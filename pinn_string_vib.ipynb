{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem to solve\n",
    "\n",
    "We consider the following DPE :\n",
    "$$\n",
    "    \\frac{\\partial ²u}{\\partial t²} = \\frac{T}{\\mu} \\frac{\\partial ²u}{\\partial x²} \n",
    "$$\n",
    "and its boundary conditions : \n",
    "$$\n",
    "\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; \\forall t > 0, \\; u(x=0, t) = 0 \\;\\; (1)\\\\\n",
    "\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; \\forall t > 0, u(x=L, t) = 0 \\;\\; (2)\\\\\n",
    "and \\;\\; for \\;\\; t=0, \\;\\;\\; u(x, 0) = 2u_{max}sin(2\\pi\\frac{x}{\\lambda}) \\;\\; (3)\n",
    "$$\n",
    "where $\\lambda$ is the wave length and A is the maximum positive value reach by the string.  \n",
    "  \n",
    "To solve this equation, we'll use a PINN.\n",
    "We define $f(t,x)$ as $f := \\frac{\\partial ²u}{\\partial t²} - \\frac{T}{\\mu} \\frac{\\partial ²u}{\\partial x²}, \\;$ u the approximated function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "from math import pi\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class vanilla_PINN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, nof_hidden_layers):\n",
    "        super().__init__()\n",
    "\n",
    "        # To avoid bad entry: the inputs are x and t and the output is a scalar \n",
    "        try:\n",
    "\n",
    "            if input_size != 2:\n",
    "                raise ValueError(\"Input size must be equal to 2\")\n",
    "\n",
    "            elif output_size != 1:\n",
    "                raise ValueError(\"Output size must be equal to 1\")\n",
    "\n",
    "            self.input_layer = nn.Sequential(nn.Linear(input_size, hidden_size), nn.GELU())\n",
    "            self.output_layer = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "        except ValueError as error:\n",
    "            print(error)\n",
    "            return None\n",
    "\n",
    "        # To be flexible on the number of hidden layers when instanciate the NN\n",
    "        self.hidden_layers = nn.Sequential()\n",
    "\n",
    "        for i in range(nof_hidden_layers):\n",
    "            self.hidden_layers.add_module(\"hidden_layer_\" + str(i), nn.Linear(hidden_size, hidden_size))\n",
    "            self.hidden_layers.add_module(\"GELU_\" + str(i), nn.GELU())\n",
    "\n",
    "\n",
    "    def forward(self, t, x):\n",
    "        \n",
    "        V = torch.cat([t, x], dim=1)\n",
    "        V = self.input_layer(V)\n",
    "        V = self.hidden_layers(V)\n",
    "        V = self.output_layer(V)\n",
    "        \n",
    "        return V"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Partial Derivative Equation (PDE) as a one of the loss\n",
    "def f(t, x, neural_net, pb_params_list):\n",
    "    \"\"\"\n",
    "        (from the problem) PDE : u_tt - (T / mu) * u_xx\n",
    "        pb_params_list = [T, mu]\n",
    "    \"\"\"\n",
    "\n",
    "    u    = neural_net(t, x)\n",
    "    \n",
    "    #d²u/dx² = u_xx \n",
    "    u_x  = torch.autograd.grad(u.sum(), x, create_graph=True)[0]\n",
    "    u_xx = torch.autograd.grad(u_x.sum(), x, create_graph=True)[0]\n",
    "    \n",
    "    # d²u/dt² = u_tt\n",
    "    u_t  = torch.autograd.grad(u.sum(), t, create_graph=True)[0]\n",
    "    u_tt = torch.autograd.grad(u.sum(), t, create_graph=True)[0]\n",
    "\n",
    "    PDE = u_tt - (pb_params_list[0] / pb_params_list[1]) * u_xx\n",
    "\n",
    "    return PDE\n",
    "\n",
    "\n",
    "# Loss function on boundary conditions points\n",
    "def MSE_loss(groung_truth_pts, net_output_pts):\n",
    "    mse_bc = nn.MSELoss()\n",
    "    return mse_bc(groung_truth_pts, net_output_pts)\n",
    "\n",
    "\n",
    "def train(model, nof_collocations_pts,\n",
    "          x_bc1_pts, t_bc1_pts, u_bc1_pts,\n",
    "          x_bc2_pts, t_bc2_pts, u_bc2_pts,\n",
    "          x_bc3_pts, t_bc3_pts, u_bc3_pts,\n",
    "          pb_params_list,\n",
    "          optimizer, weight_loss1, weight_loss2, weight_loss3,\n",
    "          nof_iterations, \n",
    "          device):\n",
    "\n",
    "    training_loss = []\n",
    "\n",
    "    for epoch in range(nof_iterations):\n",
    "\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #Loss on first BC (u(x=0, t) = 0 for all t)\n",
    "        x_bc1_pts = Variable(x_bc1_pts, requires_grad=False).to(device)\n",
    "        t_bc1_pts = Variable(t_bc1_pts, requires_grad=False).to(device)\n",
    "        u_bc1_pts = Variable(u_bc1_pts, requires_grad=False).to(device)\n",
    "\n",
    "        net_output_bc1 = model(t_bc1_pts, x_bc1_pts)\n",
    "        mse_u_bc1      = MSE_loss(u_bc1_pts, net_output_bc1)\n",
    "\n",
    "        #Loss on second BC (u(x=L, t) = 0 for all t)\n",
    "        x_bc2_pts = Variable(x_bc2_pts, requires_grad=False).to(device)\n",
    "        t_bc2_pts = Variable(t_bc2_pts, requires_grad=False).to(device)\n",
    "        u_bc2_pts = Variable(u_bc2_pts, requires_grad=False).to(device)\n",
    "\n",
    "        net_output_bc2 = model(t_bc2_pts, x_bc2_pts)\n",
    "        mse_u_bc2      = MSE_loss(u_bc2_pts, net_output_bc2)\n",
    "\n",
    "        #Loss in third BC (u(x, t=0) = u_max * cos(2 * pi * x / lambda) for all x and where lambda is the wave length)\n",
    "        #x_bc3_pts = Variable(x_bc3_pts, requires_grad=False).to(device)\n",
    "        #t_bc3_pts = Variable(t_bc3_pts, requires_grad=False).to(device)\n",
    "        #u_bc3_pts = Variable(u_bc3_pts, requires_grad=False).to(device)\n",
    "        #\n",
    "        #net_output_bc3 = model(t_bc3_pts, x_bc3_pts)\n",
    "        #mse_u_bc3      = MSE_loss(u_bc3_pts, net_output_bc3)\n",
    "\n",
    "        # PDE loss : collocations points form an equally spaced (t,x)-grid where x_max = L\n",
    "        L     = x_bc2_pts.data[0].item()\n",
    "        t_max = torch.max(t_bc1_pts).item()\n",
    "\n",
    "        x_collocation_pts = torch.linspace(0, L, nof_collocations_pts)\n",
    "        x_collocation_pts = x_collocation_pts.reshape(x_collocation_pts.shape[0], 1)\n",
    "        x_collocation_pts = Variable(x_collocation_pts, requires_grad=True).to(device)\n",
    "\n",
    "        t_collocation_pts = torch.linspace(0, t_max, nof_collocations_pts)\n",
    "        t_collocation_pts = t_collocation_pts.reshape(t_collocation_pts.shape[0], 1)\n",
    "        t_collocation_pts = Variable(t_collocation_pts, requires_grad=True).to(device)\n",
    "\n",
    "        f_output = f(t_collocation_pts, x_collocation_pts, model, pb_params_list)\n",
    "        mse_pde  = MSE_loss(f_output, torch.zeros_like(f_output))\n",
    "\n",
    "        #Compute loss as the sum of all cost\n",
    "        total_running_loss = weight_loss1 * mse_u_bc1 + weight_loss2 * mse_u_bc2 + weight_loss3 * mse_pde #+ mse_u_bc3 \n",
    "        training_loss.append(total_running_loss.detach().to('cpu'))\n",
    "\n",
    "        total_running_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        with torch.autograd.no_grad():\n",
    "            print('Epoch: {} | training loss: {:.12f}'.format(epoch, total_running_loss))\n",
    "\n",
    "    return training_loss\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | training loss: 2454.888916015625\n",
      "Epoch: 1 | training loss: 2399.732421875000\n",
      "Epoch: 2 | training loss: 2345.847167968750\n",
      "Epoch: 3 | training loss: 2293.227539062500\n",
      "Epoch: 4 | training loss: 2241.855957031250\n",
      "Epoch: 5 | training loss: 2191.706542968750\n",
      "Epoch: 6 | training loss: 2142.746582031250\n",
      "Epoch: 7 | training loss: 2094.937744140625\n",
      "Epoch: 8 | training loss: 2048.237304687500\n",
      "Epoch: 9 | training loss: 2002.600097656250\n",
      "Epoch: 10 | training loss: 1957.976562500000\n",
      "Epoch: 11 | training loss: 1914.316162109375\n",
      "Epoch: 12 | training loss: 1871.567871093750\n",
      "Epoch: 13 | training loss: 1829.681274414062\n",
      "Epoch: 14 | training loss: 1788.609741210938\n",
      "Epoch: 15 | training loss: 1748.309326171875\n",
      "Epoch: 16 | training loss: 1708.740478515625\n",
      "Epoch: 17 | training loss: 1669.867431640625\n",
      "Epoch: 18 | training loss: 1631.658081054688\n",
      "Epoch: 19 | training loss: 1594.083129882812\n",
      "Epoch: 20 | training loss: 1557.116821289062\n",
      "Epoch: 21 | training loss: 1520.735839843750\n",
      "Epoch: 22 | training loss: 1484.920043945312\n",
      "Epoch: 23 | training loss: 1449.651000976562\n",
      "Epoch: 24 | training loss: 1414.913452148438\n",
      "Epoch: 25 | training loss: 1380.693725585938\n",
      "Epoch: 26 | training loss: 1346.979736328125\n",
      "Epoch: 27 | training loss: 1313.761962890625\n",
      "Epoch: 28 | training loss: 1281.031860351562\n",
      "Epoch: 29 | training loss: 1248.782836914062\n",
      "Epoch: 30 | training loss: 1217.008666992188\n",
      "Epoch: 31 | training loss: 1185.704101562500\n",
      "Epoch: 32 | training loss: 1154.865478515625\n",
      "Epoch: 33 | training loss: 1124.488525390625\n",
      "Epoch: 34 | training loss: 1094.570434570312\n",
      "Epoch: 35 | training loss: 1065.108032226562\n",
      "Epoch: 36 | training loss: 1036.097900390625\n",
      "Epoch: 37 | training loss: 1007.536804199219\n",
      "Epoch: 38 | training loss: 979.421447753906\n",
      "Epoch: 39 | training loss: 951.748168945312\n",
      "Epoch: 40 | training loss: 924.513671875000\n",
      "Epoch: 41 | training loss: 897.714172363281\n",
      "Epoch: 42 | training loss: 871.346008300781\n",
      "Epoch: 43 | training loss: 845.405700683594\n",
      "Epoch: 44 | training loss: 819.889709472656\n",
      "Epoch: 45 | training loss: 794.794372558594\n",
      "Epoch: 46 | training loss: 770.116455078125\n",
      "Epoch: 47 | training loss: 745.852172851562\n",
      "Epoch: 48 | training loss: 721.997680664062\n",
      "Epoch: 49 | training loss: 698.549377441406\n",
      "Epoch: 50 | training loss: 675.502990722656\n",
      "Epoch: 51 | training loss: 652.853637695312\n",
      "Epoch: 52 | training loss: 630.596191406250\n",
      "Epoch: 53 | training loss: 608.725036621094\n",
      "Epoch: 54 | training loss: 587.233886718750\n",
      "Epoch: 55 | training loss: 566.116210937500\n",
      "Epoch: 56 | training loss: 545.364868164062\n",
      "Epoch: 57 | training loss: 524.973022460938\n",
      "Epoch: 58 | training loss: 504.933898925781\n",
      "Epoch: 59 | training loss: 485.241851806641\n",
      "Epoch: 60 | training loss: 465.892242431641\n",
      "Epoch: 61 | training loss: 446.882141113281\n",
      "Epoch: 62 | training loss: 428.210327148438\n",
      "Epoch: 63 | training loss: 409.877410888672\n",
      "Epoch: 64 | training loss: 391.885711669922\n",
      "Epoch: 65 | training loss: 374.239105224609\n",
      "Epoch: 66 | training loss: 356.942047119141\n",
      "Epoch: 67 | training loss: 339.999603271484\n",
      "Epoch: 68 | training loss: 323.416809082031\n",
      "Epoch: 69 | training loss: 307.197967529297\n",
      "Epoch: 70 | training loss: 291.346435546875\n",
      "Epoch: 71 | training loss: 275.864440917969\n",
      "Epoch: 72 | training loss: 260.752807617188\n",
      "Epoch: 73 | training loss: 246.011230468750\n",
      "Epoch: 74 | training loss: 231.638656616211\n",
      "Epoch: 75 | training loss: 217.634292602539\n",
      "Epoch: 76 | training loss: 203.998397827148\n",
      "Epoch: 77 | training loss: 190.734039306641\n",
      "Epoch: 78 | training loss: 177.847991943359\n",
      "Epoch: 79 | training loss: 165.351898193359\n",
      "Epoch: 80 | training loss: 153.262191772461\n",
      "Epoch: 81 | training loss: 141.598632812500\n",
      "Epoch: 82 | training loss: 130.382080078125\n",
      "Epoch: 83 | training loss: 119.631500244141\n",
      "Epoch: 84 | training loss: 109.362045288086\n",
      "Epoch: 85 | training loss: 99.584609985352\n",
      "Epoch: 86 | training loss: 90.307319641113\n",
      "Epoch: 87 | training loss: 81.538246154785\n",
      "Epoch: 88 | training loss: 73.288246154785\n",
      "Epoch: 89 | training loss: 65.572616577148\n",
      "Epoch: 90 | training loss: 58.410919189453\n",
      "Epoch: 91 | training loss: 51.823852539062\n",
      "Epoch: 92 | training loss: 45.827545166016\n",
      "Epoch: 93 | training loss: 40.427490234375\n",
      "Epoch: 94 | training loss: 35.616741180420\n",
      "Epoch: 95 | training loss: 31.380796432495\n",
      "Epoch: 96 | training loss: 27.704608917236\n",
      "Epoch: 97 | training loss: 24.575408935547\n",
      "Epoch: 98 | training loss: 21.977575302124\n",
      "Epoch: 99 | training loss: 19.884536743164\n",
      "Epoch: 100 | training loss: 18.253644943237\n",
      "Epoch: 101 | training loss: 17.028886795044\n",
      "Epoch: 102 | training loss: 16.150608062744\n",
      "Epoch: 103 | training loss: 15.564052581787\n",
      "Epoch: 104 | training loss: 15.217672348022\n",
      "Epoch: 105 | training loss: 15.055437088013\n",
      "Epoch: 106 | training loss: 15.015826225281\n",
      "Epoch: 107 | training loss: 15.039752006531\n",
      "Epoch: 108 | training loss: 15.079889297485\n",
      "Epoch: 109 | training loss: 15.104265213013\n",
      "Epoch: 110 | training loss: 15.090979576111\n",
      "Epoch: 111 | training loss: 15.021289825439\n",
      "Epoch: 112 | training loss: 14.880715370178\n",
      "Epoch: 113 | training loss: 14.664331436157\n",
      "Epoch: 114 | training loss: 14.377546310425\n",
      "Epoch: 115 | training loss: 14.031843185425\n",
      "Epoch: 116 | training loss: 13.639393806458\n",
      "Epoch: 117 | training loss: 13.210996627808\n",
      "Epoch: 118 | training loss: 12.758204460144\n",
      "Epoch: 119 | training loss: 12.294616699219\n",
      "Epoch: 120 | training loss: 11.833641052246\n",
      "Epoch: 121 | training loss: 11.385725975037\n",
      "Epoch: 122 | training loss: 10.957597732544\n",
      "Epoch: 123 | training loss: 10.553491592407\n",
      "Epoch: 124 | training loss: 10.176839828491\n",
      "Epoch: 125 | training loss: 9.830480575562\n",
      "Epoch: 126 | training loss: 9.515538215637\n",
      "Epoch: 127 | training loss: 9.230941772461\n",
      "Epoch: 128 | training loss: 8.974198341370\n",
      "Epoch: 129 | training loss: 8.742453575134\n",
      "Epoch: 130 | training loss: 8.533059120178\n",
      "Epoch: 131 | training loss: 8.343439102173\n",
      "Epoch: 132 | training loss: 8.170659065247\n",
      "Epoch: 133 | training loss: 8.011475563049\n",
      "Epoch: 134 | training loss: 7.862866878510\n",
      "Epoch: 135 | training loss: 7.722445487976\n",
      "Epoch: 136 | training loss: 7.588406562805\n",
      "Epoch: 137 | training loss: 7.459231376648\n",
      "Epoch: 138 | training loss: 7.333509445190\n",
      "Epoch: 139 | training loss: 7.209952831268\n",
      "Epoch: 140 | training loss: 7.087641239166\n",
      "Epoch: 141 | training loss: 6.966109752655\n",
      "Epoch: 142 | training loss: 6.845191001892\n",
      "Epoch: 143 | training loss: 6.724807262421\n",
      "Epoch: 144 | training loss: 6.604887962341\n",
      "Epoch: 145 | training loss: 6.485444068909\n",
      "Epoch: 146 | training loss: 6.366638183594\n",
      "Epoch: 147 | training loss: 6.248782634735\n",
      "Epoch: 148 | training loss: 6.132205963135\n",
      "Epoch: 149 | training loss: 6.017161369324\n",
      "Epoch: 150 | training loss: 5.903828620911\n",
      "Epoch: 151 | training loss: 5.792395591736\n",
      "Epoch: 152 | training loss: 5.683066368103\n",
      "Epoch: 153 | training loss: 5.576057434082\n",
      "Epoch: 154 | training loss: 5.471521854401\n",
      "Epoch: 155 | training loss: 5.369524955750\n",
      "Epoch: 156 | training loss: 5.270078659058\n",
      "Epoch: 157 | training loss: 5.173192977905\n",
      "Epoch: 158 | training loss: 5.078874111176\n",
      "Epoch: 159 | training loss: 4.987095832825\n",
      "Epoch: 160 | training loss: 4.897787570953\n",
      "Epoch: 161 | training loss: 4.810839176178\n",
      "Epoch: 162 | training loss: 4.726131439209\n",
      "Epoch: 163 | training loss: 4.643558502197\n",
      "Epoch: 164 | training loss: 4.563027858734\n",
      "Epoch: 165 | training loss: 4.484424591064\n",
      "Epoch: 166 | training loss: 4.407621860504\n",
      "Epoch: 167 | training loss: 4.332497119904\n",
      "Epoch: 168 | training loss: 4.258946895599\n",
      "Epoch: 169 | training loss: 4.186887741089\n",
      "Epoch: 170 | training loss: 4.116242408752\n",
      "Epoch: 171 | training loss: 4.046925544739\n",
      "Epoch: 172 | training loss: 3.978868722916\n",
      "Epoch: 173 | training loss: 3.912016868591\n",
      "Epoch: 174 | training loss: 3.846332788467\n",
      "Epoch: 175 | training loss: 3.781786918640\n",
      "Epoch: 176 | training loss: 3.718346357346\n",
      "Epoch: 177 | training loss: 3.655985116959\n",
      "Epoch: 178 | training loss: 3.594683885574\n",
      "Epoch: 179 | training loss: 3.534429311752\n",
      "Epoch: 180 | training loss: 3.475214481354\n",
      "Epoch: 181 | training loss: 3.417027235031\n",
      "Epoch: 182 | training loss: 3.359855651855\n",
      "Epoch: 183 | training loss: 3.303687095642\n",
      "Epoch: 184 | training loss: 3.248515844345\n",
      "Epoch: 185 | training loss: 3.194329261780\n",
      "Epoch: 186 | training loss: 3.141116857529\n",
      "Epoch: 187 | training loss: 3.088867425919\n",
      "Epoch: 188 | training loss: 3.037563562393\n",
      "Epoch: 189 | training loss: 2.987192630768\n",
      "Epoch: 190 | training loss: 2.937740802765\n",
      "Epoch: 191 | training loss: 2.889192819595\n",
      "Epoch: 192 | training loss: 2.841531515121\n",
      "Epoch: 193 | training loss: 2.794736385345\n",
      "Epoch: 194 | training loss: 2.748792648315\n",
      "Epoch: 195 | training loss: 2.703684091568\n",
      "Epoch: 196 | training loss: 2.659393787384\n",
      "Epoch: 197 | training loss: 2.615904331207\n",
      "Epoch: 198 | training loss: 2.573197364807\n",
      "Epoch: 199 | training loss: 2.531257152557\n",
      "Epoch: 200 | training loss: 2.490068435669\n",
      "Epoch: 201 | training loss: 2.449616432190\n",
      "Epoch: 202 | training loss: 2.409882068634\n",
      "Epoch: 203 | training loss: 2.370855093002\n",
      "Epoch: 204 | training loss: 2.332518815994\n",
      "Epoch: 205 | training loss: 2.294859647751\n",
      "Epoch: 206 | training loss: 2.257864475250\n",
      "Epoch: 207 | training loss: 2.221521615982\n",
      "Epoch: 208 | training loss: 2.185817241669\n",
      "Epoch: 209 | training loss: 2.150740146637\n",
      "Epoch: 210 | training loss: 2.116279602051\n",
      "Epoch: 211 | training loss: 2.082421541214\n",
      "Epoch: 212 | training loss: 2.049159526825\n",
      "Epoch: 213 | training loss: 2.016478300095\n",
      "Epoch: 214 | training loss: 1.984368205070\n",
      "Epoch: 215 | training loss: 1.952821373940\n",
      "Epoch: 216 | training loss: 1.921827077866\n",
      "Epoch: 217 | training loss: 1.891373515129\n",
      "Epoch: 218 | training loss: 1.861451387405\n",
      "Epoch: 219 | training loss: 1.832052230835\n",
      "Epoch: 220 | training loss: 1.803166508675\n",
      "Epoch: 221 | training loss: 1.774783492088\n",
      "Epoch: 222 | training loss: 1.746896147728\n",
      "Epoch: 223 | training loss: 1.719493746758\n",
      "Epoch: 224 | training loss: 1.692566394806\n",
      "Epoch: 225 | training loss: 1.666105628014\n",
      "Epoch: 226 | training loss: 1.640104055405\n",
      "Epoch: 227 | training loss: 1.614551901817\n",
      "Epoch: 228 | training loss: 1.589441537857\n",
      "Epoch: 229 | training loss: 1.564764499664\n",
      "Epoch: 230 | training loss: 1.540513277054\n",
      "Epoch: 231 | training loss: 1.516680121422\n",
      "Epoch: 232 | training loss: 1.493256211281\n",
      "Epoch: 233 | training loss: 1.470235943794\n",
      "Epoch: 234 | training loss: 1.447609782219\n",
      "Epoch: 235 | training loss: 1.425370812416\n",
      "Epoch: 236 | training loss: 1.403512358665\n",
      "Epoch: 237 | training loss: 1.382027983665\n",
      "Epoch: 238 | training loss: 1.360909223557\n",
      "Epoch: 239 | training loss: 1.340151309967\n",
      "Epoch: 240 | training loss: 1.319746613503\n",
      "Epoch: 241 | training loss: 1.299689888954\n",
      "Epoch: 242 | training loss: 1.279973506927\n",
      "Epoch: 243 | training loss: 1.260591983795\n",
      "Epoch: 244 | training loss: 1.241539359093\n",
      "Epoch: 245 | training loss: 1.222808837891\n",
      "Epoch: 246 | training loss: 1.204395651817\n",
      "Epoch: 247 | training loss: 1.186293840408\n",
      "Epoch: 248 | training loss: 1.168497800827\n",
      "Epoch: 249 | training loss: 1.151002049446\n",
      "Epoch: 250 | training loss: 1.133801102638\n",
      "Epoch: 251 | training loss: 1.116890192032\n",
      "Epoch: 252 | training loss: 1.100263953209\n",
      "Epoch: 253 | training loss: 1.083916187286\n",
      "Epoch: 254 | training loss: 1.067843794823\n",
      "Epoch: 255 | training loss: 1.052040576935\n",
      "Epoch: 256 | training loss: 1.036501765251\n",
      "Epoch: 257 | training loss: 1.021222949028\n",
      "Epoch: 258 | training loss: 1.006199240685\n",
      "Epoch: 259 | training loss: 0.991426348686\n",
      "Epoch: 260 | training loss: 0.976899683475\n",
      "Epoch: 261 | training loss: 0.962615072727\n",
      "Epoch: 262 | training loss: 0.948567867279\n",
      "Epoch: 263 | training loss: 0.934753477573\n",
      "Epoch: 264 | training loss: 0.921168684959\n",
      "Epoch: 265 | training loss: 0.907809913158\n",
      "Epoch: 266 | training loss: 0.894672036171\n",
      "Epoch: 267 | training loss: 0.881751418114\n",
      "Epoch: 268 | training loss: 0.869043946266\n",
      "Epoch: 269 | training loss: 0.856546223164\n",
      "Epoch: 270 | training loss: 0.844253897667\n",
      "Epoch: 271 | training loss: 0.832164406776\n",
      "Epoch: 272 | training loss: 0.820273041725\n",
      "Epoch: 273 | training loss: 0.808577656746\n",
      "Epoch: 274 | training loss: 0.797073185444\n",
      "Epoch: 275 | training loss: 0.785757124424\n",
      "Epoch: 276 | training loss: 0.774627029896\n",
      "Epoch: 277 | training loss: 0.763678133488\n",
      "Epoch: 278 | training loss: 0.752908349037\n",
      "Epoch: 279 | training loss: 0.742313146591\n",
      "Epoch: 280 | training loss: 0.731890320778\n",
      "Epoch: 281 | training loss: 0.721636533737\n",
      "Epoch: 282 | training loss: 0.711549043655\n",
      "Epoch: 283 | training loss: 0.701625108719\n",
      "Epoch: 284 | training loss: 0.691861629486\n",
      "Epoch: 285 | training loss: 0.682255804539\n",
      "Epoch: 286 | training loss: 0.672804713249\n",
      "Epoch: 287 | training loss: 0.663506329060\n",
      "Epoch: 288 | training loss: 0.654356896877\n",
      "Epoch: 289 | training loss: 0.645354866982\n",
      "Epoch: 290 | training loss: 0.636496841908\n",
      "Epoch: 291 | training loss: 0.627780139446\n",
      "Epoch: 292 | training loss: 0.619203269482\n",
      "Epoch: 293 | training loss: 0.610762655735\n",
      "Epoch: 294 | training loss: 0.602456867695\n",
      "Epoch: 295 | training loss: 0.594282627106\n",
      "Epoch: 296 | training loss: 0.586238741875\n",
      "Epoch: 297 | training loss: 0.578322291374\n",
      "Epoch: 298 | training loss: 0.570531547070\n",
      "Epoch: 299 | training loss: 0.562863230705\n",
      "Epoch: 300 | training loss: 0.555315494537\n",
      "Epoch: 301 | training loss: 0.547886729240\n",
      "Epoch: 302 | training loss: 0.540574967861\n",
      "Epoch: 303 | training loss: 0.533377587795\n",
      "Epoch: 304 | training loss: 0.526292920113\n",
      "Epoch: 305 | training loss: 0.519319295883\n",
      "Epoch: 306 | training loss: 0.512454509735\n",
      "Epoch: 307 | training loss: 0.505696058273\n",
      "Epoch: 308 | training loss: 0.499043762684\n",
      "Epoch: 309 | training loss: 0.492494046688\n",
      "Epoch: 310 | training loss: 0.486045449972\n",
      "Epoch: 311 | training loss: 0.479696571827\n",
      "Epoch: 312 | training loss: 0.473445981741\n",
      "Epoch: 313 | training loss: 0.467291414738\n",
      "Epoch: 314 | training loss: 0.461231768131\n",
      "Epoch: 315 | training loss: 0.455264776945\n",
      "Epoch: 316 | training loss: 0.449389666319\n",
      "Epoch: 317 | training loss: 0.443603694439\n",
      "Epoch: 318 | training loss: 0.437906086445\n",
      "Epoch: 319 | training loss: 0.432295113802\n",
      "Epoch: 320 | training loss: 0.426769763231\n",
      "Epoch: 321 | training loss: 0.421327829361\n",
      "Epoch: 322 | training loss: 0.415967941284\n",
      "Epoch: 323 | training loss: 0.410689622164\n",
      "Epoch: 324 | training loss: 0.405490636826\n",
      "Epoch: 325 | training loss: 0.400370210409\n",
      "Epoch: 326 | training loss: 0.395326167345\n",
      "Epoch: 327 | training loss: 0.390357881784\n",
      "Epoch: 328 | training loss: 0.385463595390\n",
      "Epoch: 329 | training loss: 0.380642563105\n",
      "Epoch: 330 | training loss: 0.375893235207\n",
      "Epoch: 331 | training loss: 0.371214330196\n",
      "Epoch: 332 | training loss: 0.366604834795\n",
      "Epoch: 333 | training loss: 0.362063527107\n",
      "Epoch: 334 | training loss: 0.357589572668\n",
      "Epoch: 335 | training loss: 0.353180885315\n",
      "Epoch: 336 | training loss: 0.348837375641\n",
      "Epoch: 337 | training loss: 0.344557374716\n",
      "Epoch: 338 | training loss: 0.340340137482\n",
      "Epoch: 339 | training loss: 0.336184561253\n",
      "Epoch: 340 | training loss: 0.332089126110\n",
      "Epoch: 341 | training loss: 0.328053385019\n",
      "Epoch: 342 | training loss: 0.324076384306\n",
      "Epoch: 343 | training loss: 0.320156544447\n",
      "Epoch: 344 | training loss: 0.316293537617\n",
      "Epoch: 345 | training loss: 0.312486380339\n",
      "Epoch: 346 | training loss: 0.308734029531\n",
      "Epoch: 347 | training loss: 0.305035471916\n",
      "Epoch: 348 | training loss: 0.301389932632\n",
      "Epoch: 349 | training loss: 0.297796338797\n",
      "Epoch: 350 | training loss: 0.294253736734\n",
      "Epoch: 351 | training loss: 0.290761828423\n",
      "Epoch: 352 | training loss: 0.287319213152\n",
      "Epoch: 353 | training loss: 0.283925890923\n",
      "Epoch: 354 | training loss: 0.280580133200\n",
      "Epoch: 355 | training loss: 0.277281731367\n",
      "Epoch: 356 | training loss: 0.274029731750\n",
      "Epoch: 357 | training loss: 0.270823210478\n",
      "Epoch: 358 | training loss: 0.267662048340\n",
      "Epoch: 359 | training loss: 0.264544546604\n",
      "Epoch: 360 | training loss: 0.261470913887\n",
      "Epoch: 361 | training loss: 0.258440166712\n",
      "Epoch: 362 | training loss: 0.255451232195\n",
      "Epoch: 363 | training loss: 0.252503901720\n",
      "Epoch: 364 | training loss: 0.249597221613\n",
      "Epoch: 365 | training loss: 0.246730536222\n",
      "Epoch: 366 | training loss: 0.243903964758\n",
      "Epoch: 367 | training loss: 0.241115778685\n",
      "Epoch: 368 | training loss: 0.238365784287\n",
      "Epoch: 369 | training loss: 0.235653862357\n",
      "Epoch: 370 | training loss: 0.232978716493\n",
      "Epoch: 371 | training loss: 0.230340078473\n",
      "Epoch: 372 | training loss: 0.227737441659\n",
      "Epoch: 373 | training loss: 0.225170135498\n",
      "Epoch: 374 | training loss: 0.222637310624\n",
      "Epoch: 375 | training loss: 0.220139175653\n",
      "Epoch: 376 | training loss: 0.217674463987\n",
      "Epoch: 377 | training loss: 0.215243458748\n",
      "Epoch: 378 | training loss: 0.212844729424\n",
      "Epoch: 379 | training loss: 0.210477873683\n",
      "Epoch: 380 | training loss: 0.208142966032\n",
      "Epoch: 381 | training loss: 0.205839186907\n",
      "Epoch: 382 | training loss: 0.203566074371\n",
      "Epoch: 383 | training loss: 0.201323255897\n",
      "Epoch: 384 | training loss: 0.199110195041\n",
      "Epoch: 385 | training loss: 0.196926608682\n",
      "Epoch: 386 | training loss: 0.194771826267\n",
      "Epoch: 387 | training loss: 0.192645549774\n",
      "Epoch: 388 | training loss: 0.190547108650\n",
      "Epoch: 389 | training loss: 0.188476294279\n",
      "Epoch: 390 | training loss: 0.186432927847\n",
      "Epoch: 391 | training loss: 0.184416070580\n",
      "Epoch: 392 | training loss: 0.182425647974\n",
      "Epoch: 393 | training loss: 0.180461347103\n",
      "Epoch: 394 | training loss: 0.178522452712\n",
      "Epoch: 395 | training loss: 0.176608696580\n",
      "Epoch: 396 | training loss: 0.174720004201\n",
      "Epoch: 397 | training loss: 0.172855556011\n",
      "Epoch: 398 | training loss: 0.171015456319\n",
      "Epoch: 399 | training loss: 0.169198781252\n",
      "Epoch: 400 | training loss: 0.167405158281\n",
      "Epoch: 401 | training loss: 0.165635153651\n",
      "Epoch: 402 | training loss: 0.163887679577\n",
      "Epoch: 403 | training loss: 0.162162557244\n",
      "Epoch: 404 | training loss: 0.160459548235\n",
      "Epoch: 405 | training loss: 0.158778354526\n",
      "Epoch: 406 | training loss: 0.157118603587\n",
      "Epoch: 407 | training loss: 0.155479684472\n",
      "Epoch: 408 | training loss: 0.153861954808\n",
      "Epoch: 409 | training loss: 0.152264401317\n",
      "Epoch: 410 | training loss: 0.150686874986\n",
      "Epoch: 411 | training loss: 0.149129539728\n",
      "Epoch: 412 | training loss: 0.147591829300\n",
      "Epoch: 413 | training loss: 0.146073296666\n",
      "Epoch: 414 | training loss: 0.144574061036\n",
      "Epoch: 415 | training loss: 0.143093392253\n",
      "Epoch: 416 | training loss: 0.141631484032\n",
      "Epoch: 417 | training loss: 0.140187516809\n",
      "Epoch: 418 | training loss: 0.138761803508\n",
      "Epoch: 419 | training loss: 0.137353882194\n",
      "Epoch: 420 | training loss: 0.135963410139\n",
      "Epoch: 421 | training loss: 0.134590223432\n",
      "Epoch: 422 | training loss: 0.133233681321\n",
      "Epoch: 423 | training loss: 0.131894156337\n",
      "Epoch: 424 | training loss: 0.130571097136\n",
      "Epoch: 425 | training loss: 0.129264265299\n",
      "Epoch: 426 | training loss: 0.127973735332\n",
      "Epoch: 427 | training loss: 0.126699194312\n",
      "Epoch: 428 | training loss: 0.125440016389\n",
      "Epoch: 429 | training loss: 0.124196484685\n",
      "Epoch: 430 | training loss: 0.122968040407\n",
      "Epoch: 431 | training loss: 0.121754653752\n",
      "Epoch: 432 | training loss: 0.120555989444\n",
      "Epoch: 433 | training loss: 0.119372069836\n",
      "Epoch: 434 | training loss: 0.118202462792\n",
      "Epoch: 435 | training loss: 0.117047116160\n",
      "Epoch: 436 | training loss: 0.115905657411\n",
      "Epoch: 437 | training loss: 0.114778049290\n",
      "Epoch: 438 | training loss: 0.113664299250\n",
      "Epoch: 439 | training loss: 0.112563744187\n",
      "Epoch: 440 | training loss: 0.111476339400\n",
      "Epoch: 441 | training loss: 0.110402017832\n",
      "Epoch: 442 | training loss: 0.109340995550\n",
      "Epoch: 443 | training loss: 0.108292602003\n",
      "Epoch: 444 | training loss: 0.107256546617\n",
      "Epoch: 445 | training loss: 0.106232985854\n",
      "Epoch: 446 | training loss: 0.105221800506\n",
      "Epoch: 447 | training loss: 0.104222707450\n",
      "Epoch: 448 | training loss: 0.103235498071\n",
      "Epoch: 449 | training loss: 0.102260120213\n",
      "Epoch: 450 | training loss: 0.101296335459\n",
      "Epoch: 451 | training loss: 0.100343950093\n",
      "Epoch: 452 | training loss: 0.099402941763\n",
      "Epoch: 453 | training loss: 0.098472937942\n",
      "Epoch: 454 | training loss: 0.097553938627\n",
      "Epoch: 455 | training loss: 0.096645891666\n",
      "Epoch: 456 | training loss: 0.095748677850\n",
      "Epoch: 457 | training loss: 0.094861939549\n",
      "Epoch: 458 | training loss: 0.093985885382\n",
      "Epoch: 459 | training loss: 0.093119829893\n",
      "Epoch: 460 | training loss: 0.092264041305\n",
      "Epoch: 461 | training loss: 0.091418132186\n",
      "Epoch: 462 | training loss: 0.090582370758\n",
      "Epoch: 463 | training loss: 0.089756563306\n",
      "Epoch: 464 | training loss: 0.088940136135\n",
      "Epoch: 465 | training loss: 0.088133327663\n",
      "Epoch: 466 | training loss: 0.087335959077\n",
      "Epoch: 467 | training loss: 0.086547873914\n",
      "Epoch: 468 | training loss: 0.085769034922\n",
      "Epoch: 469 | training loss: 0.084999255836\n",
      "Epoch: 470 | training loss: 0.084238491952\n",
      "Epoch: 471 | training loss: 0.083486527205\n",
      "Epoch: 472 | training loss: 0.082743257284\n",
      "Epoch: 473 | training loss: 0.082008577883\n",
      "Epoch: 474 | training loss: 0.081282719970\n",
      "Epoch: 475 | training loss: 0.080564975739\n",
      "Epoch: 476 | training loss: 0.079855643213\n",
      "Epoch: 477 | training loss: 0.079154312611\n",
      "Epoch: 478 | training loss: 0.078461423516\n",
      "Epoch: 479 | training loss: 0.077776305377\n",
      "Epoch: 480 | training loss: 0.077099248767\n",
      "Epoch: 481 | training loss: 0.076429963112\n",
      "Epoch: 482 | training loss: 0.075768217444\n",
      "Epoch: 483 | training loss: 0.075114227831\n",
      "Epoch: 484 | training loss: 0.074467681348\n",
      "Epoch: 485 | training loss: 0.073828764260\n",
      "Epoch: 486 | training loss: 0.073196925223\n",
      "Epoch: 487 | training loss: 0.072572529316\n",
      "Epoch: 488 | training loss: 0.071955040097\n",
      "Epoch: 489 | training loss: 0.071344852448\n",
      "Epoch: 490 | training loss: 0.070741571486\n",
      "Epoch: 491 | training loss: 0.070145249367\n",
      "Epoch: 492 | training loss: 0.069555871189\n",
      "Epoch: 493 | training loss: 0.068972870708\n",
      "Epoch: 494 | training loss: 0.068396933377\n",
      "Epoch: 495 | training loss: 0.067827388644\n",
      "Epoch: 496 | training loss: 0.067264333367\n",
      "Epoch: 497 | training loss: 0.066707789898\n",
      "Epoch: 498 | training loss: 0.066157527268\n",
      "Epoch: 499 | training loss: 0.065613627434\n",
      "Epoch: 500 | training loss: 0.065075762570\n",
      "Epoch: 501 | training loss: 0.064544111490\n",
      "Epoch: 502 | training loss: 0.064018346369\n",
      "Epoch: 503 | training loss: 0.063498653471\n",
      "Epoch: 504 | training loss: 0.062984794378\n",
      "Epoch: 505 | training loss: 0.062476962805\n",
      "Epoch: 506 | training loss: 0.061974767596\n",
      "Epoch: 507 | training loss: 0.061478361487\n",
      "Epoch: 508 | training loss: 0.060987543315\n",
      "Epoch: 509 | training loss: 0.060502167791\n",
      "Epoch: 510 | training loss: 0.060022525489\n",
      "Epoch: 511 | training loss: 0.059548083693\n",
      "Epoch: 512 | training loss: 0.059079132974\n",
      "Epoch: 513 | training loss: 0.058615427464\n",
      "Epoch: 514 | training loss: 0.058156993240\n",
      "Epoch: 515 | training loss: 0.057703681290\n",
      "Epoch: 516 | training loss: 0.057255618274\n",
      "Epoch: 517 | training loss: 0.056812554598\n",
      "Epoch: 518 | training loss: 0.056374505162\n",
      "Epoch: 519 | training loss: 0.055941332132\n",
      "Epoch: 520 | training loss: 0.055513076484\n",
      "Epoch: 521 | training loss: 0.055089637637\n",
      "Epoch: 522 | training loss: 0.054671052843\n",
      "Epoch: 523 | training loss: 0.054257180542\n",
      "Epoch: 524 | training loss: 0.053847949952\n",
      "Epoch: 525 | training loss: 0.053443435580\n",
      "Epoch: 526 | training loss: 0.053043361753\n",
      "Epoch: 527 | training loss: 0.052647788078\n",
      "Epoch: 528 | training loss: 0.052256677300\n",
      "Epoch: 529 | training loss: 0.051870018244\n",
      "Epoch: 530 | training loss: 0.051487736404\n",
      "Epoch: 531 | training loss: 0.051109567285\n",
      "Epoch: 532 | training loss: 0.050735838711\n",
      "Epoch: 533 | training loss: 0.050366237760\n",
      "Epoch: 534 | training loss: 0.050000727177\n",
      "Epoch: 535 | training loss: 0.049639459699\n",
      "Epoch: 536 | training loss: 0.049282200634\n",
      "Epoch: 537 | training loss: 0.048928882927\n",
      "Epoch: 538 | training loss: 0.048579685390\n",
      "Epoch: 539 | training loss: 0.048234224319\n",
      "Epoch: 540 | training loss: 0.047892712057\n",
      "Epoch: 541 | training loss: 0.047555085272\n",
      "Epoch: 542 | training loss: 0.047221213579\n",
      "Epoch: 543 | training loss: 0.046891178936\n",
      "Epoch: 544 | training loss: 0.046564809978\n",
      "Epoch: 545 | training loss: 0.046242088079\n",
      "Epoch: 546 | training loss: 0.045922961086\n",
      "Epoch: 547 | training loss: 0.045607466251\n",
      "Epoch: 548 | training loss: 0.045295383781\n",
      "Epoch: 549 | training loss: 0.044987004250\n",
      "Epoch: 550 | training loss: 0.044681861997\n",
      "Epoch: 551 | training loss: 0.044380236417\n",
      "Epoch: 552 | training loss: 0.044082012028\n",
      "Epoch: 553 | training loss: 0.043787181377\n",
      "Epoch: 554 | training loss: 0.043495584279\n",
      "Epoch: 555 | training loss: 0.043207157403\n",
      "Epoch: 556 | training loss: 0.042922075838\n",
      "Epoch: 557 | training loss: 0.042640153319\n",
      "Epoch: 558 | training loss: 0.042361352593\n",
      "Epoch: 559 | training loss: 0.042085804045\n",
      "Epoch: 560 | training loss: 0.041813243181\n",
      "Epoch: 561 | training loss: 0.041543677449\n",
      "Epoch: 562 | training loss: 0.041277155280\n",
      "Epoch: 563 | training loss: 0.041013654321\n",
      "Epoch: 564 | training loss: 0.040753129870\n",
      "Epoch: 565 | training loss: 0.040495581925\n",
      "Epoch: 566 | training loss: 0.040240850300\n",
      "Epoch: 567 | training loss: 0.039988849312\n",
      "Epoch: 568 | training loss: 0.039739757776\n",
      "Epoch: 569 | training loss: 0.039493538439\n",
      "Epoch: 570 | training loss: 0.039249930531\n",
      "Epoch: 571 | training loss: 0.039009157568\n",
      "Epoch: 572 | training loss: 0.038771014661\n",
      "Epoch: 573 | training loss: 0.038535512984\n",
      "Epoch: 574 | training loss: 0.038302645087\n",
      "Epoch: 575 | training loss: 0.038072504103\n",
      "Epoch: 576 | training loss: 0.037844821811\n",
      "Epoch: 577 | training loss: 0.037619750947\n",
      "Epoch: 578 | training loss: 0.037397097796\n",
      "Epoch: 579 | training loss: 0.037176996469\n",
      "Epoch: 580 | training loss: 0.036959365010\n",
      "Epoch: 581 | training loss: 0.036744162440\n",
      "Epoch: 582 | training loss: 0.036531332880\n",
      "Epoch: 583 | training loss: 0.036320813000\n",
      "Epoch: 584 | training loss: 0.036112703383\n",
      "Epoch: 585 | training loss: 0.035906948149\n",
      "Epoch: 586 | training loss: 0.035703428090\n",
      "Epoch: 587 | training loss: 0.035502217710\n",
      "Epoch: 588 | training loss: 0.035303268582\n",
      "Epoch: 589 | training loss: 0.035106502473\n",
      "Epoch: 590 | training loss: 0.034911926836\n",
      "Epoch: 591 | training loss: 0.034719593823\n",
      "Epoch: 592 | training loss: 0.034529417753\n",
      "Epoch: 593 | training loss: 0.034341275692\n",
      "Epoch: 594 | training loss: 0.034155283123\n",
      "Epoch: 595 | training loss: 0.033971346915\n",
      "Epoch: 596 | training loss: 0.033789400011\n",
      "Epoch: 597 | training loss: 0.033609576523\n",
      "Epoch: 598 | training loss: 0.033431701362\n",
      "Epoch: 599 | training loss: 0.033255774528\n",
      "Epoch: 600 | training loss: 0.033081803471\n",
      "Epoch: 601 | training loss: 0.032909955829\n",
      "Epoch: 602 | training loss: 0.032739784569\n",
      "Epoch: 603 | training loss: 0.032571569085\n",
      "Epoch: 604 | training loss: 0.032405316830\n",
      "Epoch: 605 | training loss: 0.032240871340\n",
      "Epoch: 606 | training loss: 0.032078165561\n",
      "Epoch: 607 | training loss: 0.031917367131\n",
      "Epoch: 608 | training loss: 0.031758356839\n",
      "Epoch: 609 | training loss: 0.031601071358\n",
      "Epoch: 610 | training loss: 0.031445480883\n",
      "Epoch: 611 | training loss: 0.031291689724\n",
      "Epoch: 612 | training loss: 0.031139675528\n",
      "Epoch: 613 | training loss: 0.030989233404\n",
      "Epoch: 614 | training loss: 0.030840486288\n",
      "Epoch: 615 | training loss: 0.030693348497\n",
      "Epoch: 616 | training loss: 0.030547874048\n",
      "Epoch: 617 | training loss: 0.030404033139\n",
      "Epoch: 618 | training loss: 0.030261758715\n",
      "Epoch: 619 | training loss: 0.030121035874\n",
      "Epoch: 620 | training loss: 0.029981888831\n",
      "Epoch: 621 | training loss: 0.029844278470\n",
      "Epoch: 622 | training loss: 0.029708204791\n",
      "Epoch: 623 | training loss: 0.029573630542\n",
      "Epoch: 624 | training loss: 0.029440559447\n",
      "Epoch: 625 | training loss: 0.029308926314\n",
      "Epoch: 626 | training loss: 0.029178772122\n",
      "Epoch: 627 | training loss: 0.029049979523\n",
      "Epoch: 628 | training loss: 0.028922686353\n",
      "Epoch: 629 | training loss: 0.028796777129\n",
      "Epoch: 630 | training loss: 0.028672277927\n",
      "Epoch: 631 | training loss: 0.028549155220\n",
      "Epoch: 632 | training loss: 0.028427340090\n",
      "Epoch: 633 | training loss: 0.028306890279\n",
      "Epoch: 634 | training loss: 0.028187748045\n",
      "Epoch: 635 | training loss: 0.028069920838\n",
      "Epoch: 636 | training loss: 0.027953341603\n",
      "Epoch: 637 | training loss: 0.027838112786\n",
      "Epoch: 638 | training loss: 0.027724150568\n",
      "Epoch: 639 | training loss: 0.027611356229\n",
      "Epoch: 640 | training loss: 0.027499837801\n",
      "Epoch: 641 | training loss: 0.027389522642\n",
      "Epoch: 642 | training loss: 0.027280425653\n",
      "Epoch: 643 | training loss: 0.027172584087\n",
      "Epoch: 644 | training loss: 0.027065830305\n",
      "Epoch: 645 | training loss: 0.026960249990\n",
      "Epoch: 646 | training loss: 0.026855843142\n",
      "Epoch: 647 | training loss: 0.026752602309\n",
      "Epoch: 648 | training loss: 0.026650430635\n",
      "Epoch: 649 | training loss: 0.026549356058\n",
      "Epoch: 650 | training loss: 0.026449393481\n",
      "Epoch: 651 | training loss: 0.026350544766\n",
      "Epoch: 652 | training loss: 0.026252737269\n",
      "Epoch: 653 | training loss: 0.026156045496\n",
      "Epoch: 654 | training loss: 0.026060353965\n",
      "Epoch: 655 | training loss: 0.025965685025\n",
      "Epoch: 656 | training loss: 0.025872027501\n",
      "Epoch: 657 | training loss: 0.025779366493\n",
      "Epoch: 658 | training loss: 0.025687722489\n",
      "Epoch: 659 | training loss: 0.025597143918\n",
      "Epoch: 660 | training loss: 0.025507479906\n",
      "Epoch: 661 | training loss: 0.025418750942\n",
      "Epoch: 662 | training loss: 0.025331016630\n",
      "Epoch: 663 | training loss: 0.025244202465\n",
      "Epoch: 664 | training loss: 0.025158286095\n",
      "Epoch: 665 | training loss: 0.025073288009\n",
      "Epoch: 666 | training loss: 0.024989252910\n",
      "Epoch: 667 | training loss: 0.024906177074\n",
      "Epoch: 668 | training loss: 0.024823836982\n",
      "Epoch: 669 | training loss: 0.024742476642\n",
      "Epoch: 670 | training loss: 0.024661885574\n",
      "Epoch: 671 | training loss: 0.024582158774\n",
      "Epoch: 672 | training loss: 0.024503288791\n",
      "Epoch: 673 | training loss: 0.024425294250\n",
      "Epoch: 674 | training loss: 0.024348137900\n",
      "Epoch: 675 | training loss: 0.024271761999\n",
      "Epoch: 676 | training loss: 0.024196218699\n",
      "Epoch: 677 | training loss: 0.024121394381\n",
      "Epoch: 678 | training loss: 0.024047389627\n",
      "Epoch: 679 | training loss: 0.023974165320\n",
      "Epoch: 680 | training loss: 0.023901697248\n",
      "Epoch: 681 | training loss: 0.023830037564\n",
      "Epoch: 682 | training loss: 0.023759070784\n",
      "Epoch: 683 | training loss: 0.023688882589\n",
      "Epoch: 684 | training loss: 0.023619368672\n",
      "Epoch: 685 | training loss: 0.023550577462\n",
      "Epoch: 686 | training loss: 0.023482523859\n",
      "Epoch: 687 | training loss: 0.023415163159\n",
      "Epoch: 688 | training loss: 0.023348530754\n",
      "Epoch: 689 | training loss: 0.023282550275\n",
      "Epoch: 690 | training loss: 0.023217275739\n",
      "Epoch: 691 | training loss: 0.023152695969\n",
      "Epoch: 692 | training loss: 0.023088723421\n",
      "Epoch: 693 | training loss: 0.023025410250\n",
      "Epoch: 694 | training loss: 0.022962700576\n",
      "Epoch: 695 | training loss: 0.022900681943\n",
      "Epoch: 696 | training loss: 0.022839307785\n",
      "Epoch: 697 | training loss: 0.022778525949\n",
      "Epoch: 698 | training loss: 0.022718381137\n",
      "Epoch: 699 | training loss: 0.022658860311\n",
      "Epoch: 700 | training loss: 0.022599913180\n",
      "Epoch: 701 | training loss: 0.022541543469\n",
      "Epoch: 702 | training loss: 0.022483790293\n",
      "Epoch: 703 | training loss: 0.022426579148\n",
      "Epoch: 704 | training loss: 0.022369951010\n",
      "Epoch: 705 | training loss: 0.022313861176\n",
      "Epoch: 706 | training loss: 0.022258326411\n",
      "Epoch: 707 | training loss: 0.022203419358\n",
      "Epoch: 708 | training loss: 0.022148985416\n",
      "Epoch: 709 | training loss: 0.022095132619\n",
      "Epoch: 710 | training loss: 0.022041765973\n",
      "Epoch: 711 | training loss: 0.021988987923\n",
      "Epoch: 712 | training loss: 0.021936636418\n",
      "Epoch: 713 | training loss: 0.021884890273\n",
      "Epoch: 714 | training loss: 0.021833552048\n",
      "Epoch: 715 | training loss: 0.021782720461\n",
      "Epoch: 716 | training loss: 0.021732445806\n",
      "Epoch: 717 | training loss: 0.021682670340\n",
      "Epoch: 718 | training loss: 0.021633323282\n",
      "Epoch: 719 | training loss: 0.021584454924\n",
      "Epoch: 720 | training loss: 0.021536065266\n",
      "Epoch: 721 | training loss: 0.021488133818\n",
      "Epoch: 722 | training loss: 0.021440599114\n",
      "Epoch: 723 | training loss: 0.021393582225\n",
      "Epoch: 724 | training loss: 0.021346999332\n",
      "Epoch: 725 | training loss: 0.021300844848\n",
      "Epoch: 726 | training loss: 0.021255148575\n",
      "Epoch: 727 | training loss: 0.021209901199\n",
      "Epoch: 728 | training loss: 0.021165004000\n",
      "Epoch: 729 | training loss: 0.021120548248\n",
      "Epoch: 730 | training loss: 0.021076506004\n",
      "Epoch: 731 | training loss: 0.021032877266\n",
      "Epoch: 732 | training loss: 0.020989688113\n",
      "Epoch: 733 | training loss: 0.020946856588\n",
      "Epoch: 734 | training loss: 0.020904416218\n",
      "Epoch: 735 | training loss: 0.020862370729\n",
      "Epoch: 736 | training loss: 0.020820679143\n",
      "Epoch: 737 | training loss: 0.020779376850\n",
      "Epoch: 738 | training loss: 0.020738456398\n",
      "Epoch: 739 | training loss: 0.020697891712\n",
      "Epoch: 740 | training loss: 0.020657731220\n",
      "Epoch: 741 | training loss: 0.020617879927\n",
      "Epoch: 742 | training loss: 0.020578423515\n",
      "Epoch: 743 | training loss: 0.020539317280\n",
      "Epoch: 744 | training loss: 0.020500484854\n",
      "Epoch: 745 | training loss: 0.020462067798\n",
      "Epoch: 746 | training loss: 0.020423913375\n",
      "Epoch: 747 | training loss: 0.020386140794\n",
      "Epoch: 748 | training loss: 0.020348727703\n",
      "Epoch: 749 | training loss: 0.020311582834\n",
      "Epoch: 750 | training loss: 0.020274775103\n",
      "Epoch: 751 | training loss: 0.020238310099\n",
      "Epoch: 752 | training loss: 0.020202130079\n",
      "Epoch: 753 | training loss: 0.020166218281\n",
      "Epoch: 754 | training loss: 0.020130660385\n",
      "Epoch: 755 | training loss: 0.020095353946\n",
      "Epoch: 756 | training loss: 0.020060388371\n",
      "Epoch: 757 | training loss: 0.020025704056\n",
      "Epoch: 758 | training loss: 0.019991280511\n",
      "Epoch: 759 | training loss: 0.019957171753\n",
      "Epoch: 760 | training loss: 0.019923333079\n",
      "Epoch: 761 | training loss: 0.019889768213\n",
      "Epoch: 762 | training loss: 0.019856469706\n",
      "Epoch: 763 | training loss: 0.019823465496\n",
      "Epoch: 764 | training loss: 0.019790718332\n",
      "Epoch: 765 | training loss: 0.019758220762\n",
      "Epoch: 766 | training loss: 0.019725993276\n",
      "Epoch: 767 | training loss: 0.019694035873\n",
      "Epoch: 768 | training loss: 0.019662316889\n",
      "Epoch: 769 | training loss: 0.019630849361\n",
      "Epoch: 770 | training loss: 0.019599633291\n",
      "Epoch: 771 | training loss: 0.019568622112\n",
      "Epoch: 772 | training loss: 0.019537886605\n",
      "Epoch: 773 | training loss: 0.019507378340\n",
      "Epoch: 774 | training loss: 0.019477071241\n",
      "Epoch: 775 | training loss: 0.019447037950\n",
      "Epoch: 776 | training loss: 0.019417207688\n",
      "Epoch: 777 | training loss: 0.019387641922\n",
      "Epoch: 778 | training loss: 0.019358281046\n",
      "Epoch: 779 | training loss: 0.019329147413\n",
      "Epoch: 780 | training loss: 0.019300211221\n",
      "Epoch: 781 | training loss: 0.019271492958\n",
      "Epoch: 782 | training loss: 0.019242942333\n",
      "Epoch: 783 | training loss: 0.019214648753\n",
      "Epoch: 784 | training loss: 0.019186507910\n",
      "Epoch: 785 | training loss: 0.019158646464\n",
      "Epoch: 786 | training loss: 0.019130924717\n",
      "Epoch: 787 | training loss: 0.019103454426\n",
      "Epoch: 788 | training loss: 0.019076142460\n",
      "Epoch: 789 | training loss: 0.019049035385\n",
      "Epoch: 790 | training loss: 0.019022073597\n",
      "Epoch: 791 | training loss: 0.018995372579\n",
      "Epoch: 792 | training loss: 0.018968788907\n",
      "Epoch: 793 | training loss: 0.018942426890\n",
      "Epoch: 794 | training loss: 0.018916213885\n",
      "Epoch: 795 | training loss: 0.018890196458\n",
      "Epoch: 796 | training loss: 0.018864335492\n",
      "Epoch: 797 | training loss: 0.018838685006\n",
      "Epoch: 798 | training loss: 0.018813192844\n",
      "Epoch: 799 | training loss: 0.018787864596\n",
      "Epoch: 800 | training loss: 0.018762705848\n",
      "Epoch: 801 | training loss: 0.018737703562\n",
      "Epoch: 802 | training loss: 0.018712833524\n",
      "Epoch: 803 | training loss: 0.018688151613\n",
      "Epoch: 804 | training loss: 0.018663626164\n",
      "Epoch: 805 | training loss: 0.018639238551\n",
      "Epoch: 806 | training loss: 0.018614994362\n",
      "Epoch: 807 | training loss: 0.018590928987\n",
      "Epoch: 808 | training loss: 0.018566964194\n",
      "Epoch: 809 | training loss: 0.018543228507\n",
      "Epoch: 810 | training loss: 0.018519615754\n",
      "Epoch: 811 | training loss: 0.018496155739\n",
      "Epoch: 812 | training loss: 0.018472801894\n",
      "Epoch: 813 | training loss: 0.018449604511\n",
      "Epoch: 814 | training loss: 0.018426517025\n",
      "Epoch: 815 | training loss: 0.018403597176\n",
      "Epoch: 816 | training loss: 0.018380802125\n",
      "Epoch: 817 | training loss: 0.018358133733\n",
      "Epoch: 818 | training loss: 0.018335606903\n",
      "Epoch: 819 | training loss: 0.018313189968\n",
      "Epoch: 820 | training loss: 0.018290877342\n",
      "Epoch: 821 | training loss: 0.018268769607\n",
      "Epoch: 822 | training loss: 0.018246689811\n",
      "Epoch: 823 | training loss: 0.018224809319\n",
      "Epoch: 824 | training loss: 0.018203029409\n",
      "Epoch: 825 | training loss: 0.018181350082\n",
      "Epoch: 826 | training loss: 0.018159782514\n",
      "Epoch: 827 | training loss: 0.018138369545\n",
      "Epoch: 828 | training loss: 0.018117083237\n",
      "Epoch: 829 | training loss: 0.018095828593\n",
      "Epoch: 830 | training loss: 0.018074732274\n",
      "Epoch: 831 | training loss: 0.018053734675\n",
      "Epoch: 832 | training loss: 0.018032828346\n",
      "Epoch: 833 | training loss: 0.018012076616\n",
      "Epoch: 834 | training loss: 0.017991425470\n",
      "Epoch: 835 | training loss: 0.017970833927\n",
      "Epoch: 836 | training loss: 0.017950367182\n",
      "Epoch: 837 | training loss: 0.017930008471\n",
      "Epoch: 838 | training loss: 0.017909748480\n",
      "Epoch: 839 | training loss: 0.017889579758\n",
      "Epoch: 840 | training loss: 0.017869524658\n",
      "Epoch: 841 | training loss: 0.017849532887\n",
      "Epoch: 842 | training loss: 0.017829678953\n",
      "Epoch: 843 | training loss: 0.017809871584\n",
      "Epoch: 844 | training loss: 0.017790187150\n",
      "Epoch: 845 | training loss: 0.017770573497\n",
      "Epoch: 846 | training loss: 0.017751060426\n",
      "Epoch: 847 | training loss: 0.017731612548\n",
      "Epoch: 848 | training loss: 0.017712267116\n",
      "Epoch: 849 | training loss: 0.017693027854\n",
      "Epoch: 850 | training loss: 0.017673885450\n",
      "Epoch: 851 | training loss: 0.017654787749\n",
      "Epoch: 852 | training loss: 0.017635788769\n",
      "Epoch: 853 | training loss: 0.017616868019\n",
      "Epoch: 854 | training loss: 0.017598032951\n",
      "Epoch: 855 | training loss: 0.017579276115\n",
      "Epoch: 856 | training loss: 0.017560604960\n",
      "Epoch: 857 | training loss: 0.017542006448\n",
      "Epoch: 858 | training loss: 0.017523478717\n",
      "Epoch: 859 | training loss: 0.017505057156\n",
      "Epoch: 860 | training loss: 0.017486670986\n",
      "Epoch: 861 | training loss: 0.017468344420\n",
      "Epoch: 862 | training loss: 0.017450151965\n",
      "Epoch: 863 | training loss: 0.017431994900\n",
      "Epoch: 864 | training loss: 0.017413875088\n",
      "Epoch: 865 | training loss: 0.017395867035\n",
      "Epoch: 866 | training loss: 0.017377898097\n",
      "Epoch: 867 | training loss: 0.017359996215\n",
      "Epoch: 868 | training loss: 0.017342185602\n",
      "Epoch: 869 | training loss: 0.017324449494\n",
      "Epoch: 870 | training loss: 0.017306784168\n",
      "Epoch: 871 | training loss: 0.017289154232\n",
      "Epoch: 872 | training loss: 0.017271585763\n",
      "Epoch: 873 | training loss: 0.017254121602\n",
      "Epoch: 874 | training loss: 0.017236698419\n",
      "Epoch: 875 | training loss: 0.017219316214\n",
      "Epoch: 876 | training loss: 0.017202012241\n",
      "Epoch: 877 | training loss: 0.017184758559\n",
      "Epoch: 878 | training loss: 0.017167558894\n",
      "Epoch: 879 | training loss: 0.017150452361\n",
      "Epoch: 880 | training loss: 0.017133381218\n",
      "Epoch: 881 | training loss: 0.017116352916\n",
      "Epoch: 882 | training loss: 0.017099361867\n",
      "Epoch: 883 | training loss: 0.017082460225\n",
      "Epoch: 884 | training loss: 0.017065618187\n",
      "Epoch: 885 | training loss: 0.017048798501\n",
      "Epoch: 886 | training loss: 0.017032044008\n",
      "Epoch: 887 | training loss: 0.017015352845\n",
      "Epoch: 888 | training loss: 0.016998691484\n",
      "Epoch: 889 | training loss: 0.016982106492\n",
      "Epoch: 890 | training loss: 0.016965573654\n",
      "Epoch: 891 | training loss: 0.016949074343\n",
      "Epoch: 892 | training loss: 0.016932623461\n",
      "Epoch: 893 | training loss: 0.016916230321\n",
      "Epoch: 894 | training loss: 0.016899911687\n",
      "Epoch: 895 | training loss: 0.016883568838\n",
      "Epoch: 896 | training loss: 0.016867291182\n",
      "Epoch: 897 | training loss: 0.016851136461\n",
      "Epoch: 898 | training loss: 0.016834966838\n",
      "Epoch: 899 | training loss: 0.016818804666\n",
      "Epoch: 900 | training loss: 0.016802754253\n",
      "Epoch: 901 | training loss: 0.016786707565\n",
      "Epoch: 902 | training loss: 0.016770742834\n",
      "Epoch: 903 | training loss: 0.016754796728\n",
      "Epoch: 904 | training loss: 0.016738869250\n",
      "Epoch: 905 | training loss: 0.016723003238\n",
      "Epoch: 906 | training loss: 0.016707170755\n",
      "Epoch: 907 | training loss: 0.016691394150\n",
      "Epoch: 908 | training loss: 0.016675662249\n",
      "Epoch: 909 | training loss: 0.016659937799\n",
      "Epoch: 910 | training loss: 0.016644302756\n",
      "Epoch: 911 | training loss: 0.016628660262\n",
      "Epoch: 912 | training loss: 0.016613081098\n",
      "Epoch: 913 | training loss: 0.016597513109\n",
      "Epoch: 914 | training loss: 0.016582025215\n",
      "Epoch: 915 | training loss: 0.016566572711\n",
      "Epoch: 916 | training loss: 0.016551135108\n",
      "Epoch: 917 | training loss: 0.016535721719\n",
      "Epoch: 918 | training loss: 0.016520354897\n",
      "Epoch: 919 | training loss: 0.016505001113\n",
      "Epoch: 920 | training loss: 0.016489703208\n",
      "Epoch: 921 | training loss: 0.016474477947\n",
      "Epoch: 922 | training loss: 0.016459213570\n",
      "Epoch: 923 | training loss: 0.016444016248\n",
      "Epoch: 924 | training loss: 0.016428850591\n",
      "Epoch: 925 | training loss: 0.016413729638\n",
      "Epoch: 926 | training loss: 0.016398642212\n",
      "Epoch: 927 | training loss: 0.016383543611\n",
      "Epoch: 928 | training loss: 0.016368513927\n",
      "Epoch: 929 | training loss: 0.016353493556\n",
      "Epoch: 930 | training loss: 0.016338491812\n",
      "Epoch: 931 | training loss: 0.016323536634\n",
      "Epoch: 932 | training loss: 0.016308633611\n",
      "Epoch: 933 | training loss: 0.016293713823\n",
      "Epoch: 934 | training loss: 0.016278851777\n",
      "Epoch: 935 | training loss: 0.016264004633\n",
      "Epoch: 936 | training loss: 0.016249241307\n",
      "Epoch: 937 | training loss: 0.016234427691\n",
      "Epoch: 938 | training loss: 0.016219682992\n",
      "Epoch: 939 | training loss: 0.016204971820\n",
      "Epoch: 940 | training loss: 0.016190307215\n",
      "Epoch: 941 | training loss: 0.016175640747\n",
      "Epoch: 942 | training loss: 0.016160994768\n",
      "Epoch: 943 | training loss: 0.016146380454\n",
      "Epoch: 944 | training loss: 0.016131781042\n",
      "Epoch: 945 | training loss: 0.016117213294\n",
      "Epoch: 946 | training loss: 0.016102679074\n",
      "Epoch: 947 | training loss: 0.016088161618\n",
      "Epoch: 948 | training loss: 0.016073672101\n",
      "Epoch: 949 | training loss: 0.016059234738\n",
      "Epoch: 950 | training loss: 0.016044786200\n",
      "Epoch: 951 | training loss: 0.016030360013\n",
      "Epoch: 952 | training loss: 0.016015997157\n",
      "Epoch: 953 | training loss: 0.016001604497\n",
      "Epoch: 954 | training loss: 0.015987267718\n",
      "Epoch: 955 | training loss: 0.015972962603\n",
      "Epoch: 956 | training loss: 0.015958657488\n",
      "Epoch: 957 | training loss: 0.015944359824\n",
      "Epoch: 958 | training loss: 0.015930105001\n",
      "Epoch: 959 | training loss: 0.015915879980\n",
      "Epoch: 960 | training loss: 0.015901662409\n",
      "Epoch: 961 | training loss: 0.015887457877\n",
      "Epoch: 962 | training loss: 0.015873286873\n",
      "Epoch: 963 | training loss: 0.015859141946\n",
      "Epoch: 964 | training loss: 0.015845011920\n",
      "Epoch: 965 | training loss: 0.015830896795\n",
      "Epoch: 966 | training loss: 0.015816813335\n",
      "Epoch: 967 | training loss: 0.015802707523\n",
      "Epoch: 968 | training loss: 0.015788683668\n",
      "Epoch: 969 | training loss: 0.015774635598\n",
      "Epoch: 970 | training loss: 0.015760600567\n",
      "Epoch: 971 | training loss: 0.015746619552\n",
      "Epoch: 972 | training loss: 0.015732673928\n",
      "Epoch: 973 | training loss: 0.015718711540\n",
      "Epoch: 974 | training loss: 0.015704765916\n",
      "Epoch: 975 | training loss: 0.015690840781\n",
      "Epoch: 976 | training loss: 0.015676993877\n",
      "Epoch: 977 | training loss: 0.015663089231\n",
      "Epoch: 978 | training loss: 0.015649229288\n",
      "Epoch: 979 | training loss: 0.015635358170\n",
      "Epoch: 980 | training loss: 0.015621578321\n",
      "Epoch: 981 | training loss: 0.015607759356\n",
      "Epoch: 982 | training loss: 0.015593955293\n",
      "Epoch: 983 | training loss: 0.015580175444\n",
      "Epoch: 984 | training loss: 0.015566444956\n",
      "Epoch: 985 | training loss: 0.015552680939\n",
      "Epoch: 986 | training loss: 0.015538973734\n",
      "Epoch: 987 | training loss: 0.015525265597\n",
      "Epoch: 988 | training loss: 0.015511593781\n",
      "Epoch: 989 | training loss: 0.015497872606\n",
      "Epoch: 990 | training loss: 0.015484254807\n",
      "Epoch: 991 | training loss: 0.015470610000\n",
      "Epoch: 992 | training loss: 0.015456979163\n",
      "Epoch: 993 | training loss: 0.015443376265\n",
      "Epoch: 994 | training loss: 0.015429805033\n",
      "Epoch: 995 | training loss: 0.015416218899\n",
      "Epoch: 996 | training loss: 0.015402658843\n",
      "Epoch: 997 | training loss: 0.015389097854\n",
      "Epoch: 998 | training loss: 0.015375580639\n",
      "Epoch: 999 | training loss: 0.015362065285\n",
      "Epoch: 1000 | training loss: 0.015348570421\n",
      "Epoch: 1001 | training loss: 0.015335090458\n",
      "Epoch: 1002 | training loss: 0.015321593732\n",
      "Epoch: 1003 | training loss: 0.015308137052\n",
      "Epoch: 1004 | training loss: 0.015294722281\n",
      "Epoch: 1005 | training loss: 0.015281261876\n",
      "Epoch: 1006 | training loss: 0.015267855488\n",
      "Epoch: 1007 | training loss: 0.015254442580\n",
      "Epoch: 1008 | training loss: 0.015241071582\n",
      "Epoch: 1009 | training loss: 0.015227660537\n",
      "Epoch: 1010 | training loss: 0.015214323066\n",
      "Epoch: 1011 | training loss: 0.015200944617\n",
      "Epoch: 1012 | training loss: 0.015187614597\n",
      "Epoch: 1013 | training loss: 0.015174306929\n",
      "Epoch: 1014 | training loss: 0.015160979703\n",
      "Epoch: 1015 | training loss: 0.015147687867\n",
      "Epoch: 1016 | training loss: 0.015134403482\n",
      "Epoch: 1017 | training loss: 0.015121122822\n",
      "Epoch: 1018 | training loss: 0.015107845888\n",
      "Epoch: 1019 | training loss: 0.015094612725\n",
      "Epoch: 1020 | training loss: 0.015081380494\n",
      "Epoch: 1021 | training loss: 0.015068193898\n",
      "Epoch: 1022 | training loss: 0.015054972842\n",
      "Epoch: 1023 | training loss: 0.015041769482\n",
      "Epoch: 1024 | training loss: 0.015028580092\n",
      "Epoch: 1025 | training loss: 0.015015391633\n",
      "Epoch: 1026 | training loss: 0.015002259985\n",
      "Epoch: 1027 | training loss: 0.014989113435\n",
      "Epoch: 1028 | training loss: 0.014975986443\n",
      "Epoch: 1029 | training loss: 0.014962852933\n",
      "Epoch: 1030 | training loss: 0.014949725010\n",
      "Epoch: 1031 | training loss: 0.014936654828\n",
      "Epoch: 1032 | training loss: 0.014923560433\n",
      "Epoch: 1033 | training loss: 0.014910464175\n",
      "Epoch: 1034 | training loss: 0.014897426590\n",
      "Epoch: 1035 | training loss: 0.014884360135\n",
      "Epoch: 1036 | training loss: 0.014871325344\n",
      "Epoch: 1037 | training loss: 0.014858288690\n",
      "Epoch: 1038 | training loss: 0.014845278114\n",
      "Epoch: 1039 | training loss: 0.014832277782\n",
      "Epoch: 1040 | training loss: 0.014819262549\n",
      "Epoch: 1041 | training loss: 0.014806280844\n",
      "Epoch: 1042 | training loss: 0.014793295413\n",
      "Epoch: 1043 | training loss: 0.014780337922\n",
      "Epoch: 1044 | training loss: 0.014767354354\n",
      "Epoch: 1045 | training loss: 0.014754425734\n",
      "Epoch: 1046 | training loss: 0.014741480350\n",
      "Epoch: 1047 | training loss: 0.014728597365\n",
      "Epoch: 1048 | training loss: 0.014715626836\n",
      "Epoch: 1049 | training loss: 0.014702729881\n",
      "Epoch: 1050 | training loss: 0.014689853415\n",
      "Epoch: 1051 | training loss: 0.014676950872\n",
      "Epoch: 1052 | training loss: 0.014664074406\n",
      "Epoch: 1053 | training loss: 0.014651229605\n",
      "Epoch: 1054 | training loss: 0.014638384804\n",
      "Epoch: 1055 | training loss: 0.014625533484\n",
      "Epoch: 1056 | training loss: 0.014612713829\n",
      "Epoch: 1057 | training loss: 0.014599874616\n",
      "Epoch: 1058 | training loss: 0.014587067068\n",
      "Epoch: 1059 | training loss: 0.014574281871\n",
      "Epoch: 1060 | training loss: 0.014561462216\n",
      "Epoch: 1061 | training loss: 0.014548653737\n",
      "Epoch: 1062 | training loss: 0.014535907656\n",
      "Epoch: 1063 | training loss: 0.014523151331\n",
      "Epoch: 1064 | training loss: 0.014510398731\n",
      "Epoch: 1065 | training loss: 0.014497632161\n",
      "Epoch: 1066 | training loss: 0.014484900981\n",
      "Epoch: 1067 | training loss: 0.014472190291\n",
      "Epoch: 1068 | training loss: 0.014459482394\n",
      "Epoch: 1069 | training loss: 0.014446747489\n",
      "Epoch: 1070 | training loss: 0.014434042387\n",
      "Epoch: 1071 | training loss: 0.014421369880\n",
      "Epoch: 1072 | training loss: 0.014408716932\n",
      "Epoch: 1073 | training loss: 0.014396031387\n",
      "Epoch: 1074 | training loss: 0.014383388683\n",
      "Epoch: 1075 | training loss: 0.014370726421\n",
      "Epoch: 1076 | training loss: 0.014358108863\n",
      "Epoch: 1077 | training loss: 0.014345482923\n",
      "Epoch: 1078 | training loss: 0.014332853258\n",
      "Epoch: 1079 | training loss: 0.014320230111\n",
      "Epoch: 1080 | training loss: 0.014307641424\n",
      "Epoch: 1081 | training loss: 0.014295071363\n",
      "Epoch: 1082 | training loss: 0.014282476157\n",
      "Epoch: 1083 | training loss: 0.014269907027\n",
      "Epoch: 1084 | training loss: 0.014257326722\n",
      "Epoch: 1085 | training loss: 0.014244779013\n",
      "Epoch: 1086 | training loss: 0.014232259244\n",
      "Epoch: 1087 | training loss: 0.014219714329\n",
      "Epoch: 1088 | training loss: 0.014207185246\n",
      "Epoch: 1089 | training loss: 0.014194654301\n",
      "Epoch: 1090 | training loss: 0.014182154089\n",
      "Epoch: 1091 | training loss: 0.014169649221\n",
      "Epoch: 1092 | training loss: 0.014157151803\n",
      "Epoch: 1093 | training loss: 0.014144705608\n",
      "Epoch: 1094 | training loss: 0.014132223092\n",
      "Epoch: 1095 | training loss: 0.014119727537\n",
      "Epoch: 1096 | training loss: 0.014107296243\n",
      "Epoch: 1097 | training loss: 0.014094844460\n",
      "Epoch: 1098 | training loss: 0.014082415961\n",
      "Epoch: 1099 | training loss: 0.014069970697\n",
      "Epoch: 1100 | training loss: 0.014057541266\n",
      "Epoch: 1101 | training loss: 0.014045117423\n",
      "Epoch: 1102 | training loss: 0.014032725245\n",
      "Epoch: 1103 | training loss: 0.014020318165\n",
      "Epoch: 1104 | training loss: 0.014007917605\n",
      "Epoch: 1105 | training loss: 0.013995573856\n",
      "Epoch: 1106 | training loss: 0.013983189128\n",
      "Epoch: 1107 | training loss: 0.013970839791\n",
      "Epoch: 1108 | training loss: 0.013958463445\n",
      "Epoch: 1109 | training loss: 0.013946142048\n",
      "Epoch: 1110 | training loss: 0.013933770359\n",
      "Epoch: 1111 | training loss: 0.013921441510\n",
      "Epoch: 1112 | training loss: 0.013909127563\n",
      "Epoch: 1113 | training loss: 0.013896834105\n",
      "Epoch: 1114 | training loss: 0.013884537853\n",
      "Epoch: 1115 | training loss: 0.013872249052\n",
      "Epoch: 1116 | training loss: 0.013859921135\n",
      "Epoch: 1117 | training loss: 0.013847687282\n",
      "Epoch: 1118 | training loss: 0.013835392892\n",
      "Epoch: 1119 | training loss: 0.013823124580\n",
      "Epoch: 1120 | training loss: 0.013810854405\n",
      "Epoch: 1121 | training loss: 0.013798629865\n",
      "Epoch: 1122 | training loss: 0.013786345720\n",
      "Epoch: 1123 | training loss: 0.013774160296\n",
      "Epoch: 1124 | training loss: 0.013761893846\n",
      "Epoch: 1125 | training loss: 0.013749686070\n",
      "Epoch: 1126 | training loss: 0.013737484813\n",
      "Epoch: 1127 | training loss: 0.013725307770\n",
      "Epoch: 1128 | training loss: 0.013713087887\n",
      "Epoch: 1129 | training loss: 0.013700935058\n",
      "Epoch: 1130 | training loss: 0.013688745908\n",
      "Epoch: 1131 | training loss: 0.013676563278\n",
      "Epoch: 1132 | training loss: 0.013664419763\n",
      "Epoch: 1133 | training loss: 0.013652289286\n",
      "Epoch: 1134 | training loss: 0.013640124351\n",
      "Epoch: 1135 | training loss: 0.013627984561\n",
      "Epoch: 1136 | training loss: 0.013615865260\n",
      "Epoch: 1137 | training loss: 0.013603725471\n",
      "Epoch: 1138 | training loss: 0.013591620140\n",
      "Epoch: 1139 | training loss: 0.013579498976\n",
      "Epoch: 1140 | training loss: 0.013567417860\n",
      "Epoch: 1141 | training loss: 0.013555311598\n",
      "Epoch: 1142 | training loss: 0.013543267734\n",
      "Epoch: 1143 | training loss: 0.013531174511\n",
      "Epoch: 1144 | training loss: 0.013519103639\n",
      "Epoch: 1145 | training loss: 0.013507042080\n",
      "Epoch: 1146 | training loss: 0.013494995423\n",
      "Epoch: 1147 | training loss: 0.013482979499\n",
      "Epoch: 1148 | training loss: 0.013470934704\n",
      "Epoch: 1149 | training loss: 0.013458924368\n",
      "Epoch: 1150 | training loss: 0.013446905650\n",
      "Epoch: 1151 | training loss: 0.013434899971\n",
      "Epoch: 1152 | training loss: 0.013422889635\n",
      "Epoch: 1153 | training loss: 0.013410870917\n",
      "Epoch: 1154 | training loss: 0.013398867100\n",
      "Epoch: 1155 | training loss: 0.013386879116\n",
      "Epoch: 1156 | training loss: 0.013374924660\n",
      "Epoch: 1157 | training loss: 0.013362975791\n",
      "Epoch: 1158 | training loss: 0.013350994326\n",
      "Epoch: 1159 | training loss: 0.013339043595\n",
      "Epoch: 1160 | training loss: 0.013327124529\n",
      "Epoch: 1161 | training loss: 0.013315167278\n",
      "Epoch: 1162 | training loss: 0.013303237967\n",
      "Epoch: 1163 | training loss: 0.013291307725\n",
      "Epoch: 1164 | training loss: 0.013279408216\n",
      "Epoch: 1165 | training loss: 0.013267496601\n",
      "Epoch: 1166 | training loss: 0.013255588710\n",
      "Epoch: 1167 | training loss: 0.013243682683\n",
      "Epoch: 1168 | training loss: 0.013231819496\n",
      "Epoch: 1169 | training loss: 0.013219919056\n",
      "Epoch: 1170 | training loss: 0.013208054006\n",
      "Epoch: 1171 | training loss: 0.013196196407\n",
      "Epoch: 1172 | training loss: 0.013184342533\n",
      "Epoch: 1173 | training loss: 0.013172476552\n",
      "Epoch: 1174 | training loss: 0.013160668314\n",
      "Epoch: 1175 | training loss: 0.013148816302\n",
      "Epoch: 1176 | training loss: 0.013136999682\n",
      "Epoch: 1177 | training loss: 0.013125206344\n",
      "Epoch: 1178 | training loss: 0.013113384135\n",
      "Epoch: 1179 | training loss: 0.013101576827\n",
      "Epoch: 1180 | training loss: 0.013089814223\n",
      "Epoch: 1181 | training loss: 0.013078017160\n",
      "Epoch: 1182 | training loss: 0.013066234067\n",
      "Epoch: 1183 | training loss: 0.013054450043\n",
      "Epoch: 1184 | training loss: 0.013042693958\n",
      "Epoch: 1185 | training loss: 0.013030925766\n",
      "Epoch: 1186 | training loss: 0.013019187376\n",
      "Epoch: 1187 | training loss: 0.013007420115\n",
      "Epoch: 1188 | training loss: 0.012995669618\n",
      "Epoch: 1189 | training loss: 0.012983964756\n",
      "Epoch: 1190 | training loss: 0.012972228229\n",
      "Epoch: 1191 | training loss: 0.012960525230\n",
      "Epoch: 1192 | training loss: 0.012948822230\n",
      "Epoch: 1193 | training loss: 0.012937146239\n",
      "Epoch: 1194 | training loss: 0.012925455347\n",
      "Epoch: 1195 | training loss: 0.012913771905\n",
      "Epoch: 1196 | training loss: 0.012902092189\n",
      "Epoch: 1197 | training loss: 0.012890435755\n",
      "Epoch: 1198 | training loss: 0.012878781185\n",
      "Epoch: 1199 | training loss: 0.012867124751\n",
      "Epoch: 1200 | training loss: 0.012855468318\n",
      "Epoch: 1201 | training loss: 0.012843851931\n",
      "Epoch: 1202 | training loss: 0.012832220644\n",
      "Epoch: 1203 | training loss: 0.012820590287\n",
      "Epoch: 1204 | training loss: 0.012808972970\n",
      "Epoch: 1205 | training loss: 0.012797382660\n",
      "Epoch: 1206 | training loss: 0.012785778381\n",
      "Epoch: 1207 | training loss: 0.012774204835\n",
      "Epoch: 1208 | training loss: 0.012762610801\n",
      "Epoch: 1209 | training loss: 0.012751024216\n",
      "Epoch: 1210 | training loss: 0.012739455327\n",
      "Epoch: 1211 | training loss: 0.012727912515\n",
      "Epoch: 1212 | training loss: 0.012716352008\n",
      "Epoch: 1213 | training loss: 0.012704780325\n",
      "Epoch: 1214 | training loss: 0.012693237513\n",
      "Epoch: 1215 | training loss: 0.012681727298\n",
      "Epoch: 1216 | training loss: 0.012670239434\n",
      "Epoch: 1217 | training loss: 0.012658704072\n",
      "Epoch: 1218 | training loss: 0.012647185475\n",
      "Epoch: 1219 | training loss: 0.012635681778\n",
      "Epoch: 1220 | training loss: 0.012624183670\n",
      "Epoch: 1221 | training loss: 0.012612705119\n",
      "Epoch: 1222 | training loss: 0.012601193041\n",
      "Epoch: 1223 | training loss: 0.012589736842\n",
      "Epoch: 1224 | training loss: 0.012578276917\n",
      "Epoch: 1225 | training loss: 0.012566825375\n",
      "Epoch: 1226 | training loss: 0.012555344962\n",
      "Epoch: 1227 | training loss: 0.012543926947\n",
      "Epoch: 1228 | training loss: 0.012532495894\n",
      "Epoch: 1229 | training loss: 0.012521042489\n",
      "Epoch: 1230 | training loss: 0.012509621680\n",
      "Epoch: 1231 | training loss: 0.012498247437\n",
      "Epoch: 1232 | training loss: 0.012486844324\n",
      "Epoch: 1233 | training loss: 0.012475435622\n",
      "Epoch: 1234 | training loss: 0.012464033440\n",
      "Epoch: 1235 | training loss: 0.012452680618\n",
      "Epoch: 1236 | training loss: 0.012441288680\n",
      "Epoch: 1237 | training loss: 0.012429919094\n",
      "Epoch: 1238 | training loss: 0.012418545783\n",
      "Epoch: 1239 | training loss: 0.012407208793\n",
      "Epoch: 1240 | training loss: 0.012395869009\n",
      "Epoch: 1241 | training loss: 0.012384499423\n",
      "Epoch: 1242 | training loss: 0.012373157777\n",
      "Epoch: 1243 | training loss: 0.012361846864\n",
      "Epoch: 1244 | training loss: 0.012350594625\n",
      "Epoch: 1245 | training loss: 0.012339252979\n",
      "Epoch: 1246 | training loss: 0.012327922508\n",
      "Epoch: 1247 | training loss: 0.012316657230\n",
      "Epoch: 1248 | training loss: 0.012305361219\n",
      "Epoch: 1249 | training loss: 0.012294087559\n",
      "Epoch: 1250 | training loss: 0.012282801792\n",
      "Epoch: 1251 | training loss: 0.012271570042\n",
      "Epoch: 1252 | training loss: 0.012260289863\n",
      "Epoch: 1253 | training loss: 0.012249040417\n",
      "Epoch: 1254 | training loss: 0.012237795629\n",
      "Epoch: 1255 | training loss: 0.012226577848\n",
      "Epoch: 1256 | training loss: 0.012215314433\n",
      "Epoch: 1257 | training loss: 0.012204125524\n",
      "Epoch: 1258 | training loss: 0.012192914262\n",
      "Epoch: 1259 | training loss: 0.012181710452\n",
      "Epoch: 1260 | training loss: 0.012170505710\n",
      "Epoch: 1261 | training loss: 0.012159310281\n",
      "Epoch: 1262 | training loss: 0.012148149312\n",
      "Epoch: 1263 | training loss: 0.012136962265\n",
      "Epoch: 1264 | training loss: 0.012125798501\n",
      "Epoch: 1265 | training loss: 0.012114644982\n",
      "Epoch: 1266 | training loss: 0.012103484012\n",
      "Epoch: 1267 | training loss: 0.012092348188\n",
      "Epoch: 1268 | training loss: 0.012081202120\n",
      "Epoch: 1269 | training loss: 0.012070097029\n",
      "Epoch: 1270 | training loss: 0.012058978900\n",
      "Epoch: 1271 | training loss: 0.012047843076\n",
      "Epoch: 1272 | training loss: 0.012036764994\n",
      "Epoch: 1273 | training loss: 0.012025641277\n",
      "Epoch: 1274 | training loss: 0.012014559470\n",
      "Epoch: 1275 | training loss: 0.012003456242\n",
      "Epoch: 1276 | training loss: 0.011992385611\n",
      "Epoch: 1277 | training loss: 0.011981333606\n",
      "Epoch: 1278 | training loss: 0.011970303021\n",
      "Epoch: 1279 | training loss: 0.011959221214\n",
      "Epoch: 1280 | training loss: 0.011948191561\n",
      "Epoch: 1281 | training loss: 0.011937137693\n",
      "Epoch: 1282 | training loss: 0.011926134117\n",
      "Epoch: 1283 | training loss: 0.011915070936\n",
      "Epoch: 1284 | training loss: 0.011904078536\n",
      "Epoch: 1285 | training loss: 0.011893044226\n",
      "Epoch: 1286 | training loss: 0.011882073246\n",
      "Epoch: 1287 | training loss: 0.011871097609\n",
      "Epoch: 1288 | training loss: 0.011860094033\n",
      "Epoch: 1289 | training loss: 0.011849116534\n",
      "Epoch: 1290 | training loss: 0.011838130653\n",
      "Epoch: 1291 | training loss: 0.011827189475\n",
      "Epoch: 1292 | training loss: 0.011816233397\n",
      "Epoch: 1293 | training loss: 0.011805283837\n",
      "Epoch: 1294 | training loss: 0.011794332415\n",
      "Epoch: 1295 | training loss: 0.011783423834\n",
      "Epoch: 1296 | training loss: 0.011772505939\n",
      "Epoch: 1297 | training loss: 0.011761571281\n",
      "Epoch: 1298 | training loss: 0.011750656180\n",
      "Epoch: 1299 | training loss: 0.011739761569\n",
      "Epoch: 1300 | training loss: 0.011728890240\n",
      "Epoch: 1301 | training loss: 0.011718006805\n",
      "Epoch: 1302 | training loss: 0.011707121506\n",
      "Epoch: 1303 | training loss: 0.011696239002\n",
      "Epoch: 1304 | training loss: 0.011685399339\n",
      "Epoch: 1305 | training loss: 0.011674541049\n",
      "Epoch: 1306 | training loss: 0.011663682759\n",
      "Epoch: 1307 | training loss: 0.011652823538\n",
      "Epoch: 1308 | training loss: 0.011641999707\n",
      "Epoch: 1309 | training loss: 0.011631188914\n",
      "Epoch: 1310 | training loss: 0.011620346457\n",
      "Epoch: 1311 | training loss: 0.011609550565\n",
      "Epoch: 1312 | training loss: 0.011598754674\n",
      "Epoch: 1313 | training loss: 0.011587927118\n",
      "Epoch: 1314 | training loss: 0.011577157304\n",
      "Epoch: 1315 | training loss: 0.011566361412\n",
      "Epoch: 1316 | training loss: 0.011555610225\n",
      "Epoch: 1317 | training loss: 0.011544814333\n",
      "Epoch: 1318 | training loss: 0.011534041725\n",
      "Epoch: 1319 | training loss: 0.011523309164\n",
      "Epoch: 1320 | training loss: 0.011512559839\n",
      "Epoch: 1321 | training loss: 0.011501828209\n",
      "Epoch: 1322 | training loss: 0.011491081677\n",
      "Epoch: 1323 | training loss: 0.011480381712\n",
      "Epoch: 1324 | training loss: 0.011469651014\n",
      "Epoch: 1325 | training loss: 0.011458938941\n",
      "Epoch: 1326 | training loss: 0.011448226869\n",
      "Epoch: 1327 | training loss: 0.011437560432\n",
      "Epoch: 1328 | training loss: 0.011426880956\n",
      "Epoch: 1329 | training loss: 0.011416200548\n",
      "Epoch: 1330 | training loss: 0.011405535974\n",
      "Epoch: 1331 | training loss: 0.011394866742\n",
      "Epoch: 1332 | training loss: 0.011384190060\n",
      "Epoch: 1333 | training loss: 0.011373530142\n",
      "Epoch: 1334 | training loss: 0.011362919584\n",
      "Epoch: 1335 | training loss: 0.011352278292\n",
      "Epoch: 1336 | training loss: 0.011341640726\n",
      "Epoch: 1337 | training loss: 0.011331052519\n",
      "Epoch: 1338 | training loss: 0.011320446618\n",
      "Epoch: 1339 | training loss: 0.011309813708\n",
      "Epoch: 1340 | training loss: 0.011299243197\n",
      "Epoch: 1341 | training loss: 0.011288634501\n",
      "Epoch: 1342 | training loss: 0.011278064921\n",
      "Epoch: 1343 | training loss: 0.011267505586\n",
      "Epoch: 1344 | training loss: 0.011256922968\n",
      "Epoch: 1345 | training loss: 0.011246369220\n",
      "Epoch: 1346 | training loss: 0.011235805228\n",
      "Epoch: 1347 | training loss: 0.011225258932\n",
      "Epoch: 1348 | training loss: 0.011214736849\n",
      "Epoch: 1349 | training loss: 0.011204194278\n",
      "Epoch: 1350 | training loss: 0.011193681508\n",
      "Epoch: 1351 | training loss: 0.011183179915\n",
      "Epoch: 1352 | training loss: 0.011172647588\n",
      "Epoch: 1353 | training loss: 0.011162181385\n",
      "Epoch: 1354 | training loss: 0.011151673272\n",
      "Epoch: 1355 | training loss: 0.011141178198\n",
      "Epoch: 1356 | training loss: 0.011130752042\n",
      "Epoch: 1357 | training loss: 0.011120239273\n",
      "Epoch: 1358 | training loss: 0.011109796353\n",
      "Epoch: 1359 | training loss: 0.011099342257\n",
      "Epoch: 1360 | training loss: 0.011088881642\n",
      "Epoch: 1361 | training loss: 0.011078463867\n",
      "Epoch: 1362 | training loss: 0.011068032123\n",
      "Epoch: 1363 | training loss: 0.011057583615\n",
      "Epoch: 1364 | training loss: 0.011047196575\n",
      "Epoch: 1365 | training loss: 0.011036773212\n",
      "Epoch: 1366 | training loss: 0.011026363820\n",
      "Epoch: 1367 | training loss: 0.011015989818\n",
      "Epoch: 1368 | training loss: 0.011005589738\n",
      "Epoch: 1369 | training loss: 0.010995223187\n",
      "Epoch: 1370 | training loss: 0.010984856635\n",
      "Epoch: 1371 | training loss: 0.010974491946\n",
      "Epoch: 1372 | training loss: 0.010964101180\n",
      "Epoch: 1373 | training loss: 0.010953783989\n",
      "Epoch: 1374 | training loss: 0.010943436064\n",
      "Epoch: 1375 | training loss: 0.010933119804\n",
      "Epoch: 1376 | training loss: 0.010922777466\n",
      "Epoch: 1377 | training loss: 0.010912444443\n",
      "Epoch: 1378 | training loss: 0.010902165435\n",
      "Epoch: 1379 | training loss: 0.010891828686\n",
      "Epoch: 1380 | training loss: 0.010881535709\n",
      "Epoch: 1381 | training loss: 0.010871252976\n",
      "Epoch: 1382 | training loss: 0.010860975832\n",
      "Epoch: 1383 | training loss: 0.010850707069\n",
      "Epoch: 1384 | training loss: 0.010840441100\n",
      "Epoch: 1385 | training loss: 0.010830170475\n",
      "Epoch: 1386 | training loss: 0.010819941759\n",
      "Epoch: 1387 | training loss: 0.010809668340\n",
      "Epoch: 1388 | training loss: 0.010799414478\n",
      "Epoch: 1389 | training loss: 0.010789209977\n",
      "Epoch: 1390 | training loss: 0.010778998025\n",
      "Epoch: 1391 | training loss: 0.010768797249\n",
      "Epoch: 1392 | training loss: 0.010758580640\n",
      "Epoch: 1393 | training loss: 0.010748369619\n",
      "Epoch: 1394 | training loss: 0.010738184676\n",
      "Epoch: 1395 | training loss: 0.010727994144\n",
      "Epoch: 1396 | training loss: 0.010717855766\n",
      "Epoch: 1397 | training loss: 0.010707676411\n",
      "Epoch: 1398 | training loss: 0.010697478428\n",
      "Epoch: 1399 | training loss: 0.010687340051\n",
      "Epoch: 1400 | training loss: 0.010677209124\n",
      "Epoch: 1401 | training loss: 0.010667089373\n",
      "Epoch: 1402 | training loss: 0.010656950064\n",
      "Epoch: 1403 | training loss: 0.010646835901\n",
      "Epoch: 1404 | training loss: 0.010636718012\n",
      "Epoch: 1405 | training loss: 0.010626627132\n",
      "Epoch: 1406 | training loss: 0.010616539046\n",
      "Epoch: 1407 | training loss: 0.010606401600\n",
      "Epoch: 1408 | training loss: 0.010596354492\n",
      "Epoch: 1409 | training loss: 0.010586272925\n",
      "Epoch: 1410 | training loss: 0.010576203465\n",
      "Epoch: 1411 | training loss: 0.010566161945\n",
      "Epoch: 1412 | training loss: 0.010556094348\n",
      "Epoch: 1413 | training loss: 0.010546056554\n",
      "Epoch: 1414 | training loss: 0.010536022484\n",
      "Epoch: 1415 | training loss: 0.010525970720\n",
      "Epoch: 1416 | training loss: 0.010515959933\n",
      "Epoch: 1417 | training loss: 0.010505954735\n",
      "Epoch: 1418 | training loss: 0.010495911352\n",
      "Epoch: 1419 | training loss: 0.010485931300\n",
      "Epoch: 1420 | training loss: 0.010475931689\n",
      "Epoch: 1421 | training loss: 0.010465964675\n",
      "Epoch: 1422 | training loss: 0.010455976240\n",
      "Epoch: 1423 | training loss: 0.010445997119\n",
      "Epoch: 1424 | training loss: 0.010436035693\n",
      "Epoch: 1425 | training loss: 0.010426105931\n",
      "Epoch: 1426 | training loss: 0.010416131467\n",
      "Epoch: 1427 | training loss: 0.010406197049\n",
      "Epoch: 1428 | training loss: 0.010396287777\n",
      "Epoch: 1429 | training loss: 0.010386345908\n",
      "Epoch: 1430 | training loss: 0.010376414284\n",
      "Epoch: 1431 | training loss: 0.010366529226\n",
      "Epoch: 1432 | training loss: 0.010356616229\n",
      "Epoch: 1433 | training loss: 0.010346733034\n",
      "Epoch: 1434 | training loss: 0.010336825624\n",
      "Epoch: 1435 | training loss: 0.010326959193\n",
      "Epoch: 1436 | training loss: 0.010317101143\n",
      "Epoch: 1437 | training loss: 0.010307217948\n",
      "Epoch: 1438 | training loss: 0.010297376662\n",
      "Epoch: 1439 | training loss: 0.010287526995\n",
      "Epoch: 1440 | training loss: 0.010277692229\n",
      "Epoch: 1441 | training loss: 0.010267849080\n",
      "Epoch: 1442 | training loss: 0.010258033872\n",
      "Epoch: 1443 | training loss: 0.010248208418\n",
      "Epoch: 1444 | training loss: 0.010238401592\n",
      "Epoch: 1445 | training loss: 0.010228582658\n",
      "Epoch: 1446 | training loss: 0.010218795389\n",
      "Epoch: 1447 | training loss: 0.010209013708\n",
      "Epoch: 1448 | training loss: 0.010199238546\n",
      "Epoch: 1449 | training loss: 0.010189501569\n",
      "Epoch: 1450 | training loss: 0.010179704987\n",
      "Epoch: 1451 | training loss: 0.010169980116\n",
      "Epoch: 1452 | training loss: 0.010160199367\n",
      "Epoch: 1453 | training loss: 0.010150470771\n",
      "Epoch: 1454 | training loss: 0.010140717030\n",
      "Epoch: 1455 | training loss: 0.010131013580\n",
      "Epoch: 1456 | training loss: 0.010121252388\n",
      "Epoch: 1457 | training loss: 0.010111609474\n",
      "Epoch: 1458 | training loss: 0.010101856664\n",
      "Epoch: 1459 | training loss: 0.010092180222\n",
      "Epoch: 1460 | training loss: 0.010082467459\n",
      "Epoch: 1461 | training loss: 0.010072806850\n",
      "Epoch: 1462 | training loss: 0.010063116439\n",
      "Epoch: 1463 | training loss: 0.010053456761\n",
      "Epoch: 1464 | training loss: 0.010043812916\n",
      "Epoch: 1465 | training loss: 0.010034168139\n",
      "Epoch: 1466 | training loss: 0.010024514981\n",
      "Epoch: 1467 | training loss: 0.010014868341\n",
      "Epoch: 1468 | training loss: 0.010005254298\n",
      "Epoch: 1469 | training loss: 0.009995632805\n",
      "Epoch: 1470 | training loss: 0.009986013174\n",
      "Epoch: 1471 | training loss: 0.009976422414\n",
      "Epoch: 1472 | training loss: 0.009966837242\n",
      "Epoch: 1473 | training loss: 0.009957263246\n",
      "Epoch: 1474 | training loss: 0.009947653860\n",
      "Epoch: 1475 | training loss: 0.009938091040\n",
      "Epoch: 1476 | training loss: 0.009928513318\n",
      "Epoch: 1477 | training loss: 0.009918963537\n",
      "Epoch: 1478 | training loss: 0.009909395128\n",
      "Epoch: 1479 | training loss: 0.009899854660\n",
      "Epoch: 1480 | training loss: 0.009890317917\n",
      "Epoch: 1481 | training loss: 0.009880803525\n",
      "Epoch: 1482 | training loss: 0.009871262126\n",
      "Epoch: 1483 | training loss: 0.009861773811\n",
      "Epoch: 1484 | training loss: 0.009852255695\n",
      "Epoch: 1485 | training loss: 0.009842768312\n",
      "Epoch: 1486 | training loss: 0.009833262302\n",
      "Epoch: 1487 | training loss: 0.009823782369\n",
      "Epoch: 1488 | training loss: 0.009814333171\n",
      "Epoch: 1489 | training loss: 0.009804842994\n",
      "Epoch: 1490 | training loss: 0.009795384482\n",
      "Epoch: 1491 | training loss: 0.009785935283\n",
      "Epoch: 1492 | training loss: 0.009776493534\n",
      "Epoch: 1493 | training loss: 0.009767054580\n",
      "Epoch: 1494 | training loss: 0.009757625870\n",
      "Epoch: 1495 | training loss: 0.009748197161\n",
      "Epoch: 1496 | training loss: 0.009738798253\n",
      "Epoch: 1497 | training loss: 0.009729382582\n",
      "Epoch: 1498 | training loss: 0.009719991125\n",
      "Epoch: 1499 | training loss: 0.009710563347\n",
      "Epoch: 1500 | training loss: 0.009701214731\n",
      "Epoch: 1501 | training loss: 0.009691861458\n",
      "Epoch: 1502 | training loss: 0.009682479315\n",
      "Epoch: 1503 | training loss: 0.009673104621\n",
      "Epoch: 1504 | training loss: 0.009663765319\n",
      "Epoch: 1505 | training loss: 0.009654396214\n",
      "Epoch: 1506 | training loss: 0.009645064361\n",
      "Epoch: 1507 | training loss: 0.009635760449\n",
      "Epoch: 1508 | training loss: 0.009626424871\n",
      "Epoch: 1509 | training loss: 0.009617118165\n",
      "Epoch: 1510 | training loss: 0.009607825428\n",
      "Epoch: 1511 | training loss: 0.009598515928\n",
      "Epoch: 1512 | training loss: 0.009589207359\n",
      "Epoch: 1513 | training loss: 0.009579939768\n",
      "Epoch: 1514 | training loss: 0.009570655413\n",
      "Epoch: 1515 | training loss: 0.009561387822\n",
      "Epoch: 1516 | training loss: 0.009552142583\n",
      "Epoch: 1517 | training loss: 0.009542862885\n",
      "Epoch: 1518 | training loss: 0.009533640929\n",
      "Epoch: 1519 | training loss: 0.009524408728\n",
      "Epoch: 1520 | training loss: 0.009515153244\n",
      "Epoch: 1521 | training loss: 0.009505914524\n",
      "Epoch: 1522 | training loss: 0.009496720508\n",
      "Epoch: 1523 | training loss: 0.009487528354\n",
      "Epoch: 1524 | training loss: 0.009478306398\n",
      "Epoch: 1525 | training loss: 0.009469145909\n",
      "Epoch: 1526 | training loss: 0.009459962137\n",
      "Epoch: 1527 | training loss: 0.009450747631\n",
      "Epoch: 1528 | training loss: 0.009441589005\n",
      "Epoch: 1529 | training loss: 0.009432444349\n",
      "Epoch: 1530 | training loss: 0.009423292242\n",
      "Epoch: 1531 | training loss: 0.009414166212\n",
      "Epoch: 1532 | training loss: 0.009405014105\n",
      "Epoch: 1533 | training loss: 0.009395866655\n",
      "Epoch: 1534 | training loss: 0.009386740625\n",
      "Epoch: 1535 | training loss: 0.009377630427\n",
      "Epoch: 1536 | training loss: 0.009368532337\n",
      "Epoch: 1537 | training loss: 0.009359441698\n",
      "Epoch: 1538 | training loss: 0.009350324050\n",
      "Epoch: 1539 | training loss: 0.009341240861\n",
      "Epoch: 1540 | training loss: 0.009332180023\n",
      "Epoch: 1541 | training loss: 0.009323096834\n",
      "Epoch: 1542 | training loss: 0.009314033203\n",
      "Epoch: 1543 | training loss: 0.009304987267\n",
      "Epoch: 1544 | training loss: 0.009295951575\n",
      "Epoch: 1545 | training loss: 0.009286896326\n",
      "Epoch: 1546 | training loss: 0.009277860634\n",
      "Epoch: 1547 | training loss: 0.009268832393\n",
      "Epoch: 1548 | training loss: 0.009259836748\n",
      "Epoch: 1549 | training loss: 0.009250812232\n",
      "Epoch: 1550 | training loss: 0.009241822176\n",
      "Epoch: 1551 | training loss: 0.009232820943\n",
      "Epoch: 1552 | training loss: 0.009223853238\n",
      "Epoch: 1553 | training loss: 0.009214856662\n",
      "Epoch: 1554 | training loss: 0.009205876850\n",
      "Epoch: 1555 | training loss: 0.009196913801\n",
      "Epoch: 1556 | training loss: 0.009187976830\n",
      "Epoch: 1557 | training loss: 0.009179020301\n",
      "Epoch: 1558 | training loss: 0.009170060046\n",
      "Epoch: 1559 | training loss: 0.009161144495\n",
      "Epoch: 1560 | training loss: 0.009152233601\n",
      "Epoch: 1561 | training loss: 0.009143304080\n",
      "Epoch: 1562 | training loss: 0.009134407155\n",
      "Epoch: 1563 | training loss: 0.009125474840\n",
      "Epoch: 1564 | training loss: 0.009116590954\n",
      "Epoch: 1565 | training loss: 0.009107700549\n",
      "Epoch: 1566 | training loss: 0.009098845534\n",
      "Epoch: 1567 | training loss: 0.009089948609\n",
      "Epoch: 1568 | training loss: 0.009081070311\n",
      "Epoch: 1569 | training loss: 0.009072230197\n",
      "Epoch: 1570 | training loss: 0.009063406847\n",
      "Epoch: 1571 | training loss: 0.009054546244\n",
      "Epoch: 1572 | training loss: 0.009045721032\n",
      "Epoch: 1573 | training loss: 0.009036887437\n",
      "Epoch: 1574 | training loss: 0.009028048255\n",
      "Epoch: 1575 | training loss: 0.009019234218\n",
      "Epoch: 1576 | training loss: 0.009010458365\n",
      "Epoch: 1577 | training loss: 0.009001650847\n",
      "Epoch: 1578 | training loss: 0.008992858231\n",
      "Epoch: 1579 | training loss: 0.008984081447\n",
      "Epoch: 1580 | training loss: 0.008975308388\n",
      "Epoch: 1581 | training loss: 0.008966523223\n",
      "Epoch: 1582 | training loss: 0.008957758546\n",
      "Epoch: 1583 | training loss: 0.008949024603\n",
      "Epoch: 1584 | training loss: 0.008940296248\n",
      "Epoch: 1585 | training loss: 0.008931534365\n",
      "Epoch: 1586 | training loss: 0.008922823705\n",
      "Epoch: 1587 | training loss: 0.008914100006\n",
      "Epoch: 1588 | training loss: 0.008905368857\n",
      "Epoch: 1589 | training loss: 0.008896679617\n",
      "Epoch: 1590 | training loss: 0.008887985721\n",
      "Epoch: 1591 | training loss: 0.008879298344\n",
      "Epoch: 1592 | training loss: 0.008870609105\n",
      "Epoch: 1593 | training loss: 0.008861929178\n",
      "Epoch: 1594 | training loss: 0.008853276260\n",
      "Epoch: 1595 | training loss: 0.008844613098\n",
      "Epoch: 1596 | training loss: 0.008835923858\n",
      "Epoch: 1597 | training loss: 0.008827310987\n",
      "Epoch: 1598 | training loss: 0.008818679489\n",
      "Epoch: 1599 | training loss: 0.008810025640\n",
      "Epoch: 1600 | training loss: 0.008801404387\n",
      "Epoch: 1601 | training loss: 0.008792813867\n",
      "Epoch: 1602 | training loss: 0.008784201927\n",
      "Epoch: 1603 | training loss: 0.008775601164\n",
      "Epoch: 1604 | training loss: 0.008767024614\n",
      "Epoch: 1605 | training loss: 0.008758448996\n",
      "Epoch: 1606 | training loss: 0.008749868721\n",
      "Epoch: 1607 | training loss: 0.008741268888\n",
      "Epoch: 1608 | training loss: 0.008732713759\n",
      "Epoch: 1609 | training loss: 0.008724144660\n",
      "Epoch: 1610 | training loss: 0.008715619333\n",
      "Epoch: 1611 | training loss: 0.008707079105\n",
      "Epoch: 1612 | training loss: 0.008698558435\n",
      "Epoch: 1613 | training loss: 0.008690021001\n",
      "Epoch: 1614 | training loss: 0.008681526408\n",
      "Epoch: 1615 | training loss: 0.008673014119\n",
      "Epoch: 1616 | training loss: 0.008664530702\n",
      "Epoch: 1617 | training loss: 0.008656013757\n",
      "Epoch: 1618 | training loss: 0.008647534996\n",
      "Epoch: 1619 | training loss: 0.008639060892\n",
      "Epoch: 1620 | training loss: 0.008630599827\n",
      "Epoch: 1621 | training loss: 0.008622133173\n",
      "Epoch: 1622 | training loss: 0.008613675833\n",
      "Epoch: 1623 | training loss: 0.008605220355\n",
      "Epoch: 1624 | training loss: 0.008596803062\n",
      "Epoch: 1625 | training loss: 0.008588355966\n",
      "Epoch: 1626 | training loss: 0.008579927497\n",
      "Epoch: 1627 | training loss: 0.008571521379\n",
      "Epoch: 1628 | training loss: 0.008563095704\n",
      "Epoch: 1629 | training loss: 0.008554703556\n",
      "Epoch: 1630 | training loss: 0.008546293713\n",
      "Epoch: 1631 | training loss: 0.008537915535\n",
      "Epoch: 1632 | training loss: 0.008529540151\n",
      "Epoch: 1633 | training loss: 0.008521156386\n",
      "Epoch: 1634 | training loss: 0.008512804285\n",
      "Epoch: 1635 | training loss: 0.008504441008\n",
      "Epoch: 1636 | training loss: 0.008496066555\n",
      "Epoch: 1637 | training loss: 0.008487760089\n",
      "Epoch: 1638 | training loss: 0.008479400538\n",
      "Epoch: 1639 | training loss: 0.008471079171\n",
      "Epoch: 1640 | training loss: 0.008462784812\n",
      "Epoch: 1641 | training loss: 0.008454466239\n",
      "Epoch: 1642 | training loss: 0.008446187712\n",
      "Epoch: 1643 | training loss: 0.008437876590\n",
      "Epoch: 1644 | training loss: 0.008429592475\n",
      "Epoch: 1645 | training loss: 0.008421316743\n",
      "Epoch: 1646 | training loss: 0.008413026109\n",
      "Epoch: 1647 | training loss: 0.008404784836\n",
      "Epoch: 1648 | training loss: 0.008396507241\n",
      "Epoch: 1649 | training loss: 0.008388248272\n",
      "Epoch: 1650 | training loss: 0.008380018175\n",
      "Epoch: 1651 | training loss: 0.008371781558\n",
      "Epoch: 1652 | training loss: 0.008363555185\n",
      "Epoch: 1653 | training loss: 0.008355317637\n",
      "Epoch: 1654 | training loss: 0.008347114548\n",
      "Epoch: 1655 | training loss: 0.008338921703\n",
      "Epoch: 1656 | training loss: 0.008330699988\n",
      "Epoch: 1657 | training loss: 0.008322499692\n",
      "Epoch: 1658 | training loss: 0.008314345032\n",
      "Epoch: 1659 | training loss: 0.008306154050\n",
      "Epoch: 1660 | training loss: 0.008297994733\n",
      "Epoch: 1661 | training loss: 0.008289856836\n",
      "Epoch: 1662 | training loss: 0.008281681687\n",
      "Epoch: 1663 | training loss: 0.008273541927\n",
      "Epoch: 1664 | training loss: 0.008265401237\n",
      "Epoch: 1665 | training loss: 0.008257263340\n",
      "Epoch: 1666 | training loss: 0.008249144070\n",
      "Epoch: 1667 | training loss: 0.008241005242\n",
      "Epoch: 1668 | training loss: 0.008232912049\n",
      "Epoch: 1669 | training loss: 0.008224812336\n",
      "Epoch: 1670 | training loss: 0.008216728456\n",
      "Epoch: 1671 | training loss: 0.008208636194\n",
      "Epoch: 1672 | training loss: 0.008200555108\n",
      "Epoch: 1673 | training loss: 0.008192487992\n",
      "Epoch: 1674 | training loss: 0.008184425533\n",
      "Epoch: 1675 | training loss: 0.008176363073\n",
      "Epoch: 1676 | training loss: 0.008168322966\n",
      "Epoch: 1677 | training loss: 0.008160287514\n",
      "Epoch: 1678 | training loss: 0.008152251132\n",
      "Epoch: 1679 | training loss: 0.008144231513\n",
      "Epoch: 1680 | training loss: 0.008136221208\n",
      "Epoch: 1681 | training loss: 0.008128216490\n",
      "Epoch: 1682 | training loss: 0.008120179176\n",
      "Epoch: 1683 | training loss: 0.008112194948\n",
      "Epoch: 1684 | training loss: 0.008104213513\n",
      "Epoch: 1685 | training loss: 0.008096230216\n",
      "Epoch: 1686 | training loss: 0.008088234812\n",
      "Epoch: 1687 | training loss: 0.008080302738\n",
      "Epoch: 1688 | training loss: 0.008072326891\n",
      "Epoch: 1689 | training loss: 0.008064394817\n",
      "Epoch: 1690 | training loss: 0.008056432940\n",
      "Epoch: 1691 | training loss: 0.008048490621\n",
      "Epoch: 1692 | training loss: 0.008040549234\n",
      "Epoch: 1693 | training loss: 0.008032672107\n",
      "Epoch: 1694 | training loss: 0.008024710231\n",
      "Epoch: 1695 | training loss: 0.008016818203\n",
      "Epoch: 1696 | training loss: 0.008008942008\n",
      "Epoch: 1697 | training loss: 0.008001046255\n",
      "Epoch: 1698 | training loss: 0.007993148640\n",
      "Epoch: 1699 | training loss: 0.007985256612\n",
      "Epoch: 1700 | training loss: 0.007977405563\n",
      "Epoch: 1701 | training loss: 0.007969559170\n",
      "Epoch: 1702 | training loss: 0.007961694151\n",
      "Epoch: 1703 | training loss: 0.007953837514\n",
      "Epoch: 1704 | training loss: 0.007945993915\n",
      "Epoch: 1705 | training loss: 0.007938157767\n",
      "Epoch: 1706 | training loss: 0.007930357940\n",
      "Epoch: 1707 | training loss: 0.007922532037\n",
      "Epoch: 1708 | training loss: 0.007914727554\n",
      "Epoch: 1709 | training loss: 0.007906911895\n",
      "Epoch: 1710 | training loss: 0.007899114862\n",
      "Epoch: 1711 | training loss: 0.007891305722\n",
      "Epoch: 1712 | training loss: 0.007883559912\n",
      "Epoch: 1713 | training loss: 0.007875771262\n",
      "Epoch: 1714 | training loss: 0.007868045941\n",
      "Epoch: 1715 | training loss: 0.007860247046\n",
      "Epoch: 1716 | training loss: 0.007852504030\n",
      "Epoch: 1717 | training loss: 0.007844758220\n",
      "Epoch: 1718 | training loss: 0.007837022655\n",
      "Epoch: 1719 | training loss: 0.007829308510\n",
      "Epoch: 1720 | training loss: 0.007821582258\n",
      "Epoch: 1721 | training loss: 0.007813880220\n",
      "Epoch: 1722 | training loss: 0.007806167006\n",
      "Epoch: 1723 | training loss: 0.007798453793\n",
      "Epoch: 1724 | training loss: 0.007790777832\n",
      "Epoch: 1725 | training loss: 0.007783123758\n",
      "Epoch: 1726 | training loss: 0.007775415666\n",
      "Epoch: 1727 | training loss: 0.007767756470\n",
      "Epoch: 1728 | training loss: 0.007760109380\n",
      "Epoch: 1729 | training loss: 0.007752459962\n",
      "Epoch: 1730 | training loss: 0.007744800299\n",
      "Epoch: 1731 | training loss: 0.007737190463\n",
      "Epoch: 1732 | training loss: 0.007729523350\n",
      "Epoch: 1733 | training loss: 0.007721920032\n",
      "Epoch: 1734 | training loss: 0.007714277133\n",
      "Epoch: 1735 | training loss: 0.007706697099\n",
      "Epoch: 1736 | training loss: 0.007699076086\n",
      "Epoch: 1737 | training loss: 0.007691472303\n",
      "Epoch: 1738 | training loss: 0.007683904842\n",
      "Epoch: 1739 | training loss: 0.007676323876\n",
      "Epoch: 1740 | training loss: 0.007668738719\n",
      "Epoch: 1741 | training loss: 0.007661179174\n",
      "Epoch: 1742 | training loss: 0.007653625216\n",
      "Epoch: 1743 | training loss: 0.007646058686\n",
      "Epoch: 1744 | training loss: 0.007638521958\n",
      "Epoch: 1745 | training loss: 0.007631008048\n",
      "Epoch: 1746 | training loss: 0.007623498794\n",
      "Epoch: 1747 | training loss: 0.007615952753\n",
      "Epoch: 1748 | training loss: 0.007608454674\n",
      "Epoch: 1749 | training loss: 0.007600966375\n",
      "Epoch: 1750 | training loss: 0.007593441755\n",
      "Epoch: 1751 | training loss: 0.007585958578\n",
      "Epoch: 1752 | training loss: 0.007578453049\n",
      "Epoch: 1753 | training loss: 0.007570990361\n",
      "Epoch: 1754 | training loss: 0.007563522086\n",
      "Epoch: 1755 | training loss: 0.007556077559\n",
      "Epoch: 1756 | training loss: 0.007548589259\n",
      "Epoch: 1757 | training loss: 0.007541179657\n",
      "Epoch: 1758 | training loss: 0.007533723488\n",
      "Epoch: 1759 | training loss: 0.007526310161\n",
      "Epoch: 1760 | training loss: 0.007518862840\n",
      "Epoch: 1761 | training loss: 0.007511465345\n",
      "Epoch: 1762 | training loss: 0.007504057139\n",
      "Epoch: 1763 | training loss: 0.007496641017\n",
      "Epoch: 1764 | training loss: 0.007489252836\n",
      "Epoch: 1765 | training loss: 0.007481875829\n",
      "Epoch: 1766 | training loss: 0.007474477869\n",
      "Epoch: 1767 | training loss: 0.007467120886\n",
      "Epoch: 1768 | training loss: 0.007459752262\n",
      "Epoch: 1769 | training loss: 0.007452399470\n",
      "Epoch: 1770 | training loss: 0.007445043884\n",
      "Epoch: 1771 | training loss: 0.007437710650\n",
      "Epoch: 1772 | training loss: 0.007430364378\n",
      "Epoch: 1773 | training loss: 0.007423040457\n",
      "Epoch: 1774 | training loss: 0.007415717002\n",
      "Epoch: 1775 | training loss: 0.007408394013\n",
      "Epoch: 1776 | training loss: 0.007401080336\n",
      "Epoch: 1777 | training loss: 0.007393795066\n",
      "Epoch: 1778 | training loss: 0.007386477198\n",
      "Epoch: 1779 | training loss: 0.007379201241\n",
      "Epoch: 1780 | training loss: 0.007371924818\n",
      "Epoch: 1781 | training loss: 0.007364645135\n",
      "Epoch: 1782 | training loss: 0.007357389666\n",
      "Epoch: 1783 | training loss: 0.007350146770\n",
      "Epoch: 1784 | training loss: 0.007342881523\n",
      "Epoch: 1785 | training loss: 0.007335657720\n",
      "Epoch: 1786 | training loss: 0.007328417618\n",
      "Epoch: 1787 | training loss: 0.007321220357\n",
      "Epoch: 1788 | training loss: 0.007313955575\n",
      "Epoch: 1789 | training loss: 0.007306757849\n",
      "Epoch: 1790 | training loss: 0.007299554069\n",
      "Epoch: 1791 | training loss: 0.007292380556\n",
      "Epoch: 1792 | training loss: 0.007285200059\n",
      "Epoch: 1793 | training loss: 0.007278010249\n",
      "Epoch: 1794 | training loss: 0.007270860486\n",
      "Epoch: 1795 | training loss: 0.007263735402\n",
      "Epoch: 1796 | training loss: 0.007256618235\n",
      "Epoch: 1797 | training loss: 0.007249529008\n",
      "Epoch: 1798 | training loss: 0.007242471445\n",
      "Epoch: 1799 | training loss: 0.007235449739\n",
      "Epoch: 1800 | training loss: 0.007228516974\n",
      "Epoch: 1801 | training loss: 0.007221659645\n",
      "Epoch: 1802 | training loss: 0.007215036079\n",
      "Epoch: 1803 | training loss: 0.007208646275\n",
      "Epoch: 1804 | training loss: 0.007202672772\n",
      "Epoch: 1805 | training loss: 0.007197428495\n",
      "Epoch: 1806 | training loss: 0.007193308789\n",
      "Epoch: 1807 | training loss: 0.007191026118\n",
      "Epoch: 1808 | training loss: 0.007191811688\n",
      "Epoch: 1809 | training loss: 0.007197561674\n",
      "Epoch: 1810 | training loss: 0.007211627439\n",
      "Epoch: 1811 | training loss: 0.007239858154\n",
      "Epoch: 1812 | training loss: 0.007292077877\n",
      "Epoch: 1813 | training loss: 0.007385311648\n",
      "Epoch: 1814 | training loss: 0.007549305446\n",
      "Epoch: 1815 | training loss: 0.007835114375\n",
      "Epoch: 1816 | training loss: 0.008332826197\n",
      "Epoch: 1817 | training loss: 0.009197289124\n",
      "Epoch: 1818 | training loss: 0.010698027909\n",
      "Epoch: 1819 | training loss: 0.013278188184\n",
      "Epoch: 1820 | training loss: 0.017662221566\n",
      "Epoch: 1821 | training loss: 0.024833632633\n",
      "Epoch: 1822 | training loss: 0.035941779613\n",
      "Epoch: 1823 | training loss: 0.051085002720\n",
      "Epoch: 1824 | training loss: 0.067702583969\n",
      "Epoch: 1825 | training loss: 0.077329210937\n",
      "Epoch: 1826 | training loss: 0.070086054504\n",
      "Epoch: 1827 | training loss: 0.044252507389\n",
      "Epoch: 1828 | training loss: 0.016632813960\n",
      "Epoch: 1829 | training loss: 0.007166269235\n",
      "Epoch: 1830 | training loss: 0.018475575373\n",
      "Epoch: 1831 | training loss: 0.034484006464\n",
      "Epoch: 1832 | training loss: 0.037069257349\n",
      "Epoch: 1833 | training loss: 0.023843560368\n",
      "Epoch: 1834 | training loss: 0.009521013126\n",
      "Epoch: 1835 | training loss: 0.008400368504\n",
      "Epoch: 1836 | training loss: 0.017867669463\n",
      "Epoch: 1837 | training loss: 0.024030115455\n",
      "Epoch: 1838 | training loss: 0.019257575274\n",
      "Epoch: 1839 | training loss: 0.009991543368\n",
      "Epoch: 1840 | training loss: 0.007171075325\n",
      "Epoch: 1841 | training loss: 0.012115355581\n",
      "Epoch: 1842 | training loss: 0.016621042043\n",
      "Epoch: 1843 | training loss: 0.014597620815\n",
      "Epoch: 1844 | training loss: 0.009079178795\n",
      "Epoch: 1845 | training loss: 0.006988360547\n",
      "Epoch: 1846 | training loss: 0.009758995846\n",
      "Epoch: 1847 | training loss: 0.012567428872\n",
      "Epoch: 1848 | training loss: 0.011497933418\n",
      "Epoch: 1849 | training loss: 0.008225237019\n",
      "Epoch: 1850 | training loss: 0.006927500013\n",
      "Epoch: 1851 | training loss: 0.008549699560\n",
      "Epoch: 1852 | training loss: 0.010233130306\n",
      "Epoch: 1853 | training loss: 0.009615231305\n",
      "Epoch: 1854 | training loss: 0.007672487292\n",
      "Epoch: 1855 | training loss: 0.006881925743\n",
      "Epoch: 1856 | training loss: 0.007827838883\n",
      "Epoch: 1857 | training loss: 0.008838474751\n",
      "Epoch: 1858 | training loss: 0.008497893810\n",
      "Epoch: 1859 | training loss: 0.007346603554\n",
      "Epoch: 1860 | training loss: 0.006841365248\n",
      "Epoch: 1861 | training loss: 0.007372586988\n",
      "Epoch: 1862 | training loss: 0.007990712300\n",
      "Epoch: 1863 | training loss: 0.007832800969\n",
      "Epoch: 1864 | training loss: 0.007155408617\n",
      "Epoch: 1865 | training loss: 0.006806266494\n",
      "Epoch: 1866 | training loss: 0.007077887189\n",
      "Epoch: 1867 | training loss: 0.007463742048\n",
      "Epoch: 1868 | training loss: 0.007423440460\n",
      "Epoch: 1869 | training loss: 0.007034265436\n",
      "Epoch: 1870 | training loss: 0.006779206451\n",
      "Epoch: 1871 | training loss: 0.006892182864\n",
      "Epoch: 1872 | training loss: 0.007131345104\n",
      "Epoch: 1873 | training loss: 0.007156879641\n",
      "Epoch: 1874 | training loss: 0.006946720183\n",
      "Epoch: 1875 | training loss: 0.006759498734\n",
      "Epoch: 1876 | training loss: 0.006779308431\n",
      "Epoch: 1877 | training loss: 0.006918865256\n",
      "Epoch: 1878 | training loss: 0.006972447969\n",
      "Epoch: 1879 | training loss: 0.006873320788\n",
      "Epoch: 1880 | training loss: 0.006742273457\n",
      "Epoch: 1881 | training loss: 0.006713873241\n",
      "Epoch: 1882 | training loss: 0.006782777142\n",
      "Epoch: 1883 | training loss: 0.006837259978\n",
      "Epoch: 1884 | training loss: 0.006804330274\n",
      "Epoch: 1885 | training loss: 0.006722181570\n",
      "Epoch: 1886 | training loss: 0.006676206365\n",
      "Epoch: 1887 | training loss: 0.006696656346\n",
      "Epoch: 1888 | training loss: 0.006736601703\n",
      "Epoch: 1889 | training loss: 0.006737804040\n",
      "Epoch: 1890 | training loss: 0.006695258431\n",
      "Epoch: 1891 | training loss: 0.006651775911\n",
      "Epoch: 1892 | training loss: 0.006643187255\n",
      "Epoch: 1893 | training loss: 0.006662218366\n",
      "Epoch: 1894 | training loss: 0.006674392149\n",
      "Epoch: 1895 | training loss: 0.006659435108\n",
      "Epoch: 1896 | training loss: 0.006629251875\n",
      "Epoch: 1897 | training loss: 0.006609220989\n",
      "Epoch: 1898 | training loss: 0.006609641016\n",
      "Epoch: 1899 | training loss: 0.006618327461\n",
      "Epoch: 1900 | training loss: 0.006617407314\n",
      "Epoch: 1901 | training loss: 0.006602213718\n",
      "Epoch: 1902 | training loss: 0.006583257113\n",
      "Epoch: 1903 | training loss: 0.006572739687\n",
      "Epoch: 1904 | training loss: 0.006572455633\n",
      "Epoch: 1905 | training loss: 0.006573994644\n",
      "Epoch: 1906 | training loss: 0.006568960845\n",
      "Epoch: 1907 | training loss: 0.006556937471\n",
      "Epoch: 1908 | training loss: 0.006544266827\n",
      "Epoch: 1909 | training loss: 0.006536760367\n",
      "Epoch: 1910 | training loss: 0.006534236018\n",
      "Epoch: 1911 | training loss: 0.006532029249\n",
      "Epoch: 1912 | training loss: 0.006526166108\n",
      "Epoch: 1913 | training loss: 0.006516920868\n",
      "Epoch: 1914 | training loss: 0.006507498212\n",
      "Epoch: 1915 | training loss: 0.006500733085\n",
      "Epoch: 1916 | training loss: 0.006496594753\n",
      "Epoch: 1917 | training loss: 0.006492725573\n",
      "Epoch: 1918 | training loss: 0.006487013772\n",
      "Epoch: 1919 | training loss: 0.006479460280\n",
      "Epoch: 1920 | training loss: 0.006471564993\n",
      "Epoch: 1921 | training loss: 0.006464924663\n",
      "Epoch: 1922 | training loss: 0.006459752098\n",
      "Epoch: 1923 | training loss: 0.006455013528\n",
      "Epoch: 1924 | training loss: 0.006449521519\n",
      "Epoch: 1925 | training loss: 0.006442984100\n",
      "Epoch: 1926 | training loss: 0.006435996853\n",
      "Epoch: 1927 | training loss: 0.006429448258\n",
      "Epoch: 1928 | training loss: 0.006423738319\n",
      "Epoch: 1929 | training loss: 0.006418454461\n",
      "Epoch: 1930 | training loss: 0.006413004827\n",
      "Epoch: 1931 | training loss: 0.006407021079\n",
      "Epoch: 1932 | training loss: 0.006400647108\n",
      "Epoch: 1933 | training loss: 0.006394273601\n",
      "Epoch: 1934 | training loss: 0.006388292182\n",
      "Epoch: 1935 | training loss: 0.006382638589\n",
      "Epoch: 1936 | training loss: 0.006377065089\n",
      "Epoch: 1937 | training loss: 0.006371352822\n",
      "Epoch: 1938 | training loss: 0.006365403533\n",
      "Epoch: 1939 | training loss: 0.006359321065\n",
      "Epoch: 1940 | training loss: 0.006353276316\n",
      "Epoch: 1941 | training loss: 0.006347401999\n",
      "Epoch: 1942 | training loss: 0.006341705099\n",
      "Epoch: 1943 | training loss: 0.006336042657\n",
      "Epoch: 1944 | training loss: 0.006330306176\n",
      "Epoch: 1945 | training loss: 0.006324484013\n",
      "Epoch: 1946 | training loss: 0.006318552885\n",
      "Epoch: 1947 | training loss: 0.006312640384\n",
      "Epoch: 1948 | training loss: 0.006306824740\n",
      "Epoch: 1949 | training loss: 0.006301098038\n",
      "Epoch: 1950 | training loss: 0.006295425817\n",
      "Epoch: 1951 | training loss: 0.006289691664\n",
      "Epoch: 1952 | training loss: 0.006283903494\n",
      "Epoch: 1953 | training loss: 0.006278089713\n",
      "Epoch: 1954 | training loss: 0.006272296421\n",
      "Epoch: 1955 | training loss: 0.006266537588\n",
      "Epoch: 1956 | training loss: 0.006260813214\n",
      "Epoch: 1957 | training loss: 0.006255104207\n",
      "Epoch: 1958 | training loss: 0.006249390543\n",
      "Epoch: 1959 | training loss: 0.006243697368\n",
      "Epoch: 1960 | training loss: 0.006237967871\n",
      "Epoch: 1961 | training loss: 0.006232210901\n",
      "Epoch: 1962 | training loss: 0.006226516329\n",
      "Epoch: 1963 | training loss: 0.006220814306\n",
      "Epoch: 1964 | training loss: 0.006215156056\n",
      "Epoch: 1965 | training loss: 0.006209447049\n",
      "Epoch: 1966 | training loss: 0.006203815807\n",
      "Epoch: 1967 | training loss: 0.006198125891\n",
      "Epoch: 1968 | training loss: 0.006192442030\n",
      "Epoch: 1969 | training loss: 0.006186794490\n",
      "Epoch: 1970 | training loss: 0.006181118079\n",
      "Epoch: 1971 | training loss: 0.006175456569\n",
      "Epoch: 1972 | training loss: 0.006169835106\n",
      "Epoch: 1973 | training loss: 0.006164214108\n",
      "Epoch: 1974 | training loss: 0.006158576347\n",
      "Epoch: 1975 | training loss: 0.006152965128\n",
      "Epoch: 1976 | training loss: 0.006147339940\n",
      "Epoch: 1977 | training loss: 0.006141716149\n",
      "Epoch: 1978 | training loss: 0.006136110052\n",
      "Epoch: 1979 | training loss: 0.006130485330\n",
      "Epoch: 1980 | training loss: 0.006124895066\n",
      "Epoch: 1981 | training loss: 0.006119302940\n",
      "Epoch: 1982 | training loss: 0.006113719195\n",
      "Epoch: 1983 | training loss: 0.006108140573\n",
      "Epoch: 1984 | training loss: 0.006102597807\n",
      "Epoch: 1985 | training loss: 0.006097028032\n",
      "Epoch: 1986 | training loss: 0.006091451738\n",
      "Epoch: 1987 | training loss: 0.006085894536\n",
      "Epoch: 1988 | training loss: 0.006080340594\n",
      "Epoch: 1989 | training loss: 0.006074794568\n",
      "Epoch: 1990 | training loss: 0.006069259718\n",
      "Epoch: 1991 | training loss: 0.006063709967\n",
      "Epoch: 1992 | training loss: 0.006058208179\n",
      "Epoch: 1993 | training loss: 0.006052693352\n",
      "Epoch: 1994 | training loss: 0.006047167815\n",
      "Epoch: 1995 | training loss: 0.006041647866\n",
      "Epoch: 1996 | training loss: 0.006036129780\n",
      "Epoch: 1997 | training loss: 0.006030660588\n",
      "Epoch: 1998 | training loss: 0.006025148090\n",
      "Epoch: 1999 | training loss: 0.006019655615\n",
      "Epoch: 2000 | training loss: 0.006014183164\n",
      "Epoch: 2001 | training loss: 0.006008685101\n",
      "Epoch: 2002 | training loss: 0.006003222428\n",
      "Epoch: 2003 | training loss: 0.005997763947\n",
      "Epoch: 2004 | training loss: 0.005992302671\n",
      "Epoch: 2005 | training loss: 0.005986869801\n",
      "Epoch: 2006 | training loss: 0.005981407128\n",
      "Epoch: 2007 | training loss: 0.005975949112\n",
      "Epoch: 2008 | training loss: 0.005970515311\n",
      "Epoch: 2009 | training loss: 0.005965089891\n",
      "Epoch: 2010 | training loss: 0.005959655158\n",
      "Epoch: 2011 | training loss: 0.005954251159\n",
      "Epoch: 2012 | training loss: 0.005948825739\n",
      "Epoch: 2013 | training loss: 0.005943437107\n",
      "Epoch: 2014 | training loss: 0.005938015878\n",
      "Epoch: 2015 | training loss: 0.005932614207\n",
      "Epoch: 2016 | training loss: 0.005927241407\n",
      "Epoch: 2017 | training loss: 0.005921852309\n",
      "Epoch: 2018 | training loss: 0.005916471593\n",
      "Epoch: 2019 | training loss: 0.005911094602\n",
      "Epoch: 2020 | training loss: 0.005905718077\n",
      "Epoch: 2021 | training loss: 0.005900348537\n",
      "Epoch: 2022 | training loss: 0.005894973408\n",
      "Epoch: 2023 | training loss: 0.005889649037\n",
      "Epoch: 2024 | training loss: 0.005884299055\n",
      "Epoch: 2025 | training loss: 0.005878944416\n",
      "Epoch: 2026 | training loss: 0.005873609334\n",
      "Epoch: 2027 | training loss: 0.005868284963\n",
      "Epoch: 2028 | training loss: 0.005862930790\n",
      "Epoch: 2029 | training loss: 0.005857623648\n",
      "Epoch: 2030 | training loss: 0.005852307659\n",
      "Epoch: 2031 | training loss: 0.005846998654\n",
      "Epoch: 2032 | training loss: 0.005841689650\n",
      "Epoch: 2033 | training loss: 0.005836407654\n",
      "Epoch: 2034 | training loss: 0.005831116810\n",
      "Epoch: 2035 | training loss: 0.005825840868\n",
      "Epoch: 2036 | training loss: 0.005820541643\n",
      "Epoch: 2037 | training loss: 0.005815268029\n",
      "Epoch: 2038 | training loss: 0.005809991620\n",
      "Epoch: 2039 | training loss: 0.005804731045\n",
      "Epoch: 2040 | training loss: 0.005799470469\n",
      "Epoch: 2041 | training loss: 0.005794216879\n",
      "Epoch: 2042 | training loss: 0.005788971204\n",
      "Epoch: 2043 | training loss: 0.005783718079\n",
      "Epoch: 2044 | training loss: 0.005778498482\n",
      "Epoch: 2045 | training loss: 0.005773249082\n",
      "Epoch: 2046 | training loss: 0.005768051371\n",
      "Epoch: 2047 | training loss: 0.005762794986\n",
      "Epoch: 2048 | training loss: 0.005757624749\n",
      "Epoch: 2049 | training loss: 0.005752398167\n",
      "Epoch: 2050 | training loss: 0.005747178104\n",
      "Epoch: 2051 | training loss: 0.005742002279\n",
      "Epoch: 2052 | training loss: 0.005736812018\n",
      "Epoch: 2053 | training loss: 0.005731611047\n",
      "Epoch: 2054 | training loss: 0.005726458505\n",
      "Epoch: 2055 | training loss: 0.005721258931\n",
      "Epoch: 2056 | training loss: 0.005716124084\n",
      "Epoch: 2057 | training loss: 0.005710970610\n",
      "Epoch: 2058 | training loss: 0.005705818068\n",
      "Epoch: 2059 | training loss: 0.005700682756\n",
      "Epoch: 2060 | training loss: 0.005695587955\n",
      "Epoch: 2061 | training loss: 0.005690488033\n",
      "Epoch: 2062 | training loss: 0.005685416050\n",
      "Epoch: 2063 | training loss: 0.005680437665\n",
      "Epoch: 2064 | training loss: 0.005675524473\n",
      "Epoch: 2065 | training loss: 0.005670779385\n",
      "Epoch: 2066 | training loss: 0.005666298792\n",
      "Epoch: 2067 | training loss: 0.005662205629\n",
      "Epoch: 2068 | training loss: 0.005658807233\n",
      "Epoch: 2069 | training loss: 0.005656516179\n",
      "Epoch: 2070 | training loss: 0.005656205118\n",
      "Epoch: 2071 | training loss: 0.005659266375\n",
      "Epoch: 2072 | training loss: 0.005668153986\n",
      "Epoch: 2073 | training loss: 0.005687245633\n",
      "Epoch: 2074 | training loss: 0.005724514835\n",
      "Epoch: 2075 | training loss: 0.005794025958\n",
      "Epoch: 2076 | training loss: 0.005921239499\n",
      "Epoch: 2077 | training loss: 0.006151685957\n",
      "Epoch: 2078 | training loss: 0.006568354554\n",
      "Epoch: 2079 | training loss: 0.007320187986\n",
      "Epoch: 2080 | training loss: 0.008679169230\n",
      "Epoch: 2081 | training loss: 0.011121107265\n",
      "Epoch: 2082 | training loss: 0.015483630821\n",
      "Epoch: 2083 | training loss: 0.023072499782\n",
      "Epoch: 2084 | training loss: 0.035786800086\n",
      "Epoch: 2085 | training loss: 0.055105213076\n",
      "Epoch: 2086 | training loss: 0.080118194222\n",
      "Epoch: 2087 | training loss: 0.101440757513\n",
      "Epoch: 2088 | training loss: 0.102514833212\n",
      "Epoch: 2089 | training loss: 0.071212872863\n",
      "Epoch: 2090 | training loss: 0.026855576783\n",
      "Epoch: 2091 | training loss: 0.005604633596\n",
      "Epoch: 2092 | training loss: 0.020001489669\n",
      "Epoch: 2093 | training loss: 0.045385785401\n",
      "Epoch: 2094 | training loss: 0.049061700702\n",
      "Epoch: 2095 | training loss: 0.027084387839\n",
      "Epoch: 2096 | training loss: 0.007017882541\n",
      "Epoch: 2097 | training loss: 0.010881792754\n",
      "Epoch: 2098 | training loss: 0.026975939050\n",
      "Epoch: 2099 | training loss: 0.030054301023\n",
      "Epoch: 2100 | training loss: 0.016435910016\n",
      "Epoch: 2101 | training loss: 0.005744239315\n",
      "Epoch: 2102 | training loss: 0.010549137369\n",
      "Epoch: 2103 | training loss: 0.019860301167\n",
      "Epoch: 2104 | training loss: 0.018485281616\n",
      "Epoch: 2105 | training loss: 0.009187228978\n",
      "Epoch: 2106 | training loss: 0.005649088882\n",
      "Epoch: 2107 | training loss: 0.010926811025\n",
      "Epoch: 2108 | training loss: 0.014903249219\n",
      "Epoch: 2109 | training loss: 0.011265728623\n",
      "Epoch: 2110 | training loss: 0.006052492652\n",
      "Epoch: 2111 | training loss: 0.006547756027\n",
      "Epoch: 2112 | training loss: 0.010378471576\n",
      "Epoch: 2113 | training loss: 0.010792993940\n",
      "Epoch: 2114 | training loss: 0.007355079986\n",
      "Epoch: 2115 | training loss: 0.005443295464\n",
      "Epoch: 2116 | training loss: 0.007209669799\n",
      "Epoch: 2117 | training loss: 0.009020155296\n",
      "Epoch: 2118 | training loss: 0.007901081815\n",
      "Epoch: 2119 | training loss: 0.005774987396\n",
      "Epoch: 2120 | training loss: 0.005716852844\n",
      "Epoch: 2121 | training loss: 0.007198486477\n",
      "Epoch: 2122 | training loss: 0.007553509437\n",
      "Epoch: 2123 | training loss: 0.006295552012\n",
      "Epoch: 2124 | training loss: 0.005404199939\n",
      "Epoch: 2125 | training loss: 0.005964071490\n",
      "Epoch: 2126 | training loss: 0.006752816029\n",
      "Epoch: 2127 | training loss: 0.006460446864\n",
      "Epoch: 2128 | training loss: 0.005606880412\n",
      "Epoch: 2129 | training loss: 0.005432151258\n",
      "Epoch: 2130 | training loss: 0.005969754420\n",
      "Epoch: 2131 | training loss: 0.006232281215\n",
      "Epoch: 2132 | training loss: 0.005822286941\n",
      "Epoch: 2133 | training loss: 0.005386365578\n",
      "Epoch: 2134 | training loss: 0.005489367060\n",
      "Epoch: 2135 | training loss: 0.005829010159\n",
      "Epoch: 2136 | training loss: 0.005831363611\n",
      "Epoch: 2137 | training loss: 0.005511353724\n",
      "Epoch: 2138 | training loss: 0.005337389186\n",
      "Epoch: 2139 | training loss: 0.005486648064\n",
      "Epoch: 2140 | training loss: 0.005655311048\n",
      "Epoch: 2141 | training loss: 0.005574217066\n",
      "Epoch: 2142 | training loss: 0.005373563152\n",
      "Epoch: 2143 | training loss: 0.005325432401\n",
      "Epoch: 2144 | training loss: 0.005442356225\n",
      "Epoch: 2145 | training loss: 0.005510606337\n",
      "Epoch: 2146 | training loss: 0.005425694864\n",
      "Epoch: 2147 | training loss: 0.005312281661\n",
      "Epoch: 2148 | training loss: 0.005310653709\n",
      "Epoch: 2149 | training loss: 0.005385116674\n",
      "Epoch: 2150 | training loss: 0.005406060256\n",
      "Epoch: 2151 | training loss: 0.005341255106\n",
      "Epoch: 2152 | training loss: 0.005279110279\n",
      "Epoch: 2153 | training loss: 0.005288369022\n",
      "Epoch: 2154 | training loss: 0.005331312772\n",
      "Epoch: 2155 | training loss: 0.005333855748\n",
      "Epoch: 2156 | training loss: 0.005289534107\n",
      "Epoch: 2157 | training loss: 0.005254488904\n",
      "Epoch: 2158 | training loss: 0.005262400024\n",
      "Epoch: 2159 | training loss: 0.005285677034\n",
      "Epoch: 2160 | training loss: 0.005282769445\n",
      "Epoch: 2161 | training loss: 0.005253768992\n",
      "Epoch: 2162 | training loss: 0.005232227966\n",
      "Epoch: 2163 | training loss: 0.005235997029\n",
      "Epoch: 2164 | training loss: 0.005247996189\n",
      "Epoch: 2165 | training loss: 0.005244290922\n",
      "Epoch: 2166 | training loss: 0.005225325935\n",
      "Epoch: 2167 | training loss: 0.005210558884\n",
      "Epoch: 2168 | training loss: 0.005210632458\n",
      "Epoch: 2169 | training loss: 0.005216068123\n",
      "Epoch: 2170 | training loss: 0.005212837830\n",
      "Epoch: 2171 | training loss: 0.005200302228\n",
      "Epoch: 2172 | training loss: 0.005189208779\n",
      "Epoch: 2173 | training loss: 0.005186604802\n",
      "Epoch: 2174 | training loss: 0.005188126117\n",
      "Epoch: 2175 | training loss: 0.005185459740\n",
      "Epoch: 2176 | training loss: 0.005176880863\n",
      "Epoch: 2177 | training loss: 0.005168047268\n",
      "Epoch: 2178 | training loss: 0.005163869355\n",
      "Epoch: 2179 | training loss: 0.005162926391\n",
      "Epoch: 2180 | training loss: 0.005160348490\n",
      "Epoch: 2181 | training loss: 0.005154171959\n",
      "Epoch: 2182 | training loss: 0.005146997049\n",
      "Epoch: 2183 | training loss: 0.005142052192\n",
      "Epoch: 2184 | training loss: 0.005139454268\n",
      "Epoch: 2185 | training loss: 0.005136651453\n",
      "Epoch: 2186 | training loss: 0.005131922662\n",
      "Epoch: 2187 | training loss: 0.005125990137\n",
      "Epoch: 2188 | training loss: 0.005120823160\n",
      "Epoch: 2189 | training loss: 0.005117152818\n",
      "Epoch: 2190 | training loss: 0.005113996565\n",
      "Epoch: 2191 | training loss: 0.005110002123\n",
      "Epoch: 2192 | training loss: 0.005104979500\n",
      "Epoch: 2193 | training loss: 0.005099962465\n",
      "Epoch: 2194 | training loss: 0.005095696077\n",
      "Epoch: 2195 | training loss: 0.005092044827\n",
      "Epoch: 2196 | training loss: 0.005088284146\n",
      "Epoch: 2197 | training loss: 0.005083913915\n",
      "Epoch: 2198 | training loss: 0.005079211202\n",
      "Epoch: 2199 | training loss: 0.005074719433\n",
      "Epoch: 2200 | training loss: 0.005070708226\n",
      "Epoch: 2201 | training loss: 0.005066857208\n",
      "Epoch: 2202 | training loss: 0.005062805489\n",
      "Epoch: 2203 | training loss: 0.005058442242\n",
      "Epoch: 2204 | training loss: 0.005054037087\n",
      "Epoch: 2205 | training loss: 0.005049812607\n",
      "Epoch: 2206 | training loss: 0.005045786966\n",
      "Epoch: 2207 | training loss: 0.005041804630\n",
      "Epoch: 2208 | training loss: 0.005037667230\n",
      "Epoch: 2209 | training loss: 0.005033416674\n",
      "Epoch: 2210 | training loss: 0.005029177293\n",
      "Epoch: 2211 | training loss: 0.005025041755\n",
      "Epoch: 2212 | training loss: 0.005021019839\n",
      "Epoch: 2213 | training loss: 0.005016986281\n",
      "Epoch: 2214 | training loss: 0.005012852140\n",
      "Epoch: 2215 | training loss: 0.005008673295\n",
      "Epoch: 2216 | training loss: 0.005004524719\n",
      "Epoch: 2217 | training loss: 0.005000412464\n",
      "Epoch: 2218 | training loss: 0.004996400792\n",
      "Epoch: 2219 | training loss: 0.004992330913\n",
      "Epoch: 2220 | training loss: 0.004988258705\n",
      "Epoch: 2221 | training loss: 0.004984148778\n",
      "Epoch: 2222 | training loss: 0.004980039783\n",
      "Epoch: 2223 | training loss: 0.004975967109\n",
      "Epoch: 2224 | training loss: 0.004971918184\n",
      "Epoch: 2225 | training loss: 0.004967877641\n",
      "Epoch: 2226 | training loss: 0.004963835701\n",
      "Epoch: 2227 | training loss: 0.004959769081\n",
      "Epoch: 2228 | training loss: 0.004955705255\n",
      "Epoch: 2229 | training loss: 0.004951654002\n",
      "Epoch: 2230 | training loss: 0.004947618581\n",
      "Epoch: 2231 | training loss: 0.004943589680\n",
      "Epoch: 2232 | training loss: 0.004939572886\n",
      "Epoch: 2233 | training loss: 0.004935555626\n",
      "Epoch: 2234 | training loss: 0.004931539297\n",
      "Epoch: 2235 | training loss: 0.004927487578\n",
      "Epoch: 2236 | training loss: 0.004923481029\n",
      "Epoch: 2237 | training loss: 0.004919474013\n",
      "Epoch: 2238 | training loss: 0.004915474914\n",
      "Epoch: 2239 | training loss: 0.004911491647\n",
      "Epoch: 2240 | training loss: 0.004907481372\n",
      "Epoch: 2241 | training loss: 0.004903480876\n",
      "Epoch: 2242 | training loss: 0.004899498075\n",
      "Epoch: 2243 | training loss: 0.004895501304\n",
      "Epoch: 2244 | training loss: 0.004891516641\n",
      "Epoch: 2245 | training loss: 0.004887579940\n",
      "Epoch: 2246 | training loss: 0.004883583635\n",
      "Epoch: 2247 | training loss: 0.004879631102\n",
      "Epoch: 2248 | training loss: 0.004875660874\n",
      "Epoch: 2249 | training loss: 0.004871702753\n",
      "Epoch: 2250 | training loss: 0.004867743701\n",
      "Epoch: 2251 | training loss: 0.004863818642\n",
      "Epoch: 2252 | training loss: 0.004859843291\n",
      "Epoch: 2253 | training loss: 0.004855925683\n",
      "Epoch: 2254 | training loss: 0.004851976410\n",
      "Epoch: 2255 | training loss: 0.004848044831\n",
      "Epoch: 2256 | training loss: 0.004844113719\n",
      "Epoch: 2257 | training loss: 0.004840193316\n",
      "Epoch: 2258 | training loss: 0.004836261738\n",
      "Epoch: 2259 | training loss: 0.004832349718\n",
      "Epoch: 2260 | training loss: 0.004828435369\n",
      "Epoch: 2261 | training loss: 0.004824543372\n",
      "Epoch: 2262 | training loss: 0.004820614122\n",
      "Epoch: 2263 | training loss: 0.004816721193\n",
      "Epoch: 2264 | training loss: 0.004812842701\n",
      "Epoch: 2265 | training loss: 0.004808955360\n",
      "Epoch: 2266 | training loss: 0.004805079661\n",
      "Epoch: 2267 | training loss: 0.004801178351\n",
      "Epoch: 2268 | training loss: 0.004797279835\n",
      "Epoch: 2269 | training loss: 0.004793420434\n",
      "Epoch: 2270 | training loss: 0.004789568949\n",
      "Epoch: 2271 | training loss: 0.004785672761\n",
      "Epoch: 2272 | training loss: 0.004781816620\n",
      "Epoch: 2273 | training loss: 0.004777951166\n",
      "Epoch: 2274 | training loss: 0.004774106666\n",
      "Epoch: 2275 | training loss: 0.004770250525\n",
      "Epoch: 2276 | training loss: 0.004766415805\n",
      "Epoch: 2277 | training loss: 0.004762569442\n",
      "Epoch: 2278 | training loss: 0.004758724943\n",
      "Epoch: 2279 | training loss: 0.004754877649\n",
      "Epoch: 2280 | training loss: 0.004751058761\n",
      "Epoch: 2281 | training loss: 0.004747231491\n",
      "Epoch: 2282 | training loss: 0.004743414931\n",
      "Epoch: 2283 | training loss: 0.004739589058\n",
      "Epoch: 2284 | training loss: 0.004735786468\n",
      "Epoch: 2285 | training loss: 0.004731975961\n",
      "Epoch: 2286 | training loss: 0.004728169646\n",
      "Epoch: 2287 | training loss: 0.004724370781\n",
      "Epoch: 2288 | training loss: 0.004720566794\n",
      "Epoch: 2289 | training loss: 0.004716763739\n",
      "Epoch: 2290 | training loss: 0.004712992348\n",
      "Epoch: 2291 | training loss: 0.004709191620\n",
      "Epoch: 2292 | training loss: 0.004705413710\n",
      "Epoch: 2293 | training loss: 0.004701649770\n",
      "Epoch: 2294 | training loss: 0.004697844852\n",
      "Epoch: 2295 | training loss: 0.004694084637\n",
      "Epoch: 2296 | training loss: 0.004690337460\n",
      "Epoch: 2297 | training loss: 0.004686547443\n",
      "Epoch: 2298 | training loss: 0.004682802130\n",
      "Epoch: 2299 | training loss: 0.004679026082\n",
      "Epoch: 2300 | training loss: 0.004675299861\n",
      "Epoch: 2301 | training loss: 0.004671549890\n",
      "Epoch: 2302 | training loss: 0.004667809233\n",
      "Epoch: 2303 | training loss: 0.004664086737\n",
      "Epoch: 2304 | training loss: 0.004660345614\n",
      "Epoch: 2305 | training loss: 0.004656607285\n",
      "Epoch: 2306 | training loss: 0.004652892239\n",
      "Epoch: 2307 | training loss: 0.004649179522\n",
      "Epoch: 2308 | training loss: 0.004645445384\n",
      "Epoch: 2309 | training loss: 0.004641725682\n",
      "Epoch: 2310 | training loss: 0.004638028797\n",
      "Epoch: 2311 | training loss: 0.004634321202\n",
      "Epoch: 2312 | training loss: 0.004630595911\n",
      "Epoch: 2313 | training loss: 0.004626913927\n",
      "Epoch: 2314 | training loss: 0.004623225890\n",
      "Epoch: 2315 | training loss: 0.004619518761\n",
      "Epoch: 2316 | training loss: 0.004615845624\n",
      "Epoch: 2317 | training loss: 0.004612170625\n",
      "Epoch: 2318 | training loss: 0.004608470947\n",
      "Epoch: 2319 | training loss: 0.004604807124\n",
      "Epoch: 2320 | training loss: 0.004601136316\n",
      "Epoch: 2321 | training loss: 0.004597472027\n",
      "Epoch: 2322 | training loss: 0.004593788181\n",
      "Epoch: 2323 | training loss: 0.004590159282\n",
      "Epoch: 2324 | training loss: 0.004586500581\n",
      "Epoch: 2325 | training loss: 0.004582846537\n",
      "Epoch: 2326 | training loss: 0.004579207860\n",
      "Epoch: 2327 | training loss: 0.004575562198\n",
      "Epoch: 2328 | training loss: 0.004571935628\n",
      "Epoch: 2329 | training loss: 0.004568285309\n",
      "Epoch: 2330 | training loss: 0.004564654548\n",
      "Epoch: 2331 | training loss: 0.004561046138\n",
      "Epoch: 2332 | training loss: 0.004557417240\n",
      "Epoch: 2333 | training loss: 0.004553791601\n",
      "Epoch: 2334 | training loss: 0.004550173879\n",
      "Epoch: 2335 | training loss: 0.004546570126\n",
      "Epoch: 2336 | training loss: 0.004542971030\n",
      "Epoch: 2337 | training loss: 0.004539384041\n",
      "Epoch: 2338 | training loss: 0.004535759799\n",
      "Epoch: 2339 | training loss: 0.004532162566\n",
      "Epoch: 2340 | training loss: 0.004528574646\n",
      "Epoch: 2341 | training loss: 0.004524999298\n",
      "Epoch: 2342 | training loss: 0.004521406721\n",
      "Epoch: 2343 | training loss: 0.004517832305\n",
      "Epoch: 2344 | training loss: 0.004514260683\n",
      "Epoch: 2345 | training loss: 0.004510683939\n",
      "Epoch: 2346 | training loss: 0.004507122096\n",
      "Epoch: 2347 | training loss: 0.004503550474\n",
      "Epoch: 2348 | training loss: 0.004500002600\n",
      "Epoch: 2349 | training loss: 0.004496450536\n",
      "Epoch: 2350 | training loss: 0.004492884036\n",
      "Epoch: 2351 | training loss: 0.004489372019\n",
      "Epoch: 2352 | training loss: 0.004485786892\n",
      "Epoch: 2353 | training loss: 0.004482277669\n",
      "Epoch: 2354 | training loss: 0.004478734918\n",
      "Epoch: 2355 | training loss: 0.004475220107\n",
      "Epoch: 2356 | training loss: 0.004471706692\n",
      "Epoch: 2357 | training loss: 0.004468198866\n",
      "Epoch: 2358 | training loss: 0.004464698024\n",
      "Epoch: 2359 | training loss: 0.004461202305\n",
      "Epoch: 2360 | training loss: 0.004457719158\n",
      "Epoch: 2361 | training loss: 0.004454265814\n",
      "Epoch: 2362 | training loss: 0.004450849257\n",
      "Epoch: 2363 | training loss: 0.004447451793\n",
      "Epoch: 2364 | training loss: 0.004444134422\n",
      "Epoch: 2365 | training loss: 0.004440875724\n",
      "Epoch: 2366 | training loss: 0.004437714815\n",
      "Epoch: 2367 | training loss: 0.004434797913\n",
      "Epoch: 2368 | training loss: 0.004432114772\n",
      "Epoch: 2369 | training loss: 0.004429889377\n",
      "Epoch: 2370 | training loss: 0.004428360611\n",
      "Epoch: 2371 | training loss: 0.004427899141\n",
      "Epoch: 2372 | training loss: 0.004429128487\n",
      "Epoch: 2373 | training loss: 0.004432984628\n",
      "Epoch: 2374 | training loss: 0.004441034049\n",
      "Epoch: 2375 | training loss: 0.004455888644\n",
      "Epoch: 2376 | training loss: 0.004481875803\n",
      "Epoch: 2377 | training loss: 0.004526134580\n",
      "Epoch: 2378 | training loss: 0.004600556567\n",
      "Epoch: 2379 | training loss: 0.004725095350\n",
      "Epoch: 2380 | training loss: 0.004933942109\n",
      "Epoch: 2381 | training loss: 0.005283321720\n",
      "Epoch: 2382 | training loss: 0.005870027468\n",
      "Epoch: 2383 | training loss: 0.006854454055\n",
      "Epoch: 2384 | training loss: 0.008510399610\n",
      "Epoch: 2385 | training loss: 0.011271459982\n",
      "Epoch: 2386 | training loss: 0.015842266381\n",
      "Epoch: 2387 | training loss: 0.023165166378\n",
      "Epoch: 2388 | training loss: 0.034444008023\n",
      "Epoch: 2389 | training loss: 0.050121184438\n",
      "Epoch: 2390 | training loss: 0.068836152554\n",
      "Epoch: 2391 | training loss: 0.083719529212\n",
      "Epoch: 2392 | training loss: 0.084665894508\n",
      "Epoch: 2393 | training loss: 0.063657350838\n",
      "Epoch: 2394 | training loss: 0.030465355143\n",
      "Epoch: 2395 | training loss: 0.007049123757\n",
      "Epoch: 2396 | training loss: 0.007816560566\n",
      "Epoch: 2397 | training loss: 0.025243366137\n",
      "Epoch: 2398 | training loss: 0.038827165961\n",
      "Epoch: 2399 | training loss: 0.034566696733\n",
      "Epoch: 2400 | training loss: 0.017214149237\n",
      "Epoch: 2401 | training loss: 0.004981771111\n",
      "Epoch: 2402 | training loss: 0.008100463077\n",
      "Epoch: 2403 | training loss: 0.018928941339\n",
      "Epoch: 2404 | training loss: 0.023057980463\n",
      "Epoch: 2405 | training loss: 0.015831451863\n",
      "Epoch: 2406 | training loss: 0.006387568545\n",
      "Epoch: 2407 | training loss: 0.004940656479\n",
      "Epoch: 2408 | training loss: 0.010777369142\n",
      "Epoch: 2409 | training loss: 0.014978384599\n",
      "Epoch: 2410 | training loss: 0.012179497629\n",
      "Epoch: 2411 | training loss: 0.006343770772\n",
      "Epoch: 2412 | training loss: 0.004383107647\n",
      "Epoch: 2413 | training loss: 0.007381307427\n",
      "Epoch: 2414 | training loss: 0.010346246883\n",
      "Epoch: 2415 | training loss: 0.009285775945\n",
      "Epoch: 2416 | training loss: 0.005849920213\n",
      "Epoch: 2417 | training loss: 0.004279261455\n",
      "Epoch: 2418 | training loss: 0.005806238391\n",
      "Epoch: 2419 | training loss: 0.007721589878\n",
      "Epoch: 2420 | training loss: 0.007380957715\n",
      "Epoch: 2421 | training loss: 0.005394725129\n",
      "Epoch: 2422 | training loss: 0.004262778908\n",
      "Epoch: 2423 | training loss: 0.005002942402\n",
      "Epoch: 2424 | training loss: 0.006201654673\n",
      "Epoch: 2425 | training loss: 0.006179679651\n",
      "Epoch: 2426 | training loss: 0.005060442723\n",
      "Epoch: 2427 | training loss: 0.004266493022\n",
      "Epoch: 2428 | training loss: 0.004573366605\n",
      "Epoch: 2429 | training loss: 0.005307795480\n",
      "Epoch: 2430 | training loss: 0.005428123288\n",
      "Epoch: 2431 | training loss: 0.004828280304\n",
      "Epoch: 2432 | training loss: 0.004277347121\n",
      "Epoch: 2433 | training loss: 0.004346554633\n",
      "Epoch: 2434 | training loss: 0.004777437542\n",
      "Epoch: 2435 | training loss: 0.004946170840\n",
      "Epoch: 2436 | training loss: 0.004658652004\n",
      "Epoch: 2437 | training loss: 0.004288668744\n",
      "Epoch: 2438 | training loss: 0.004237350542\n",
      "Epoch: 2439 | training loss: 0.004466650076\n",
      "Epoch: 2440 | training loss: 0.004629242700\n",
      "Epoch: 2441 | training loss: 0.004523651674\n",
      "Epoch: 2442 | training loss: 0.004292670172\n",
      "Epoch: 2443 | training loss: 0.004194818903\n",
      "Epoch: 2444 | training loss: 0.004291169811\n",
      "Epoch: 2445 | training loss: 0.004416670650\n",
      "Epoch: 2446 | training loss: 0.004408063833\n",
      "Epoch: 2447 | training loss: 0.004283132497\n",
      "Epoch: 2448 | training loss: 0.004185340833\n",
      "Epoch: 2449 | training loss: 0.004201668315\n",
      "Epoch: 2450 | training loss: 0.004279523622\n",
      "Epoch: 2451 | training loss: 0.004311098251\n",
      "Epoch: 2452 | training loss: 0.004259865731\n",
      "Epoch: 2453 | training loss: 0.004185877740\n",
      "Epoch: 2454 | training loss: 0.004162284546\n",
      "Epoch: 2455 | training loss: 0.004195306450\n",
      "Epoch: 2456 | training loss: 0.004231228493\n",
      "Epoch: 2457 | training loss: 0.004224342294\n",
      "Epoch: 2458 | training loss: 0.004182582255\n",
      "Epoch: 2459 | training loss: 0.004148624837\n",
      "Epoch: 2460 | training loss: 0.004149332643\n",
      "Epoch: 2461 | training loss: 0.004171247594\n",
      "Epoch: 2462 | training loss: 0.004182463046\n",
      "Epoch: 2463 | training loss: 0.004168319050\n",
      "Epoch: 2464 | training loss: 0.004142555408\n",
      "Epoch: 2465 | training loss: 0.004127976019\n",
      "Epoch: 2466 | training loss: 0.004132242873\n",
      "Epoch: 2467 | training loss: 0.004142980091\n",
      "Epoch: 2468 | training loss: 0.004144238774\n",
      "Epoch: 2469 | training loss: 0.004132424947\n",
      "Epoch: 2470 | training loss: 0.004117397591\n",
      "Epoch: 2471 | training loss: 0.004110200331\n",
      "Epoch: 2472 | training loss: 0.004112320486\n",
      "Epoch: 2473 | training loss: 0.004116359632\n",
      "Epoch: 2474 | training loss: 0.004114625044\n",
      "Epoch: 2475 | training loss: 0.004106470849\n",
      "Epoch: 2476 | training loss: 0.004097289406\n",
      "Epoch: 2477 | training loss: 0.004092525691\n",
      "Epoch: 2478 | training loss: 0.004092433956\n",
      "Epoch: 2479 | training loss: 0.004093168303\n",
      "Epoch: 2480 | training loss: 0.004090907983\n",
      "Epoch: 2481 | training loss: 0.004085266031\n",
      "Epoch: 2482 | training loss: 0.004079024307\n",
      "Epoch: 2483 | training loss: 0.004074893426\n",
      "Epoch: 2484 | training loss: 0.004073306452\n",
      "Epoch: 2485 | training loss: 0.004072411917\n",
      "Epoch: 2486 | training loss: 0.004070136230\n",
      "Epoch: 2487 | training loss: 0.004066055175\n",
      "Epoch: 2488 | training loss: 0.004061316140\n",
      "Epoch: 2489 | training loss: 0.004057450220\n",
      "Epoch: 2490 | training loss: 0.004054958932\n",
      "Epoch: 2491 | training loss: 0.004053101409\n",
      "Epoch: 2492 | training loss: 0.004050854594\n",
      "Epoch: 2493 | training loss: 0.004047599155\n",
      "Epoch: 2494 | training loss: 0.004043797031\n",
      "Epoch: 2495 | training loss: 0.004040225875\n",
      "Epoch: 2496 | training loss: 0.004037299659\n",
      "Epoch: 2497 | training loss: 0.004034910817\n",
      "Epoch: 2498 | training loss: 0.004032511264\n",
      "Epoch: 2499 | training loss: 0.004029691219\n",
      "Epoch: 2500 | training loss: 0.004026459530\n",
      "Epoch: 2501 | training loss: 0.004023118876\n",
      "Epoch: 2502 | training loss: 0.004020024091\n",
      "Epoch: 2503 | training loss: 0.004017305095\n",
      "Epoch: 2504 | training loss: 0.004014716484\n",
      "Epoch: 2505 | training loss: 0.004012074322\n",
      "Epoch: 2506 | training loss: 0.004009190481\n",
      "Epoch: 2507 | training loss: 0.004006120376\n",
      "Epoch: 2508 | training loss: 0.004003029317\n",
      "Epoch: 2509 | training loss: 0.004000124987\n",
      "Epoch: 2510 | training loss: 0.003997324966\n",
      "Epoch: 2511 | training loss: 0.003994637169\n",
      "Epoch: 2512 | training loss: 0.003991898615\n",
      "Epoch: 2513 | training loss: 0.003989038058\n",
      "Epoch: 2514 | training loss: 0.003986161202\n",
      "Epoch: 2515 | training loss: 0.003983195405\n",
      "Epoch: 2516 | training loss: 0.003980328795\n",
      "Epoch: 2517 | training loss: 0.003977557644\n",
      "Epoch: 2518 | training loss: 0.003974812105\n",
      "Epoch: 2519 | training loss: 0.003972055856\n",
      "Epoch: 2520 | training loss: 0.003969238605\n",
      "Epoch: 2521 | training loss: 0.003966408782\n",
      "Epoch: 2522 | training loss: 0.003963562194\n",
      "Epoch: 2523 | training loss: 0.003960718866\n",
      "Epoch: 2524 | training loss: 0.003957930021\n",
      "Epoch: 2525 | training loss: 0.003955172841\n",
      "Epoch: 2526 | training loss: 0.003952417988\n",
      "Epoch: 2527 | training loss: 0.003949658480\n",
      "Epoch: 2528 | training loss: 0.003946891055\n",
      "Epoch: 2529 | training loss: 0.003944059368\n",
      "Epoch: 2530 | training loss: 0.003941261675\n",
      "Epoch: 2531 | training loss: 0.003938467242\n",
      "Epoch: 2532 | training loss: 0.003935729153\n",
      "Epoch: 2533 | training loss: 0.003932979424\n",
      "Epoch: 2534 | training loss: 0.003930223174\n",
      "Epoch: 2535 | training loss: 0.003927461337\n",
      "Epoch: 2536 | training loss: 0.003924709745\n",
      "Epoch: 2537 | training loss: 0.003921950702\n",
      "Epoch: 2538 | training loss: 0.003919177689\n",
      "Epoch: 2539 | training loss: 0.003916435875\n",
      "Epoch: 2540 | training loss: 0.003913712688\n",
      "Epoch: 2541 | training loss: 0.003910957836\n",
      "Epoch: 2542 | training loss: 0.003908229526\n",
      "Epoch: 2543 | training loss: 0.003905489575\n",
      "Epoch: 2544 | training loss: 0.003902768018\n",
      "Epoch: 2545 | training loss: 0.003900047624\n",
      "Epoch: 2546 | training loss: 0.003897320479\n",
      "Epoch: 2547 | training loss: 0.003894592170\n",
      "Epoch: 2548 | training loss: 0.003891862929\n",
      "Epoch: 2549 | training loss: 0.003889153246\n",
      "Epoch: 2550 | training loss: 0.003886455670\n",
      "Epoch: 2551 | training loss: 0.003883739701\n",
      "Epoch: 2552 | training loss: 0.003881041659\n",
      "Epoch: 2553 | training loss: 0.003878303804\n",
      "Epoch: 2554 | training loss: 0.003875628114\n",
      "Epoch: 2555 | training loss: 0.003872929141\n",
      "Epoch: 2556 | training loss: 0.003870232962\n",
      "Epoch: 2557 | training loss: 0.003867538180\n",
      "Epoch: 2558 | training loss: 0.003864843166\n",
      "Epoch: 2559 | training loss: 0.003862160258\n",
      "Epoch: 2560 | training loss: 0.003859475954\n",
      "Epoch: 2561 | training loss: 0.003856800962\n",
      "Epoch: 2562 | training loss: 0.003854118753\n",
      "Epoch: 2563 | training loss: 0.003851450048\n",
      "Epoch: 2564 | training loss: 0.003848777618\n",
      "Epoch: 2565 | training loss: 0.003846117761\n",
      "Epoch: 2566 | training loss: 0.003843427869\n",
      "Epoch: 2567 | training loss: 0.003840781748\n",
      "Epoch: 2568 | training loss: 0.003838122822\n",
      "Epoch: 2569 | training loss: 0.003835459938\n",
      "Epoch: 2570 | training loss: 0.003832798218\n",
      "Epoch: 2571 | training loss: 0.003830166068\n",
      "Epoch: 2572 | training loss: 0.003827506676\n",
      "Epoch: 2573 | training loss: 0.003824872430\n",
      "Epoch: 2574 | training loss: 0.003822226543\n",
      "Epoch: 2575 | training loss: 0.003819584846\n",
      "Epoch: 2576 | training loss: 0.003816954792\n",
      "Epoch: 2577 | training loss: 0.003814322175\n",
      "Epoch: 2578 | training loss: 0.003811701201\n",
      "Epoch: 2579 | training loss: 0.003809057409\n",
      "Epoch: 2580 | training loss: 0.003806419205\n",
      "Epoch: 2581 | training loss: 0.003803811269\n",
      "Epoch: 2582 | training loss: 0.003801187035\n",
      "Epoch: 2583 | training loss: 0.003798582591\n",
      "Epoch: 2584 | training loss: 0.003795946017\n",
      "Epoch: 2585 | training loss: 0.003793361131\n",
      "Epoch: 2586 | training loss: 0.003790743649\n",
      "Epoch: 2587 | training loss: 0.003788146190\n",
      "Epoch: 2588 | training loss: 0.003785539418\n",
      "Epoch: 2589 | training loss: 0.003782934276\n",
      "Epoch: 2590 | training loss: 0.003780350788\n",
      "Epoch: 2591 | training loss: 0.003777757287\n",
      "Epoch: 2592 | training loss: 0.003775153775\n",
      "Epoch: 2593 | training loss: 0.003772585653\n",
      "Epoch: 2594 | training loss: 0.003769989125\n",
      "Epoch: 2595 | training loss: 0.003767410293\n",
      "Epoch: 2596 | training loss: 0.003764821682\n",
      "Epoch: 2597 | training loss: 0.003762253793\n",
      "Epoch: 2598 | training loss: 0.003759687301\n",
      "Epoch: 2599 | training loss: 0.003757126862\n",
      "Epoch: 2600 | training loss: 0.003754574573\n",
      "Epoch: 2601 | training loss: 0.003751989920\n",
      "Epoch: 2602 | training loss: 0.003749435535\n",
      "Epoch: 2603 | training loss: 0.003746886738\n",
      "Epoch: 2604 | training loss: 0.003744332120\n",
      "Epoch: 2605 | training loss: 0.003741770051\n",
      "Epoch: 2606 | training loss: 0.003739231965\n",
      "Epoch: 2607 | training loss: 0.003736703657\n",
      "Epoch: 2608 | training loss: 0.003734155092\n",
      "Epoch: 2609 | training loss: 0.003731633304\n",
      "Epoch: 2610 | training loss: 0.003729108255\n",
      "Epoch: 2611 | training loss: 0.003726604860\n",
      "Epoch: 2612 | training loss: 0.003724076319\n",
      "Epoch: 2613 | training loss: 0.003721586661\n",
      "Epoch: 2614 | training loss: 0.003719117725\n",
      "Epoch: 2615 | training loss: 0.003716624342\n",
      "Epoch: 2616 | training loss: 0.003714169608\n",
      "Epoch: 2617 | training loss: 0.003711736761\n",
      "Epoch: 2618 | training loss: 0.003709337208\n",
      "Epoch: 2619 | training loss: 0.003706961870\n",
      "Epoch: 2620 | training loss: 0.003704675473\n",
      "Epoch: 2621 | training loss: 0.003702446120\n",
      "Epoch: 2622 | training loss: 0.003700340632\n",
      "Epoch: 2623 | training loss: 0.003698394867\n",
      "Epoch: 2624 | training loss: 0.003696658649\n",
      "Epoch: 2625 | training loss: 0.003695230000\n",
      "Epoch: 2626 | training loss: 0.003694263287\n",
      "Epoch: 2627 | training loss: 0.003693950828\n",
      "Epoch: 2628 | training loss: 0.003694577608\n",
      "Epoch: 2629 | training loss: 0.003696577158\n",
      "Epoch: 2630 | training loss: 0.003700690111\n",
      "Epoch: 2631 | training loss: 0.003707936034\n",
      "Epoch: 2632 | training loss: 0.003719973145\n",
      "Epoch: 2633 | training loss: 0.003739329521\n",
      "Epoch: 2634 | training loss: 0.003770061303\n",
      "Epoch: 2635 | training loss: 0.003818536643\n",
      "Epoch: 2636 | training loss: 0.003894736990\n",
      "Epoch: 2637 | training loss: 0.004014044069\n",
      "Epoch: 2638 | training loss: 0.004202016164\n",
      "Epoch: 2639 | training loss: 0.004498628899\n",
      "Epoch: 2640 | training loss: 0.004969516769\n",
      "Epoch: 2641 | training loss: 0.005717141554\n",
      "Epoch: 2642 | training loss: 0.006908832584\n",
      "Epoch: 2643 | training loss: 0.008795498870\n",
      "Epoch: 2644 | training loss: 0.011776867323\n",
      "Epoch: 2645 | training loss: 0.016378253698\n",
      "Epoch: 2646 | training loss: 0.023330194876\n",
      "Epoch: 2647 | training loss: 0.033160723746\n",
      "Epoch: 2648 | training loss: 0.045983657241\n",
      "Epoch: 2649 | training loss: 0.059653319418\n",
      "Epoch: 2650 | training loss: 0.069789789617\n",
      "Epoch: 2651 | training loss: 0.068751357496\n",
      "Epoch: 2652 | training loss: 0.053276069462\n",
      "Epoch: 2653 | training loss: 0.028413100168\n",
      "Epoch: 2654 | training loss: 0.008481562138\n",
      "Epoch: 2655 | training loss: 0.004164803773\n",
      "Epoch: 2656 | training loss: 0.014011729509\n",
      "Epoch: 2657 | training loss: 0.026727212593\n",
      "Epoch: 2658 | training loss: 0.030519945547\n",
      "Epoch: 2659 | training loss: 0.022446267307\n",
      "Epoch: 2660 | training loss: 0.009916842915\n",
      "Epoch: 2661 | training loss: 0.003665925469\n",
      "Epoch: 2662 | training loss: 0.007124593481\n",
      "Epoch: 2663 | training loss: 0.014442022890\n",
      "Epoch: 2664 | training loss: 0.017397452146\n",
      "Epoch: 2665 | training loss: 0.013206835836\n",
      "Epoch: 2666 | training loss: 0.006501687225\n",
      "Epoch: 2667 | training loss: 0.003596641589\n",
      "Epoch: 2668 | training loss: 0.006064238027\n",
      "Epoch: 2669 | training loss: 0.009997727349\n",
      "Epoch: 2670 | training loss: 0.010809933767\n",
      "Epoch: 2671 | training loss: 0.007884318009\n",
      "Epoch: 2672 | training loss: 0.004448656924\n",
      "Epoch: 2673 | training loss: 0.003712193575\n",
      "Epoch: 2674 | training loss: 0.005606602877\n",
      "Epoch: 2675 | training loss: 0.007474495564\n",
      "Epoch: 2676 | training loss: 0.007218564861\n",
      "Epoch: 2677 | training loss: 0.005272445269\n",
      "Epoch: 2678 | training loss: 0.003719302826\n",
      "Epoch: 2679 | training loss: 0.003867266467\n",
      "Epoch: 2680 | training loss: 0.005088945851\n",
      "Epoch: 2681 | training loss: 0.005822935142\n",
      "Epoch: 2682 | training loss: 0.005302076694\n",
      "Epoch: 2683 | training loss: 0.004169757944\n",
      "Epoch: 2684 | training loss: 0.003561530262\n",
      "Epoch: 2685 | training loss: 0.003888204461\n",
      "Epoch: 2686 | training loss: 0.004568706732\n",
      "Epoch: 2687 | training loss: 0.004794107750\n",
      "Epoch: 2688 | training loss: 0.004364168271\n",
      "Epoch: 2689 | training loss: 0.003759841900\n",
      "Epoch: 2690 | training loss: 0.003545472631\n",
      "Epoch: 2691 | training loss: 0.003804110689\n",
      "Epoch: 2692 | training loss: 0.004153615795\n",
      "Epoch: 2693 | training loss: 0.004203824326\n",
      "Epoch: 2694 | training loss: 0.003927053418\n",
      "Epoch: 2695 | training loss: 0.003614569316\n",
      "Epoch: 2696 | training loss: 0.003536926117\n",
      "Epoch: 2697 | training loss: 0.003695150372\n",
      "Epoch: 2698 | training loss: 0.003873298410\n",
      "Epoch: 2699 | training loss: 0.003881022101\n",
      "Epoch: 2700 | training loss: 0.003724101931\n",
      "Epoch: 2701 | training loss: 0.003558847355\n",
      "Epoch: 2702 | training loss: 0.003520361846\n",
      "Epoch: 2703 | training loss: 0.003604355734\n",
      "Epoch: 2704 | training loss: 0.003698803950\n",
      "Epoch: 2705 | training loss: 0.003705276642\n",
      "Epoch: 2706 | training loss: 0.003623489290\n",
      "Epoch: 2707 | training loss: 0.003531993367\n",
      "Epoch: 2708 | training loss: 0.003502965905\n",
      "Epoch: 2709 | training loss: 0.003540824167\n",
      "Epoch: 2710 | training loss: 0.003592154710\n",
      "Epoch: 2711 | training loss: 0.003603313118\n",
      "Epoch: 2712 | training loss: 0.003565939143\n",
      "Epoch: 2713 | training loss: 0.003514477285\n",
      "Epoch: 2714 | training loss: 0.003488990478\n",
      "Epoch: 2715 | training loss: 0.003500303719\n",
      "Epoch: 2716 | training loss: 0.003527080640\n",
      "Epoch: 2717 | training loss: 0.003539957572\n",
      "Epoch: 2718 | training loss: 0.003526909277\n",
      "Epoch: 2719 | training loss: 0.003499405459\n",
      "Epoch: 2720 | training loss: 0.003478376660\n",
      "Epoch: 2721 | training loss: 0.003475815756\n",
      "Epoch: 2722 | training loss: 0.003486908739\n",
      "Epoch: 2723 | training loss: 0.003497294849\n",
      "Epoch: 2724 | training loss: 0.003496161895\n",
      "Epoch: 2725 | training loss: 0.003483740613\n",
      "Epoch: 2726 | training loss: 0.003468939103\n",
      "Epoch: 2727 | training loss: 0.003460787004\n",
      "Epoch: 2728 | training loss: 0.003461645450\n",
      "Epoch: 2729 | training loss: 0.003466787748\n",
      "Epoch: 2730 | training loss: 0.003469448537\n",
      "Epoch: 2731 | training loss: 0.003465993097\n",
      "Epoch: 2732 | training loss: 0.003458050312\n",
      "Epoch: 2733 | training loss: 0.003450039774\n",
      "Epoch: 2734 | training loss: 0.003445750801\n",
      "Epoch: 2735 | training loss: 0.003445586655\n",
      "Epoch: 2736 | training loss: 0.003446999704\n",
      "Epoch: 2737 | training loss: 0.003446978983\n",
      "Epoch: 2738 | training loss: 0.003444074420\n",
      "Epoch: 2739 | training loss: 0.003439124906\n",
      "Epoch: 2740 | training loss: 0.003434182145\n",
      "Epoch: 2741 | training loss: 0.003430899931\n",
      "Epoch: 2742 | training loss: 0.003429582343\n",
      "Epoch: 2743 | training loss: 0.003429151140\n",
      "Epoch: 2744 | training loss: 0.003428234719\n",
      "Epoch: 2745 | training loss: 0.003426065203\n",
      "Epoch: 2746 | training loss: 0.003422782989\n",
      "Epoch: 2747 | training loss: 0.003419244662\n",
      "Epoch: 2748 | training loss: 0.003416273743\n",
      "Epoch: 2749 | training loss: 0.003414259059\n",
      "Epoch: 2750 | training loss: 0.003412830643\n",
      "Epoch: 2751 | training loss: 0.003411423182\n",
      "Epoch: 2752 | training loss: 0.003409596859\n",
      "Epoch: 2753 | training loss: 0.003407184966\n",
      "Epoch: 2754 | training loss: 0.003404455725\n",
      "Epoch: 2755 | training loss: 0.003401823575\n",
      "Epoch: 2756 | training loss: 0.003399522277\n",
      "Epoch: 2757 | training loss: 0.003397584427\n",
      "Epoch: 2758 | training loss: 0.003395813983\n",
      "Epoch: 2759 | training loss: 0.003394024912\n",
      "Epoch: 2760 | training loss: 0.003391999286\n",
      "Epoch: 2761 | training loss: 0.003389759455\n",
      "Epoch: 2762 | training loss: 0.003387425561\n",
      "Epoch: 2763 | training loss: 0.003385130083\n",
      "Epoch: 2764 | training loss: 0.003382975934\n",
      "Epoch: 2765 | training loss: 0.003380976850\n",
      "Epoch: 2766 | training loss: 0.003379039001\n",
      "Epoch: 2767 | training loss: 0.003377086716\n",
      "Epoch: 2768 | training loss: 0.003375070402\n",
      "Epoch: 2769 | training loss: 0.003372994717\n",
      "Epoch: 2770 | training loss: 0.003370868275\n",
      "Epoch: 2771 | training loss: 0.003368705744\n",
      "Epoch: 2772 | training loss: 0.003366608173\n",
      "Epoch: 2773 | training loss: 0.003364540637\n",
      "Epoch: 2774 | training loss: 0.003362558084\n",
      "Epoch: 2775 | training loss: 0.003360590898\n",
      "Epoch: 2776 | training loss: 0.003358600894\n",
      "Epoch: 2777 | training loss: 0.003356561530\n",
      "Epoch: 2778 | training loss: 0.003354508430\n",
      "Epoch: 2779 | training loss: 0.003352428786\n",
      "Epoch: 2780 | training loss: 0.003350372892\n",
      "Epoch: 2781 | training loss: 0.003348351922\n",
      "Epoch: 2782 | training loss: 0.003346309531\n",
      "Epoch: 2783 | training loss: 0.003344286233\n",
      "Epoch: 2784 | training loss: 0.003342282725\n",
      "Epoch: 2785 | training loss: 0.003340311814\n",
      "Epoch: 2786 | training loss: 0.003338303184\n",
      "Epoch: 2787 | training loss: 0.003336317372\n",
      "Epoch: 2788 | training loss: 0.003334302921\n",
      "Epoch: 2789 | training loss: 0.003332278924\n",
      "Epoch: 2790 | training loss: 0.003330264241\n",
      "Epoch: 2791 | training loss: 0.003328236286\n",
      "Epoch: 2792 | training loss: 0.003326224629\n",
      "Epoch: 2793 | training loss: 0.003324229736\n",
      "Epoch: 2794 | training loss: 0.003322233446\n",
      "Epoch: 2795 | training loss: 0.003320270684\n",
      "Epoch: 2796 | training loss: 0.003318282776\n",
      "Epoch: 2797 | training loss: 0.003316310467\n",
      "Epoch: 2798 | training loss: 0.003314328380\n",
      "Epoch: 2799 | training loss: 0.003312371904\n",
      "Epoch: 2800 | training loss: 0.003310384229\n",
      "Epoch: 2801 | training loss: 0.003308397252\n",
      "Epoch: 2802 | training loss: 0.003306390252\n",
      "Epoch: 2803 | training loss: 0.003304422367\n",
      "Epoch: 2804 | training loss: 0.003302432364\n",
      "Epoch: 2805 | training loss: 0.003300479613\n",
      "Epoch: 2806 | training loss: 0.003298509168\n",
      "Epoch: 2807 | training loss: 0.003296548966\n",
      "Epoch: 2808 | training loss: 0.003294588299\n",
      "Epoch: 2809 | training loss: 0.003292603884\n",
      "Epoch: 2810 | training loss: 0.003290657885\n",
      "Epoch: 2811 | training loss: 0.003288716078\n",
      "Epoch: 2812 | training loss: 0.003286774969\n",
      "Epoch: 2813 | training loss: 0.003284829203\n",
      "Epoch: 2814 | training loss: 0.003282899037\n",
      "Epoch: 2815 | training loss: 0.003280936508\n",
      "Epoch: 2816 | training loss: 0.003278978867\n",
      "Epoch: 2817 | training loss: 0.003277050797\n",
      "Epoch: 2818 | training loss: 0.003275082912\n",
      "Epoch: 2819 | training loss: 0.003273155307\n",
      "Epoch: 2820 | training loss: 0.003271210939\n",
      "Epoch: 2821 | training loss: 0.003269245615\n",
      "Epoch: 2822 | training loss: 0.003267319873\n",
      "Epoch: 2823 | training loss: 0.003265379695\n",
      "Epoch: 2824 | training loss: 0.003263453254\n",
      "Epoch: 2825 | training loss: 0.003261512145\n",
      "Epoch: 2826 | training loss: 0.003259597812\n",
      "Epoch: 2827 | training loss: 0.003257668810\n",
      "Epoch: 2828 | training loss: 0.003255761927\n",
      "Epoch: 2829 | training loss: 0.003253834322\n",
      "Epoch: 2830 | training loss: 0.003251931630\n",
      "Epoch: 2831 | training loss: 0.003250016365\n",
      "Epoch: 2832 | training loss: 0.003248104826\n",
      "Epoch: 2833 | training loss: 0.003246191889\n",
      "Epoch: 2834 | training loss: 0.003244282678\n",
      "Epoch: 2835 | training loss: 0.003242396051\n",
      "Epoch: 2836 | training loss: 0.003240477992\n",
      "Epoch: 2837 | training loss: 0.003238584846\n",
      "Epoch: 2838 | training loss: 0.003236689139\n",
      "Epoch: 2839 | training loss: 0.003234799951\n",
      "Epoch: 2840 | training loss: 0.003232924268\n",
      "Epoch: 2841 | training loss: 0.003231039736\n",
      "Epoch: 2842 | training loss: 0.003229174297\n",
      "Epoch: 2843 | training loss: 0.003227319801\n",
      "Epoch: 2844 | training loss: 0.003225459717\n",
      "Epoch: 2845 | training loss: 0.003223637119\n",
      "Epoch: 2846 | training loss: 0.003221809864\n",
      "Epoch: 2847 | training loss: 0.003220038489\n",
      "Epoch: 2848 | training loss: 0.003218258731\n",
      "Epoch: 2849 | training loss: 0.003216516227\n",
      "Epoch: 2850 | training loss: 0.003214855678\n",
      "Epoch: 2851 | training loss: 0.003213254269\n",
      "Epoch: 2852 | training loss: 0.003211752977\n",
      "Epoch: 2853 | training loss: 0.003210415598\n",
      "Epoch: 2854 | training loss: 0.003209300572\n",
      "Epoch: 2855 | training loss: 0.003208481707\n",
      "Epoch: 2856 | training loss: 0.003208101960\n",
      "Epoch: 2857 | training loss: 0.003208413487\n",
      "Epoch: 2858 | training loss: 0.003209775779\n",
      "Epoch: 2859 | training loss: 0.003212677781\n",
      "Epoch: 2860 | training loss: 0.003217948135\n",
      "Epoch: 2861 | training loss: 0.003226843197\n",
      "Epoch: 2862 | training loss: 0.003241208382\n",
      "Epoch: 2863 | training loss: 0.003264158266\n",
      "Epoch: 2864 | training loss: 0.003300433978\n",
      "Epoch: 2865 | training loss: 0.003357595298\n",
      "Epoch: 2866 | training loss: 0.003447782714\n",
      "Epoch: 2867 | training loss: 0.003590557491\n",
      "Epoch: 2868 | training loss: 0.003817324992\n",
      "Epoch: 2869 | training loss: 0.004179867916\n",
      "Epoch: 2870 | training loss: 0.004759877454\n",
      "Epoch: 2871 | training loss: 0.005691981874\n",
      "Epoch: 2872 | training loss: 0.007184164599\n",
      "Epoch: 2873 | training loss: 0.009576851502\n",
      "Epoch: 2874 | training loss: 0.013349587098\n",
      "Epoch: 2875 | training loss: 0.019224850461\n",
      "Epoch: 2876 | training loss: 0.027937848121\n",
      "Epoch: 2877 | training loss: 0.040177766234\n",
      "Epoch: 2878 | training loss: 0.055090371519\n",
      "Epoch: 2879 | training loss: 0.069804608822\n",
      "Epoch: 2880 | training loss: 0.076804049313\n",
      "Epoch: 2881 | training loss: 0.069276228547\n",
      "Epoch: 2882 | training loss: 0.045775718987\n",
      "Epoch: 2883 | training loss: 0.018606508151\n",
      "Epoch: 2884 | training loss: 0.003817429533\n",
      "Epoch: 2885 | training loss: 0.007877764292\n",
      "Epoch: 2886 | training loss: 0.022467037663\n",
      "Epoch: 2887 | training loss: 0.032610300928\n",
      "Epoch: 2888 | training loss: 0.029180807993\n",
      "Epoch: 2889 | training loss: 0.015612028539\n",
      "Epoch: 2890 | training loss: 0.004591322504\n",
      "Epoch: 2891 | training loss: 0.004617386963\n",
      "Epoch: 2892 | training loss: 0.012517500669\n",
      "Epoch: 2893 | training loss: 0.018359353766\n",
      "Epoch: 2894 | training loss: 0.015894072130\n",
      "Epoch: 2895 | training loss: 0.008266294375\n",
      "Epoch: 2896 | training loss: 0.003323759651\n",
      "Epoch: 2897 | training loss: 0.004934639670\n",
      "Epoch: 2898 | training loss: 0.009630921297\n",
      "Epoch: 2899 | training loss: 0.011467634700\n",
      "Epoch: 2900 | training loss: 0.008601307869\n",
      "Epoch: 2901 | training loss: 0.004435613751\n",
      "Epoch: 2902 | training loss: 0.003198218066\n",
      "Epoch: 2903 | training loss: 0.005280780140\n",
      "Epoch: 2904 | training loss: 0.007544266991\n",
      "Epoch: 2905 | training loss: 0.007288233377\n",
      "Epoch: 2906 | training loss: 0.004987412132\n",
      "Epoch: 2907 | training loss: 0.003235052805\n",
      "Epoch: 2908 | training loss: 0.003571042791\n",
      "Epoch: 2909 | training loss: 0.005055987276\n",
      "Epoch: 2910 | training loss: 0.005739566870\n",
      "Epoch: 2911 | training loss: 0.004896593280\n",
      "Epoch: 2912 | training loss: 0.003568229731\n",
      "Epoch: 2913 | training loss: 0.003117435379\n",
      "Epoch: 2914 | training loss: 0.003739549313\n",
      "Epoch: 2915 | training loss: 0.004481913522\n",
      "Epoch: 2916 | training loss: 0.004458098672\n",
      "Epoch: 2917 | training loss: 0.003758403240\n",
      "Epoch: 2918 | training loss: 0.003160968190\n",
      "Epoch: 2919 | training loss: 0.003196887206\n",
      "Epoch: 2920 | training loss: 0.003649926279\n",
      "Epoch: 2921 | training loss: 0.003924495541\n",
      "Epoch: 2922 | training loss: 0.003727872390\n",
      "Epoch: 2923 | training loss: 0.003304864978\n",
      "Epoch: 2924 | training loss: 0.003086324781\n",
      "Epoch: 2925 | training loss: 0.003217474092\n",
      "Epoch: 2926 | training loss: 0.003469053190\n",
      "Epoch: 2927 | training loss: 0.003539800644\n",
      "Epoch: 2928 | training loss: 0.003368158126\n",
      "Epoch: 2929 | training loss: 0.003145823488\n",
      "Epoch: 2930 | training loss: 0.003078452311\n",
      "Epoch: 2931 | training loss: 0.003182658926\n",
      "Epoch: 2932 | training loss: 0.003307532985\n",
      "Epoch: 2933 | training loss: 0.003313228488\n",
      "Epoch: 2934 | training loss: 0.003202905180\n",
      "Epoch: 2935 | training loss: 0.003090544837\n",
      "Epoch: 2936 | training loss: 0.003071188461\n",
      "Epoch: 2937 | training loss: 0.003134664847\n",
      "Epoch: 2938 | training loss: 0.003196185920\n",
      "Epoch: 2939 | training loss: 0.003190104850\n",
      "Epoch: 2940 | training loss: 0.003126841737\n",
      "Epoch: 2941 | training loss: 0.003067826387\n",
      "Epoch: 2942 | training loss: 0.003059157636\n",
      "Epoch: 2943 | training loss: 0.003092885250\n",
      "Epoch: 2944 | training loss: 0.003124828450\n",
      "Epoch: 2945 | training loss: 0.003121507587\n",
      "Epoch: 2946 | training loss: 0.003087630263\n",
      "Epoch: 2947 | training loss: 0.003054373432\n",
      "Epoch: 2948 | training loss: 0.003046585713\n",
      "Epoch: 2949 | training loss: 0.003062260337\n",
      "Epoch: 2950 | training loss: 0.003079781774\n",
      "Epoch: 2951 | training loss: 0.003080334980\n",
      "Epoch: 2952 | training loss: 0.003063464072\n",
      "Epoch: 2953 | training loss: 0.003043840639\n",
      "Epoch: 2954 | training loss: 0.003035588656\n",
      "Epoch: 2955 | training loss: 0.003040713025\n",
      "Epoch: 2956 | training loss: 0.003049878404\n",
      "Epoch: 2957 | training loss: 0.003052462358\n",
      "Epoch: 2958 | training loss: 0.003045449033\n",
      "Epoch: 2959 | training loss: 0.003034141613\n",
      "Epoch: 2960 | training loss: 0.003026344813\n",
      "Epoch: 2961 | training loss: 0.003025619546\n",
      "Epoch: 2962 | training loss: 0.003029274056\n",
      "Epoch: 2963 | training loss: 0.003031803295\n",
      "Epoch: 2964 | training loss: 0.003029745538\n",
      "Epoch: 2965 | training loss: 0.003023893572\n",
      "Epoch: 2966 | training loss: 0.003017853480\n",
      "Epoch: 2967 | training loss: 0.003014670685\n",
      "Epoch: 2968 | training loss: 0.003014662769\n",
      "Epoch: 2969 | training loss: 0.003015727270\n",
      "Epoch: 2970 | training loss: 0.003015490947\n",
      "Epoch: 2971 | training loss: 0.003012970323\n",
      "Epoch: 2972 | training loss: 0.003009120468\n",
      "Epoch: 2973 | training loss: 0.003005627776\n",
      "Epoch: 2974 | training loss: 0.003003585152\n",
      "Epoch: 2975 | training loss: 0.003002937883\n",
      "Epoch: 2976 | training loss: 0.003002609359\n",
      "Epoch: 2977 | training loss: 0.003001589095\n",
      "Epoch: 2978 | training loss: 0.002999485936\n",
      "Epoch: 2979 | training loss: 0.002996802796\n",
      "Epoch: 2980 | training loss: 0.002994324081\n",
      "Epoch: 2981 | training loss: 0.002992526628\n",
      "Epoch: 2982 | training loss: 0.002991364803\n",
      "Epoch: 2983 | training loss: 0.002990416018\n",
      "Epoch: 2984 | training loss: 0.002989165019\n",
      "Epoch: 2985 | training loss: 0.002987486776\n",
      "Epoch: 2986 | training loss: 0.002985421335\n",
      "Epoch: 2987 | training loss: 0.002983359154\n",
      "Epoch: 2988 | training loss: 0.002981569385\n",
      "Epoch: 2989 | training loss: 0.002980119083\n",
      "Epoch: 2990 | training loss: 0.002978835022\n",
      "Epoch: 2991 | training loss: 0.002977526281\n",
      "Epoch: 2992 | training loss: 0.002976029878\n",
      "Epoch: 2993 | training loss: 0.002974337665\n",
      "Epoch: 2994 | training loss: 0.002972524380\n",
      "Epoch: 2995 | training loss: 0.002970769769\n",
      "Epoch: 2996 | training loss: 0.002969147870\n",
      "Epoch: 2997 | training loss: 0.002967687789\n",
      "Epoch: 2998 | training loss: 0.002966242842\n",
      "Epoch: 2999 | training loss: 0.002964794869\n",
      "Epoch: 3000 | training loss: 0.002963250270\n",
      "Epoch: 3001 | training loss: 0.002961635357\n",
      "Epoch: 3002 | training loss: 0.002960005542\n",
      "Epoch: 3003 | training loss: 0.002958401572\n",
      "Epoch: 3004 | training loss: 0.002956838114\n",
      "Epoch: 3005 | training loss: 0.002955338452\n",
      "Epoch: 3006 | training loss: 0.002953844843\n",
      "Epoch: 3007 | training loss: 0.002952360082\n",
      "Epoch: 3008 | training loss: 0.002950838767\n",
      "Epoch: 3009 | training loss: 0.002949276473\n",
      "Epoch: 3010 | training loss: 0.002947717439\n",
      "Epoch: 3011 | training loss: 0.002946183085\n",
      "Epoch: 3012 | training loss: 0.002944628242\n",
      "Epoch: 3013 | training loss: 0.002943115775\n",
      "Epoch: 3014 | training loss: 0.002941612620\n",
      "Epoch: 3015 | training loss: 0.002940103877\n",
      "Epoch: 3016 | training loss: 0.002938595833\n",
      "Epoch: 3017 | training loss: 0.002937069628\n",
      "Epoch: 3018 | training loss: 0.002935562050\n",
      "Epoch: 3019 | training loss: 0.002934051212\n",
      "Epoch: 3020 | training loss: 0.002932513831\n",
      "Epoch: 3021 | training loss: 0.002931006253\n",
      "Epoch: 3022 | training loss: 0.002929509152\n",
      "Epoch: 3023 | training loss: 0.002927988302\n",
      "Epoch: 3024 | training loss: 0.002926517744\n",
      "Epoch: 3025 | training loss: 0.002925011795\n",
      "Epoch: 3026 | training loss: 0.002923526103\n",
      "Epoch: 3027 | training loss: 0.002922008745\n",
      "Epoch: 3028 | training loss: 0.002920527011\n",
      "Epoch: 3029 | training loss: 0.002919022460\n",
      "Epoch: 3030 | training loss: 0.002917514415\n",
      "Epoch: 3031 | training loss: 0.002916019410\n",
      "Epoch: 3032 | training loss: 0.002914531156\n",
      "Epoch: 3033 | training loss: 0.002913038014\n",
      "Epoch: 3034 | training loss: 0.002911553718\n",
      "Epoch: 3035 | training loss: 0.002910078969\n",
      "Epoch: 3036 | training loss: 0.002908595605\n",
      "Epoch: 3037 | training loss: 0.002907105722\n",
      "Epoch: 3038 | training loss: 0.002905637259\n",
      "Epoch: 3039 | training loss: 0.002904160414\n",
      "Epoch: 3040 | training loss: 0.002902678447\n",
      "Epoch: 3041 | training loss: 0.002901193453\n",
      "Epoch: 3042 | training loss: 0.002899726853\n",
      "Epoch: 3043 | training loss: 0.002898260020\n",
      "Epoch: 3044 | training loss: 0.002896769671\n",
      "Epoch: 3045 | training loss: 0.002895290963\n",
      "Epoch: 3046 | training loss: 0.002893818077\n",
      "Epoch: 3047 | training loss: 0.002892352641\n",
      "Epoch: 3048 | training loss: 0.002890895121\n",
      "Epoch: 3049 | training loss: 0.002889443887\n",
      "Epoch: 3050 | training loss: 0.002887978684\n",
      "Epoch: 3051 | training loss: 0.002886506962\n",
      "Epoch: 3052 | training loss: 0.002885052236\n",
      "Epoch: 3053 | training loss: 0.002883582842\n",
      "Epoch: 3054 | training loss: 0.002882124158\n",
      "Epoch: 3055 | training loss: 0.002880689222\n",
      "Epoch: 3056 | training loss: 0.002879228909\n",
      "Epoch: 3057 | training loss: 0.002877768595\n",
      "Epoch: 3058 | training loss: 0.002876312705\n",
      "Epoch: 3059 | training loss: 0.002874883125\n",
      "Epoch: 3060 | training loss: 0.002873433288\n",
      "Epoch: 3061 | training loss: 0.002871974604\n",
      "Epoch: 3062 | training loss: 0.002870528027\n",
      "Epoch: 3063 | training loss: 0.002869081683\n",
      "Epoch: 3064 | training loss: 0.002867645118\n",
      "Epoch: 3065 | training loss: 0.002866186434\n",
      "Epoch: 3066 | training loss: 0.002864755690\n",
      "Epoch: 3067 | training loss: 0.002863313304\n",
      "Epoch: 3068 | training loss: 0.002861890243\n",
      "Epoch: 3069 | training loss: 0.002860438544\n",
      "Epoch: 3070 | training loss: 0.002859000117\n",
      "Epoch: 3071 | training loss: 0.002857573796\n",
      "Epoch: 3072 | training loss: 0.002856154460\n",
      "Epoch: 3073 | training loss: 0.002854721388\n",
      "Epoch: 3074 | training loss: 0.002853310434\n",
      "Epoch: 3075 | training loss: 0.002851897385\n",
      "Epoch: 3076 | training loss: 0.002850471530\n",
      "Epoch: 3077 | training loss: 0.002849071287\n",
      "Epoch: 3078 | training loss: 0.002847661031\n",
      "Epoch: 3079 | training loss: 0.002846234711\n",
      "Epoch: 3080 | training loss: 0.002844861709\n",
      "Epoch: 3081 | training loss: 0.002843449125\n",
      "Epoch: 3082 | training loss: 0.002842070535\n",
      "Epoch: 3083 | training loss: 0.002840705914\n",
      "Epoch: 3084 | training loss: 0.002839333843\n",
      "Epoch: 3085 | training loss: 0.002837947104\n",
      "Epoch: 3086 | training loss: 0.002836619969\n",
      "Epoch: 3087 | training loss: 0.002835302148\n",
      "Epoch: 3088 | training loss: 0.002834007610\n",
      "Epoch: 3089 | training loss: 0.002832733095\n",
      "Epoch: 3090 | training loss: 0.002831524238\n",
      "Epoch: 3091 | training loss: 0.002830340760\n",
      "Epoch: 3092 | training loss: 0.002829225967\n",
      "Epoch: 3093 | training loss: 0.002828230383\n",
      "Epoch: 3094 | training loss: 0.002827391028\n",
      "Epoch: 3095 | training loss: 0.002826800570\n",
      "Epoch: 3096 | training loss: 0.002826571232\n",
      "Epoch: 3097 | training loss: 0.002826793352\n",
      "Epoch: 3098 | training loss: 0.002827723743\n",
      "Epoch: 3099 | training loss: 0.002829633653\n",
      "Epoch: 3100 | training loss: 0.002833022270\n",
      "Epoch: 3101 | training loss: 0.002838638611\n",
      "Epoch: 3102 | training loss: 0.002847649623\n",
      "Epoch: 3103 | training loss: 0.002861706540\n",
      "Epoch: 3104 | training loss: 0.002883459209\n",
      "Epoch: 3105 | training loss: 0.002917014761\n",
      "Epoch: 3106 | training loss: 0.002968747634\n",
      "Epoch: 3107 | training loss: 0.003048557788\n",
      "Epoch: 3108 | training loss: 0.003172407858\n",
      "Epoch: 3109 | training loss: 0.003365546698\n",
      "Epoch: 3110 | training loss: 0.003668772522\n",
      "Epoch: 3111 | training loss: 0.004145603627\n",
      "Epoch: 3112 | training loss: 0.004899755120\n",
      "Epoch: 3113 | training loss: 0.006088404916\n",
      "Epoch: 3114 | training loss: 0.007967995480\n",
      "Epoch: 3115 | training loss: 0.010902870446\n",
      "Epoch: 3116 | training loss: 0.015454745851\n",
      "Epoch: 3117 | training loss: 0.022247994319\n",
      "Epoch: 3118 | training loss: 0.032025303692\n",
      "Epoch: 3119 | training loss: 0.044699244201\n",
      "Epoch: 3120 | training loss: 0.059066016227\n",
      "Epoch: 3121 | training loss: 0.070240050554\n",
      "Epoch: 3122 | training loss: 0.072019629180\n",
      "Epoch: 3123 | training loss: 0.058452397585\n",
      "Epoch: 3124 | training loss: 0.033902119845\n",
      "Epoch: 3125 | training loss: 0.011065971106\n",
      "Epoch: 3126 | training loss: 0.002807935933\n",
      "Epoch: 3127 | training loss: 0.010458127595\n",
      "Epoch: 3128 | training loss: 0.023965265602\n",
      "Epoch: 3129 | training loss: 0.030727691948\n",
      "Epoch: 3130 | training loss: 0.024869363755\n",
      "Epoch: 3131 | training loss: 0.012105061673\n",
      "Epoch: 3132 | training loss: 0.003433809616\n",
      "Epoch: 3133 | training loss: 0.004777348135\n",
      "Epoch: 3134 | training loss: 0.012118973769\n",
      "Epoch: 3135 | training loss: 0.016788613051\n",
      "Epoch: 3136 | training loss: 0.014190930873\n",
      "Epoch: 3137 | training loss: 0.007323491387\n",
      "Epoch: 3138 | training loss: 0.002952824812\n",
      "Epoch: 3139 | training loss: 0.004265755415\n",
      "Epoch: 3140 | training loss: 0.008395908400\n",
      "Epoch: 3141 | training loss: 0.010320684873\n",
      "Epoch: 3142 | training loss: 0.008108651266\n",
      "Epoch: 3143 | training loss: 0.004359900951\n",
      "Epoch: 3144 | training loss: 0.002759739291\n",
      "Epoch: 3145 | training loss: 0.004201227799\n",
      "Epoch: 3146 | training loss: 0.006391729228\n",
      "Epoch: 3147 | training loss: 0.006740023848\n",
      "Epoch: 3148 | training loss: 0.005024918821\n",
      "Epoch: 3149 | training loss: 0.003153879195\n",
      "Epoch: 3150 | training loss: 0.002861885820\n",
      "Epoch: 3151 | training loss: 0.003980561160\n",
      "Epoch: 3152 | training loss: 0.004975005519\n",
      "Epoch: 3153 | training loss: 0.004745007493\n",
      "Epoch: 3154 | training loss: 0.003630338702\n",
      "Epoch: 3155 | training loss: 0.002805726137\n",
      "Epoch: 3156 | training loss: 0.002939650789\n",
      "Epoch: 3157 | training loss: 0.003631457686\n",
      "Epoch: 3158 | training loss: 0.004016538151\n",
      "Epoch: 3159 | training loss: 0.003703855677\n",
      "Epoch: 3160 | training loss: 0.003070638515\n",
      "Epoch: 3161 | training loss: 0.002738735639\n",
      "Epoch: 3162 | training loss: 0.002922665793\n",
      "Epoch: 3163 | training loss: 0.003300496843\n",
      "Epoch: 3164 | training loss: 0.003430002136\n",
      "Epoch: 3165 | training loss: 0.003197596408\n",
      "Epoch: 3166 | training loss: 0.002860172652\n",
      "Epoch: 3167 | training loss: 0.002728609834\n",
      "Epoch: 3168 | training loss: 0.002860671841\n",
      "Epoch: 3169 | training loss: 0.003057480324\n",
      "Epoch: 3170 | training loss: 0.003100858536\n",
      "Epoch: 3171 | training loss: 0.002959635807\n",
      "Epoch: 3172 | training loss: 0.002781804651\n",
      "Epoch: 3173 | training loss: 0.002721555065\n",
      "Epoch: 3174 | training loss: 0.002796886489\n",
      "Epoch: 3175 | training loss: 0.002900208347\n",
      "Epoch: 3176 | training loss: 0.002920727013\n",
      "Epoch: 3177 | training loss: 0.002845197450\n",
      "Epoch: 3178 | training loss: 0.002749334788\n",
      "Epoch: 3179 | training loss: 0.002712880494\n",
      "Epoch: 3180 | training loss: 0.002748451661\n",
      "Epoch: 3181 | training loss: 0.002803732874\n",
      "Epoch: 3182 | training loss: 0.002820347203\n",
      "Epoch: 3183 | training loss: 0.002785119228\n",
      "Epoch: 3184 | training loss: 0.002732582390\n",
      "Epoch: 3185 | training loss: 0.002705462044\n",
      "Epoch: 3186 | training loss: 0.002717094030\n",
      "Epoch: 3187 | training loss: 0.002745953854\n",
      "Epoch: 3188 | training loss: 0.002760702744\n",
      "Epoch: 3189 | training loss: 0.002748263068\n",
      "Epoch: 3190 | training loss: 0.002720629098\n",
      "Epoch: 3191 | training loss: 0.002700095298\n",
      "Epoch: 3192 | training loss: 0.002698900644\n",
      "Epoch: 3193 | training loss: 0.002711627632\n",
      "Epoch: 3194 | training loss: 0.002722946927\n",
      "Epoch: 3195 | training loss: 0.002721842611\n",
      "Epoch: 3196 | training loss: 0.002709435765\n",
      "Epoch: 3197 | training loss: 0.002695437521\n",
      "Epoch: 3198 | training loss: 0.002688990207\n",
      "Epoch: 3199 | training loss: 0.002691798843\n",
      "Epoch: 3200 | training loss: 0.002698210534\n",
      "Epoch: 3201 | training loss: 0.002701184247\n",
      "Epoch: 3202 | training loss: 0.002697568387\n",
      "Epoch: 3203 | training loss: 0.002689798363\n",
      "Epoch: 3204 | training loss: 0.002683012979\n",
      "Epoch: 3205 | training loss: 0.002680599922\n",
      "Epoch: 3206 | training loss: 0.002682188526\n",
      "Epoch: 3207 | training loss: 0.002684639534\n",
      "Epoch: 3208 | training loss: 0.002684852574\n",
      "Epoch: 3209 | training loss: 0.002682003891\n",
      "Epoch: 3210 | training loss: 0.002677527256\n",
      "Epoch: 3211 | training loss: 0.002673735842\n",
      "Epoch: 3212 | training loss: 0.002672034083\n",
      "Epoch: 3213 | training loss: 0.002672173316\n",
      "Epoch: 3214 | training loss: 0.002672692994\n",
      "Epoch: 3215 | training loss: 0.002672257368\n",
      "Epoch: 3216 | training loss: 0.002670404268\n",
      "Epoch: 3217 | training loss: 0.002667682478\n",
      "Epoch: 3218 | training loss: 0.002665148815\n",
      "Epoch: 3219 | training loss: 0.002663512249\n",
      "Epoch: 3220 | training loss: 0.002662803512\n",
      "Epoch: 3221 | training loss: 0.002662455430\n",
      "Epoch: 3222 | training loss: 0.002661796287\n",
      "Epoch: 3223 | training loss: 0.002660515485\n",
      "Epoch: 3224 | training loss: 0.002658717567\n",
      "Epoch: 3225 | training loss: 0.002656792291\n",
      "Epoch: 3226 | training loss: 0.002655193210\n",
      "Epoch: 3227 | training loss: 0.002654005541\n",
      "Epoch: 3228 | training loss: 0.002653154545\n",
      "Epoch: 3229 | training loss: 0.002652293770\n",
      "Epoch: 3230 | training loss: 0.002651239978\n",
      "Epoch: 3231 | training loss: 0.002649914939\n",
      "Epoch: 3232 | training loss: 0.002648432972\n",
      "Epoch: 3233 | training loss: 0.002646960318\n",
      "Epoch: 3234 | training loss: 0.002645640867\n",
      "Epoch: 3235 | training loss: 0.002644507913\n",
      "Epoch: 3236 | training loss: 0.002643486485\n",
      "Epoch: 3237 | training loss: 0.002642445499\n",
      "Epoch: 3238 | training loss: 0.002641321626\n",
      "Epoch: 3239 | training loss: 0.002640092280\n",
      "Epoch: 3240 | training loss: 0.002638799604\n",
      "Epoch: 3241 | training loss: 0.002637490863\n",
      "Epoch: 3242 | training loss: 0.002636234742\n",
      "Epoch: 3243 | training loss: 0.002635049168\n",
      "Epoch: 3244 | training loss: 0.002633933676\n",
      "Epoch: 3245 | training loss: 0.002632828662\n",
      "Epoch: 3246 | training loss: 0.002631674521\n",
      "Epoch: 3247 | training loss: 0.002630534116\n",
      "Epoch: 3248 | training loss: 0.002629333176\n",
      "Epoch: 3249 | training loss: 0.002628104528\n",
      "Epoch: 3250 | training loss: 0.002626883797\n",
      "Epoch: 3251 | training loss: 0.002625719411\n",
      "Epoch: 3252 | training loss: 0.002624543151\n",
      "Epoch: 3253 | training loss: 0.002623414621\n",
      "Epoch: 3254 | training loss: 0.002622296335\n",
      "Epoch: 3255 | training loss: 0.002621139167\n",
      "Epoch: 3256 | training loss: 0.002619972918\n",
      "Epoch: 3257 | training loss: 0.002618799917\n",
      "Epoch: 3258 | training loss: 0.002617609221\n",
      "Epoch: 3259 | training loss: 0.002616447397\n",
      "Epoch: 3260 | training loss: 0.002615287900\n",
      "Epoch: 3261 | training loss: 0.002614116063\n",
      "Epoch: 3262 | training loss: 0.002612980083\n",
      "Epoch: 3263 | training loss: 0.002611843403\n",
      "Epoch: 3264 | training loss: 0.002610689728\n",
      "Epoch: 3265 | training loss: 0.002609548625\n",
      "Epoch: 3266 | training loss: 0.002608380513\n",
      "Epoch: 3267 | training loss: 0.002607239410\n",
      "Epoch: 3268 | training loss: 0.002606088063\n",
      "Epoch: 3269 | training loss: 0.002604937647\n",
      "Epoch: 3270 | training loss: 0.002603788860\n",
      "Epoch: 3271 | training loss: 0.002602632856\n",
      "Epoch: 3272 | training loss: 0.002601514803\n",
      "Epoch: 3273 | training loss: 0.002600368811\n",
      "Epoch: 3274 | training loss: 0.002599210944\n",
      "Epoch: 3275 | training loss: 0.002598086139\n",
      "Epoch: 3276 | training loss: 0.002596929204\n",
      "Epoch: 3277 | training loss: 0.002595788101\n",
      "Epoch: 3278 | training loss: 0.002594660036\n",
      "Epoch: 3279 | training loss: 0.002593520330\n",
      "Epoch: 3280 | training loss: 0.002592392731\n",
      "Epoch: 3281 | training loss: 0.002591273282\n",
      "Epoch: 3282 | training loss: 0.002590120770\n",
      "Epoch: 3283 | training loss: 0.002588985255\n",
      "Epoch: 3284 | training loss: 0.002587872790\n",
      "Epoch: 3285 | training loss: 0.002586731687\n",
      "Epoch: 3286 | training loss: 0.002585608047\n",
      "Epoch: 3287 | training loss: 0.002584482543\n",
      "Epoch: 3288 | training loss: 0.002583349589\n",
      "Epoch: 3289 | training loss: 0.002582242945\n",
      "Epoch: 3290 | training loss: 0.002581092296\n",
      "Epoch: 3291 | training loss: 0.002579974942\n",
      "Epoch: 3292 | training loss: 0.002578852233\n",
      "Epoch: 3293 | training loss: 0.002577734413\n",
      "Epoch: 3294 | training loss: 0.002576605650\n",
      "Epoch: 3295 | training loss: 0.002575491555\n",
      "Epoch: 3296 | training loss: 0.002574385144\n",
      "Epoch: 3297 | training loss: 0.002573257778\n",
      "Epoch: 3298 | training loss: 0.002572135534\n",
      "Epoch: 3299 | training loss: 0.002571011195\n",
      "Epoch: 3300 | training loss: 0.002569906181\n",
      "Epoch: 3301 | training loss: 0.002568783937\n",
      "Epoch: 3302 | training loss: 0.002567683347\n",
      "Epoch: 3303 | training loss: 0.002566569950\n",
      "Epoch: 3304 | training loss: 0.002565458184\n",
      "Epoch: 3305 | training loss: 0.002564345021\n",
      "Epoch: 3306 | training loss: 0.002563241171\n",
      "Epoch: 3307 | training loss: 0.002562121022\n",
      "Epoch: 3308 | training loss: 0.002561012516\n",
      "Epoch: 3309 | training loss: 0.002559906337\n",
      "Epoch: 3310 | training loss: 0.002558812499\n",
      "Epoch: 3311 | training loss: 0.002557702130\n",
      "Epoch: 3312 | training loss: 0.002556591062\n",
      "Epoch: 3313 | training loss: 0.002555490937\n",
      "Epoch: 3314 | training loss: 0.002554393141\n",
      "Epoch: 3315 | training loss: 0.002553295344\n",
      "Epoch: 3316 | training loss: 0.002552191028\n",
      "Epoch: 3317 | training loss: 0.002551101847\n",
      "Epoch: 3318 | training loss: 0.002549994038\n",
      "Epoch: 3319 | training loss: 0.002548889490\n",
      "Epoch: 3320 | training loss: 0.002547801472\n",
      "Epoch: 3321 | training loss: 0.002546708798\n",
      "Epoch: 3322 | training loss: 0.002545606811\n",
      "Epoch: 3323 | training loss: 0.002544512274\n",
      "Epoch: 3324 | training loss: 0.002543427050\n",
      "Epoch: 3325 | training loss: 0.002542335074\n",
      "Epoch: 3326 | training loss: 0.002541249618\n",
      "Epoch: 3327 | training loss: 0.002540149493\n",
      "Epoch: 3328 | training loss: 0.002539069857\n",
      "Epoch: 3329 | training loss: 0.002537984867\n",
      "Epoch: 3330 | training loss: 0.002536904300\n",
      "Epoch: 3331 | training loss: 0.002535839099\n",
      "Epoch: 3332 | training loss: 0.002534766449\n",
      "Epoch: 3333 | training loss: 0.002533692401\n",
      "Epoch: 3334 | training loss: 0.002532643266\n",
      "Epoch: 3335 | training loss: 0.002531595295\n",
      "Epoch: 3336 | training loss: 0.002530577593\n",
      "Epoch: 3337 | training loss: 0.002529588062\n",
      "Epoch: 3338 | training loss: 0.002528616926\n",
      "Epoch: 3339 | training loss: 0.002527737990\n",
      "Epoch: 3340 | training loss: 0.002526971279\n",
      "Epoch: 3341 | training loss: 0.002526334487\n",
      "Epoch: 3342 | training loss: 0.002525919117\n",
      "Epoch: 3343 | training loss: 0.002525848337\n",
      "Epoch: 3344 | training loss: 0.002526321448\n",
      "Epoch: 3345 | training loss: 0.002527694684\n",
      "Epoch: 3346 | training loss: 0.002530427184\n",
      "Epoch: 3347 | training loss: 0.002535386011\n",
      "Epoch: 3348 | training loss: 0.002543903887\n",
      "Epoch: 3349 | training loss: 0.002558215521\n",
      "Epoch: 3350 | training loss: 0.002581936773\n",
      "Epoch: 3351 | training loss: 0.002621185267\n",
      "Epoch: 3352 | training loss: 0.002686235122\n",
      "Epoch: 3353 | training loss: 0.002794212662\n",
      "Epoch: 3354 | training loss: 0.002974151168\n",
      "Epoch: 3355 | training loss: 0.003274354618\n",
      "Epoch: 3356 | training loss: 0.003778599435\n",
      "Epoch: 3357 | training loss: 0.004625599831\n",
      "Epoch: 3358 | training loss: 0.006055578124\n",
      "Epoch: 3359 | training loss: 0.008454387076\n",
      "Epoch: 3360 | training loss: 0.012473403476\n",
      "Epoch: 3361 | training loss: 0.019042611122\n",
      "Epoch: 3362 | training loss: 0.029538840055\n",
      "Epoch: 3363 | training loss: 0.045080795884\n",
      "Epoch: 3364 | training loss: 0.065996058285\n",
      "Epoch: 3365 | training loss: 0.087768130004\n",
      "Epoch: 3366 | training loss: 0.101039752364\n",
      "Epoch: 3367 | training loss: 0.091129690409\n",
      "Epoch: 3368 | training loss: 0.057347472757\n",
      "Epoch: 3369 | training loss: 0.018653579056\n",
      "Epoch: 3370 | training loss: 0.002522262279\n",
      "Epoch: 3371 | training loss: 0.015165846795\n",
      "Epoch: 3372 | training loss: 0.037181731313\n",
      "Epoch: 3373 | training loss: 0.043880805373\n",
      "Epoch: 3374 | training loss: 0.028126683086\n",
      "Epoch: 3375 | training loss: 0.007763941772\n",
      "Epoch: 3376 | training loss: 0.003343045246\n",
      "Epoch: 3377 | training loss: 0.014872023836\n",
      "Epoch: 3378 | training loss: 0.024758737534\n",
      "Epoch: 3379 | training loss: 0.020262863487\n",
      "Epoch: 3380 | training loss: 0.007871704176\n",
      "Epoch: 3381 | training loss: 0.002531173406\n",
      "Epoch: 3382 | training loss: 0.008243320510\n",
      "Epoch: 3383 | training loss: 0.014910478145\n",
      "Epoch: 3384 | training loss: 0.013128602877\n",
      "Epoch: 3385 | training loss: 0.005854079500\n",
      "Epoch: 3386 | training loss: 0.002495200140\n",
      "Epoch: 3387 | training loss: 0.005889623426\n",
      "Epoch: 3388 | training loss: 0.009814864025\n",
      "Epoch: 3389 | training loss: 0.008563265204\n",
      "Epoch: 3390 | training loss: 0.004222028423\n",
      "Epoch: 3391 | training loss: 0.002518222202\n",
      "Epoch: 3392 | training loss: 0.004752022214\n",
      "Epoch: 3393 | training loss: 0.006917057093\n",
      "Epoch: 3394 | training loss: 0.005876095034\n",
      "Epoch: 3395 | training loss: 0.003293536138\n",
      "Epoch: 3396 | training loss: 0.002546626143\n",
      "Epoch: 3397 | training loss: 0.004017146304\n",
      "Epoch: 3398 | training loss: 0.005160646513\n",
      "Epoch: 3399 | training loss: 0.004356181715\n",
      "Epoch: 3400 | training loss: 0.002842188580\n",
      "Epoch: 3401 | training loss: 0.002553913975\n",
      "Epoch: 3402 | training loss: 0.003489185823\n",
      "Epoch: 3403 | training loss: 0.004081613384\n",
      "Epoch: 3404 | training loss: 0.003516210476\n",
      "Epoch: 3405 | training loss: 0.002636395860\n",
      "Epoch: 3406 | training loss: 0.002536079613\n",
      "Epoch: 3407 | training loss: 0.003111786209\n",
      "Epoch: 3408 | training loss: 0.003425099887\n",
      "Epoch: 3409 | training loss: 0.003058008617\n",
      "Epoch: 3410 | training loss: 0.002544707619\n",
      "Epoch: 3411 | training loss: 0.002507189754\n",
      "Epoch: 3412 | training loss: 0.002852761652\n",
      "Epoch: 3413 | training loss: 0.003029417247\n",
      "Epoch: 3414 | training loss: 0.002806929871\n",
      "Epoch: 3415 | training loss: 0.002502751537\n",
      "Epoch: 3416 | training loss: 0.002477857051\n",
      "Epoch: 3417 | training loss: 0.002680108650\n",
      "Epoch: 3418 | training loss: 0.002789539751\n",
      "Epoch: 3419 | training loss: 0.002664842643\n",
      "Epoch: 3420 | training loss: 0.002481826348\n",
      "Epoch: 3421 | training loss: 0.002454992151\n",
      "Epoch: 3422 | training loss: 0.002569246572\n",
      "Epoch: 3423 | training loss: 0.002643408719\n",
      "Epoch: 3424 | training loss: 0.002581146080\n",
      "Epoch: 3425 | training loss: 0.002469948959\n",
      "Epoch: 3426 | training loss: 0.002439878415\n",
      "Epoch: 3427 | training loss: 0.002500289120\n",
      "Epoch: 3428 | training loss: 0.002552695107\n",
      "Epoch: 3429 | training loss: 0.002527864650\n",
      "Epoch: 3430 | training loss: 0.002461470896\n",
      "Epoch: 3431 | training loss: 0.002431473462\n",
      "Epoch: 3432 | training loss: 0.002458913950\n",
      "Epoch: 3433 | training loss: 0.002494718879\n",
      "Epoch: 3434 | training loss: 0.002490361920\n",
      "Epoch: 3435 | training loss: 0.002453300403\n",
      "Epoch: 3436 | training loss: 0.002427302068\n",
      "Epoch: 3437 | training loss: 0.002435310045\n",
      "Epoch: 3438 | training loss: 0.002457737923\n",
      "Epoch: 3439 | training loss: 0.002462803852\n",
      "Epoch: 3440 | training loss: 0.002444732701\n",
      "Epoch: 3441 | training loss: 0.002424848732\n",
      "Epoch: 3442 | training loss: 0.002422553021\n",
      "Epoch: 3443 | training loss: 0.002434244612\n",
      "Epoch: 3444 | training loss: 0.002441792982\n",
      "Epoch: 3445 | training loss: 0.002435318194\n",
      "Epoch: 3446 | training loss: 0.002422261517\n",
      "Epoch: 3447 | training loss: 0.002415828640\n",
      "Epoch: 3448 | training loss: 0.002419637516\n",
      "Epoch: 3449 | training loss: 0.002425760031\n",
      "Epoch: 3450 | training loss: 0.002425465034\n",
      "Epoch: 3451 | training loss: 0.002418492688\n",
      "Epoch: 3452 | training loss: 0.002411843976\n",
      "Epoch: 3453 | training loss: 0.002410835586\n",
      "Epoch: 3454 | training loss: 0.002413929207\n",
      "Epoch: 3455 | training loss: 0.002415711526\n",
      "Epoch: 3456 | training loss: 0.002413153416\n",
      "Epoch: 3457 | training loss: 0.002408384345\n",
      "Epoch: 3458 | training loss: 0.002405372448\n",
      "Epoch: 3459 | training loss: 0.002405598527\n",
      "Epoch: 3460 | training loss: 0.002406997606\n",
      "Epoch: 3461 | training loss: 0.002406782005\n",
      "Epoch: 3462 | training loss: 0.002404296305\n",
      "Epoch: 3463 | training loss: 0.002401309088\n",
      "Epoch: 3464 | training loss: 0.002399751451\n",
      "Epoch: 3465 | training loss: 0.002399787540\n",
      "Epoch: 3466 | training loss: 0.002400042024\n",
      "Epoch: 3467 | training loss: 0.002399173332\n",
      "Epoch: 3468 | training loss: 0.002397233620\n",
      "Epoch: 3469 | training loss: 0.002395260148\n",
      "Epoch: 3470 | training loss: 0.002394166775\n",
      "Epoch: 3471 | training loss: 0.002393872244\n",
      "Epoch: 3472 | training loss: 0.002393561415\n",
      "Epoch: 3473 | training loss: 0.002392588649\n",
      "Epoch: 3474 | training loss: 0.002391069429\n",
      "Epoch: 3475 | training loss: 0.002389566507\n",
      "Epoch: 3476 | training loss: 0.002388557885\n",
      "Epoch: 3477 | training loss: 0.002387991641\n",
      "Epoch: 3478 | training loss: 0.002387393499\n",
      "Epoch: 3479 | training loss: 0.002386464737\n",
      "Epoch: 3480 | training loss: 0.002385226311\n",
      "Epoch: 3481 | training loss: 0.002383987419\n",
      "Epoch: 3482 | training loss: 0.002382993232\n",
      "Epoch: 3483 | training loss: 0.002382250037\n",
      "Epoch: 3484 | training loss: 0.002381531289\n",
      "Epoch: 3485 | training loss: 0.002380633028\n",
      "Epoch: 3486 | training loss: 0.002379554324\n",
      "Epoch: 3487 | training loss: 0.002378460020\n",
      "Epoch: 3488 | training loss: 0.002377460711\n",
      "Epoch: 3489 | training loss: 0.002376617398\n",
      "Epoch: 3490 | training loss: 0.002375799697\n",
      "Epoch: 3491 | training loss: 0.002374927513\n",
      "Epoch: 3492 | training loss: 0.002373973140\n",
      "Epoch: 3493 | training loss: 0.002372950781\n",
      "Epoch: 3494 | training loss: 0.002371984534\n",
      "Epoch: 3495 | training loss: 0.002371074632\n",
      "Epoch: 3496 | training loss: 0.002370208735\n",
      "Epoch: 3497 | training loss: 0.002369347727\n",
      "Epoch: 3498 | training loss: 0.002368437825\n",
      "Epoch: 3499 | training loss: 0.002367505105\n",
      "Epoch: 3500 | training loss: 0.002366525587\n",
      "Epoch: 3501 | training loss: 0.002365601249\n",
      "Epoch: 3502 | training loss: 0.002364706248\n",
      "Epoch: 3503 | training loss: 0.002363826614\n",
      "Epoch: 3504 | training loss: 0.002362953266\n",
      "Epoch: 3505 | training loss: 0.002362039173\n",
      "Epoch: 3506 | training loss: 0.002361112507\n",
      "Epoch: 3507 | training loss: 0.002360170940\n",
      "Epoch: 3508 | training loss: 0.002359274076\n",
      "Epoch: 3509 | training loss: 0.002358383033\n",
      "Epoch: 3510 | training loss: 0.002357498975\n",
      "Epoch: 3511 | training loss: 0.002356619108\n",
      "Epoch: 3512 | training loss: 0.002355712233\n",
      "Epoch: 3513 | training loss: 0.002354770433\n",
      "Epoch: 3514 | training loss: 0.002353860764\n",
      "Epoch: 3515 | training loss: 0.002352956217\n",
      "Epoch: 3516 | training loss: 0.002352054929\n",
      "Epoch: 3517 | training loss: 0.002351174597\n",
      "Epoch: 3518 | training loss: 0.002350288443\n",
      "Epoch: 3519 | training loss: 0.002349383431\n",
      "Epoch: 3520 | training loss: 0.002348485868\n",
      "Epoch: 3521 | training loss: 0.002347595058\n",
      "Epoch: 3522 | training loss: 0.002346691443\n",
      "Epoch: 3523 | training loss: 0.002345786896\n",
      "Epoch: 3524 | training loss: 0.002344903070\n",
      "Epoch: 3525 | training loss: 0.002344010631\n",
      "Epoch: 3526 | training loss: 0.002343121916\n",
      "Epoch: 3527 | training loss: 0.002342216903\n",
      "Epoch: 3528 | training loss: 0.002341328654\n",
      "Epoch: 3529 | training loss: 0.002340438776\n",
      "Epoch: 3530 | training loss: 0.002339555649\n",
      "Epoch: 3531 | training loss: 0.002338651568\n",
      "Epoch: 3532 | training loss: 0.002337783575\n",
      "Epoch: 3533 | training loss: 0.002336891834\n",
      "Epoch: 3534 | training loss: 0.002336008474\n",
      "Epoch: 3535 | training loss: 0.002335107885\n",
      "Epoch: 3536 | training loss: 0.002334216610\n",
      "Epoch: 3537 | training loss: 0.002333342563\n",
      "Epoch: 3538 | training loss: 0.002332453849\n",
      "Epoch: 3539 | training loss: 0.002331566066\n",
      "Epoch: 3540 | training loss: 0.002330699470\n",
      "Epoch: 3541 | training loss: 0.002329818439\n",
      "Epoch: 3542 | training loss: 0.002328925068\n",
      "Epoch: 3543 | training loss: 0.002328049159\n",
      "Epoch: 3544 | training loss: 0.002327162307\n",
      "Epoch: 3545 | training loss: 0.002326274291\n",
      "Epoch: 3546 | training loss: 0.002325411420\n",
      "Epoch: 3547 | training loss: 0.002324527130\n",
      "Epoch: 3548 | training loss: 0.002323641675\n",
      "Epoch: 3549 | training loss: 0.002322778571\n",
      "Epoch: 3550 | training loss: 0.002321902663\n",
      "Epoch: 3551 | training loss: 0.002321022097\n",
      "Epoch: 3552 | training loss: 0.002320152009\n",
      "Epoch: 3553 | training loss: 0.002319263062\n",
      "Epoch: 3554 | training loss: 0.002318389714\n",
      "Epoch: 3555 | training loss: 0.002317521954\n",
      "Epoch: 3556 | training loss: 0.002316634636\n",
      "Epoch: 3557 | training loss: 0.002315771999\n",
      "Epoch: 3558 | training loss: 0.002314902144\n",
      "Epoch: 3559 | training loss: 0.002314024838\n",
      "Epoch: 3560 | training loss: 0.002313160338\n",
      "Epoch: 3561 | training loss: 0.002312283963\n",
      "Epoch: 3562 | training loss: 0.002311405260\n",
      "Epoch: 3563 | training loss: 0.002310539596\n",
      "Epoch: 3564 | training loss: 0.002309668809\n",
      "Epoch: 3565 | training loss: 0.002308806172\n",
      "Epoch: 3566 | training loss: 0.002307947259\n",
      "Epoch: 3567 | training loss: 0.002307068091\n",
      "Epoch: 3568 | training loss: 0.002306210110\n",
      "Epoch: 3569 | training loss: 0.002305338159\n",
      "Epoch: 3570 | training loss: 0.002304457594\n",
      "Epoch: 3571 | training loss: 0.002303608926\n",
      "Epoch: 3572 | training loss: 0.002302739071\n",
      "Epoch: 3573 | training loss: 0.002301876899\n",
      "Epoch: 3574 | training loss: 0.002301008673\n",
      "Epoch: 3575 | training loss: 0.002300152788\n",
      "Epoch: 3576 | training loss: 0.002299290150\n",
      "Epoch: 3577 | training loss: 0.002298430074\n",
      "Epoch: 3578 | training loss: 0.002297565574\n",
      "Epoch: 3579 | training loss: 0.002296705265\n",
      "Epoch: 3580 | training loss: 0.002295856830\n",
      "Epoch: 3581 | training loss: 0.002294994425\n",
      "Epoch: 3582 | training loss: 0.002294135280\n",
      "Epoch: 3583 | training loss: 0.002293277998\n",
      "Epoch: 3584 | training loss: 0.002292422345\n",
      "Epoch: 3585 | training loss: 0.002291546436\n",
      "Epoch: 3586 | training loss: 0.002290696371\n",
      "Epoch: 3587 | training loss: 0.002289845143\n",
      "Epoch: 3588 | training loss: 0.002288987394\n",
      "Epoch: 3589 | training loss: 0.002288131975\n",
      "Epoch: 3590 | training loss: 0.002287284937\n",
      "Epoch: 3591 | training loss: 0.002286420204\n",
      "Epoch: 3592 | training loss: 0.002285564318\n",
      "Epoch: 3593 | training loss: 0.002284709830\n",
      "Epoch: 3594 | training loss: 0.002283866284\n",
      "Epoch: 3595 | training loss: 0.002283008303\n",
      "Epoch: 3596 | training loss: 0.002282161731\n",
      "Epoch: 3597 | training loss: 0.002281296998\n",
      "Epoch: 3598 | training loss: 0.002280452056\n",
      "Epoch: 3599 | training loss: 0.002279603388\n",
      "Epoch: 3600 | training loss: 0.002278758679\n",
      "Epoch: 3601 | training loss: 0.002277921652\n",
      "Epoch: 3602 | training loss: 0.002277067397\n",
      "Epoch: 3603 | training loss: 0.002276224550\n",
      "Epoch: 3604 | training loss: 0.002275372390\n",
      "Epoch: 3605 | training loss: 0.002274532802\n",
      "Epoch: 3606 | training loss: 0.002273666672\n",
      "Epoch: 3607 | training loss: 0.002272836166\n",
      "Epoch: 3608 | training loss: 0.002271975391\n",
      "Epoch: 3609 | training loss: 0.002271144418\n",
      "Epoch: 3610 | training loss: 0.002270288300\n",
      "Epoch: 3611 | training loss: 0.002269452671\n",
      "Epoch: 3612 | training loss: 0.002268610988\n",
      "Epoch: 3613 | training loss: 0.002267775126\n",
      "Epoch: 3614 | training loss: 0.002266923431\n",
      "Epoch: 3615 | training loss: 0.002266097814\n",
      "Epoch: 3616 | training loss: 0.002265251009\n",
      "Epoch: 3617 | training loss: 0.002264429349\n",
      "Epoch: 3618 | training loss: 0.002263578121\n",
      "Epoch: 3619 | training loss: 0.002262735507\n",
      "Epoch: 3620 | training loss: 0.002261917572\n",
      "Epoch: 3621 | training loss: 0.002261074958\n",
      "Epoch: 3622 | training loss: 0.002260246081\n",
      "Epoch: 3623 | training loss: 0.002259403002\n",
      "Epoch: 3624 | training loss: 0.002258567372\n",
      "Epoch: 3625 | training loss: 0.002257737564\n",
      "Epoch: 3626 | training loss: 0.002256914042\n",
      "Epoch: 3627 | training loss: 0.002256082371\n",
      "Epoch: 3628 | training loss: 0.002255276311\n",
      "Epoch: 3629 | training loss: 0.002254452091\n",
      "Epoch: 3630 | training loss: 0.002253666986\n",
      "Epoch: 3631 | training loss: 0.002252878156\n",
      "Epoch: 3632 | training loss: 0.002252107486\n",
      "Epoch: 3633 | training loss: 0.002251361031\n",
      "Epoch: 3634 | training loss: 0.002250639489\n",
      "Epoch: 3635 | training loss: 0.002249958925\n",
      "Epoch: 3636 | training loss: 0.002249334240\n",
      "Epoch: 3637 | training loss: 0.002248814795\n",
      "Epoch: 3638 | training loss: 0.002248436678\n",
      "Epoch: 3639 | training loss: 0.002248268574\n",
      "Epoch: 3640 | training loss: 0.002248408739\n",
      "Epoch: 3641 | training loss: 0.002248992678\n",
      "Epoch: 3642 | training loss: 0.002250265330\n",
      "Epoch: 3643 | training loss: 0.002252611332\n",
      "Epoch: 3644 | training loss: 0.002256574109\n",
      "Epoch: 3645 | training loss: 0.002262949944\n",
      "Epoch: 3646 | training loss: 0.002273120452\n",
      "Epoch: 3647 | training loss: 0.002289198805\n",
      "Epoch: 3648 | training loss: 0.002314631129\n",
      "Epoch: 3649 | training loss: 0.002354901284\n",
      "Epoch: 3650 | training loss: 0.002418736462\n",
      "Epoch: 3651 | training loss: 0.002520429436\n",
      "Epoch: 3652 | training loss: 0.002683428116\n",
      "Epoch: 3653 | training loss: 0.002945298795\n",
      "Epoch: 3654 | training loss: 0.003368841950\n",
      "Epoch: 3655 | training loss: 0.004053608980\n",
      "Epoch: 3656 | training loss: 0.005168058909\n",
      "Epoch: 3657 | training loss: 0.006971729919\n",
      "Epoch: 3658 | training loss: 0.009893437847\n",
      "Epoch: 3659 | training loss: 0.014533473179\n",
      "Epoch: 3660 | training loss: 0.021801294759\n",
      "Epoch: 3661 | training loss: 0.032548882067\n",
      "Epoch: 3662 | training loss: 0.047497116029\n",
      "Epoch: 3663 | training loss: 0.065023295581\n",
      "Epoch: 3664 | training loss: 0.080862954259\n",
      "Epoch: 3665 | training loss: 0.085017561913\n",
      "Epoch: 3666 | training loss: 0.070684976876\n",
      "Epoch: 3667 | training loss: 0.040132746100\n",
      "Epoch: 3668 | training loss: 0.011607596651\n",
      "Epoch: 3669 | training loss: 0.002384843538\n",
      "Epoch: 3670 | training loss: 0.013709370978\n",
      "Epoch: 3671 | training loss: 0.030686607584\n",
      "Epoch: 3672 | training loss: 0.035921029747\n",
      "Epoch: 3673 | training loss: 0.024854918942\n",
      "Epoch: 3674 | training loss: 0.008551181294\n",
      "Epoch: 3675 | training loss: 0.002264268463\n",
      "Epoch: 3676 | training loss: 0.008950162679\n",
      "Epoch: 3677 | training loss: 0.018178079277\n",
      "Epoch: 3678 | training loss: 0.018907023594\n",
      "Epoch: 3679 | training loss: 0.010756149888\n",
      "Epoch: 3680 | training loss: 0.003163223155\n",
      "Epoch: 3681 | training loss: 0.003392016515\n",
      "Epoch: 3682 | training loss: 0.008950392716\n",
      "Epoch: 3683 | training loss: 0.012031051330\n",
      "Epoch: 3684 | training loss: 0.009003631771\n",
      "Epoch: 3685 | training loss: 0.003863201244\n",
      "Epoch: 3686 | training loss: 0.002312378958\n",
      "Epoch: 3687 | training loss: 0.004984244704\n",
      "Epoch: 3688 | training loss: 0.007626479026\n",
      "Epoch: 3689 | training loss: 0.006838768255\n",
      "Epoch: 3690 | training loss: 0.003846845124\n",
      "Epoch: 3691 | training loss: 0.002207488287\n",
      "Epoch: 3692 | training loss: 0.003289134707\n",
      "Epoch: 3693 | training loss: 0.005089638755\n",
      "Epoch: 3694 | training loss: 0.005177264102\n",
      "Epoch: 3695 | training loss: 0.003582932288\n",
      "Epoch: 3696 | training loss: 0.002282729838\n",
      "Epoch: 3697 | training loss: 0.002561898902\n",
      "Epoch: 3698 | training loss: 0.003658095840\n",
      "Epoch: 3699 | training loss: 0.004029582255\n",
      "Epoch: 3700 | training loss: 0.003275066614\n",
      "Epoch: 3701 | training loss: 0.002364713699\n",
      "Epoch: 3702 | training loss: 0.002274671104\n",
      "Epoch: 3703 | training loss: 0.002867761068\n",
      "Epoch: 3704 | training loss: 0.003265587380\n",
      "Epoch: 3705 | training loss: 0.002988121938\n",
      "Epoch: 3706 | training loss: 0.002414347604\n",
      "Epoch: 3707 | training loss: 0.002191879787\n",
      "Epoch: 3708 | training loss: 0.002453893423\n",
      "Epoch: 3709 | training loss: 0.002770983614\n",
      "Epoch: 3710 | training loss: 0.002740404569\n",
      "Epoch: 3711 | training loss: 0.002425428713\n",
      "Epoch: 3712 | training loss: 0.002195646055\n",
      "Epoch: 3713 | training loss: 0.002258136636\n",
      "Epoch: 3714 | training loss: 0.002462544013\n",
      "Epoch: 3715 | training loss: 0.002535495674\n",
      "Epoch: 3716 | training loss: 0.002401333768\n",
      "Epoch: 3717 | training loss: 0.002225256292\n",
      "Epoch: 3718 | training loss: 0.002186792437\n",
      "Epoch: 3719 | training loss: 0.002285255352\n",
      "Epoch: 3720 | training loss: 0.002375805518\n",
      "Epoch: 3721 | training loss: 0.002351149917\n",
      "Epoch: 3722 | training loss: 0.002247612691\n",
      "Epoch: 3723 | training loss: 0.002178212162\n",
      "Epoch: 3724 | training loss: 0.002199411392\n",
      "Epoch: 3725 | training loss: 0.002263447270\n",
      "Epoch: 3726 | training loss: 0.002288322197\n",
      "Epoch: 3727 | training loss: 0.002248811536\n",
      "Epoch: 3728 | training loss: 0.002191022970\n",
      "Epoch: 3729 | training loss: 0.002171323402\n",
      "Epoch: 3730 | training loss: 0.002197050489\n",
      "Epoch: 3731 | training loss: 0.002228785539\n",
      "Epoch: 3732 | training loss: 0.002229248406\n",
      "Epoch: 3733 | training loss: 0.002199868206\n",
      "Epoch: 3734 | training loss: 0.002171628177\n",
      "Epoch: 3735 | training loss: 0.002168498002\n",
      "Epoch: 3736 | training loss: 0.002185469493\n",
      "Epoch: 3737 | training loss: 0.002199231647\n",
      "Epoch: 3738 | training loss: 0.002194450703\n",
      "Epoch: 3739 | training loss: 0.002176879672\n",
      "Epoch: 3740 | training loss: 0.002163421828\n",
      "Epoch: 3741 | training loss: 0.002163930330\n",
      "Epoch: 3742 | training loss: 0.002173310611\n",
      "Epoch: 3743 | training loss: 0.002179320436\n",
      "Epoch: 3744 | training loss: 0.002175270813\n",
      "Epoch: 3745 | training loss: 0.002165221376\n",
      "Epoch: 3746 | training loss: 0.002158082556\n",
      "Epoch: 3747 | training loss: 0.002158477902\n",
      "Epoch: 3748 | training loss: 0.002163288882\n",
      "Epoch: 3749 | training loss: 0.002166057937\n",
      "Epoch: 3750 | training loss: 0.002163511934\n",
      "Epoch: 3751 | training loss: 0.002157770097\n",
      "Epoch: 3752 | training loss: 0.002153439214\n",
      "Epoch: 3753 | training loss: 0.002153040841\n",
      "Epoch: 3754 | training loss: 0.002155202907\n",
      "Epoch: 3755 | training loss: 0.002156628761\n",
      "Epoch: 3756 | training loss: 0.002155330032\n",
      "Epoch: 3757 | training loss: 0.002152015222\n",
      "Epoch: 3758 | training loss: 0.002149028704\n",
      "Epoch: 3759 | training loss: 0.002147990745\n",
      "Epoch: 3760 | training loss: 0.002148595173\n",
      "Epoch: 3761 | training loss: 0.002149241045\n",
      "Epoch: 3762 | training loss: 0.002148631262\n",
      "Epoch: 3763 | training loss: 0.002146737184\n",
      "Epoch: 3764 | training loss: 0.002144643571\n",
      "Epoch: 3765 | training loss: 0.002143376507\n",
      "Epoch: 3766 | training loss: 0.002143101301\n",
      "Epoch: 3767 | training loss: 0.002143181162\n",
      "Epoch: 3768 | training loss: 0.002142779529\n",
      "Epoch: 3769 | training loss: 0.002141675446\n",
      "Epoch: 3770 | training loss: 0.002140229801\n",
      "Epoch: 3771 | training loss: 0.002138982993\n",
      "Epoch: 3772 | training loss: 0.002138236072\n",
      "Epoch: 3773 | training loss: 0.002137866337\n",
      "Epoch: 3774 | training loss: 0.002137478208\n",
      "Epoch: 3775 | training loss: 0.002136770403\n",
      "Epoch: 3776 | training loss: 0.002135766670\n",
      "Epoch: 3777 | training loss: 0.002134675160\n",
      "Epoch: 3778 | training loss: 0.002133744536\n",
      "Epoch: 3779 | training loss: 0.002133082831\n",
      "Epoch: 3780 | training loss: 0.002132556401\n",
      "Epoch: 3781 | training loss: 0.002131950110\n",
      "Epoch: 3782 | training loss: 0.002131192479\n",
      "Epoch: 3783 | training loss: 0.002130316105\n",
      "Epoch: 3784 | training loss: 0.002129410626\n",
      "Epoch: 3785 | training loss: 0.002128604334\n",
      "Epoch: 3786 | training loss: 0.002127903514\n",
      "Epoch: 3787 | training loss: 0.002127242507\n",
      "Epoch: 3788 | training loss: 0.002126587089\n",
      "Epoch: 3789 | training loss: 0.002125842730\n",
      "Epoch: 3790 | training loss: 0.002125056460\n",
      "Epoch: 3791 | training loss: 0.002124244813\n",
      "Epoch: 3792 | training loss: 0.002123469021\n",
      "Epoch: 3793 | training loss: 0.002122764941\n",
      "Epoch: 3794 | training loss: 0.002122072503\n",
      "Epoch: 3795 | training loss: 0.002121379599\n",
      "Epoch: 3796 | training loss: 0.002120652702\n",
      "Epoch: 3797 | training loss: 0.002119892742\n",
      "Epoch: 3798 | training loss: 0.002119129989\n",
      "Epoch: 3799 | training loss: 0.002118372824\n",
      "Epoch: 3800 | training loss: 0.002117645461\n",
      "Epoch: 3801 | training loss: 0.002116926247\n",
      "Epoch: 3802 | training loss: 0.002116227057\n",
      "Epoch: 3803 | training loss: 0.002115512267\n",
      "Epoch: 3804 | training loss: 0.002114779083\n",
      "Epoch: 3805 | training loss: 0.002114050323\n",
      "Epoch: 3806 | training loss: 0.002113298979\n",
      "Epoch: 3807 | training loss: 0.002112566959\n",
      "Epoch: 3808 | training loss: 0.002111835871\n",
      "Epoch: 3809 | training loss: 0.002111118287\n",
      "Epoch: 3810 | training loss: 0.002110410715\n",
      "Epoch: 3811 | training loss: 0.002109694295\n",
      "Epoch: 3812 | training loss: 0.002108976478\n",
      "Epoch: 3813 | training loss: 0.002108235843\n",
      "Epoch: 3814 | training loss: 0.002107509412\n",
      "Epoch: 3815 | training loss: 0.002106774133\n",
      "Epoch: 3816 | training loss: 0.002106049564\n",
      "Epoch: 3817 | training loss: 0.002105327323\n",
      "Epoch: 3818 | training loss: 0.002104606945\n",
      "Epoch: 3819 | training loss: 0.002103886101\n",
      "Epoch: 3820 | training loss: 0.002103167819\n",
      "Epoch: 3821 | training loss: 0.002102453262\n",
      "Epoch: 3822 | training loss: 0.002101723803\n",
      "Epoch: 3823 | training loss: 0.002101007383\n",
      "Epoch: 3824 | training loss: 0.002100284444\n",
      "Epoch: 3825 | training loss: 0.002099586651\n",
      "Epoch: 3826 | training loss: 0.002098838566\n",
      "Epoch: 3827 | training loss: 0.002098126803\n",
      "Epoch: 3828 | training loss: 0.002097413642\n",
      "Epoch: 3829 | training loss: 0.002096696990\n",
      "Epoch: 3830 | training loss: 0.002095980803\n",
      "Epoch: 3831 | training loss: 0.002095267875\n",
      "Epoch: 3832 | training loss: 0.002094538650\n",
      "Epoch: 3833 | training loss: 0.002093837596\n",
      "Epoch: 3834 | training loss: 0.002093119547\n",
      "Epoch: 3835 | training loss: 0.002092402894\n",
      "Epoch: 3836 | training loss: 0.002091678791\n",
      "Epoch: 3837 | training loss: 0.002090968424\n",
      "Epoch: 3838 | training loss: 0.002090242226\n",
      "Epoch: 3839 | training loss: 0.002089546062\n",
      "Epoch: 3840 | training loss: 0.002088825218\n",
      "Epoch: 3841 | training loss: 0.002088107402\n",
      "Epoch: 3842 | training loss: 0.002087404020\n",
      "Epoch: 3843 | training loss: 0.002086684108\n",
      "Epoch: 3844 | training loss: 0.002085975138\n",
      "Epoch: 3845 | training loss: 0.002085268032\n",
      "Epoch: 3846 | training loss: 0.002084550913\n",
      "Epoch: 3847 | training loss: 0.002083844505\n",
      "Epoch: 3848 | training loss: 0.002083131112\n",
      "Epoch: 3849 | training loss: 0.002082418650\n",
      "Epoch: 3850 | training loss: 0.002081713406\n",
      "Epoch: 3851 | training loss: 0.002080991399\n",
      "Epoch: 3852 | training loss: 0.002080284059\n",
      "Epoch: 3853 | training loss: 0.002079579746\n",
      "Epoch: 3854 | training loss: 0.002078866353\n",
      "Epoch: 3855 | training loss: 0.002078165300\n",
      "Epoch: 3856 | training loss: 0.002077449579\n",
      "Epoch: 3857 | training loss: 0.002076741308\n",
      "Epoch: 3858 | training loss: 0.002076039091\n",
      "Epoch: 3859 | training loss: 0.002075338271\n",
      "Epoch: 3860 | training loss: 0.002074631862\n",
      "Epoch: 3861 | training loss: 0.002073899843\n",
      "Epoch: 3862 | training loss: 0.002073195996\n",
      "Epoch: 3863 | training loss: 0.002072505653\n",
      "Epoch: 3864 | training loss: 0.002071782481\n",
      "Epoch: 3865 | training loss: 0.002071098424\n",
      "Epoch: 3866 | training loss: 0.002070379909\n",
      "Epoch: 3867 | training loss: 0.002069667680\n",
      "Epoch: 3868 | training loss: 0.002068979898\n",
      "Epoch: 3869 | training loss: 0.002068267902\n",
      "Epoch: 3870 | training loss: 0.002067566849\n",
      "Epoch: 3871 | training loss: 0.002066869754\n",
      "Epoch: 3872 | training loss: 0.002066158690\n",
      "Epoch: 3873 | training loss: 0.002065452281\n",
      "Epoch: 3874 | training loss: 0.002064752625\n",
      "Epoch: 3875 | training loss: 0.002064057393\n",
      "Epoch: 3876 | training loss: 0.002063350054\n",
      "Epoch: 3877 | training loss: 0.002062645275\n",
      "Epoch: 3878 | training loss: 0.002061947249\n",
      "Epoch: 3879 | training loss: 0.002061247127\n",
      "Epoch: 3880 | training loss: 0.002060544677\n",
      "Epoch: 3881 | training loss: 0.002059828956\n",
      "Epoch: 3882 | training loss: 0.002059151419\n",
      "Epoch: 3883 | training loss: 0.002058445942\n",
      "Epoch: 3884 | training loss: 0.002057733480\n",
      "Epoch: 3885 | training loss: 0.002057038713\n",
      "Epoch: 3886 | training loss: 0.002056339756\n",
      "Epoch: 3887 | training loss: 0.002055633115\n",
      "Epoch: 3888 | training loss: 0.002054943470\n",
      "Epoch: 3889 | training loss: 0.002054244978\n",
      "Epoch: 3890 | training loss: 0.002053545788\n",
      "Epoch: 3891 | training loss: 0.002052837051\n",
      "Epoch: 3892 | training loss: 0.002052163472\n",
      "Epoch: 3893 | training loss: 0.002051464282\n",
      "Epoch: 3894 | training loss: 0.002050757408\n",
      "Epoch: 3895 | training loss: 0.002050074516\n",
      "Epoch: 3896 | training loss: 0.002049364615\n",
      "Epoch: 3897 | training loss: 0.002048674040\n",
      "Epoch: 3898 | training loss: 0.002047994640\n",
      "Epoch: 3899 | training loss: 0.002047301270\n",
      "Epoch: 3900 | training loss: 0.002046615351\n",
      "Epoch: 3901 | training loss: 0.002045931295\n",
      "Epoch: 3902 | training loss: 0.002045240253\n",
      "Epoch: 3903 | training loss: 0.002044573892\n",
      "Epoch: 3904 | training loss: 0.002043914050\n",
      "Epoch: 3905 | training loss: 0.002043273300\n",
      "Epoch: 3906 | training loss: 0.002042666078\n",
      "Epoch: 3907 | training loss: 0.002042067237\n",
      "Epoch: 3908 | training loss: 0.002041529631\n",
      "Epoch: 3909 | training loss: 0.002041018102\n",
      "Epoch: 3910 | training loss: 0.002040579449\n",
      "Epoch: 3911 | training loss: 0.002040265128\n",
      "Epoch: 3912 | training loss: 0.002040150343\n",
      "Epoch: 3913 | training loss: 0.002040267922\n",
      "Epoch: 3914 | training loss: 0.002040779684\n",
      "Epoch: 3915 | training loss: 0.002041863743\n",
      "Epoch: 3916 | training loss: 0.002043856075\n",
      "Epoch: 3917 | training loss: 0.002047216054\n",
      "Epoch: 3918 | training loss: 0.002052735537\n",
      "Epoch: 3919 | training loss: 0.002061511856\n",
      "Epoch: 3920 | training loss: 0.002075325465\n",
      "Epoch: 3921 | training loss: 0.002097130055\n",
      "Epoch: 3922 | training loss: 0.002131445101\n",
      "Epoch: 3923 | training loss: 0.002185748192\n",
      "Epoch: 3924 | training loss: 0.002271897392\n",
      "Epoch: 3925 | training loss: 0.002409517299\n",
      "Epoch: 3926 | training loss: 0.002630121540\n",
      "Epoch: 3927 | training loss: 0.002985919593\n",
      "Epoch: 3928 | training loss: 0.003560595913\n",
      "Epoch: 3929 | training loss: 0.004494898953\n",
      "Epoch: 3930 | training loss: 0.006007332355\n",
      "Epoch: 3931 | training loss: 0.008463097736\n",
      "Epoch: 3932 | training loss: 0.012385400012\n",
      "Epoch: 3933 | training loss: 0.018593186513\n",
      "Epoch: 3934 | training loss: 0.027957934886\n",
      "Epoch: 3935 | training loss: 0.041433572769\n",
      "Epoch: 3936 | training loss: 0.058317050338\n",
      "Epoch: 3937 | training loss: 0.075823664665\n",
      "Epoch: 3938 | training loss: 0.085324816406\n",
      "Epoch: 3939 | training loss: 0.078510180116\n",
      "Epoch: 3940 | training loss: 0.052199676633\n",
      "Epoch: 3941 | training loss: 0.020444814116\n",
      "Epoch: 3942 | training loss: 0.002810061909\n",
      "Epoch: 3943 | training loss: 0.007718598004\n",
      "Epoch: 3944 | training loss: 0.024923777208\n",
      "Epoch: 3945 | training loss: 0.035831097513\n",
      "Epoch: 3946 | training loss: 0.030068492517\n",
      "Epoch: 3947 | training loss: 0.013572668657\n",
      "Epoch: 3948 | training loss: 0.002574456390\n",
      "Epoch: 3949 | training loss: 0.005457558669\n",
      "Epoch: 3950 | training loss: 0.015390965156\n",
      "Epoch: 3951 | training loss: 0.019719552249\n",
      "Epoch: 3952 | training loss: 0.013720097020\n",
      "Epoch: 3953 | training loss: 0.004750733729\n",
      "Epoch: 3954 | training loss: 0.002220664406\n",
      "Epoch: 3955 | training loss: 0.006911106873\n",
      "Epoch: 3956 | training loss: 0.011539699510\n",
      "Epoch: 3957 | training loss: 0.010239665397\n",
      "Epoch: 3958 | training loss: 0.005013965070\n",
      "Epoch: 3959 | training loss: 0.002024848945\n",
      "Epoch: 3960 | training loss: 0.003816666547\n",
      "Epoch: 3961 | training loss: 0.006992584560\n",
      "Epoch: 3962 | training loss: 0.007220662199\n",
      "Epoch: 3963 | training loss: 0.004450526554\n",
      "Epoch: 3964 | training loss: 0.002151791006\n",
      "Epoch: 3965 | training loss: 0.002640698105\n",
      "Epoch: 3966 | training loss: 0.004565719981\n",
      "Epoch: 3967 | training loss: 0.005178614520\n",
      "Epoch: 3968 | training loss: 0.003811535891\n",
      "Epoch: 3969 | training loss: 0.002245991724\n",
      "Epoch: 3970 | training loss: 0.002183980308\n",
      "Epoch: 3971 | training loss: 0.003263157792\n",
      "Epoch: 3972 | training loss: 0.003875245573\n",
      "Epoch: 3973 | training loss: 0.003278747667\n",
      "Epoch: 3974 | training loss: 0.002284036018\n",
      "Epoch: 3975 | training loss: 0.002023380250\n",
      "Epoch: 3976 | training loss: 0.002571700141\n",
      "Epoch: 3977 | training loss: 0.003063925076\n",
      "Epoch: 3978 | training loss: 0.002874205355\n",
      "Epoch: 3979 | training loss: 0.002284535673\n",
      "Epoch: 3980 | training loss: 0.001988623291\n",
      "Epoch: 3981 | training loss: 0.002216278110\n",
      "Epoch: 3982 | training loss: 0.002562909387\n",
      "Epoch: 3983 | training loss: 0.002572361380\n",
      "Epoch: 3984 | training loss: 0.002259716624\n",
      "Epoch: 3985 | training loss: 0.002003697213\n",
      "Epoch: 3986 | training loss: 0.002049348783\n",
      "Epoch: 3987 | training loss: 0.002260985551\n",
      "Epoch: 3988 | training loss: 0.002347365953\n",
      "Epoch: 3989 | training loss: 0.002214312088\n",
      "Epoch: 3990 | training loss: 0.002029984957\n",
      "Epoch: 3991 | training loss: 0.001987755299\n",
      "Epoch: 3992 | training loss: 0.002090470400\n",
      "Epoch: 3993 | training loss: 0.002184181707\n",
      "Epoch: 3994 | training loss: 0.002155749127\n",
      "Epoch: 3995 | training loss: 0.002047036774\n",
      "Epoch: 3996 | training loss: 0.001978808315\n",
      "Epoch: 3997 | training loss: 0.002006230643\n",
      "Epoch: 3998 | training loss: 0.002073111711\n",
      "Epoch: 3999 | training loss: 0.002093255753\n",
      "Epoch: 4000 | training loss: 0.002047211397\n",
      "Epoch: 4001 | training loss: 0.001988917822\n",
      "Epoch: 4002 | training loss: 0.001975079067\n",
      "Epoch: 4003 | training loss: 0.002006009687\n",
      "Epoch: 4004 | training loss: 0.002036356600\n",
      "Epoch: 4005 | training loss: 0.002030798700\n",
      "Epoch: 4006 | training loss: 0.001997683197\n",
      "Epoch: 4007 | training loss: 0.001971874386\n",
      "Epoch: 4008 | training loss: 0.001974395942\n",
      "Epoch: 4009 | training loss: 0.001994167455\n",
      "Epoch: 4010 | training loss: 0.002005394548\n",
      "Epoch: 4011 | training loss: 0.001995890401\n",
      "Epoch: 4012 | training loss: 0.001976546366\n",
      "Epoch: 4013 | training loss: 0.001965841744\n",
      "Epoch: 4014 | training loss: 0.001970463898\n",
      "Epoch: 4015 | training loss: 0.001981124748\n",
      "Epoch: 4016 | training loss: 0.001984607428\n",
      "Epoch: 4017 | training loss: 0.001977320993\n",
      "Epoch: 4018 | training loss: 0.001966638723\n",
      "Epoch: 4019 | training loss: 0.001961800503\n",
      "Epoch: 4020 | training loss: 0.001964786090\n",
      "Epoch: 4021 | training loss: 0.001970037585\n",
      "Epoch: 4022 | training loss: 0.001971073449\n",
      "Epoch: 4023 | training loss: 0.001966641285\n",
      "Epoch: 4024 | training loss: 0.001960715745\n",
      "Epoch: 4025 | training loss: 0.001957913162\n",
      "Epoch: 4026 | training loss: 0.001959175570\n",
      "Epoch: 4027 | training loss: 0.001961700153\n",
      "Epoch: 4028 | training loss: 0.001962034032\n",
      "Epoch: 4029 | training loss: 0.001959509682\n",
      "Epoch: 4030 | training loss: 0.001956044463\n",
      "Epoch: 4031 | training loss: 0.001954044448\n",
      "Epoch: 4032 | training loss: 0.001954212086\n",
      "Epoch: 4033 | training loss: 0.001955252606\n",
      "Epoch: 4034 | training loss: 0.001955403714\n",
      "Epoch: 4035 | training loss: 0.001954032807\n",
      "Epoch: 4036 | training loss: 0.001951881917\n",
      "Epoch: 4037 | training loss: 0.001950268866\n",
      "Epoch: 4038 | training loss: 0.001949785277\n",
      "Epoch: 4039 | training loss: 0.001949959900\n",
      "Epoch: 4040 | training loss: 0.001949938480\n",
      "Epoch: 4041 | training loss: 0.001949150814\n",
      "Epoch: 4042 | training loss: 0.001947848359\n",
      "Epoch: 4043 | training loss: 0.001946559642\n",
      "Epoch: 4044 | training loss: 0.001945764991\n",
      "Epoch: 4045 | training loss: 0.001945469645\n",
      "Epoch: 4046 | training loss: 0.001945236232\n",
      "Epoch: 4047 | training loss: 0.001944733900\n",
      "Epoch: 4048 | training loss: 0.001943860552\n",
      "Epoch: 4049 | training loss: 0.001942864386\n",
      "Epoch: 4050 | training loss: 0.001941977534\n",
      "Epoch: 4051 | training loss: 0.001941371709\n",
      "Epoch: 4052 | training loss: 0.001940937713\n",
      "Epoch: 4053 | training loss: 0.001940485789\n",
      "Epoch: 4054 | training loss: 0.001939828275\n",
      "Epoch: 4055 | training loss: 0.001939043053\n",
      "Epoch: 4056 | training loss: 0.001938236644\n",
      "Epoch: 4057 | training loss: 0.001937524416\n",
      "Epoch: 4058 | training loss: 0.001936945133\n",
      "Epoch: 4059 | training loss: 0.001936407527\n",
      "Epoch: 4060 | training loss: 0.001935863402\n",
      "Epoch: 4061 | training loss: 0.001935222070\n",
      "Epoch: 4062 | training loss: 0.001934502972\n",
      "Epoch: 4063 | training loss: 0.001933793421\n",
      "Epoch: 4064 | training loss: 0.001933111809\n",
      "Epoch: 4065 | training loss: 0.001932509709\n",
      "Epoch: 4066 | training loss: 0.001931942068\n",
      "Epoch: 4067 | training loss: 0.001931340317\n",
      "Epoch: 4068 | training loss: 0.001930730417\n",
      "Epoch: 4069 | training loss: 0.001930048224\n",
      "Epoch: 4070 | training loss: 0.001929363701\n",
      "Epoch: 4071 | training loss: 0.001928713056\n",
      "Epoch: 4072 | training loss: 0.001928094425\n",
      "Epoch: 4073 | training loss: 0.001927501056\n",
      "Epoch: 4074 | training loss: 0.001926888246\n",
      "Epoch: 4075 | training loss: 0.001926288940\n",
      "Epoch: 4076 | training loss: 0.001925642020\n",
      "Epoch: 4077 | training loss: 0.001925016986\n",
      "Epoch: 4078 | training loss: 0.001924362383\n",
      "Epoch: 4079 | training loss: 0.001923711039\n",
      "Epoch: 4080 | training loss: 0.001923081116\n",
      "Epoch: 4081 | training loss: 0.001922477502\n",
      "Epoch: 4082 | training loss: 0.001921870513\n",
      "Epoch: 4083 | training loss: 0.001921244198\n",
      "Epoch: 4084 | training loss: 0.001920598093\n",
      "Epoch: 4085 | training loss: 0.001919966307\n",
      "Epoch: 4086 | training loss: 0.001919343136\n",
      "Epoch: 4087 | training loss: 0.001918718102\n",
      "Epoch: 4088 | training loss: 0.001918100985\n",
      "Epoch: 4089 | training loss: 0.001917476766\n",
      "Epoch: 4090 | training loss: 0.001916865120\n",
      "Epoch: 4091 | training loss: 0.001916242996\n",
      "Epoch: 4092 | training loss: 0.001915612491\n",
      "Epoch: 4093 | training loss: 0.001914984663\n",
      "Epoch: 4094 | training loss: 0.001914354740\n",
      "Epoch: 4095 | training loss: 0.001913742744\n",
      "Epoch: 4096 | training loss: 0.001913110027\n",
      "Epoch: 4097 | training loss: 0.001912504667\n",
      "Epoch: 4098 | training loss: 0.001911892556\n",
      "Epoch: 4099 | training loss: 0.001911265543\n",
      "Epoch: 4100 | training loss: 0.001910622232\n",
      "Epoch: 4101 | training loss: 0.001910003601\n",
      "Epoch: 4102 | training loss: 0.001909391955\n",
      "Epoch: 4103 | training loss: 0.001908764243\n",
      "Epoch: 4104 | training loss: 0.001908144681\n",
      "Epoch: 4105 | training loss: 0.001907549798\n",
      "Epoch: 4106 | training loss: 0.001906932797\n",
      "Epoch: 4107 | training loss: 0.001906315563\n",
      "Epoch: 4108 | training loss: 0.001905676327\n",
      "Epoch: 4109 | training loss: 0.001905065030\n",
      "Epoch: 4110 | training loss: 0.001904443838\n",
      "Epoch: 4111 | training loss: 0.001903825090\n",
      "Epoch: 4112 | training loss: 0.001903213444\n",
      "Epoch: 4113 | training loss: 0.001902598189\n",
      "Epoch: 4114 | training loss: 0.001901993062\n",
      "Epoch: 4115 | training loss: 0.001901362091\n",
      "Epoch: 4116 | training loss: 0.001900743926\n",
      "Epoch: 4117 | training loss: 0.001900112722\n",
      "Epoch: 4118 | training loss: 0.001899500843\n",
      "Epoch: 4119 | training loss: 0.001898881746\n",
      "Epoch: 4120 | training loss: 0.001898265909\n",
      "Epoch: 4121 | training loss: 0.001897638431\n",
      "Epoch: 4122 | training loss: 0.001897015376\n",
      "Epoch: 4123 | training loss: 0.001896414557\n",
      "Epoch: 4124 | training loss: 0.001895797439\n",
      "Epoch: 4125 | training loss: 0.001895180088\n",
      "Epoch: 4126 | training loss: 0.001894550631\n",
      "Epoch: 4127 | training loss: 0.001893945038\n",
      "Epoch: 4128 | training loss: 0.001893332577\n",
      "Epoch: 4129 | training loss: 0.001892715110\n",
      "Epoch: 4130 | training loss: 0.001892107539\n",
      "Epoch: 4131 | training loss: 0.001891505555\n",
      "Epoch: 4132 | training loss: 0.001890880289\n",
      "Epoch: 4133 | training loss: 0.001890268526\n",
      "Epoch: 4134 | training loss: 0.001889658743\n",
      "Epoch: 4135 | training loss: 0.001889039297\n",
      "Epoch: 4136 | training loss: 0.001888414612\n",
      "Epoch: 4137 | training loss: 0.001887803082\n",
      "Epoch: 4138 | training loss: 0.001887183054\n",
      "Epoch: 4139 | training loss: 0.001886567334\n",
      "Epoch: 4140 | training loss: 0.001885955920\n",
      "Epoch: 4141 | training loss: 0.001885343576\n",
      "Epoch: 4142 | training loss: 0.001884724712\n",
      "Epoch: 4143 | training loss: 0.001884104917\n",
      "Epoch: 4144 | training loss: 0.001883505611\n",
      "Epoch: 4145 | training loss: 0.001882897690\n",
      "Epoch: 4146 | training loss: 0.001882285578\n",
      "Epoch: 4147 | training loss: 0.001881672186\n",
      "Epoch: 4148 | training loss: 0.001881063450\n",
      "Epoch: 4149 | training loss: 0.001880456461\n",
      "Epoch: 4150 | training loss: 0.001879842719\n",
      "Epoch: 4151 | training loss: 0.001879226882\n",
      "Epoch: 4152 | training loss: 0.001878609648\n",
      "Epoch: 4153 | training loss: 0.001877991017\n",
      "Epoch: 4154 | training loss: 0.001877376111\n",
      "Epoch: 4155 | training loss: 0.001876777271\n",
      "Epoch: 4156 | training loss: 0.001876172260\n",
      "Epoch: 4157 | training loss: 0.001875557937\n",
      "Epoch: 4158 | training loss: 0.001874940470\n",
      "Epoch: 4159 | training loss: 0.001874332782\n",
      "Epoch: 4160 | training loss: 0.001873721951\n",
      "Epoch: 4161 | training loss: 0.001873116242\n",
      "Epoch: 4162 | training loss: 0.001872505411\n",
      "Epoch: 4163 | training loss: 0.001871883054\n",
      "Epoch: 4164 | training loss: 0.001871282933\n",
      "Epoch: 4165 | training loss: 0.001870668726\n",
      "Epoch: 4166 | training loss: 0.001870052423\n",
      "Epoch: 4167 | training loss: 0.001869451255\n",
      "Epoch: 4168 | training loss: 0.001868838444\n",
      "Epoch: 4169 | training loss: 0.001868230291\n",
      "Epoch: 4170 | training loss: 0.001867620158\n",
      "Epoch: 4171 | training loss: 0.001867018873\n",
      "Epoch: 4172 | training loss: 0.001866422361\n",
      "Epoch: 4173 | training loss: 0.001865795581\n",
      "Epoch: 4174 | training loss: 0.001865191385\n",
      "Epoch: 4175 | training loss: 0.001864585094\n",
      "Epoch: 4176 | training loss: 0.001863966696\n",
      "Epoch: 4177 | training loss: 0.001863370184\n",
      "Epoch: 4178 | training loss: 0.001862762612\n",
      "Epoch: 4179 | training loss: 0.001862144563\n",
      "Epoch: 4180 | training loss: 0.001861551078\n",
      "Epoch: 4181 | training loss: 0.001860942342\n",
      "Epoch: 4182 | training loss: 0.001860337332\n",
      "Epoch: 4183 | training loss: 0.001859738957\n",
      "Epoch: 4184 | training loss: 0.001859141514\n",
      "Epoch: 4185 | training loss: 0.001858524512\n",
      "Epoch: 4186 | training loss: 0.001857935684\n",
      "Epoch: 4187 | training loss: 0.001857333351\n",
      "Epoch: 4188 | training loss: 0.001856751041\n",
      "Epoch: 4189 | training loss: 0.001856161980\n",
      "Epoch: 4190 | training loss: 0.001855573151\n",
      "Epoch: 4191 | training loss: 0.001855021343\n",
      "Epoch: 4192 | training loss: 0.001854478149\n",
      "Epoch: 4193 | training loss: 0.001853967085\n",
      "Epoch: 4194 | training loss: 0.001853507594\n",
      "Epoch: 4195 | training loss: 0.001853093156\n",
      "Epoch: 4196 | training loss: 0.001852798159\n",
      "Epoch: 4197 | training loss: 0.001852663700\n",
      "Epoch: 4198 | training loss: 0.001852800255\n",
      "Epoch: 4199 | training loss: 0.001853353111\n",
      "Epoch: 4200 | training loss: 0.001854524249\n",
      "Epoch: 4201 | training loss: 0.001856696792\n",
      "Epoch: 4202 | training loss: 0.001860495191\n",
      "Epoch: 4203 | training loss: 0.001866967650\n",
      "Epoch: 4204 | training loss: 0.001877660048\n",
      "Epoch: 4205 | training loss: 0.001895206980\n",
      "Epoch: 4206 | training loss: 0.001924035838\n",
      "Epoch: 4207 | training loss: 0.001971518388\n",
      "Epoch: 4208 | training loss: 0.002050072420\n",
      "Epoch: 4209 | training loss: 0.002180322539\n",
      "Epoch: 4210 | training loss: 0.002397647593\n",
      "Epoch: 4211 | training loss: 0.002760680392\n",
      "Epoch: 4212 | training loss: 0.003371250117\n",
      "Epoch: 4213 | training loss: 0.004398031160\n",
      "Epoch: 4214 | training loss: 0.006132909097\n",
      "Epoch: 4215 | training loss: 0.009036132134\n",
      "Epoch: 4216 | training loss: 0.013882789761\n",
      "Epoch: 4217 | training loss: 0.021724808961\n",
      "Epoch: 4218 | training loss: 0.034064084291\n",
      "Epoch: 4219 | training loss: 0.051728677005\n",
      "Epoch: 4220 | training loss: 0.074200652540\n",
      "Epoch: 4221 | training loss: 0.094574898481\n",
      "Epoch: 4222 | training loss: 0.101630866528\n",
      "Epoch: 4223 | training loss: 0.082019500434\n",
      "Epoch: 4224 | training loss: 0.042668141425\n",
      "Epoch: 4225 | training loss: 0.008821055293\n",
      "Epoch: 4226 | training loss: 0.003743925132\n",
      "Epoch: 4227 | training loss: 0.023311369121\n",
      "Epoch: 4228 | training loss: 0.042130716145\n",
      "Epoch: 4229 | training loss: 0.039436891675\n",
      "Epoch: 4230 | training loss: 0.018450474367\n",
      "Epoch: 4231 | training loss: 0.002723169513\n",
      "Epoch: 4232 | training loss: 0.006612611003\n",
      "Epoch: 4233 | training loss: 0.020099319518\n",
      "Epoch: 4234 | training loss: 0.024067126215\n",
      "Epoch: 4235 | training loss: 0.013782707043\n",
      "Epoch: 4236 | training loss: 0.003017814830\n",
      "Epoch: 4237 | training loss: 0.003897017101\n",
      "Epoch: 4238 | training loss: 0.011951988563\n",
      "Epoch: 4239 | training loss: 0.014584790915\n",
      "Epoch: 4240 | training loss: 0.008417451754\n",
      "Epoch: 4241 | training loss: 0.002291745972\n",
      "Epoch: 4242 | training loss: 0.003456755308\n",
      "Epoch: 4243 | training loss: 0.008303471841\n",
      "Epoch: 4244 | training loss: 0.009101882577\n",
      "Epoch: 4245 | training loss: 0.004965172149\n",
      "Epoch: 4246 | training loss: 0.001878339797\n",
      "Epoch: 4247 | training loss: 0.003335098503\n",
      "Epoch: 4248 | training loss: 0.006102832034\n",
      "Epoch: 4249 | training loss: 0.005841473117\n",
      "Epoch: 4250 | training loss: 0.003137194086\n",
      "Epoch: 4251 | training loss: 0.001823834144\n",
      "Epoch: 4252 | training loss: 0.003149309196\n",
      "Epoch: 4253 | training loss: 0.004575838801\n",
      "Epoch: 4254 | training loss: 0.003937850706\n",
      "Epoch: 4255 | training loss: 0.002291775076\n",
      "Epoch: 4256 | training loss: 0.001887914841\n",
      "Epoch: 4257 | training loss: 0.002880173502\n",
      "Epoch: 4258 | training loss: 0.003519039135\n",
      "Epoch: 4259 | training loss: 0.002882381435\n",
      "Epoch: 4260 | training loss: 0.001954536652\n",
      "Epoch: 4261 | training loss: 0.001939267502\n",
      "Epoch: 4262 | training loss: 0.002591854660\n",
      "Epoch: 4263 | training loss: 0.002826259937\n",
      "Epoch: 4264 | training loss: 0.002333366545\n",
      "Epoch: 4265 | training loss: 0.001841028454\n",
      "Epoch: 4266 | training loss: 0.001944453921\n",
      "Epoch: 4267 | training loss: 0.002338728867\n",
      "Epoch: 4268 | training loss: 0.002397174016\n",
      "Epoch: 4269 | training loss: 0.002062313491\n",
      "Epoch: 4270 | training loss: 0.001809063368\n",
      "Epoch: 4271 | training loss: 0.001918498776\n",
      "Epoch: 4272 | training loss: 0.002146274783\n",
      "Epoch: 4273 | training loss: 0.002143518068\n",
      "Epoch: 4274 | training loss: 0.001931209350\n",
      "Epoch: 4275 | training loss: 0.001800373662\n",
      "Epoch: 4276 | training loss: 0.001883214107\n",
      "Epoch: 4277 | training loss: 0.002013331046\n",
      "Epoch: 4278 | training loss: 0.001997544896\n",
      "Epoch: 4279 | training loss: 0.001867944491\n",
      "Epoch: 4280 | training loss: 0.001796503784\n",
      "Epoch: 4281 | training loss: 0.001849772176\n",
      "Epoch: 4282 | training loss: 0.001925122924\n",
      "Epoch: 4283 | training loss: 0.001913171029\n",
      "Epoch: 4284 | training loss: 0.001835996169\n",
      "Epoch: 4285 | training loss: 0.001793406438\n",
      "Epoch: 4286 | training loss: 0.001823738334\n",
      "Epoch: 4287 | training loss: 0.001868571155\n",
      "Epoch: 4288 | training loss: 0.001863497542\n",
      "Epoch: 4289 | training loss: 0.001818317920\n",
      "Epoch: 4290 | training loss: 0.001790508046\n",
      "Epoch: 4291 | training loss: 0.001805793261\n",
      "Epoch: 4292 | training loss: 0.001833044458\n",
      "Epoch: 4293 | training loss: 0.001833043760\n",
      "Epoch: 4294 | training loss: 0.001807282562\n",
      "Epoch: 4295 | training loss: 0.001788003137\n",
      "Epoch: 4296 | training loss: 0.001793776988\n",
      "Epoch: 4297 | training loss: 0.001810104121\n",
      "Epoch: 4298 | training loss: 0.001813099836\n",
      "Epoch: 4299 | training loss: 0.001799352001\n",
      "Epoch: 4300 | training loss: 0.001785793807\n",
      "Epoch: 4301 | training loss: 0.001786104636\n",
      "Epoch: 4302 | training loss: 0.001795305754\n",
      "Epoch: 4303 | training loss: 0.001799270278\n",
      "Epoch: 4304 | training loss: 0.001792754396\n",
      "Epoch: 4305 | training loss: 0.001783628250\n",
      "Epoch: 4306 | training loss: 0.001781210536\n",
      "Epoch: 4307 | training loss: 0.001785536529\n",
      "Epoch: 4308 | training loss: 0.001789140864\n",
      "Epoch: 4309 | training loss: 0.001786851790\n",
      "Epoch: 4310 | training loss: 0.001781173749\n",
      "Epoch: 4311 | training loss: 0.001777944854\n",
      "Epoch: 4312 | training loss: 0.001779191894\n",
      "Epoch: 4313 | training loss: 0.001781681902\n",
      "Epoch: 4314 | training loss: 0.001781480270\n",
      "Epoch: 4315 | training loss: 0.001778375125\n",
      "Epoch: 4316 | training loss: 0.001775398385\n",
      "Epoch: 4317 | training loss: 0.001774843317\n",
      "Epoch: 4318 | training loss: 0.001776017481\n",
      "Epoch: 4319 | training loss: 0.001776553807\n",
      "Epoch: 4320 | training loss: 0.001775186975\n",
      "Epoch: 4321 | training loss: 0.001772982068\n",
      "Epoch: 4322 | training loss: 0.001771649462\n",
      "Epoch: 4323 | training loss: 0.001771695679\n",
      "Epoch: 4324 | training loss: 0.001772100921\n",
      "Epoch: 4325 | training loss: 0.001771669835\n",
      "Epoch: 4326 | training loss: 0.001770331059\n",
      "Epoch: 4327 | training loss: 0.001768972143\n",
      "Epoch: 4328 | training loss: 0.001768301474\n",
      "Epoch: 4329 | training loss: 0.001768245362\n",
      "Epoch: 4330 | training loss: 0.001768113929\n",
      "Epoch: 4331 | training loss: 0.001767430920\n",
      "Epoch: 4332 | training loss: 0.001766359201\n",
      "Epoch: 4333 | training loss: 0.001765442314\n",
      "Epoch: 4334 | training loss: 0.001764931250\n",
      "Epoch: 4335 | training loss: 0.001764660934\n",
      "Epoch: 4336 | training loss: 0.001764271059\n",
      "Epoch: 4337 | training loss: 0.001763600274\n",
      "Epoch: 4338 | training loss: 0.001762770815\n",
      "Epoch: 4339 | training loss: 0.001762029948\n",
      "Epoch: 4340 | training loss: 0.001761523890\n",
      "Epoch: 4341 | training loss: 0.001761120977\n",
      "Epoch: 4342 | training loss: 0.001760629355\n",
      "Epoch: 4343 | training loss: 0.001759996987\n",
      "Epoch: 4344 | training loss: 0.001759298495\n",
      "Epoch: 4345 | training loss: 0.001758658327\n",
      "Epoch: 4346 | training loss: 0.001758140163\n",
      "Epoch: 4347 | training loss: 0.001757659484\n",
      "Epoch: 4348 | training loss: 0.001757127466\n",
      "Epoch: 4349 | training loss: 0.001756541547\n",
      "Epoch: 4350 | training loss: 0.001755906036\n",
      "Epoch: 4351 | training loss: 0.001755298581\n",
      "Epoch: 4352 | training loss: 0.001754744444\n",
      "Epoch: 4353 | training loss: 0.001754230005\n",
      "Epoch: 4354 | training loss: 0.001753709046\n",
      "Epoch: 4355 | training loss: 0.001753132790\n",
      "Epoch: 4356 | training loss: 0.001752540702\n",
      "Epoch: 4357 | training loss: 0.001751949778\n",
      "Epoch: 4358 | training loss: 0.001751379226\n",
      "Epoch: 4359 | training loss: 0.001750839991\n",
      "Epoch: 4360 | training loss: 0.001750303665\n",
      "Epoch: 4361 | training loss: 0.001749745337\n",
      "Epoch: 4362 | training loss: 0.001749172923\n",
      "Epoch: 4363 | training loss: 0.001748601790\n",
      "Epoch: 4364 | training loss: 0.001748032169\n",
      "Epoch: 4365 | training loss: 0.001747473842\n",
      "Epoch: 4366 | training loss: 0.001746953465\n",
      "Epoch: 4367 | training loss: 0.001746395603\n",
      "Epoch: 4368 | training loss: 0.001745823771\n",
      "Epoch: 4369 | training loss: 0.001745254965\n",
      "Epoch: 4370 | training loss: 0.001744686160\n",
      "Epoch: 4371 | training loss: 0.001744133537\n",
      "Epoch: 4372 | training loss: 0.001743589761\n",
      "Epoch: 4373 | training loss: 0.001743041445\n",
      "Epoch: 4374 | training loss: 0.001742489054\n",
      "Epoch: 4375 | training loss: 0.001741921064\n",
      "Epoch: 4376 | training loss: 0.001741351327\n",
      "Epoch: 4377 | training loss: 0.001740807900\n",
      "Epoch: 4378 | training loss: 0.001740247477\n",
      "Epoch: 4379 | training loss: 0.001739697065\n",
      "Epoch: 4380 | training loss: 0.001739144791\n",
      "Epoch: 4381 | training loss: 0.001738594961\n",
      "Epoch: 4382 | training loss: 0.001738037914\n",
      "Epoch: 4383 | training loss: 0.001737473765\n",
      "Epoch: 4384 | training loss: 0.001736917067\n",
      "Epoch: 4385 | training loss: 0.001736368402\n",
      "Epoch: 4386 | training loss: 0.001735805068\n",
      "Epoch: 4387 | training loss: 0.001735270722\n",
      "Epoch: 4388 | training loss: 0.001734706806\n",
      "Epoch: 4389 | training loss: 0.001734161167\n",
      "Epoch: 4390 | training loss: 0.001733596437\n",
      "Epoch: 4391 | training loss: 0.001733032754\n",
      "Epoch: 4392 | training loss: 0.001732484670\n",
      "Epoch: 4393 | training loss: 0.001731934259\n",
      "Epoch: 4394 | training loss: 0.001731381053\n",
      "Epoch: 4395 | training loss: 0.001730823424\n",
      "Epoch: 4396 | training loss: 0.001730275922\n",
      "Epoch: 4397 | training loss: 0.001729722833\n",
      "Epoch: 4398 | training loss: 0.001729184994\n",
      "Epoch: 4399 | training loss: 0.001728623640\n",
      "Epoch: 4400 | training loss: 0.001728055300\n",
      "Epoch: 4401 | training loss: 0.001727514202\n",
      "Epoch: 4402 | training loss: 0.001726959599\n",
      "Epoch: 4403 | training loss: 0.001726408489\n",
      "Epoch: 4404 | training loss: 0.001725867391\n",
      "Epoch: 4405 | training loss: 0.001725298469\n",
      "Epoch: 4406 | training loss: 0.001724746195\n",
      "Epoch: 4407 | training loss: 0.001724197995\n",
      "Epoch: 4408 | training loss: 0.001723655034\n",
      "Epoch: 4409 | training loss: 0.001723098452\n",
      "Epoch: 4410 | training loss: 0.001722544665\n",
      "Epoch: 4411 | training loss: 0.001722001005\n",
      "Epoch: 4412 | training loss: 0.001721446402\n",
      "Epoch: 4413 | training loss: 0.001720893895\n",
      "Epoch: 4414 | training loss: 0.001720349770\n",
      "Epoch: 4415 | training loss: 0.001719800523\n",
      "Epoch: 4416 | training loss: 0.001719252206\n",
      "Epoch: 4417 | training loss: 0.001718694111\n",
      "Epoch: 4418 | training loss: 0.001718148356\n",
      "Epoch: 4419 | training loss: 0.001717594685\n",
      "Epoch: 4420 | training loss: 0.001717039384\n",
      "Epoch: 4421 | training loss: 0.001716495957\n",
      "Epoch: 4422 | training loss: 0.001715934952\n",
      "Epoch: 4423 | training loss: 0.001715396880\n",
      "Epoch: 4424 | training loss: 0.001714835176\n",
      "Epoch: 4425 | training loss: 0.001714294543\n",
      "Epoch: 4426 | training loss: 0.001713743550\n",
      "Epoch: 4427 | training loss: 0.001713192789\n",
      "Epoch: 4428 | training loss: 0.001712644356\n",
      "Epoch: 4429 | training loss: 0.001712096855\n",
      "Epoch: 4430 | training loss: 0.001711539808\n",
      "Epoch: 4431 | training loss: 0.001711002551\n",
      "Epoch: 4432 | training loss: 0.001710443059\n",
      "Epoch: 4433 | training loss: 0.001709909644\n",
      "Epoch: 4434 | training loss: 0.001709350501\n",
      "Epoch: 4435 | training loss: 0.001708800322\n",
      "Epoch: 4436 | training loss: 0.001708243857\n",
      "Epoch: 4437 | training loss: 0.001707688905\n",
      "Epoch: 4438 | training loss: 0.001707153395\n",
      "Epoch: 4439 | training loss: 0.001706606010\n",
      "Epoch: 4440 | training loss: 0.001706047449\n",
      "Epoch: 4441 | training loss: 0.001705497969\n",
      "Epoch: 4442 | training loss: 0.001704959664\n",
      "Epoch: 4443 | training loss: 0.001704400522\n",
      "Epoch: 4444 | training loss: 0.001703855582\n",
      "Epoch: 4445 | training loss: 0.001703314018\n",
      "Epoch: 4446 | training loss: 0.001702762092\n",
      "Epoch: 4447 | training loss: 0.001702206559\n",
      "Epoch: 4448 | training loss: 0.001701660687\n",
      "Epoch: 4449 | training loss: 0.001701119938\n",
      "Epoch: 4450 | training loss: 0.001700568711\n",
      "Epoch: 4451 | training loss: 0.001700018649\n",
      "Epoch: 4452 | training loss: 0.001699482091\n",
      "Epoch: 4453 | training loss: 0.001698916545\n",
      "Epoch: 4454 | training loss: 0.001698374748\n",
      "Epoch: 4455 | training loss: 0.001697828993\n",
      "Epoch: 4456 | training loss: 0.001697283937\n",
      "Epoch: 4457 | training loss: 0.001696727704\n",
      "Epoch: 4458 | training loss: 0.001696181251\n",
      "Epoch: 4459 | training loss: 0.001695638406\n",
      "Epoch: 4460 | training loss: 0.001695085433\n",
      "Epoch: 4461 | training loss: 0.001694543171\n",
      "Epoch: 4462 | training loss: 0.001694003819\n",
      "Epoch: 4463 | training loss: 0.001693444094\n",
      "Epoch: 4464 | training loss: 0.001692892052\n",
      "Epoch: 4465 | training loss: 0.001692346297\n",
      "Epoch: 4466 | training loss: 0.001691805548\n",
      "Epoch: 4467 | training loss: 0.001691270154\n",
      "Epoch: 4468 | training loss: 0.001690704026\n",
      "Epoch: 4469 | training loss: 0.001690159901\n",
      "Epoch: 4470 | training loss: 0.001689626486\n",
      "Epoch: 4471 | training loss: 0.001689076656\n",
      "Epoch: 4472 | training loss: 0.001688525314\n",
      "Epoch: 4473 | training loss: 0.001687990269\n",
      "Epoch: 4474 | training loss: 0.001687440439\n",
      "Epoch: 4475 | training loss: 0.001686902251\n",
      "Epoch: 4476 | training loss: 0.001686342992\n",
      "Epoch: 4477 | training loss: 0.001685787109\n",
      "Epoch: 4478 | training loss: 0.001685244380\n",
      "Epoch: 4479 | training loss: 0.001684703748\n",
      "Epoch: 4480 | training loss: 0.001684154384\n",
      "Epoch: 4481 | training loss: 0.001683601062\n",
      "Epoch: 4482 | training loss: 0.001683056704\n",
      "Epoch: 4483 | training loss: 0.001682496048\n",
      "Epoch: 4484 | training loss: 0.001681957394\n",
      "Epoch: 4485 | training loss: 0.001681415713\n",
      "Epoch: 4486 | training loss: 0.001680877991\n",
      "Epoch: 4487 | training loss: 0.001680326182\n",
      "Epoch: 4488 | training loss: 0.001679786481\n",
      "Epoch: 4489 | training loss: 0.001679229550\n",
      "Epoch: 4490 | training loss: 0.001678693108\n",
      "Epoch: 4491 | training loss: 0.001678145374\n",
      "Epoch: 4492 | training loss: 0.001677590888\n",
      "Epoch: 4493 | training loss: 0.001677063061\n",
      "Epoch: 4494 | training loss: 0.001676508924\n",
      "Epoch: 4495 | training loss: 0.001675972482\n",
      "Epoch: 4496 | training loss: 0.001675414504\n",
      "Epoch: 4497 | training loss: 0.001674855128\n",
      "Epoch: 4498 | training loss: 0.001674319385\n",
      "Epoch: 4499 | training loss: 0.001673774677\n",
      "Epoch: 4500 | training loss: 0.001673226012\n",
      "Epoch: 4501 | training loss: 0.001672682120\n",
      "Epoch: 4502 | training loss: 0.001672139973\n",
      "Epoch: 4503 | training loss: 0.001671586884\n",
      "Epoch: 4504 | training loss: 0.001671048580\n",
      "Epoch: 4505 | training loss: 0.001670501078\n",
      "Epoch: 4506 | training loss: 0.001669954974\n",
      "Epoch: 4507 | training loss: 0.001669418532\n",
      "Epoch: 4508 | training loss: 0.001668862067\n",
      "Epoch: 4509 | training loss: 0.001668309793\n",
      "Epoch: 4510 | training loss: 0.001667773584\n",
      "Epoch: 4511 | training loss: 0.001667229342\n",
      "Epoch: 4512 | training loss: 0.001666676020\n",
      "Epoch: 4513 | training loss: 0.001666138181\n",
      "Epoch: 4514 | training loss: 0.001665598596\n",
      "Epoch: 4515 | training loss: 0.001665047603\n",
      "Epoch: 4516 | training loss: 0.001664502081\n",
      "Epoch: 4517 | training loss: 0.001663956326\n",
      "Epoch: 4518 | training loss: 0.001663414296\n",
      "Epoch: 4519 | training loss: 0.001662873081\n",
      "Epoch: 4520 | training loss: 0.001662324881\n",
      "Epoch: 4521 | training loss: 0.001661780407\n",
      "Epoch: 4522 | training loss: 0.001661242568\n",
      "Epoch: 4523 | training loss: 0.001660690526\n",
      "Epoch: 4524 | training loss: 0.001660155831\n",
      "Epoch: 4525 | training loss: 0.001659602276\n",
      "Epoch: 4526 | training loss: 0.001659077127\n",
      "Epoch: 4527 | training loss: 0.001658540103\n",
      "Epoch: 4528 | training loss: 0.001658012625\n",
      "Epoch: 4529 | training loss: 0.001657489920\n",
      "Epoch: 4530 | training loss: 0.001656986075\n",
      "Epoch: 4531 | training loss: 0.001656505861\n",
      "Epoch: 4532 | training loss: 0.001656086300\n",
      "Epoch: 4533 | training loss: 0.001655737986\n",
      "Epoch: 4534 | training loss: 0.001655513304\n",
      "Epoch: 4535 | training loss: 0.001655502361\n",
      "Epoch: 4536 | training loss: 0.001655855682\n",
      "Epoch: 4537 | training loss: 0.001656840090\n",
      "Epoch: 4538 | training loss: 0.001658916706\n",
      "Epoch: 4539 | training loss: 0.001662936760\n",
      "Epoch: 4540 | training loss: 0.001670439262\n",
      "Epoch: 4541 | training loss: 0.001684027142\n",
      "Epoch: 4542 | training loss: 0.001708335709\n",
      "Epoch: 4543 | training loss: 0.001751826960\n",
      "Epoch: 4544 | training loss: 0.001829691930\n",
      "Epoch: 4545 | training loss: 0.001969784964\n",
      "Epoch: 4546 | training loss: 0.002222422278\n",
      "Epoch: 4547 | training loss: 0.002680691425\n",
      "Epoch: 4548 | training loss: 0.003512620926\n",
      "Epoch: 4549 | training loss: 0.005030462518\n",
      "Epoch: 4550 | training loss: 0.007782517932\n",
      "Epoch: 4551 | training loss: 0.012768253684\n",
      "Epoch: 4552 | training loss: 0.021578766406\n",
      "Epoch: 4553 | training loss: 0.036775436252\n",
      "Epoch: 4554 | training loss: 0.060795426369\n",
      "Epoch: 4555 | training loss: 0.094540990889\n",
      "Epoch: 4556 | training loss: 0.128243789077\n",
      "Epoch: 4557 | training loss: 0.141239285469\n",
      "Epoch: 4558 | training loss: 0.107660338283\n",
      "Epoch: 4559 | training loss: 0.044173065573\n",
      "Epoch: 4560 | training loss: 0.003267537337\n",
      "Epoch: 4561 | training loss: 0.016608016565\n",
      "Epoch: 4562 | training loss: 0.053971238434\n",
      "Epoch: 4563 | training loss: 0.062030486763\n",
      "Epoch: 4564 | training loss: 0.030743125826\n",
      "Epoch: 4565 | training loss: 0.002880197717\n",
      "Epoch: 4566 | training loss: 0.011900546961\n",
      "Epoch: 4567 | training loss: 0.034557394683\n",
      "Epoch: 4568 | training loss: 0.032110273838\n",
      "Epoch: 4569 | training loss: 0.009811147116\n",
      "Epoch: 4570 | training loss: 0.002385020955\n",
      "Epoch: 4571 | training loss: 0.016058871523\n",
      "Epoch: 4572 | training loss: 0.023146543652\n",
      "Epoch: 4573 | training loss: 0.011438852176\n",
      "Epoch: 4574 | training loss: 0.001693795202\n",
      "Epoch: 4575 | training loss: 0.007855110802\n",
      "Epoch: 4576 | training loss: 0.015202743933\n",
      "Epoch: 4577 | training loss: 0.009948712774\n",
      "Epoch: 4578 | training loss: 0.002089772606\n",
      "Epoch: 4579 | training loss: 0.004390275106\n",
      "Epoch: 4580 | training loss: 0.010043865070\n",
      "Epoch: 4581 | training loss: 0.007858144119\n",
      "Epoch: 4582 | training loss: 0.002304197289\n",
      "Epoch: 4583 | training loss: 0.002883530688\n",
      "Epoch: 4584 | training loss: 0.006798599381\n",
      "Epoch: 4585 | training loss: 0.006074707489\n",
      "Epoch: 4586 | training loss: 0.002326427959\n",
      "Epoch: 4587 | training loss: 0.002196228597\n",
      "Epoch: 4588 | training loss: 0.004807569552\n",
      "Epoch: 4589 | training loss: 0.004704770632\n",
      "Epoch: 4590 | training loss: 0.002250229008\n",
      "Epoch: 4591 | training loss: 0.001874504844\n",
      "Epoch: 4592 | training loss: 0.003570019966\n",
      "Epoch: 4593 | training loss: 0.003738476196\n",
      "Epoch: 4594 | training loss: 0.002151959110\n",
      "Epoch: 4595 | training loss: 0.001718051615\n",
      "Epoch: 4596 | training loss: 0.002796576824\n",
      "Epoch: 4597 | training loss: 0.003058790695\n",
      "Epoch: 4598 | training loss: 0.002059670864\n",
      "Epoch: 4599 | training loss: 0.001645966666\n",
      "Epoch: 4600 | training loss: 0.002309463220\n",
      "Epoch: 4601 | training loss: 0.002590878401\n",
      "Epoch: 4602 | training loss: 0.001979561988\n",
      "Epoch: 4603 | training loss: 0.001617239555\n",
      "Epoch: 4604 | training loss: 0.002004993847\n",
      "Epoch: 4605 | training loss: 0.002262596972\n",
      "Epoch: 4606 | training loss: 0.001909700222\n",
      "Epoch: 4607 | training loss: 0.001611693413\n",
      "Epoch: 4608 | training loss: 0.001818683348\n",
      "Epoch: 4609 | training loss: 0.002035750076\n",
      "Epoch: 4610 | training loss: 0.001849245629\n",
      "Epoch: 4611 | training loss: 0.001616198686\n",
      "Epoch: 4612 | training loss: 0.001707806252\n",
      "Epoch: 4613 | training loss: 0.001877336530\n",
      "Epoch: 4614 | training loss: 0.001796434401\n",
      "Epoch: 4615 | training loss: 0.001624280238\n",
      "Epoch: 4616 | training loss: 0.001645694021\n",
      "Epoch: 4617 | training loss: 0.001767826034\n",
      "Epoch: 4618 | training loss: 0.001748849172\n",
      "Epoch: 4619 | training loss: 0.001630876213\n",
      "Epoch: 4620 | training loss: 0.001614369103\n",
      "Epoch: 4621 | training loss: 0.001694151433\n",
      "Epoch: 4622 | training loss: 0.001707704738\n",
      "Epoch: 4623 | training loss: 0.001634143991\n",
      "Epoch: 4624 | training loss: 0.001601284137\n",
      "Epoch: 4625 | training loss: 0.001646434190\n",
      "Epoch: 4626 | training loss: 0.001672593178\n",
      "Epoch: 4627 | training loss: 0.001633099280\n",
      "Epoch: 4628 | training loss: 0.001597912749\n",
      "Epoch: 4629 | training loss: 0.001617206028\n",
      "Epoch: 4630 | training loss: 0.001643760828\n",
      "Epoch: 4631 | training loss: 0.001628084574\n",
      "Epoch: 4632 | training loss: 0.001598659670\n",
      "Epoch: 4633 | training loss: 0.001600960270\n",
      "Epoch: 4634 | training loss: 0.001621142728\n",
      "Epoch: 4635 | training loss: 0.001619942952\n",
      "Epoch: 4636 | training loss: 0.001599889947\n",
      "Epoch: 4637 | training loss: 0.001593274181\n",
      "Epoch: 4638 | training loss: 0.001605092082\n",
      "Epoch: 4639 | training loss: 0.001610489679\n",
      "Epoch: 4640 | training loss: 0.001599677722\n",
      "Epoch: 4641 | training loss: 0.001590382890\n",
      "Epoch: 4642 | training loss: 0.001594685134\n",
      "Epoch: 4643 | training loss: 0.001601282042\n",
      "Epoch: 4644 | training loss: 0.001597594935\n",
      "Epoch: 4645 | training loss: 0.001589483698\n",
      "Epoch: 4646 | training loss: 0.001588605926\n",
      "Epoch: 4647 | training loss: 0.001593330991\n",
      "Epoch: 4648 | training loss: 0.001593827503\n",
      "Epoch: 4649 | training loss: 0.001588702668\n",
      "Epoch: 4650 | training loss: 0.001585452934\n",
      "Epoch: 4651 | training loss: 0.001587345731\n",
      "Epoch: 4652 | training loss: 0.001589275780\n",
      "Epoch: 4653 | training loss: 0.001587095088\n",
      "Epoch: 4654 | training loss: 0.001583711361\n",
      "Epoch: 4655 | training loss: 0.001583348960\n",
      "Epoch: 4656 | training loss: 0.001584921614\n",
      "Epoch: 4657 | training loss: 0.001584682381\n",
      "Epoch: 4658 | training loss: 0.001582295285\n",
      "Epoch: 4659 | training loss: 0.001580722630\n",
      "Epoch: 4660 | training loss: 0.001581150689\n",
      "Epoch: 4661 | training loss: 0.001581645920\n",
      "Epoch: 4662 | training loss: 0.001580520766\n",
      "Epoch: 4663 | training loss: 0.001578829484\n",
      "Epoch: 4664 | training loss: 0.001578251831\n",
      "Epoch: 4665 | training loss: 0.001578557189\n",
      "Epoch: 4666 | training loss: 0.001578303752\n",
      "Epoch: 4667 | training loss: 0.001577108400\n",
      "Epoch: 4668 | training loss: 0.001576042501\n",
      "Epoch: 4669 | training loss: 0.001575777773\n",
      "Epoch: 4670 | training loss: 0.001575726550\n",
      "Epoch: 4671 | training loss: 0.001575126196\n",
      "Epoch: 4672 | training loss: 0.001574159367\n",
      "Epoch: 4673 | training loss: 0.001573468791\n",
      "Epoch: 4674 | training loss: 0.001573212678\n",
      "Epoch: 4675 | training loss: 0.001572914771\n",
      "Epoch: 4676 | training loss: 0.001572234207\n",
      "Epoch: 4677 | training loss: 0.001571456552\n",
      "Epoch: 4678 | training loss: 0.001570930355\n",
      "Epoch: 4679 | training loss: 0.001570604043\n",
      "Epoch: 4680 | training loss: 0.001570158754\n",
      "Epoch: 4681 | training loss: 0.001569507411\n",
      "Epoch: 4682 | training loss: 0.001568854670\n",
      "Epoch: 4683 | training loss: 0.001568398322\n",
      "Epoch: 4684 | training loss: 0.001568009495\n",
      "Epoch: 4685 | training loss: 0.001567485742\n",
      "Epoch: 4686 | training loss: 0.001566879801\n",
      "Epoch: 4687 | training loss: 0.001566318679\n",
      "Epoch: 4688 | training loss: 0.001565856393\n",
      "Epoch: 4689 | training loss: 0.001565415412\n",
      "Epoch: 4690 | training loss: 0.001564887702\n",
      "Epoch: 4691 | training loss: 0.001564313658\n",
      "Epoch: 4692 | training loss: 0.001563780592\n",
      "Epoch: 4693 | training loss: 0.001563302358\n",
      "Epoch: 4694 | training loss: 0.001562815043\n",
      "Epoch: 4695 | training loss: 0.001562289777\n",
      "Epoch: 4696 | training loss: 0.001561755082\n",
      "Epoch: 4697 | training loss: 0.001561254612\n",
      "Epoch: 4698 | training loss: 0.001560781035\n",
      "Epoch: 4699 | training loss: 0.001560287434\n",
      "Epoch: 4700 | training loss: 0.001559758210\n",
      "Epoch: 4701 | training loss: 0.001559233526\n",
      "Epoch: 4702 | training loss: 0.001558728865\n",
      "Epoch: 4703 | training loss: 0.001558248419\n",
      "Epoch: 4704 | training loss: 0.001557747484\n",
      "Epoch: 4705 | training loss: 0.001557236887\n",
      "Epoch: 4706 | training loss: 0.001556717209\n",
      "Epoch: 4707 | training loss: 0.001556220930\n",
      "Epoch: 4708 | training loss: 0.001555715222\n",
      "Epoch: 4709 | training loss: 0.001555211376\n",
      "Epoch: 4710 | training loss: 0.001554700895\n",
      "Epoch: 4711 | training loss: 0.001554198214\n",
      "Epoch: 4712 | training loss: 0.001553697744\n",
      "Epoch: 4713 | training loss: 0.001553199836\n",
      "Epoch: 4714 | training loss: 0.001552702161\n",
      "Epoch: 4715 | training loss: 0.001552194473\n",
      "Epoch: 4716 | training loss: 0.001551696099\n",
      "Epoch: 4717 | training loss: 0.001551200403\n",
      "Epoch: 4718 | training loss: 0.001550691435\n",
      "Epoch: 4719 | training loss: 0.001550193410\n",
      "Epoch: 4720 | training loss: 0.001549690613\n",
      "Epoch: 4721 | training loss: 0.001549192006\n",
      "Epoch: 4722 | training loss: 0.001548687811\n",
      "Epoch: 4723 | training loss: 0.001548186643\n",
      "Epoch: 4724 | training loss: 0.001547700027\n",
      "Epoch: 4725 | training loss: 0.001547189080\n",
      "Epoch: 4726 | training loss: 0.001546693384\n",
      "Epoch: 4727 | training loss: 0.001546195126\n",
      "Epoch: 4728 | training loss: 0.001545692678\n",
      "Epoch: 4729 | training loss: 0.001545190695\n",
      "Epoch: 4730 | training loss: 0.001544682425\n",
      "Epoch: 4731 | training loss: 0.001544197090\n",
      "Epoch: 4732 | training loss: 0.001543698832\n",
      "Epoch: 4733 | training loss: 0.001543200109\n",
      "Epoch: 4734 | training loss: 0.001542702084\n",
      "Epoch: 4735 | training loss: 0.001542208949\n",
      "Epoch: 4736 | training loss: 0.001541702077\n",
      "Epoch: 4737 | training loss: 0.001541210921\n",
      "Epoch: 4738 | training loss: 0.001540702651\n",
      "Epoch: 4739 | training loss: 0.001540199621\n",
      "Epoch: 4740 | training loss: 0.001539715566\n",
      "Epoch: 4741 | training loss: 0.001539210440\n",
      "Epoch: 4742 | training loss: 0.001538710436\n",
      "Epoch: 4743 | training loss: 0.001538207522\n",
      "Epoch: 4744 | training loss: 0.001537723583\n",
      "Epoch: 4745 | training loss: 0.001537223347\n",
      "Epoch: 4746 | training loss: 0.001536721713\n",
      "Epoch: 4747 | training loss: 0.001536229160\n",
      "Epoch: 4748 | training loss: 0.001535736839\n",
      "Epoch: 4749 | training loss: 0.001535236835\n",
      "Epoch: 4750 | training loss: 0.001534730429\n",
      "Epoch: 4751 | training loss: 0.001534236129\n",
      "Epoch: 4752 | training loss: 0.001533745788\n",
      "Epoch: 4753 | training loss: 0.001533240080\n",
      "Epoch: 4754 | training loss: 0.001532748574\n",
      "Epoch: 4755 | training loss: 0.001532253111\n",
      "Epoch: 4756 | training loss: 0.001531759277\n",
      "Epoch: 4757 | training loss: 0.001531255199\n",
      "Epoch: 4758 | training loss: 0.001530761831\n",
      "Epoch: 4759 | training loss: 0.001530264970\n",
      "Epoch: 4760 | training loss: 0.001529773232\n",
      "Epoch: 4761 | training loss: 0.001529268688\n",
      "Epoch: 4762 | training loss: 0.001528772991\n",
      "Epoch: 4763 | training loss: 0.001528272638\n",
      "Epoch: 4764 | training loss: 0.001527792658\n",
      "Epoch: 4765 | training loss: 0.001527275890\n",
      "Epoch: 4766 | training loss: 0.001526789390\n",
      "Epoch: 4767 | training loss: 0.001526294858\n",
      "Epoch: 4768 | training loss: 0.001525795902\n",
      "Epoch: 4769 | training loss: 0.001525305444\n",
      "Epoch: 4770 | training loss: 0.001524803578\n",
      "Epoch: 4771 | training loss: 0.001524320454\n",
      "Epoch: 4772 | training loss: 0.001523811836\n",
      "Epoch: 4773 | training loss: 0.001523319283\n",
      "Epoch: 4774 | training loss: 0.001522829174\n",
      "Epoch: 4775 | training loss: 0.001522319973\n",
      "Epoch: 4776 | training loss: 0.001521829632\n",
      "Epoch: 4777 | training loss: 0.001521338709\n",
      "Epoch: 4778 | training loss: 0.001520839287\n",
      "Epoch: 4779 | training loss: 0.001520351507\n",
      "Epoch: 4780 | training loss: 0.001519861864\n",
      "Epoch: 4781 | training loss: 0.001519347192\n",
      "Epoch: 4782 | training loss: 0.001518851612\n",
      "Epoch: 4783 | training loss: 0.001518376521\n",
      "Epoch: 4784 | training loss: 0.001517880242\n",
      "Epoch: 4785 | training loss: 0.001517373836\n",
      "Epoch: 4786 | training loss: 0.001516881632\n",
      "Epoch: 4787 | training loss: 0.001516388613\n",
      "Epoch: 4788 | training loss: 0.001515886281\n",
      "Epoch: 4789 | training loss: 0.001515384298\n",
      "Epoch: 4790 | training loss: 0.001514903735\n",
      "Epoch: 4791 | training loss: 0.001514409319\n",
      "Epoch: 4792 | training loss: 0.001513908035\n",
      "Epoch: 4793 | training loss: 0.001513413619\n",
      "Epoch: 4794 | training loss: 0.001512915129\n",
      "Epoch: 4795 | training loss: 0.001512422459\n",
      "Epoch: 4796 | training loss: 0.001511924202\n",
      "Epoch: 4797 | training loss: 0.001511439215\n",
      "Epoch: 4798 | training loss: 0.001510947710\n",
      "Epoch: 4799 | training loss: 0.001510440372\n",
      "Epoch: 4800 | training loss: 0.001509953290\n",
      "Epoch: 4801 | training loss: 0.001509456662\n",
      "Epoch: 4802 | training loss: 0.001508958754\n",
      "Epoch: 4803 | training loss: 0.001508464920\n",
      "Epoch: 4804 | training loss: 0.001507976325\n",
      "Epoch: 4805 | training loss: 0.001507478883\n",
      "Epoch: 4806 | training loss: 0.001506980974\n",
      "Epoch: 4807 | training loss: 0.001506485511\n",
      "Epoch: 4808 | training loss: 0.001505992725\n",
      "Epoch: 4809 | training loss: 0.001505503082\n",
      "Epoch: 4810 | training loss: 0.001505004824\n",
      "Epoch: 4811 | training loss: 0.001504504820\n",
      "Epoch: 4812 | training loss: 0.001504014595\n",
      "Epoch: 4813 | training loss: 0.001503515872\n",
      "Epoch: 4814 | training loss: 0.001503025414\n",
      "Epoch: 4815 | training loss: 0.001502533094\n",
      "Epoch: 4816 | training loss: 0.001502044499\n",
      "Epoch: 4817 | training loss: 0.001501541352\n",
      "Epoch: 4818 | training loss: 0.001501051593\n",
      "Epoch: 4819 | training loss: 0.001500553684\n",
      "Epoch: 4820 | training loss: 0.001500068232\n",
      "Epoch: 4821 | training loss: 0.001499566017\n",
      "Epoch: 4822 | training loss: 0.001499074278\n",
      "Epoch: 4823 | training loss: 0.001498572994\n",
      "Epoch: 4824 | training loss: 0.001498086029\n",
      "Epoch: 4825 | training loss: 0.001497580903\n",
      "Epoch: 4826 | training loss: 0.001497098012\n",
      "Epoch: 4827 | training loss: 0.001496602315\n",
      "Epoch: 4828 | training loss: 0.001496109413\n",
      "Epoch: 4829 | training loss: 0.001495606615\n",
      "Epoch: 4830 | training loss: 0.001495115226\n",
      "Epoch: 4831 | training loss: 0.001494623022\n",
      "Epoch: 4832 | training loss: 0.001494120806\n",
      "Epoch: 4833 | training loss: 0.001493622549\n",
      "Epoch: 4834 | training loss: 0.001493139192\n",
      "Epoch: 4835 | training loss: 0.001492639305\n",
      "Epoch: 4836 | training loss: 0.001492151408\n",
      "Epoch: 4837 | training loss: 0.001491655479\n",
      "Epoch: 4838 | training loss: 0.001491152914\n",
      "Epoch: 4839 | training loss: 0.001490669209\n",
      "Epoch: 4840 | training loss: 0.001490168856\n",
      "Epoch: 4841 | training loss: 0.001489664079\n",
      "Epoch: 4842 | training loss: 0.001489178976\n",
      "Epoch: 4843 | training loss: 0.001488684444\n",
      "Epoch: 4844 | training loss: 0.001488198526\n",
      "Epoch: 4845 | training loss: 0.001487693749\n",
      "Epoch: 4846 | training loss: 0.001487207133\n",
      "Epoch: 4847 | training loss: 0.001486709341\n",
      "Epoch: 4848 | training loss: 0.001486214926\n",
      "Epoch: 4849 | training loss: 0.001485723886\n",
      "Epoch: 4850 | training loss: 0.001485215966\n",
      "Epoch: 4851 | training loss: 0.001484730630\n",
      "Epoch: 4852 | training loss: 0.001484241337\n",
      "Epoch: 4853 | training loss: 0.001483736327\n",
      "Epoch: 4854 | training loss: 0.001483249711\n",
      "Epoch: 4855 | training loss: 0.001482744352\n",
      "Epoch: 4856 | training loss: 0.001482263557\n",
      "Epoch: 4857 | training loss: 0.001481768093\n",
      "Epoch: 4858 | training loss: 0.001481272164\n",
      "Epoch: 4859 | training loss: 0.001480776118\n",
      "Epoch: 4860 | training loss: 0.001480272273\n",
      "Epoch: 4861 | training loss: 0.001479796250\n",
      "Epoch: 4862 | training loss: 0.001479296363\n",
      "Epoch: 4863 | training loss: 0.001478795893\n",
      "Epoch: 4864 | training loss: 0.001478300896\n",
      "Epoch: 4865 | training loss: 0.001477814978\n",
      "Epoch: 4866 | training loss: 0.001477308688\n",
      "Epoch: 4867 | training loss: 0.001476825564\n",
      "Epoch: 4868 | training loss: 0.001476323814\n",
      "Epoch: 4869 | training loss: 0.001475837664\n",
      "Epoch: 4870 | training loss: 0.001475353492\n",
      "Epoch: 4871 | training loss: 0.001474856399\n",
      "Epoch: 4872 | training loss: 0.001474348130\n",
      "Epoch: 4873 | training loss: 0.001473857905\n",
      "Epoch: 4874 | training loss: 0.001473369659\n",
      "Epoch: 4875 | training loss: 0.001472860575\n",
      "Epoch: 4876 | training loss: 0.001472370932\n",
      "Epoch: 4877 | training loss: 0.001471877564\n",
      "Epoch: 4878 | training loss: 0.001471384312\n",
      "Epoch: 4879 | training loss: 0.001470889430\n",
      "Epoch: 4880 | training loss: 0.001470394665\n",
      "Epoch: 4881 | training loss: 0.001469898969\n",
      "Epoch: 4882 | training loss: 0.001469401876\n",
      "Epoch: 4883 | training loss: 0.001468921080\n",
      "Epoch: 4884 | training loss: 0.001468423987\n",
      "Epoch: 4885 | training loss: 0.001467938535\n",
      "Epoch: 4886 | training loss: 0.001467442256\n",
      "Epoch: 4887 | training loss: 0.001466942835\n",
      "Epoch: 4888 | training loss: 0.001466456102\n",
      "Epoch: 4889 | training loss: 0.001465962618\n",
      "Epoch: 4890 | training loss: 0.001465467969\n",
      "Epoch: 4891 | training loss: 0.001464961446\n",
      "Epoch: 4892 | training loss: 0.001464474364\n",
      "Epoch: 4893 | training loss: 0.001463962719\n",
      "Epoch: 4894 | training loss: 0.001463469118\n",
      "Epoch: 4895 | training loss: 0.001462972839\n",
      "Epoch: 4896 | training loss: 0.001462477841\n",
      "Epoch: 4897 | training loss: 0.001461987966\n",
      "Epoch: 4898 | training loss: 0.001461493550\n",
      "Epoch: 4899 | training loss: 0.001460995991\n",
      "Epoch: 4900 | training loss: 0.001460497733\n",
      "Epoch: 4901 | training loss: 0.001460009487\n",
      "Epoch: 4902 | training loss: 0.001459517167\n",
      "Epoch: 4903 | training loss: 0.001459016232\n",
      "Epoch: 4904 | training loss: 0.001458527986\n",
      "Epoch: 4905 | training loss: 0.001458028564\n",
      "Epoch: 4906 | training loss: 0.001457550679\n",
      "Epoch: 4907 | training loss: 0.001457041129\n",
      "Epoch: 4908 | training loss: 0.001456546481\n",
      "Epoch: 4909 | training loss: 0.001456067315\n",
      "Epoch: 4910 | training loss: 0.001455574529\n",
      "Epoch: 4911 | training loss: 0.001455075690\n",
      "Epoch: 4912 | training loss: 0.001454596873\n",
      "Epoch: 4913 | training loss: 0.001454097335\n",
      "Epoch: 4914 | training loss: 0.001453614677\n",
      "Epoch: 4915 | training loss: 0.001453111647\n",
      "Epoch: 4916 | training loss: 0.001452625962\n",
      "Epoch: 4917 | training loss: 0.001452138880\n",
      "Epoch: 4918 | training loss: 0.001451649470\n",
      "Epoch: 4919 | training loss: 0.001451172982\n",
      "Epoch: 4920 | training loss: 0.001450681943\n",
      "Epoch: 4921 | training loss: 0.001450206968\n",
      "Epoch: 4922 | training loss: 0.001449731179\n",
      "Epoch: 4923 | training loss: 0.001449256437\n",
      "Epoch: 4924 | training loss: 0.001448796946\n",
      "Epoch: 4925 | training loss: 0.001448332216\n",
      "Epoch: 4926 | training loss: 0.001447863644\n",
      "Epoch: 4927 | training loss: 0.001447443268\n",
      "Epoch: 4928 | training loss: 0.001447048737\n",
      "Epoch: 4929 | training loss: 0.001446691807\n",
      "Epoch: 4930 | training loss: 0.001446380513\n",
      "Epoch: 4931 | training loss: 0.001446162816\n",
      "Epoch: 4932 | training loss: 0.001446040347\n",
      "Epoch: 4933 | training loss: 0.001446092967\n",
      "Epoch: 4934 | training loss: 0.001446382143\n",
      "Epoch: 4935 | training loss: 0.001447006129\n",
      "Epoch: 4936 | training loss: 0.001448152238\n",
      "Epoch: 4937 | training loss: 0.001450048643\n",
      "Epoch: 4938 | training loss: 0.001453102333\n",
      "Epoch: 4939 | training loss: 0.001457938924\n",
      "Epoch: 4940 | training loss: 0.001465437585\n",
      "Epoch: 4941 | training loss: 0.001477094833\n",
      "Epoch: 4942 | training loss: 0.001495118951\n",
      "Epoch: 4943 | training loss: 0.001523134997\n",
      "Epoch: 4944 | training loss: 0.001566941501\n",
      "Epoch: 4945 | training loss: 0.001635827590\n",
      "Epoch: 4946 | training loss: 0.001744397567\n",
      "Epoch: 4947 | training loss: 0.001916843699\n",
      "Epoch: 4948 | training loss: 0.002191507258\n",
      "Epoch: 4949 | training loss: 0.002632121556\n",
      "Epoch: 4950 | training loss: 0.003338135779\n",
      "Epoch: 4951 | training loss: 0.004476990551\n",
      "Epoch: 4952 | training loss: 0.006300651468\n",
      "Epoch: 4953 | training loss: 0.009225907736\n",
      "Epoch: 4954 | training loss: 0.013809688389\n",
      "Epoch: 4955 | training loss: 0.020894633606\n",
      "Epoch: 4956 | training loss: 0.031169168651\n",
      "Epoch: 4957 | training loss: 0.045164167881\n",
      "Epoch: 4958 | training loss: 0.060988306999\n",
      "Epoch: 4959 | training loss: 0.074521243572\n",
      "Epoch: 4960 | training loss: 0.076537750661\n",
      "Epoch: 4961 | training loss: 0.061856530607\n",
      "Epoch: 4962 | training loss: 0.033630046993\n",
      "Epoch: 4963 | training loss: 0.008715793490\n",
      "Epoch: 4964 | training loss: 0.001748258015\n",
      "Epoch: 4965 | training loss: 0.012674232945\n",
      "Epoch: 4966 | training loss: 0.027631428093\n",
      "Epoch: 4967 | training loss: 0.031374227256\n",
      "Epoch: 4968 | training loss: 0.020795606077\n",
      "Epoch: 4969 | training loss: 0.006407722831\n",
      "Epoch: 4970 | training loss: 0.001553614158\n",
      "Epoch: 4971 | training loss: 0.008018340915\n",
      "Epoch: 4972 | training loss: 0.015960285440\n",
      "Epoch: 4973 | training loss: 0.015864571556\n",
      "Epoch: 4974 | training loss: 0.008247008547\n",
      "Epoch: 4975 | training loss: 0.001948888879\n",
      "Epoch: 4976 | training loss: 0.002881346270\n",
      "Epoch: 4977 | training loss: 0.007951191626\n",
      "Epoch: 4978 | training loss: 0.010083110072\n",
      "Epoch: 4979 | training loss: 0.006810807157\n",
      "Epoch: 4980 | training loss: 0.002406929620\n",
      "Epoch: 4981 | training loss: 0.001704627066\n",
      "Epoch: 4982 | training loss: 0.004412756767\n",
      "Epoch: 4983 | training loss: 0.006376943085\n",
      "Epoch: 4984 | training loss: 0.005101102870\n",
      "Epoch: 4985 | training loss: 0.002401792211\n",
      "Epoch: 4986 | training loss: 0.001449678442\n",
      "Epoch: 4987 | training loss: 0.002796549350\n",
      "Epoch: 4988 | training loss: 0.004202066455\n",
      "Epoch: 4989 | training loss: 0.003794774413\n",
      "Epoch: 4990 | training loss: 0.002227782737\n",
      "Epoch: 4991 | training loss: 0.001414530561\n",
      "Epoch: 4992 | training loss: 0.002033697907\n",
      "Epoch: 4993 | training loss: 0.002954567783\n",
      "Epoch: 4994 | training loss: 0.002916523023\n",
      "Epoch: 4995 | training loss: 0.002044953173\n",
      "Epoch: 4996 | training loss: 0.001429579570\n",
      "Epoch: 4997 | training loss: 0.001662075170\n",
      "Epoch: 4998 | training loss: 0.002234329470\n",
      "Epoch: 4999 | training loss: 0.002344739158\n",
      "Epoch: 5000 | training loss: 0.001892603934\n",
      "Epoch: 5001 | training loss: 0.001455194550\n",
      "Epoch: 5002 | training loss: 0.001488424605\n",
      "Epoch: 5003 | training loss: 0.001820824109\n",
      "Epoch: 5004 | training loss: 0.001972561236\n",
      "Epoch: 5005 | training loss: 0.001768239657\n",
      "Epoch: 5006 | training loss: 0.001476887497\n",
      "Epoch: 5007 | training loss: 0.001418678439\n",
      "Epoch: 5008 | training loss: 0.001589505468\n",
      "Epoch: 5009 | training loss: 0.001729158452\n",
      "Epoch: 5010 | training loss: 0.001665316056\n",
      "Epoch: 5011 | training loss: 0.001488982467\n",
      "Epoch: 5012 | training loss: 0.001401406480\n",
      "Epoch: 5013 | training loss: 0.001467686961\n",
      "Epoch: 5014 | training loss: 0.001571180765\n",
      "Epoch: 5015 | training loss: 0.001577924937\n",
      "Epoch: 5016 | training loss: 0.001487496076\n",
      "Epoch: 5017 | training loss: 0.001407075441\n",
      "Epoch: 5018 | training loss: 0.001412952784\n",
      "Epoch: 5019 | training loss: 0.001474032993\n",
      "Epoch: 5020 | training loss: 0.001506412635\n",
      "Epoch: 5021 | training loss: 0.001473992597\n",
      "Epoch: 5022 | training loss: 0.001417508814\n",
      "Epoch: 5023 | training loss: 0.001395737403\n",
      "Epoch: 5024 | training loss: 0.001419823151\n",
      "Epoch: 5025 | training loss: 0.001451151213\n",
      "Epoch: 5026 | training loss: 0.001451424905\n",
      "Epoch: 5027 | training loss: 0.001422438654\n",
      "Epoch: 5028 | training loss: 0.001396241598\n",
      "Epoch: 5029 | training loss: 0.001396079315\n",
      "Epoch: 5030 | training loss: 0.001414274331\n",
      "Epoch: 5031 | training loss: 0.001426138100\n",
      "Epoch: 5032 | training loss: 0.001418469939\n",
      "Epoch: 5033 | training loss: 0.001400517183\n",
      "Epoch: 5034 | training loss: 0.001390048536\n",
      "Epoch: 5035 | training loss: 0.001394164399\n",
      "Epoch: 5036 | training loss: 0.001404369716\n",
      "Epoch: 5037 | training loss: 0.001407903736\n",
      "Epoch: 5038 | training loss: 0.001401078887\n",
      "Epoch: 5039 | training loss: 0.001391106751\n",
      "Epoch: 5040 | training loss: 0.001386918477\n",
      "Epoch: 5041 | training loss: 0.001390203834\n",
      "Epoch: 5042 | training loss: 0.001395279542\n",
      "Epoch: 5043 | training loss: 0.001395947300\n",
      "Epoch: 5044 | training loss: 0.001391428057\n",
      "Epoch: 5045 | training loss: 0.001385992859\n",
      "Epoch: 5046 | training loss: 0.001384033705\n",
      "Epoch: 5047 | training loss: 0.001385886688\n",
      "Epoch: 5048 | training loss: 0.001388344564\n",
      "Epoch: 5049 | training loss: 0.001388260978\n",
      "Epoch: 5050 | training loss: 0.001385468291\n",
      "Epoch: 5051 | training loss: 0.001382327406\n",
      "Epoch: 5052 | training loss: 0.001381072914\n",
      "Epoch: 5053 | training loss: 0.001381820301\n",
      "Epoch: 5054 | training loss: 0.001382894581\n",
      "Epoch: 5055 | training loss: 0.001382704009\n",
      "Epoch: 5056 | training loss: 0.001381093170\n",
      "Epoch: 5057 | training loss: 0.001379171386\n",
      "Epoch: 5058 | training loss: 0.001378148212\n",
      "Epoch: 5059 | training loss: 0.001378217945\n",
      "Epoch: 5060 | training loss: 0.001378613641\n",
      "Epoch: 5061 | training loss: 0.001378456829\n",
      "Epoch: 5062 | training loss: 0.001377480105\n",
      "Epoch: 5063 | training loss: 0.001376197324\n",
      "Epoch: 5064 | training loss: 0.001375275315\n",
      "Epoch: 5065 | training loss: 0.001374953077\n",
      "Epoch: 5066 | training loss: 0.001374935266\n",
      "Epoch: 5067 | training loss: 0.001374736545\n",
      "Epoch: 5068 | training loss: 0.001374127460\n",
      "Epoch: 5069 | training loss: 0.001373256557\n",
      "Epoch: 5070 | training loss: 0.001372451778\n",
      "Epoch: 5071 | training loss: 0.001371948631\n",
      "Epoch: 5072 | training loss: 0.001371660386\n",
      "Epoch: 5073 | training loss: 0.001371392515\n",
      "Epoch: 5074 | training loss: 0.001370953047\n",
      "Epoch: 5075 | training loss: 0.001370331622\n",
      "Epoch: 5076 | training loss: 0.001369650476\n",
      "Epoch: 5077 | training loss: 0.001369074220\n",
      "Epoch: 5078 | training loss: 0.001368629048\n",
      "Epoch: 5079 | training loss: 0.001368275145\n",
      "Epoch: 5080 | training loss: 0.001367879566\n",
      "Epoch: 5081 | training loss: 0.001367393997\n",
      "Epoch: 5082 | training loss: 0.001366823562\n",
      "Epoch: 5083 | training loss: 0.001366262557\n",
      "Epoch: 5084 | training loss: 0.001365742180\n",
      "Epoch: 5085 | training loss: 0.001365306787\n",
      "Epoch: 5086 | training loss: 0.001364874886\n",
      "Epoch: 5087 | training loss: 0.001364445663\n",
      "Epoch: 5088 | training loss: 0.001363975345\n",
      "Epoch: 5089 | training loss: 0.001363462536\n",
      "Epoch: 5090 | training loss: 0.001362939831\n",
      "Epoch: 5091 | training loss: 0.001362429233\n",
      "Epoch: 5092 | training loss: 0.001361972303\n",
      "Epoch: 5093 | training loss: 0.001361521427\n",
      "Epoch: 5094 | training loss: 0.001361071598\n",
      "Epoch: 5095 | training loss: 0.001360622584\n",
      "Epoch: 5096 | training loss: 0.001360121067\n",
      "Epoch: 5097 | training loss: 0.001359623973\n",
      "Epoch: 5098 | training loss: 0.001359140966\n",
      "Epoch: 5099 | training loss: 0.001358654350\n",
      "Epoch: 5100 | training loss: 0.001358206384\n",
      "Epoch: 5101 | training loss: 0.001357747591\n",
      "Epoch: 5102 | training loss: 0.001357284375\n",
      "Epoch: 5103 | training loss: 0.001356819295\n",
      "Epoch: 5104 | training loss: 0.001356327673\n",
      "Epoch: 5105 | training loss: 0.001355844317\n",
      "Epoch: 5106 | training loss: 0.001355364453\n",
      "Epoch: 5107 | training loss: 0.001354892040\n",
      "Epoch: 5108 | training loss: 0.001354430569\n",
      "Epoch: 5109 | training loss: 0.001353969914\n",
      "Epoch: 5110 | training loss: 0.001353502041\n",
      "Epoch: 5111 | training loss: 0.001353021944\n",
      "Epoch: 5112 | training loss: 0.001352553489\n",
      "Epoch: 5113 | training loss: 0.001352072577\n",
      "Epoch: 5114 | training loss: 0.001351602376\n",
      "Epoch: 5115 | training loss: 0.001351130777\n",
      "Epoch: 5116 | training loss: 0.001350674196\n",
      "Epoch: 5117 | training loss: 0.001350203296\n",
      "Epoch: 5118 | training loss: 0.001349729835\n",
      "Epoch: 5119 | training loss: 0.001349260216\n",
      "Epoch: 5120 | training loss: 0.001348794322\n",
      "Epoch: 5121 | training loss: 0.001348327962\n",
      "Epoch: 5122 | training loss: 0.001347850775\n",
      "Epoch: 5123 | training loss: 0.001347376965\n",
      "Epoch: 5124 | training loss: 0.001346914796\n",
      "Epoch: 5125 | training loss: 0.001346444362\n",
      "Epoch: 5126 | training loss: 0.001345980214\n",
      "Epoch: 5127 | training loss: 0.001345516066\n",
      "Epoch: 5128 | training loss: 0.001345042838\n",
      "Epoch: 5129 | training loss: 0.001344571821\n",
      "Epoch: 5130 | training loss: 0.001344097778\n",
      "Epoch: 5131 | training loss: 0.001343632932\n",
      "Epoch: 5132 | training loss: 0.001343154698\n",
      "Epoch: 5133 | training loss: 0.001342688105\n",
      "Epoch: 5134 | training loss: 0.001342225936\n",
      "Epoch: 5135 | training loss: 0.001341765514\n",
      "Epoch: 5136 | training loss: 0.001341293682\n",
      "Epoch: 5137 | training loss: 0.001340830931\n",
      "Epoch: 5138 | training loss: 0.001340349903\n",
      "Epoch: 5139 | training loss: 0.001339885755\n",
      "Epoch: 5140 | training loss: 0.001339407987\n",
      "Epoch: 5141 | training loss: 0.001338930568\n",
      "Epoch: 5142 | training loss: 0.001338481554\n",
      "Epoch: 5143 | training loss: 0.001338005415\n",
      "Epoch: 5144 | training loss: 0.001337536843\n",
      "Epoch: 5145 | training loss: 0.001337074442\n",
      "Epoch: 5146 | training loss: 0.001336597721\n",
      "Epoch: 5147 | training loss: 0.001336138230\n",
      "Epoch: 5148 | training loss: 0.001335669309\n",
      "Epoch: 5149 | training loss: 0.001335191540\n",
      "Epoch: 5150 | training loss: 0.001334736007\n",
      "Epoch: 5151 | training loss: 0.001334261266\n",
      "Epoch: 5152 | training loss: 0.001333787921\n",
      "Epoch: 5153 | training loss: 0.001333317137\n",
      "Epoch: 5154 | training loss: 0.001332847169\n",
      "Epoch: 5155 | training loss: 0.001332383603\n",
      "Epoch: 5156 | training loss: 0.001331920270\n",
      "Epoch: 5157 | training loss: 0.001331448555\n",
      "Epoch: 5158 | training loss: 0.001330987900\n",
      "Epoch: 5159 | training loss: 0.001330524217\n",
      "Epoch: 5160 | training loss: 0.001330068102\n",
      "Epoch: 5161 | training loss: 0.001329580322\n",
      "Epoch: 5162 | training loss: 0.001329114661\n",
      "Epoch: 5163 | training loss: 0.001328644576\n",
      "Epoch: 5164 | training loss: 0.001328172279\n",
      "Epoch: 5165 | training loss: 0.001327704173\n",
      "Epoch: 5166 | training loss: 0.001327237696\n",
      "Epoch: 5167 | training loss: 0.001326760277\n",
      "Epoch: 5168 | training loss: 0.001326305442\n",
      "Epoch: 5169 | training loss: 0.001325818943\n",
      "Epoch: 5170 | training loss: 0.001325365389\n",
      "Epoch: 5171 | training loss: 0.001324900542\n",
      "Epoch: 5172 | training loss: 0.001324432436\n",
      "Epoch: 5173 | training loss: 0.001323960605\n",
      "Epoch: 5174 | training loss: 0.001323497039\n",
      "Epoch: 5175 | training loss: 0.001323019154\n",
      "Epoch: 5176 | training loss: 0.001322551630\n",
      "Epoch: 5177 | training loss: 0.001322077354\n",
      "Epoch: 5178 | training loss: 0.001321611227\n",
      "Epoch: 5179 | training loss: 0.001321142539\n",
      "Epoch: 5180 | training loss: 0.001320680021\n",
      "Epoch: 5181 | training loss: 0.001320214476\n",
      "Epoch: 5182 | training loss: 0.001319736475\n",
      "Epoch: 5183 | training loss: 0.001319266157\n",
      "Epoch: 5184 | training loss: 0.001318796072\n",
      "Epoch: 5185 | training loss: 0.001318336697\n",
      "Epoch: 5186 | training loss: 0.001317875693\n",
      "Epoch: 5187 | training loss: 0.001317395363\n",
      "Epoch: 5188 | training loss: 0.001316935290\n",
      "Epoch: 5189 | training loss: 0.001316469279\n",
      "Epoch: 5190 | training loss: 0.001316007692\n",
      "Epoch: 5191 | training loss: 0.001315538306\n",
      "Epoch: 5192 | training loss: 0.001315080677\n",
      "Epoch: 5193 | training loss: 0.001314609312\n",
      "Epoch: 5194 | training loss: 0.001314138528\n",
      "Epoch: 5195 | training loss: 0.001313686022\n",
      "Epoch: 5196 | training loss: 0.001313221874\n",
      "Epoch: 5197 | training loss: 0.001312756445\n",
      "Epoch: 5198 | training loss: 0.001312304987\n",
      "Epoch: 5199 | training loss: 0.001311849337\n",
      "Epoch: 5200 | training loss: 0.001311394386\n",
      "Epoch: 5201 | training loss: 0.001310933614\n",
      "Epoch: 5202 | training loss: 0.001310502412\n",
      "Epoch: 5203 | training loss: 0.001310057705\n",
      "Epoch: 5204 | training loss: 0.001309637097\n",
      "Epoch: 5205 | training loss: 0.001309201820\n",
      "Epoch: 5206 | training loss: 0.001308815205\n",
      "Epoch: 5207 | training loss: 0.001308434876\n",
      "Epoch: 5208 | training loss: 0.001308102277\n",
      "Epoch: 5209 | training loss: 0.001307794708\n",
      "Epoch: 5210 | training loss: 0.001307552797\n",
      "Epoch: 5211 | training loss: 0.001307375147\n",
      "Epoch: 5212 | training loss: 0.001307330211\n",
      "Epoch: 5213 | training loss: 0.001307434752\n",
      "Epoch: 5214 | training loss: 0.001307804254\n",
      "Epoch: 5215 | training loss: 0.001308526611\n",
      "Epoch: 5216 | training loss: 0.001309771207\n",
      "Epoch: 5217 | training loss: 0.001311738277\n",
      "Epoch: 5218 | training loss: 0.001314804889\n",
      "Epoch: 5219 | training loss: 0.001319498988\n",
      "Epoch: 5220 | training loss: 0.001326622907\n",
      "Epoch: 5221 | training loss: 0.001337376423\n",
      "Epoch: 5222 | training loss: 0.001353691565\n",
      "Epoch: 5223 | training loss: 0.001378560090\n",
      "Epoch: 5224 | training loss: 0.001416677260\n",
      "Epoch: 5225 | training loss: 0.001475506462\n",
      "Epoch: 5226 | training loss: 0.001566338586\n",
      "Epoch: 5227 | training loss: 0.001707728836\n",
      "Epoch: 5228 | training loss: 0.001928419806\n",
      "Epoch: 5229 | training loss: 0.002275597071\n",
      "Epoch: 5230 | training loss: 0.002821776085\n",
      "Epoch: 5231 | training loss: 0.003686864395\n",
      "Epoch: 5232 | training loss: 0.005049588624\n",
      "Epoch: 5233 | training loss: 0.007204941474\n",
      "Epoch: 5234 | training loss: 0.010551362298\n",
      "Epoch: 5235 | training loss: 0.015707788989\n",
      "Epoch: 5236 | training loss: 0.023270361125\n",
      "Epoch: 5237 | training loss: 0.033912289888\n",
      "Epoch: 5238 | training loss: 0.047002479434\n",
      "Epoch: 5239 | training loss: 0.060645490885\n",
      "Epoch: 5240 | training loss: 0.068681053817\n",
      "Epoch: 5241 | training loss: 0.065226279199\n",
      "Epoch: 5242 | training loss: 0.046670529991\n",
      "Epoch: 5243 | training loss: 0.021554080769\n",
      "Epoch: 5244 | training loss: 0.003933542408\n",
      "Epoch: 5245 | training loss: 0.002975756302\n",
      "Epoch: 5246 | training loss: 0.014594456181\n",
      "Epoch: 5247 | training loss: 0.025955393910\n",
      "Epoch: 5248 | training loss: 0.026511665434\n",
      "Epoch: 5249 | training loss: 0.015903741121\n",
      "Epoch: 5250 | training loss: 0.004377717152\n",
      "Epoch: 5251 | training loss: 0.001620668918\n",
      "Epoch: 5252 | training loss: 0.007606584113\n",
      "Epoch: 5253 | training loss: 0.013996217400\n",
      "Epoch: 5254 | training loss: 0.013482572511\n",
      "Epoch: 5255 | training loss: 0.007107780781\n",
      "Epoch: 5256 | training loss: 0.001804020721\n",
      "Epoch: 5257 | training loss: 0.002333208919\n",
      "Epoch: 5258 | training loss: 0.006480251439\n",
      "Epoch: 5259 | training loss: 0.008627394214\n",
      "Epoch: 5260 | training loss: 0.006399672944\n",
      "Epoch: 5261 | training loss: 0.002601105720\n",
      "Epoch: 5262 | training loss: 0.001329109655\n",
      "Epoch: 5263 | training loss: 0.003172055352\n",
      "Epoch: 5264 | training loss: 0.005210119765\n",
      "Epoch: 5265 | training loss: 0.004892380908\n",
      "Epoch: 5266 | training loss: 0.002762704156\n",
      "Epoch: 5267 | training loss: 0.001325127902\n",
      "Epoch: 5268 | training loss: 0.001856266870\n",
      "Epoch: 5269 | training loss: 0.003213824239\n",
      "Epoch: 5270 | training loss: 0.003569256980\n",
      "Epoch: 5271 | training loss: 0.002568681259\n",
      "Epoch: 5272 | training loss: 0.001463274239\n",
      "Epoch: 5273 | training loss: 0.001391786966\n",
      "Epoch: 5274 | training loss: 0.002135107294\n",
      "Epoch: 5275 | training loss: 0.002619944513\n",
      "Epoch: 5276 | training loss: 0.002271780279\n",
      "Epoch: 5277 | training loss: 0.001561356359\n",
      "Epoch: 5278 | training loss: 0.001275504939\n",
      "Epoch: 5279 | training loss: 0.001588957151\n",
      "Epoch: 5280 | training loss: 0.001988400239\n",
      "Epoch: 5281 | training loss: 0.001969610807\n",
      "Epoch: 5282 | training loss: 0.001589335734\n",
      "Epoch: 5283 | training loss: 0.001290959422\n",
      "Epoch: 5284 | training loss: 0.001348132500\n",
      "Epoch: 5285 | training loss: 0.001599487616\n",
      "Epoch: 5286 | training loss: 0.001707764342\n",
      "Epoch: 5287 | training loss: 0.001558109885\n",
      "Epoch: 5288 | training loss: 0.001336187357\n",
      "Epoch: 5289 | training loss: 0.001271384070\n",
      "Epoch: 5290 | training loss: 0.001383540453\n",
      "Epoch: 5291 | training loss: 0.001503512496\n",
      "Epoch: 5292 | training loss: 0.001488604350\n",
      "Epoch: 5293 | training loss: 0.001366073266\n",
      "Epoch: 5294 | training loss: 0.001271514338\n",
      "Epoch: 5295 | training loss: 0.001286187209\n",
      "Epoch: 5296 | training loss: 0.001363961026\n",
      "Epoch: 5297 | training loss: 0.001403865870\n",
      "Epoch: 5298 | training loss: 0.001364027732\n",
      "Epoch: 5299 | training loss: 0.001292702043\n",
      "Epoch: 5300 | training loss: 0.001261204714\n",
      "Epoch: 5301 | training loss: 0.001287445542\n",
      "Epoch: 5302 | training loss: 0.001328652143\n",
      "Epoch: 5303 | training loss: 0.001335858367\n",
      "Epoch: 5304 | training loss: 0.001304003876\n",
      "Epoch: 5305 | training loss: 0.001267707441\n",
      "Epoch: 5306 | training loss: 0.001259520650\n",
      "Epoch: 5307 | training loss: 0.001278693439\n",
      "Epoch: 5308 | training loss: 0.001298425370\n",
      "Epoch: 5309 | training loss: 0.001296803122\n",
      "Epoch: 5310 | training loss: 0.001276858151\n",
      "Epoch: 5311 | training loss: 0.001258687349\n",
      "Epoch: 5312 | training loss: 0.001256922958\n",
      "Epoch: 5313 | training loss: 0.001268059597\n",
      "Epoch: 5314 | training loss: 0.001277388656\n",
      "Epoch: 5315 | training loss: 0.001274946611\n",
      "Epoch: 5316 | training loss: 0.001263673184\n",
      "Epoch: 5317 | training loss: 0.001254175208\n",
      "Epoch: 5318 | training loss: 0.001253458438\n",
      "Epoch: 5319 | training loss: 0.001259254292\n",
      "Epoch: 5320 | training loss: 0.001263945363\n",
      "Epoch: 5321 | training loss: 0.001262493315\n",
      "Epoch: 5322 | training loss: 0.001256387797\n",
      "Epoch: 5323 | training loss: 0.001250955276\n",
      "Epoch: 5324 | training loss: 0.001249944675\n",
      "Epoch: 5325 | training loss: 0.001252582180\n",
      "Epoch: 5326 | training loss: 0.001255147392\n",
      "Epoch: 5327 | training loss: 0.001254713628\n",
      "Epoch: 5328 | training loss: 0.001251523616\n",
      "Epoch: 5329 | training loss: 0.001248140354\n",
      "Epoch: 5330 | training loss: 0.001246805768\n",
      "Epoch: 5331 | training loss: 0.001247625798\n",
      "Epoch: 5332 | training loss: 0.001248921035\n",
      "Epoch: 5333 | training loss: 0.001248968649\n",
      "Epoch: 5334 | training loss: 0.001247442211\n",
      "Epoch: 5335 | training loss: 0.001245384687\n",
      "Epoch: 5336 | training loss: 0.001244042884\n",
      "Epoch: 5337 | training loss: 0.001243862207\n",
      "Epoch: 5338 | training loss: 0.001244293293\n",
      "Epoch: 5339 | training loss: 0.001244432759\n",
      "Epoch: 5340 | training loss: 0.001243797131\n",
      "Epoch: 5341 | training loss: 0.001242598519\n",
      "Epoch: 5342 | training loss: 0.001241456019\n",
      "Epoch: 5343 | training loss: 0.001240815502\n",
      "Epoch: 5344 | training loss: 0.001240690239\n",
      "Epoch: 5345 | training loss: 0.001240680926\n",
      "Epoch: 5346 | training loss: 0.001240403275\n",
      "Epoch: 5347 | training loss: 0.001239731675\n",
      "Epoch: 5348 | training loss: 0.001238870202\n",
      "Epoch: 5349 | training loss: 0.001238112105\n",
      "Epoch: 5350 | training loss: 0.001237639226\n",
      "Epoch: 5351 | training loss: 0.001237389748\n",
      "Epoch: 5352 | training loss: 0.001237140037\n",
      "Epoch: 5353 | training loss: 0.001236745971\n",
      "Epoch: 5354 | training loss: 0.001236156910\n",
      "Epoch: 5355 | training loss: 0.001235497999\n",
      "Epoch: 5356 | training loss: 0.001234905096\n",
      "Epoch: 5357 | training loss: 0.001234448748\n",
      "Epoch: 5358 | training loss: 0.001234113937\n",
      "Epoch: 5359 | training loss: 0.001233762130\n",
      "Epoch: 5360 | training loss: 0.001233347575\n",
      "Epoch: 5361 | training loss: 0.001232822658\n",
      "Epoch: 5362 | training loss: 0.001232263632\n",
      "Epoch: 5363 | training loss: 0.001231731381\n",
      "Epoch: 5364 | training loss: 0.001231276663\n",
      "Epoch: 5365 | training loss: 0.001230884343\n",
      "Epoch: 5366 | training loss: 0.001230485504\n",
      "Epoch: 5367 | training loss: 0.001230051508\n",
      "Epoch: 5368 | training loss: 0.001229575253\n",
      "Epoch: 5369 | training loss: 0.001229071990\n",
      "Epoch: 5370 | training loss: 0.001228584209\n",
      "Epoch: 5371 | training loss: 0.001228123670\n",
      "Epoch: 5372 | training loss: 0.001227686182\n",
      "Epoch: 5373 | training loss: 0.001227250556\n",
      "Epoch: 5374 | training loss: 0.001226824475\n",
      "Epoch: 5375 | training loss: 0.001226370572\n",
      "Epoch: 5376 | training loss: 0.001225911314\n",
      "Epoch: 5377 | training loss: 0.001225436456\n",
      "Epoch: 5378 | training loss: 0.001224982203\n",
      "Epoch: 5379 | training loss: 0.001224532607\n",
      "Epoch: 5380 | training loss: 0.001224080333\n",
      "Epoch: 5381 | training loss: 0.001223650295\n",
      "Epoch: 5382 | training loss: 0.001223197673\n",
      "Epoch: 5383 | training loss: 0.001222762745\n",
      "Epoch: 5384 | training loss: 0.001222303719\n",
      "Epoch: 5385 | training loss: 0.001221846789\n",
      "Epoch: 5386 | training loss: 0.001221393817\n",
      "Epoch: 5387 | training loss: 0.001220943290\n",
      "Epoch: 5388 | training loss: 0.001220490667\n",
      "Epoch: 5389 | training loss: 0.001220049453\n",
      "Epoch: 5390 | training loss: 0.001219608937\n",
      "Epoch: 5391 | training loss: 0.001219170401\n",
      "Epoch: 5392 | training loss: 0.001218714518\n",
      "Epoch: 5393 | training loss: 0.001218268997\n",
      "Epoch: 5394 | training loss: 0.001217821380\n",
      "Epoch: 5395 | training loss: 0.001217364683\n",
      "Epoch: 5396 | training loss: 0.001216915902\n",
      "Epoch: 5397 | training loss: 0.001216474106\n",
      "Epoch: 5398 | training loss: 0.001216027420\n",
      "Epoch: 5399 | training loss: 0.001215579221\n",
      "Epoch: 5400 | training loss: 0.001215136494\n",
      "Epoch: 5401 | training loss: 0.001214689808\n",
      "Epoch: 5402 | training loss: 0.001214245567\n",
      "Epoch: 5403 | training loss: 0.001213787473\n",
      "Epoch: 5404 | training loss: 0.001213341020\n",
      "Epoch: 5405 | training loss: 0.001212896546\n",
      "Epoch: 5406 | training loss: 0.001212450792\n",
      "Epoch: 5407 | training loss: 0.001211991301\n",
      "Epoch: 5408 | training loss: 0.001211553928\n",
      "Epoch: 5409 | training loss: 0.001211113995\n",
      "Epoch: 5410 | training loss: 0.001210663002\n",
      "Epoch: 5411 | training loss: 0.001210223651\n",
      "Epoch: 5412 | training loss: 0.001209775917\n",
      "Epoch: 5413 | training loss: 0.001209322596\n",
      "Epoch: 5414 | training loss: 0.001208884525\n",
      "Epoch: 5415 | training loss: 0.001208431786\n",
      "Epoch: 5416 | training loss: 0.001207984984\n",
      "Epoch: 5417 | training loss: 0.001207542373\n",
      "Epoch: 5418 | training loss: 0.001207095804\n",
      "Epoch: 5419 | training loss: 0.001206646441\n",
      "Epoch: 5420 | training loss: 0.001206205110\n",
      "Epoch: 5421 | training loss: 0.001205754932\n",
      "Epoch: 5422 | training loss: 0.001205308130\n",
      "Epoch: 5423 | training loss: 0.001204869244\n",
      "Epoch: 5424 | training loss: 0.001204420230\n",
      "Epoch: 5425 | training loss: 0.001203974592\n",
      "Epoch: 5426 | training loss: 0.001203531981\n",
      "Epoch: 5427 | training loss: 0.001203084714\n",
      "Epoch: 5428 | training loss: 0.001202629413\n",
      "Epoch: 5429 | training loss: 0.001202187617\n",
      "Epoch: 5430 | training loss: 0.001201736741\n",
      "Epoch: 5431 | training loss: 0.001201302512\n",
      "Epoch: 5432 | training loss: 0.001200858969\n",
      "Epoch: 5433 | training loss: 0.001200412167\n",
      "Epoch: 5434 | training loss: 0.001199960709\n",
      "Epoch: 5435 | training loss: 0.001199519029\n",
      "Epoch: 5436 | training loss: 0.001199068152\n",
      "Epoch: 5437 | training loss: 0.001198627520\n",
      "Epoch: 5438 | training loss: 0.001198171172\n",
      "Epoch: 5439 | training loss: 0.001197734615\n",
      "Epoch: 5440 | training loss: 0.001197295263\n",
      "Epoch: 5441 | training loss: 0.001196842641\n",
      "Epoch: 5442 | training loss: 0.001196394558\n",
      "Epoch: 5443 | training loss: 0.001195951132\n",
      "Epoch: 5444 | training loss: 0.001195508288\n",
      "Epoch: 5445 | training loss: 0.001195056597\n",
      "Epoch: 5446 | training loss: 0.001194604672\n",
      "Epoch: 5447 | training loss: 0.001194160781\n",
      "Epoch: 5448 | training loss: 0.001193713280\n",
      "Epoch: 5449 | training loss: 0.001193271950\n",
      "Epoch: 5450 | training loss: 0.001192824566\n",
      "Epoch: 5451 | training loss: 0.001192364609\n",
      "Epoch: 5452 | training loss: 0.001191931777\n",
      "Epoch: 5453 | training loss: 0.001191484276\n",
      "Epoch: 5454 | training loss: 0.001191040268\n",
      "Epoch: 5455 | training loss: 0.001190588810\n",
      "Epoch: 5456 | training loss: 0.001190145267\n",
      "Epoch: 5457 | training loss: 0.001189700910\n",
      "Epoch: 5458 | training loss: 0.001189250033\n",
      "Epoch: 5459 | training loss: 0.001188802882\n",
      "Epoch: 5460 | training loss: 0.001188359456\n",
      "Epoch: 5461 | training loss: 0.001187921502\n",
      "Epoch: 5462 | training loss: 0.001187475398\n",
      "Epoch: 5463 | training loss: 0.001187031856\n",
      "Epoch: 5464 | training loss: 0.001186586451\n",
      "Epoch: 5465 | training loss: 0.001186131849\n",
      "Epoch: 5466 | training loss: 0.001185694942\n",
      "Epoch: 5467 | training loss: 0.001185256988\n",
      "Epoch: 5468 | training loss: 0.001184804831\n",
      "Epoch: 5469 | training loss: 0.001184362452\n",
      "Epoch: 5470 | training loss: 0.001183916000\n",
      "Epoch: 5471 | training loss: 0.001183471875\n",
      "Epoch: 5472 | training loss: 0.001183030196\n",
      "Epoch: 5473 | training loss: 0.001182592590\n",
      "Epoch: 5474 | training loss: 0.001182161854\n",
      "Epoch: 5475 | training loss: 0.001181733329\n",
      "Epoch: 5476 | training loss: 0.001181293046\n",
      "Epoch: 5477 | training loss: 0.001180877793\n",
      "Epoch: 5478 | training loss: 0.001180473017\n",
      "Epoch: 5479 | training loss: 0.001180073363\n",
      "Epoch: 5480 | training loss: 0.001179692219\n",
      "Epoch: 5481 | training loss: 0.001179330633\n",
      "Epoch: 5482 | training loss: 0.001179002924\n",
      "Epoch: 5483 | training loss: 0.001178738195\n",
      "Epoch: 5484 | training loss: 0.001178555191\n",
      "Epoch: 5485 | training loss: 0.001178485574\n",
      "Epoch: 5486 | training loss: 0.001178557053\n",
      "Epoch: 5487 | training loss: 0.001178882550\n",
      "Epoch: 5488 | training loss: 0.001179573243\n",
      "Epoch: 5489 | training loss: 0.001180778956\n",
      "Epoch: 5490 | training loss: 0.001182781532\n",
      "Epoch: 5491 | training loss: 0.001186031965\n",
      "Epoch: 5492 | training loss: 0.001191226067\n",
      "Epoch: 5493 | training loss: 0.001199434046\n",
      "Epoch: 5494 | training loss: 0.001212393399\n",
      "Epoch: 5495 | training loss: 0.001232852228\n",
      "Epoch: 5496 | training loss: 0.001265187748\n",
      "Epoch: 5497 | training loss: 0.001316599082\n",
      "Epoch: 5498 | training loss: 0.001398422406\n",
      "Epoch: 5499 | training loss: 0.001529555302\n",
      "Epoch: 5500 | training loss: 0.001740503940\n",
      "Epoch: 5501 | training loss: 0.002082477324\n",
      "Epoch: 5502 | training loss: 0.002636573277\n",
      "Epoch: 5503 | training loss: 0.003540745005\n",
      "Epoch: 5504 | training loss: 0.005007785745\n",
      "Epoch: 5505 | training loss: 0.007396269590\n",
      "Epoch: 5506 | training loss: 0.011212036945\n",
      "Epoch: 5507 | training loss: 0.017253946513\n",
      "Epoch: 5508 | training loss: 0.026323547587\n",
      "Epoch: 5509 | training loss: 0.039289705455\n",
      "Epoch: 5510 | training loss: 0.055177226663\n",
      "Epoch: 5511 | training loss: 0.071001037955\n",
      "Epoch: 5512 | training loss: 0.077963493764\n",
      "Epoch: 5513 | training loss: 0.068801812828\n",
      "Epoch: 5514 | training loss: 0.042276855558\n",
      "Epoch: 5515 | training loss: 0.013766393997\n",
      "Epoch: 5516 | training loss: 0.001217171433\n",
      "Epoch: 5517 | training loss: 0.009355499409\n",
      "Epoch: 5518 | training loss: 0.025561437011\n",
      "Epoch: 5519 | training loss: 0.032148174942\n",
      "Epoch: 5520 | training loss: 0.022841880098\n",
      "Epoch: 5521 | training loss: 0.007398448419\n",
      "Epoch: 5522 | training loss: 0.001202049549\n",
      "Epoch: 5523 | training loss: 0.007616848685\n",
      "Epoch: 5524 | training loss: 0.016125803813\n",
      "Epoch: 5525 | training loss: 0.015907350928\n",
      "Epoch: 5526 | training loss: 0.007647187915\n",
      "Epoch: 5527 | training loss: 0.001444850117\n",
      "Epoch: 5528 | training loss: 0.003306752071\n",
      "Epoch: 5529 | training loss: 0.008747647516\n",
      "Epoch: 5530 | training loss: 0.009938129224\n",
      "Epoch: 5531 | training loss: 0.005578832235\n",
      "Epoch: 5532 | training loss: 0.001496014651\n",
      "Epoch: 5533 | training loss: 0.002151232678\n",
      "Epoch: 5534 | training loss: 0.005387185141\n",
      "Epoch: 5535 | training loss: 0.006301964168\n",
      "Epoch: 5536 | training loss: 0.003809726099\n",
      "Epoch: 5537 | training loss: 0.001365027274\n",
      "Epoch: 5538 | training loss: 0.001742090681\n",
      "Epoch: 5539 | training loss: 0.003664612770\n",
      "Epoch: 5540 | training loss: 0.004172548652\n",
      "Epoch: 5541 | training loss: 0.002667772351\n",
      "Epoch: 5542 | training loss: 0.001254490344\n",
      "Epoch: 5543 | training loss: 0.001536028227\n",
      "Epoch: 5544 | training loss: 0.002676865319\n",
      "Epoch: 5545 | training loss: 0.002927444875\n",
      "Epoch: 5546 | training loss: 0.002009041375\n",
      "Epoch: 5547 | training loss: 0.001197491423\n",
      "Epoch: 5548 | training loss: 0.001398560125\n",
      "Epoch: 5549 | training loss: 0.002073402517\n",
      "Epoch: 5550 | training loss: 0.002197814174\n",
      "Epoch: 5551 | training loss: 0.001643657568\n",
      "Epoch: 5552 | training loss: 0.001172058168\n",
      "Epoch: 5553 | training loss: 0.001299362164\n",
      "Epoch: 5554 | training loss: 0.001698076841\n",
      "Epoch: 5555 | training loss: 0.001770367613\n",
      "Epoch: 5556 | training loss: 0.001443747547\n",
      "Epoch: 5557 | training loss: 0.001161055290\n",
      "Epoch: 5558 | training loss: 0.001228677807\n",
      "Epoch: 5559 | training loss: 0.001464895089\n",
      "Epoch: 5560 | training loss: 0.001518594334\n",
      "Epoch: 5561 | training loss: 0.001332444837\n",
      "Epoch: 5562 | training loss: 0.001156375045\n",
      "Epoch: 5563 | training loss: 0.001181716798\n",
      "Epoch: 5564 | training loss: 0.001320495736\n",
      "Epoch: 5565 | training loss: 0.001366462326\n",
      "Epoch: 5566 | training loss: 0.001266464475\n",
      "Epoch: 5567 | training loss: 0.001154386438\n",
      "Epoch: 5568 | training loss: 0.001153817284\n",
      "Epoch: 5569 | training loss: 0.001233036630\n",
      "Epoch: 5570 | training loss: 0.001273007016\n",
      "Epoch: 5571 | training loss: 0.001224868000\n",
      "Epoch: 5572 | training loss: 0.001153492020\n",
      "Epoch: 5573 | training loss: 0.001139190630\n",
      "Epoch: 5574 | training loss: 0.001180928666\n",
      "Epoch: 5575 | training loss: 0.001213525655\n",
      "Epoch: 5576 | training loss: 0.001195513876\n",
      "Epoch: 5577 | training loss: 0.001152141253\n",
      "Epoch: 5578 | training loss: 0.001133075217\n",
      "Epoch: 5579 | training loss: 0.001151092583\n",
      "Epoch: 5580 | training loss: 0.001174918143\n",
      "Epoch: 5581 | training loss: 0.001173191471\n",
      "Epoch: 5582 | training loss: 0.001149465796\n",
      "Epoch: 5583 | training loss: 0.001131533878\n",
      "Epoch: 5584 | training loss: 0.001135592815\n",
      "Epoch: 5585 | training loss: 0.001150511089\n",
      "Epoch: 5586 | training loss: 0.001155581907\n",
      "Epoch: 5587 | training loss: 0.001145138987\n",
      "Epoch: 5588 | training loss: 0.001131545054\n",
      "Epoch: 5589 | training loss: 0.001128366333\n",
      "Epoch: 5590 | training loss: 0.001135436585\n",
      "Epoch: 5591 | training loss: 0.001141695539\n",
      "Epoch: 5592 | training loss: 0.001139192027\n",
      "Epoch: 5593 | training loss: 0.001130977646\n",
      "Epoch: 5594 | training loss: 0.001125609269\n",
      "Epoch: 5595 | training loss: 0.001127083320\n",
      "Epoch: 5596 | training loss: 0.001131488476\n",
      "Epoch: 5597 | training loss: 0.001132684294\n",
      "Epoch: 5598 | training loss: 0.001129122917\n",
      "Epoch: 5599 | training loss: 0.001124464441\n",
      "Epoch: 5600 | training loss: 0.001122826361\n",
      "Epoch: 5601 | training loss: 0.001124474569\n",
      "Epoch: 5602 | training loss: 0.001126358286\n",
      "Epoch: 5603 | training loss: 0.001125807874\n",
      "Epoch: 5604 | training loss: 0.001123149414\n",
      "Epoch: 5605 | training loss: 0.001120763016\n",
      "Epoch: 5606 | training loss: 0.001120302943\n",
      "Epoch: 5607 | training loss: 0.001121245325\n",
      "Epoch: 5608 | training loss: 0.001121808775\n",
      "Epoch: 5609 | training loss: 0.001120964997\n",
      "Epoch: 5610 | training loss: 0.001119220280\n",
      "Epoch: 5611 | training loss: 0.001117892447\n",
      "Epoch: 5612 | training loss: 0.001117623644\n",
      "Epoch: 5613 | training loss: 0.001117980340\n",
      "Epoch: 5614 | training loss: 0.001117999782\n",
      "Epoch: 5615 | training loss: 0.001117254142\n",
      "Epoch: 5616 | training loss: 0.001116108382\n",
      "Epoch: 5617 | training loss: 0.001115225139\n",
      "Epoch: 5618 | training loss: 0.001114896615\n",
      "Epoch: 5619 | training loss: 0.001114888932\n",
      "Epoch: 5620 | training loss: 0.001114703249\n",
      "Epoch: 5621 | training loss: 0.001114113955\n",
      "Epoch: 5622 | training loss: 0.001113291830\n",
      "Epoch: 5623 | training loss: 0.001112604165\n",
      "Epoch: 5624 | training loss: 0.001112212427\n",
      "Epoch: 5625 | training loss: 0.001112003345\n",
      "Epoch: 5626 | training loss: 0.001111731748\n",
      "Epoch: 5627 | training loss: 0.001111253165\n",
      "Epoch: 5628 | training loss: 0.001110618352\n",
      "Epoch: 5629 | training loss: 0.001110022189\n",
      "Epoch: 5630 | training loss: 0.001109578880\n",
      "Epoch: 5631 | training loss: 0.001109240693\n",
      "Epoch: 5632 | training loss: 0.001108913450\n",
      "Epoch: 5633 | training loss: 0.001108486671\n",
      "Epoch: 5634 | training loss: 0.001107973862\n",
      "Epoch: 5635 | training loss: 0.001107448479\n",
      "Epoch: 5636 | training loss: 0.001106978511\n",
      "Epoch: 5637 | training loss: 0.001106591546\n",
      "Epoch: 5638 | training loss: 0.001106220530\n",
      "Epoch: 5639 | training loss: 0.001105828676\n",
      "Epoch: 5640 | training loss: 0.001105363830\n",
      "Epoch: 5641 | training loss: 0.001104883500\n",
      "Epoch: 5642 | training loss: 0.001104413997\n",
      "Epoch: 5643 | training loss: 0.001103994786\n",
      "Epoch: 5644 | training loss: 0.001103595365\n",
      "Epoch: 5645 | training loss: 0.001103204559\n",
      "Epoch: 5646 | training loss: 0.001102789887\n",
      "Epoch: 5647 | training loss: 0.001102335053\n",
      "Epoch: 5648 | training loss: 0.001101876143\n",
      "Epoch: 5649 | training loss: 0.001101437490\n",
      "Epoch: 5650 | training loss: 0.001101005473\n",
      "Epoch: 5651 | training loss: 0.001100607333\n",
      "Epoch: 5652 | training loss: 0.001100193476\n",
      "Epoch: 5653 | training loss: 0.001099760644\n",
      "Epoch: 5654 | training loss: 0.001099328278\n",
      "Epoch: 5655 | training loss: 0.001098891720\n",
      "Epoch: 5656 | training loss: 0.001098463894\n",
      "Epoch: 5657 | training loss: 0.001098044799\n",
      "Epoch: 5658 | training loss: 0.001097624307\n",
      "Epoch: 5659 | training loss: 0.001097198110\n",
      "Epoch: 5660 | training loss: 0.001096778316\n",
      "Epoch: 5661 | training loss: 0.001096349908\n",
      "Epoch: 5662 | training loss: 0.001095929416\n",
      "Epoch: 5663 | training loss: 0.001095498446\n",
      "Epoch: 5664 | training loss: 0.001095069689\n",
      "Epoch: 5665 | training loss: 0.001094650710\n",
      "Epoch: 5666 | training loss: 0.001094232895\n",
      "Epoch: 5667 | training loss: 0.001093809726\n",
      "Epoch: 5668 | training loss: 0.001093387022\n",
      "Epoch: 5669 | training loss: 0.001092967577\n",
      "Epoch: 5670 | training loss: 0.001092539285\n",
      "Epoch: 5671 | training loss: 0.001092117047\n",
      "Epoch: 5672 | training loss: 0.001091696089\n",
      "Epoch: 5673 | training loss: 0.001091274782\n",
      "Epoch: 5674 | training loss: 0.001090852078\n",
      "Epoch: 5675 | training loss: 0.001090430655\n",
      "Epoch: 5676 | training loss: 0.001090012607\n",
      "Epoch: 5677 | training loss: 0.001089589437\n",
      "Epoch: 5678 | training loss: 0.001089173369\n",
      "Epoch: 5679 | training loss: 0.001088744262\n",
      "Epoch: 5680 | training loss: 0.001088313991\n",
      "Epoch: 5681 | training loss: 0.001087902347\n",
      "Epoch: 5682 | training loss: 0.001087482320\n",
      "Epoch: 5683 | training loss: 0.001087067300\n",
      "Epoch: 5684 | training loss: 0.001086635864\n",
      "Epoch: 5685 | training loss: 0.001086215954\n",
      "Epoch: 5686 | training loss: 0.001085792668\n",
      "Epoch: 5687 | training loss: 0.001085371478\n",
      "Epoch: 5688 | training loss: 0.001084949705\n",
      "Epoch: 5689 | training loss: 0.001084531192\n",
      "Epoch: 5690 | training loss: 0.001084115356\n",
      "Epoch: 5691 | training loss: 0.001083699171\n",
      "Epoch: 5692 | training loss: 0.001083269948\n",
      "Epoch: 5693 | training loss: 0.001082841074\n",
      "Epoch: 5694 | training loss: 0.001082425355\n",
      "Epoch: 5695 | training loss: 0.001082003582\n",
      "Epoch: 5696 | training loss: 0.001081583323\n",
      "Epoch: 5697 | training loss: 0.001081162831\n",
      "Epoch: 5698 | training loss: 0.001080740243\n",
      "Epoch: 5699 | training loss: 0.001080322778\n",
      "Epoch: 5700 | training loss: 0.001079905778\n",
      "Epoch: 5701 | training loss: 0.001079478767\n",
      "Epoch: 5702 | training loss: 0.001079058973\n",
      "Epoch: 5703 | training loss: 0.001078640926\n",
      "Epoch: 5704 | training loss: 0.001078211120\n",
      "Epoch: 5705 | training loss: 0.001077799592\n",
      "Epoch: 5706 | training loss: 0.001077377936\n",
      "Epoch: 5707 | training loss: 0.001076959074\n",
      "Epoch: 5708 | training loss: 0.001076542307\n",
      "Epoch: 5709 | training loss: 0.001076115528\n",
      "Epoch: 5710 | training loss: 0.001075692009\n",
      "Epoch: 5711 | training loss: 0.001075273147\n",
      "Epoch: 5712 | training loss: 0.001074857777\n",
      "Epoch: 5713 | training loss: 0.001074430416\n",
      "Epoch: 5714 | training loss: 0.001074011088\n",
      "Epoch: 5715 | training loss: 0.001073594671\n",
      "Epoch: 5716 | training loss: 0.001073169988\n",
      "Epoch: 5717 | training loss: 0.001072749263\n",
      "Epoch: 5718 | training loss: 0.001072324114\n",
      "Epoch: 5719 | training loss: 0.001071914332\n",
      "Epoch: 5720 | training loss: 0.001071488950\n",
      "Epoch: 5721 | training loss: 0.001071066712\n",
      "Epoch: 5722 | training loss: 0.001070637023\n",
      "Epoch: 5723 | training loss: 0.001070230152\n",
      "Epoch: 5724 | training loss: 0.001069805934\n",
      "Epoch: 5725 | training loss: 0.001069384511\n",
      "Epoch: 5726 | training loss: 0.001068957732\n",
      "Epoch: 5727 | training loss: 0.001068545738\n",
      "Epoch: 5728 | training loss: 0.001068125828\n",
      "Epoch: 5729 | training loss: 0.001067701261\n",
      "Epoch: 5730 | training loss: 0.001067282865\n",
      "Epoch: 5731 | training loss: 0.001066867495\n",
      "Epoch: 5732 | training loss: 0.001066442113\n",
      "Epoch: 5733 | training loss: 0.001066016615\n",
      "Epoch: 5734 | training loss: 0.001065602410\n",
      "Epoch: 5735 | training loss: 0.001065187389\n",
      "Epoch: 5736 | training loss: 0.001064761775\n",
      "Epoch: 5737 | training loss: 0.001064340584\n",
      "Epoch: 5738 | training loss: 0.001063918229\n",
      "Epoch: 5739 | training loss: 0.001063497039\n",
      "Epoch: 5740 | training loss: 0.001063071191\n",
      "Epoch: 5741 | training loss: 0.001062655589\n",
      "Epoch: 5742 | training loss: 0.001062233816\n",
      "Epoch: 5743 | training loss: 0.001061810064\n",
      "Epoch: 5744 | training loss: 0.001061396324\n",
      "Epoch: 5745 | training loss: 0.001060971292\n",
      "Epoch: 5746 | training loss: 0.001060548471\n",
      "Epoch: 5747 | training loss: 0.001060131704\n",
      "Epoch: 5748 | training loss: 0.001059711794\n",
      "Epoch: 5749 | training loss: 0.001059287461\n",
      "Epoch: 5750 | training loss: 0.001058869762\n",
      "Epoch: 5751 | training loss: 0.001058446942\n",
      "Epoch: 5752 | training loss: 0.001058028894\n",
      "Epoch: 5753 | training loss: 0.001057607937\n",
      "Epoch: 5754 | training loss: 0.001057189540\n",
      "Epoch: 5755 | training loss: 0.001056770794\n",
      "Epoch: 5756 | training loss: 0.001056347042\n",
      "Epoch: 5757 | training loss: 0.001055925502\n",
      "Epoch: 5758 | training loss: 0.001055505360\n",
      "Epoch: 5759 | training loss: 0.001055089291\n",
      "Epoch: 5760 | training loss: 0.001054672990\n",
      "Epoch: 5761 | training loss: 0.001054251567\n",
      "Epoch: 5762 | training loss: 0.001053825603\n",
      "Epoch: 5763 | training loss: 0.001053409069\n",
      "Epoch: 5764 | training loss: 0.001052982523\n",
      "Epoch: 5765 | training loss: 0.001052559353\n",
      "Epoch: 5766 | training loss: 0.001052138978\n",
      "Epoch: 5767 | training loss: 0.001051723259\n",
      "Epoch: 5768 | training loss: 0.001051300904\n",
      "Epoch: 5769 | training loss: 0.001050878083\n",
      "Epoch: 5770 | training loss: 0.001050451305\n",
      "Epoch: 5771 | training loss: 0.001050028834\n",
      "Epoch: 5772 | training loss: 0.001049624057\n",
      "Epoch: 5773 | training loss: 0.001049200073\n",
      "Epoch: 5774 | training loss: 0.001048777252\n",
      "Epoch: 5775 | training loss: 0.001048358507\n",
      "Epoch: 5776 | training loss: 0.001047944417\n",
      "Epoch: 5777 | training loss: 0.001047537429\n",
      "Epoch: 5778 | training loss: 0.001047124038\n",
      "Epoch: 5779 | training loss: 0.001046717050\n",
      "Epoch: 5780 | training loss: 0.001046319376\n",
      "Epoch: 5781 | training loss: 0.001045926241\n",
      "Epoch: 5782 | training loss: 0.001045543468\n",
      "Epoch: 5783 | training loss: 0.001045175595\n",
      "Epoch: 5784 | training loss: 0.001044823090\n",
      "Epoch: 5785 | training loss: 0.001044517150\n",
      "Epoch: 5786 | training loss: 0.001044269651\n",
      "Epoch: 5787 | training loss: 0.001044088393\n",
      "Epoch: 5788 | training loss: 0.001044028439\n",
      "Epoch: 5789 | training loss: 0.001044147648\n",
      "Epoch: 5790 | training loss: 0.001044498757\n",
      "Epoch: 5791 | training loss: 0.001045260695\n",
      "Epoch: 5792 | training loss: 0.001046633814\n",
      "Epoch: 5793 | training loss: 0.001048973063\n",
      "Epoch: 5794 | training loss: 0.001052770996\n",
      "Epoch: 5795 | training loss: 0.001058866852\n",
      "Epoch: 5796 | training loss: 0.001068618847\n",
      "Epoch: 5797 | training loss: 0.001084119896\n",
      "Epoch: 5798 | training loss: 0.001108732074\n",
      "Epoch: 5799 | training loss: 0.001147995237\n",
      "Epoch: 5800 | training loss: 0.001210906310\n",
      "Epoch: 5801 | training loss: 0.001312019769\n",
      "Epoch: 5802 | training loss: 0.001475817640\n",
      "Epoch: 5803 | training loss: 0.001741595450\n",
      "Epoch: 5804 | training loss: 0.002176234964\n",
      "Epoch: 5805 | training loss: 0.002886706032\n",
      "Epoch: 5806 | training loss: 0.004055108875\n",
      "Epoch: 5807 | training loss: 0.005960260984\n",
      "Epoch: 5808 | training loss: 0.009071405046\n",
      "Epoch: 5809 | training loss: 0.014021321200\n",
      "Epoch: 5810 | training loss: 0.021772162989\n",
      "Epoch: 5811 | training loss: 0.033053554595\n",
      "Epoch: 5812 | training loss: 0.048295382410\n",
      "Epoch: 5813 | training loss: 0.064741685987\n",
      "Epoch: 5814 | training loss: 0.076931923628\n",
      "Epoch: 5815 | training loss: 0.074172958732\n",
      "Epoch: 5816 | training loss: 0.053233396262\n",
      "Epoch: 5817 | training loss: 0.022645013407\n",
      "Epoch: 5818 | training loss: 0.002825503470\n",
      "Epoch: 5819 | training loss: 0.004850063473\n",
      "Epoch: 5820 | training loss: 0.020726570860\n",
      "Epoch: 5821 | training loss: 0.032025717199\n",
      "Epoch: 5822 | training loss: 0.026673691347\n",
      "Epoch: 5823 | training loss: 0.011004807428\n",
      "Epoch: 5824 | training loss: 0.001264204737\n",
      "Epoch: 5825 | training loss: 0.005362198688\n",
      "Epoch: 5826 | training loss: 0.014870972373\n",
      "Epoch: 5827 | training loss: 0.016793660820\n",
      "Epoch: 5828 | training loss: 0.009225640446\n",
      "Epoch: 5829 | training loss: 0.001786944107\n",
      "Epoch: 5830 | training loss: 0.002544589108\n",
      "Epoch: 5831 | training loss: 0.008222245611\n",
      "Epoch: 5832 | training loss: 0.010148058645\n",
      "Epoch: 5833 | training loss: 0.005986831151\n",
      "Epoch: 5834 | training loss: 0.001500161830\n",
      "Epoch: 5835 | training loss: 0.001938092988\n",
      "Epoch: 5836 | training loss: 0.005329328589\n",
      "Epoch: 5837 | training loss: 0.006308421958\n",
      "Epoch: 5838 | training loss: 0.003682313720\n",
      "Epoch: 5839 | training loss: 0.001188819297\n",
      "Epoch: 5840 | training loss: 0.001757068909\n",
      "Epoch: 5841 | training loss: 0.003778913990\n",
      "Epoch: 5842 | training loss: 0.004060749430\n",
      "Epoch: 5843 | training loss: 0.002335954923\n",
      "Epoch: 5844 | training loss: 0.001049373765\n",
      "Epoch: 5845 | training loss: 0.001634092070\n",
      "Epoch: 5846 | training loss: 0.002796704881\n",
      "Epoch: 5847 | training loss: 0.002739230637\n",
      "Epoch: 5848 | training loss: 0.001636226894\n",
      "Epoch: 5849 | training loss: 0.001019137097\n",
      "Epoch: 5850 | training loss: 0.001501739258\n",
      "Epoch: 5851 | training loss: 0.002141573234\n",
      "Epoch: 5852 | training loss: 0.001976692583\n",
      "Epoch: 5853 | training loss: 0.001298412099\n",
      "Epoch: 5854 | training loss: 0.001022234443\n",
      "Epoch: 5855 | training loss: 0.001369445701\n",
      "Epoch: 5856 | training loss: 0.001710566692\n",
      "Epoch: 5857 | training loss: 0.001549653127\n",
      "Epoch: 5858 | training loss: 0.001144904178\n",
      "Epoch: 5859 | training loss: 0.001025199075\n",
      "Epoch: 5860 | training loss: 0.001253408496\n",
      "Epoch: 5861 | training loss: 0.001434939681\n",
      "Epoch: 5862 | training loss: 0.001314256690\n",
      "Epoch: 5863 | training loss: 0.001075990265\n",
      "Epoch: 5864 | training loss: 0.001022013254\n",
      "Epoch: 5865 | training loss: 0.001163538313\n",
      "Epoch: 5866 | training loss: 0.001263731159\n",
      "Epoch: 5867 | training loss: 0.001185629051\n",
      "Epoch: 5868 | training loss: 0.001045116456\n",
      "Epoch: 5869 | training loss: 0.001015223563\n",
      "Epoch: 5870 | training loss: 0.001099073212\n",
      "Epoch: 5871 | training loss: 0.001158848172\n",
      "Epoch: 5872 | training loss: 0.001113970298\n",
      "Epoch: 5873 | training loss: 0.001029906096\n",
      "Epoch: 5874 | training loss: 0.001008693478\n",
      "Epoch: 5875 | training loss: 0.001056281733\n",
      "Epoch: 5876 | training loss: 0.001094187726\n",
      "Epoch: 5877 | training loss: 0.001071553910\n",
      "Epoch: 5878 | training loss: 0.001021320466\n",
      "Epoch: 5879 | training loss: 0.001003730576\n",
      "Epoch: 5880 | training loss: 0.001028725179\n",
      "Epoch: 5881 | training loss: 0.001053920249\n",
      "Epoch: 5882 | training loss: 0.001045252080\n",
      "Epoch: 5883 | training loss: 0.001015683170\n",
      "Epoch: 5884 | training loss: 0.001000642311\n",
      "Epoch: 5885 | training loss: 0.001011877088\n",
      "Epoch: 5886 | training loss: 0.001028381521\n",
      "Epoch: 5887 | training loss: 0.001027332619\n",
      "Epoch: 5888 | training loss: 0.001010937151\n",
      "Epoch: 5889 | training loss: 0.000998804928\n",
      "Epoch: 5890 | training loss: 0.001002016943\n",
      "Epoch: 5891 | training loss: 0.001012142515\n",
      "Epoch: 5892 | training loss: 0.001014661393\n",
      "Epoch: 5893 | training loss: 0.001006607665\n",
      "Epoch: 5894 | training loss: 0.000997574301\n",
      "Epoch: 5895 | training loss: 0.000996456947\n",
      "Epoch: 5896 | training loss: 0.001001715311\n",
      "Epoch: 5897 | training loss: 0.001005146652\n",
      "Epoch: 5898 | training loss: 0.001002215897\n",
      "Epoch: 5899 | training loss: 0.000996298506\n",
      "Epoch: 5900 | training loss: 0.000993408263\n",
      "Epoch: 5901 | training loss: 0.000995175447\n",
      "Epoch: 5902 | training loss: 0.000997899100\n",
      "Epoch: 5903 | training loss: 0.000997647643\n",
      "Epoch: 5904 | training loss: 0.000994479749\n",
      "Epoch: 5905 | training loss: 0.000991555396\n",
      "Epoch: 5906 | training loss: 0.000991211389\n",
      "Epoch: 5907 | training loss: 0.000992631307\n",
      "Epoch: 5908 | training loss: 0.000993330730\n",
      "Epoch: 5909 | training loss: 0.000992099871\n",
      "Epoch: 5910 | training loss: 0.000989954337\n",
      "Epoch: 5911 | training loss: 0.000988699961\n",
      "Epoch: 5912 | training loss: 0.000988851883\n",
      "Epoch: 5913 | training loss: 0.000989437220\n",
      "Epoch: 5914 | training loss: 0.000989234541\n",
      "Epoch: 5915 | training loss: 0.000988083542\n",
      "Epoch: 5916 | training loss: 0.000986810657\n",
      "Epoch: 5917 | training loss: 0.000986200059\n",
      "Epoch: 5918 | training loss: 0.000986252096\n",
      "Epoch: 5919 | training loss: 0.000986314029\n",
      "Epoch: 5920 | training loss: 0.000985873397\n",
      "Epoch: 5921 | training loss: 0.000985011808\n",
      "Epoch: 5922 | training loss: 0.000984167098\n",
      "Epoch: 5923 | training loss: 0.000983717619\n",
      "Epoch: 5924 | training loss: 0.000983573496\n",
      "Epoch: 5925 | training loss: 0.000983386068\n",
      "Epoch: 5926 | training loss: 0.000982930185\n",
      "Epoch: 5927 | training loss: 0.000982260797\n",
      "Epoch: 5928 | training loss: 0.000981653342\n",
      "Epoch: 5929 | training loss: 0.000981240533\n",
      "Epoch: 5930 | training loss: 0.000980967190\n",
      "Epoch: 5931 | training loss: 0.000980676152\n",
      "Epoch: 5932 | training loss: 0.000980235985\n",
      "Epoch: 5933 | training loss: 0.000979690114\n",
      "Epoch: 5934 | training loss: 0.000979182078\n",
      "Epoch: 5935 | training loss: 0.000978779513\n",
      "Epoch: 5936 | training loss: 0.000978451339\n",
      "Epoch: 5937 | training loss: 0.000978100346\n",
      "Epoch: 5938 | training loss: 0.000977689051\n",
      "Epoch: 5939 | training loss: 0.000977208489\n",
      "Epoch: 5940 | training loss: 0.000976755982\n",
      "Epoch: 5941 | training loss: 0.000976342824\n",
      "Epoch: 5942 | training loss: 0.000975978910\n",
      "Epoch: 5943 | training loss: 0.000975588104\n",
      "Epoch: 5944 | training loss: 0.000975196366\n",
      "Epoch: 5945 | training loss: 0.000974771567\n",
      "Epoch: 5946 | training loss: 0.000974336872\n",
      "Epoch: 5947 | training loss: 0.000973927672\n",
      "Epoch: 5948 | training loss: 0.000973533199\n",
      "Epoch: 5949 | training loss: 0.000973153394\n",
      "Epoch: 5950 | training loss: 0.000972758164\n",
      "Epoch: 5951 | training loss: 0.000972345297\n",
      "Epoch: 5952 | training loss: 0.000971938134\n",
      "Epoch: 5953 | training loss: 0.000971518632\n",
      "Epoch: 5954 | training loss: 0.000971119269\n",
      "Epoch: 5955 | training loss: 0.000970729045\n",
      "Epoch: 5956 | training loss: 0.000970337016\n",
      "Epoch: 5957 | training loss: 0.000969944638\n",
      "Epoch: 5958 | training loss: 0.000969543471\n",
      "Epoch: 5959 | training loss: 0.000969136250\n",
      "Epoch: 5960 | training loss: 0.000968723558\n",
      "Epoch: 5961 | training loss: 0.000968332635\n",
      "Epoch: 5962 | training loss: 0.000967935077\n",
      "Epoch: 5963 | training loss: 0.000967540778\n",
      "Epoch: 5964 | training loss: 0.000967144617\n",
      "Epoch: 5965 | training loss: 0.000966744614\n",
      "Epoch: 5966 | training loss: 0.000966338906\n",
      "Epoch: 5967 | training loss: 0.000965937623\n",
      "Epoch: 5968 | training loss: 0.000965553569\n",
      "Epoch: 5969 | training loss: 0.000965156010\n",
      "Epoch: 5970 | training loss: 0.000964755600\n",
      "Epoch: 5971 | training loss: 0.000964359730\n",
      "Epoch: 5972 | training loss: 0.000963956118\n",
      "Epoch: 5973 | training loss: 0.000963563682\n",
      "Epoch: 5974 | training loss: 0.000963165890\n",
      "Epoch: 5975 | training loss: 0.000962769729\n",
      "Epoch: 5976 | training loss: 0.000962376944\n",
      "Epoch: 5977 | training loss: 0.000961985323\n",
      "Epoch: 5978 | training loss: 0.000961581827\n",
      "Epoch: 5979 | training loss: 0.000961183920\n",
      "Epoch: 5980 | training loss: 0.000960795151\n",
      "Epoch: 5981 | training loss: 0.000960396603\n",
      "Epoch: 5982 | training loss: 0.000959992525\n",
      "Epoch: 5983 | training loss: 0.000959598343\n",
      "Epoch: 5984 | training loss: 0.000959210447\n",
      "Epoch: 5985 | training loss: 0.000958819001\n",
      "Epoch: 5986 | training loss: 0.000958412187\n",
      "Epoch: 5987 | training loss: 0.000958020915\n",
      "Epoch: 5988 | training loss: 0.000957620214\n",
      "Epoch: 5989 | training loss: 0.000957227370\n",
      "Epoch: 5990 | training loss: 0.000956834061\n",
      "Epoch: 5991 | training loss: 0.000956430915\n",
      "Epoch: 5992 | training loss: 0.000956045347\n",
      "Epoch: 5993 | training loss: 0.000955655414\n",
      "Epoch: 5994 | training loss: 0.000955251046\n",
      "Epoch: 5995 | training loss: 0.000954867457\n",
      "Epoch: 5996 | training loss: 0.000954468327\n",
      "Epoch: 5997 | training loss: 0.000954068149\n",
      "Epoch: 5998 | training loss: 0.000953684328\n",
      "Epoch: 5999 | training loss: 0.000953280716\n",
      "Epoch: 6000 | training loss: 0.000952885021\n",
      "Epoch: 6001 | training loss: 0.000952491013\n",
      "Epoch: 6002 | training loss: 0.000952095841\n",
      "Epoch: 6003 | training loss: 0.000951702008\n",
      "Epoch: 6004 | training loss: 0.000951306196\n",
      "Epoch: 6005 | training loss: 0.000950908172\n",
      "Epoch: 6006 | training loss: 0.000950519927\n",
      "Epoch: 6007 | training loss: 0.000950122252\n",
      "Epoch: 6008 | training loss: 0.000949730689\n",
      "Epoch: 6009 | training loss: 0.000949326437\n",
      "Epoch: 6010 | training loss: 0.000948936562\n",
      "Epoch: 6011 | training loss: 0.000948539295\n",
      "Epoch: 6012 | training loss: 0.000948153902\n",
      "Epoch: 6013 | training loss: 0.000947752618\n",
      "Epoch: 6014 | training loss: 0.000947363442\n",
      "Epoch: 6015 | training loss: 0.000946969609\n",
      "Epoch: 6016 | training loss: 0.000946566055\n",
      "Epoch: 6017 | training loss: 0.000946177170\n",
      "Epoch: 6018 | training loss: 0.000945776817\n",
      "Epoch: 6019 | training loss: 0.000945386710\n",
      "Epoch: 6020 | training loss: 0.000944992644\n",
      "Epoch: 6021 | training loss: 0.000944594387\n",
      "Epoch: 6022 | training loss: 0.000944203697\n",
      "Epoch: 6023 | training loss: 0.000943808991\n",
      "Epoch: 6024 | training loss: 0.000943414518\n",
      "Epoch: 6025 | training loss: 0.000943021558\n",
      "Epoch: 6026 | training loss: 0.000942625629\n",
      "Epoch: 6027 | training loss: 0.000942234299\n",
      "Epoch: 6028 | training loss: 0.000941836566\n",
      "Epoch: 6029 | training loss: 0.000941446051\n",
      "Epoch: 6030 | training loss: 0.000941047503\n",
      "Epoch: 6031 | training loss: 0.000940652913\n",
      "Epoch: 6032 | training loss: 0.000940258033\n",
      "Epoch: 6033 | training loss: 0.000939863152\n",
      "Epoch: 6034 | training loss: 0.000939468853\n",
      "Epoch: 6035 | training loss: 0.000939073681\n",
      "Epoch: 6036 | training loss: 0.000938680139\n",
      "Epoch: 6037 | training loss: 0.000938285608\n",
      "Epoch: 6038 | training loss: 0.000937889563\n",
      "Epoch: 6039 | training loss: 0.000937501551\n",
      "Epoch: 6040 | training loss: 0.000937105564\n",
      "Epoch: 6041 | training loss: 0.000936716795\n",
      "Epoch: 6042 | training loss: 0.000936323078\n",
      "Epoch: 6043 | training loss: 0.000935925636\n",
      "Epoch: 6044 | training loss: 0.000935533782\n",
      "Epoch: 6045 | training loss: 0.000935132848\n",
      "Epoch: 6046 | training loss: 0.000934740820\n",
      "Epoch: 6047 | training loss: 0.000934345764\n",
      "Epoch: 6048 | training loss: 0.000933946692\n",
      "Epoch: 6049 | training loss: 0.000933558797\n",
      "Epoch: 6050 | training loss: 0.000933156174\n",
      "Epoch: 6051 | training loss: 0.000932767522\n",
      "Epoch: 6052 | training loss: 0.000932363910\n",
      "Epoch: 6053 | training loss: 0.000931975490\n",
      "Epoch: 6054 | training loss: 0.000931574730\n",
      "Epoch: 6055 | training loss: 0.000931181537\n",
      "Epoch: 6056 | training loss: 0.000930790731\n",
      "Epoch: 6057 | training loss: 0.000930389098\n",
      "Epoch: 6058 | training loss: 0.000930002076\n",
      "Epoch: 6059 | training loss: 0.000929602655\n",
      "Epoch: 6060 | training loss: 0.000929207541\n",
      "Epoch: 6061 | training loss: 0.000928820926\n",
      "Epoch: 6062 | training loss: 0.000928418362\n",
      "Epoch: 6063 | training loss: 0.000928026042\n",
      "Epoch: 6064 | training loss: 0.000927632151\n",
      "Epoch: 6065 | training loss: 0.000927232672\n",
      "Epoch: 6066 | training loss: 0.000926839362\n",
      "Epoch: 6067 | training loss: 0.000926443841\n",
      "Epoch: 6068 | training loss: 0.000926049775\n",
      "Epoch: 6069 | training loss: 0.000925655244\n",
      "Epoch: 6070 | training loss: 0.000925267639\n",
      "Epoch: 6071 | training loss: 0.000924867112\n",
      "Epoch: 6072 | training loss: 0.000924468273\n",
      "Epoch: 6073 | training loss: 0.000924079912\n",
      "Epoch: 6074 | training loss: 0.000923683983\n",
      "Epoch: 6075 | training loss: 0.000923291023\n",
      "Epoch: 6076 | training loss: 0.000922891777\n",
      "Epoch: 6077 | training loss: 0.000922495616\n",
      "Epoch: 6078 | training loss: 0.000922097533\n",
      "Epoch: 6079 | training loss: 0.000921710394\n",
      "Epoch: 6080 | training loss: 0.000921315048\n",
      "Epoch: 6081 | training loss: 0.000920916966\n",
      "Epoch: 6082 | training loss: 0.000920525868\n",
      "Epoch: 6083 | training loss: 0.000920122431\n",
      "Epoch: 6084 | training loss: 0.000919723767\n",
      "Epoch: 6085 | training loss: 0.000919336104\n",
      "Epoch: 6086 | training loss: 0.000918942387\n",
      "Epoch: 6087 | training loss: 0.000918546459\n",
      "Epoch: 6088 | training loss: 0.000918147969\n",
      "Epoch: 6089 | training loss: 0.000917756988\n",
      "Epoch: 6090 | training loss: 0.000917359837\n",
      "Epoch: 6091 | training loss: 0.000916966004\n",
      "Epoch: 6092 | training loss: 0.000916563557\n",
      "Epoch: 6093 | training loss: 0.000916179619\n",
      "Epoch: 6094 | training loss: 0.000915779616\n",
      "Epoch: 6095 | training loss: 0.000915386248\n",
      "Epoch: 6096 | training loss: 0.000914988574\n",
      "Epoch: 6097 | training loss: 0.000914598815\n",
      "Epoch: 6098 | training loss: 0.000914204167\n",
      "Epoch: 6099 | training loss: 0.000913803291\n",
      "Epoch: 6100 | training loss: 0.000913404045\n",
      "Epoch: 6101 | training loss: 0.000913014053\n",
      "Epoch: 6102 | training loss: 0.000912613585\n",
      "Epoch: 6103 | training loss: 0.000912224641\n",
      "Epoch: 6104 | training loss: 0.000911823357\n",
      "Epoch: 6105 | training loss: 0.000911432668\n",
      "Epoch: 6106 | training loss: 0.000911034644\n",
      "Epoch: 6107 | training loss: 0.000910640461\n",
      "Epoch: 6108 | training loss: 0.000910247909\n",
      "Epoch: 6109 | training loss: 0.000909852679\n",
      "Epoch: 6110 | training loss: 0.000909453258\n",
      "Epoch: 6111 | training loss: 0.000909067225\n",
      "Epoch: 6112 | training loss: 0.000908669841\n",
      "Epoch: 6113 | training loss: 0.000908282294\n",
      "Epoch: 6114 | training loss: 0.000907884911\n",
      "Epoch: 6115 | training loss: 0.000907502021\n",
      "Epoch: 6116 | training loss: 0.000907110865\n",
      "Epoch: 6117 | training loss: 0.000906729721\n",
      "Epoch: 6118 | training loss: 0.000906372326\n",
      "Epoch: 6119 | training loss: 0.000906033441\n",
      "Epoch: 6120 | training loss: 0.000905730005\n",
      "Epoch: 6121 | training loss: 0.000905497931\n",
      "Epoch: 6122 | training loss: 0.000905364926\n",
      "Epoch: 6123 | training loss: 0.000905434194\n",
      "Epoch: 6124 | training loss: 0.000905865571\n",
      "Epoch: 6125 | training loss: 0.000906909467\n",
      "Epoch: 6126 | training loss: 0.000909044524\n",
      "Epoch: 6127 | training loss: 0.000913058524\n",
      "Epoch: 6128 | training loss: 0.000920480466\n",
      "Epoch: 6129 | training loss: 0.000933937146\n",
      "Epoch: 6130 | training loss: 0.000958207529\n",
      "Epoch: 6131 | training loss: 0.001002148376\n",
      "Epoch: 6132 | training loss: 0.001081913826\n",
      "Epoch: 6133 | training loss: 0.001227172557\n",
      "Epoch: 6134 | training loss: 0.001492151292\n",
      "Epoch: 6135 | training loss: 0.001978582470\n",
      "Epoch: 6136 | training loss: 0.002871191362\n",
      "Epoch: 6137 | training loss: 0.004517993424\n",
      "Epoch: 6138 | training loss: 0.007531331386\n",
      "Epoch: 6139 | training loss: 0.013033174910\n",
      "Epoch: 6140 | training loss: 0.022749822587\n",
      "Epoch: 6141 | training loss: 0.039364390075\n",
      "Epoch: 6142 | training loss: 0.064692229033\n",
      "Epoch: 6143 | training loss: 0.097607485950\n",
      "Epoch: 6144 | training loss: 0.123191855848\n",
      "Epoch: 6145 | training loss: 0.119055010378\n",
      "Epoch: 6146 | training loss: 0.071016043425\n",
      "Epoch: 6147 | training loss: 0.015913959593\n",
      "Epoch: 6148 | training loss: 0.002880764194\n",
      "Epoch: 6149 | training loss: 0.032817710191\n",
      "Epoch: 6150 | training loss: 0.057298768312\n",
      "Epoch: 6151 | training loss: 0.040428049862\n",
      "Epoch: 6152 | training loss: 0.007774980273\n",
      "Epoch: 6153 | training loss: 0.004295171238\n",
      "Epoch: 6154 | training loss: 0.025826327503\n",
      "Epoch: 6155 | training loss: 0.031511168927\n",
      "Epoch: 6156 | training loss: 0.012006646022\n",
      "Epoch: 6157 | training loss: 0.001062542549\n",
      "Epoch: 6158 | training loss: 0.012629009783\n",
      "Epoch: 6159 | training loss: 0.020534481853\n",
      "Epoch: 6160 | training loss: 0.009923812002\n",
      "Epoch: 6161 | training loss: 0.000942868995\n",
      "Epoch: 6162 | training loss: 0.007587719709\n",
      "Epoch: 6163 | training loss: 0.013552882709\n",
      "Epoch: 6164 | training loss: 0.006814819761\n",
      "Epoch: 6165 | training loss: 0.000919155311\n",
      "Epoch: 6166 | training loss: 0.005452353973\n",
      "Epoch: 6167 | training loss: 0.009041432291\n",
      "Epoch: 6168 | training loss: 0.004322676919\n",
      "Epoch: 6169 | training loss: 0.000929203234\n",
      "Epoch: 6170 | training loss: 0.004318597261\n",
      "Epoch: 6171 | training loss: 0.006165380124\n",
      "Epoch: 6172 | training loss: 0.002673389856\n",
      "Epoch: 6173 | training loss: 0.001008558320\n",
      "Epoch: 6174 | training loss: 0.003539474448\n",
      "Epoch: 6175 | training loss: 0.004205371253\n",
      "Epoch: 6176 | training loss: 0.001715622726\n",
      "Epoch: 6177 | training loss: 0.001108802389\n",
      "Epoch: 6178 | training loss: 0.002894003177\n",
      "Epoch: 6179 | training loss: 0.002914536744\n",
      "Epoch: 6180 | training loss: 0.001219464466\n",
      "Epoch: 6181 | training loss: 0.001183704124\n",
      "Epoch: 6182 | training loss: 0.002363489708\n",
      "Epoch: 6183 | training loss: 0.002077016281\n",
      "Epoch: 6184 | training loss: 0.000996000832\n",
      "Epoch: 6185 | training loss: 0.001205004286\n",
      "Epoch: 6186 | training loss: 0.001928061596\n",
      "Epoch: 6187 | training loss: 0.001564120408\n",
      "Epoch: 6188 | training loss: 0.000911643612\n",
      "Epoch: 6189 | training loss: 0.001181661035\n",
      "Epoch: 6190 | training loss: 0.001597616356\n",
      "Epoch: 6191 | training loss: 0.001259686775\n",
      "Epoch: 6192 | training loss: 0.000886824913\n",
      "Epoch: 6193 | training loss: 0.001133435639\n",
      "Epoch: 6194 | training loss: 0.001357068075\n",
      "Epoch: 6195 | training loss: 0.001087252167\n",
      "Epoch: 6196 | training loss: 0.000882833032\n",
      "Epoch: 6197 | training loss: 0.001077288412\n",
      "Epoch: 6198 | training loss: 0.001191003015\n",
      "Epoch: 6199 | training loss: 0.000991211855\n",
      "Epoch: 6200 | training loss: 0.000882846420\n",
      "Epoch: 6201 | training loss: 0.001024538418\n",
      "Epoch: 6202 | training loss: 0.001078454196\n",
      "Epoch: 6203 | training loss: 0.000937985955\n",
      "Epoch: 6204 | training loss: 0.000881828833\n",
      "Epoch: 6205 | training loss: 0.000980273704\n",
      "Epoch: 6206 | training loss: 0.001004681806\n",
      "Epoch: 6207 | training loss: 0.000908796792\n",
      "Epoch: 6208 | training loss: 0.000879237079\n",
      "Epoch: 6209 | training loss: 0.000945975597\n",
      "Epoch: 6210 | training loss: 0.000956774573\n",
      "Epoch: 6211 | training loss: 0.000892213837\n",
      "Epoch: 6212 | training loss: 0.000875807600\n",
      "Epoch: 6213 | training loss: 0.000920187798\n",
      "Epoch: 6214 | training loss: 0.000925224449\n",
      "Epoch: 6215 | training loss: 0.000882229127\n",
      "Epoch: 6216 | training loss: 0.000872094650\n",
      "Epoch: 6217 | training loss: 0.000901114661\n",
      "Epoch: 6218 | training loss: 0.000904103566\n",
      "Epoch: 6219 | training loss: 0.000875768601\n",
      "Epoch: 6220 | training loss: 0.000868599396\n",
      "Epoch: 6221 | training loss: 0.000887399423\n",
      "Epoch: 6222 | training loss: 0.000889891060\n",
      "Epoch: 6223 | training loss: 0.000871277181\n",
      "Epoch: 6224 | training loss: 0.000865501643\n",
      "Epoch: 6225 | training loss: 0.000877454644\n",
      "Epoch: 6226 | training loss: 0.000879921543\n",
      "Epoch: 6227 | training loss: 0.000867833965\n",
      "Epoch: 6228 | training loss: 0.000862871530\n",
      "Epoch: 6229 | training loss: 0.000870223914\n",
      "Epoch: 6230 | training loss: 0.000872642966\n",
      "Epoch: 6231 | training loss: 0.000864920439\n",
      "Epoch: 6232 | training loss: 0.000860646600\n",
      "Epoch: 6233 | training loss: 0.000864918635\n",
      "Epoch: 6234 | training loss: 0.000867143855\n",
      "Epoch: 6235 | training loss: 0.000862353889\n",
      "Epoch: 6236 | training loss: 0.000858708285\n",
      "Epoch: 6237 | training loss: 0.000860890257\n",
      "Epoch: 6238 | training loss: 0.000862760236\n",
      "Epoch: 6239 | training loss: 0.000859934080\n",
      "Epoch: 6240 | training loss: 0.000856962346\n",
      "Epoch: 6241 | training loss: 0.000857840758\n",
      "Epoch: 6242 | training loss: 0.000859278196\n",
      "Epoch: 6243 | training loss: 0.000857713749\n",
      "Epoch: 6244 | training loss: 0.000855348771\n",
      "Epoch: 6245 | training loss: 0.000855336082\n",
      "Epoch: 6246 | training loss: 0.000856320024\n",
      "Epoch: 6247 | training loss: 0.000855552731\n",
      "Epoch: 6248 | training loss: 0.000853762438\n",
      "Epoch: 6249 | training loss: 0.000853282574\n",
      "Epoch: 6250 | training loss: 0.000853836886\n",
      "Epoch: 6251 | training loss: 0.000853498583\n",
      "Epoch: 6252 | training loss: 0.000852204161\n",
      "Epoch: 6253 | training loss: 0.000851477962\n",
      "Epoch: 6254 | training loss: 0.000851658173\n",
      "Epoch: 6255 | training loss: 0.000851533259\n",
      "Epoch: 6256 | training loss: 0.000850625394\n",
      "Epoch: 6257 | training loss: 0.000849836506\n",
      "Epoch: 6258 | training loss: 0.000849713513\n",
      "Epoch: 6259 | training loss: 0.000849638600\n",
      "Epoch: 6260 | training loss: 0.000849038886\n",
      "Epoch: 6261 | training loss: 0.000848291675\n",
      "Epoch: 6262 | training loss: 0.000847947493\n",
      "Epoch: 6263 | training loss: 0.000847819145\n",
      "Epoch: 6264 | training loss: 0.000847401854\n",
      "Epoch: 6265 | training loss: 0.000846771756\n",
      "Epoch: 6266 | training loss: 0.000846324314\n",
      "Epoch: 6267 | training loss: 0.000846086012\n",
      "Epoch: 6268 | training loss: 0.000845774892\n",
      "Epoch: 6269 | training loss: 0.000845260685\n",
      "Epoch: 6270 | training loss: 0.000844778377\n",
      "Epoch: 6271 | training loss: 0.000844452181\n",
      "Epoch: 6272 | training loss: 0.000844159105\n",
      "Epoch: 6273 | training loss: 0.000843740534\n",
      "Epoch: 6274 | training loss: 0.000843268819\n",
      "Epoch: 6275 | training loss: 0.000842894078\n",
      "Epoch: 6276 | training loss: 0.000842573878\n",
      "Epoch: 6277 | training loss: 0.000842200010\n",
      "Epoch: 6278 | training loss: 0.000841776782\n",
      "Epoch: 6279 | training loss: 0.000841385685\n",
      "Epoch: 6280 | training loss: 0.000841045869\n",
      "Epoch: 6281 | training loss: 0.000840690103\n",
      "Epoch: 6282 | training loss: 0.000840295048\n",
      "Epoch: 6283 | training loss: 0.000839898712\n",
      "Epoch: 6284 | training loss: 0.000839531363\n",
      "Epoch: 6285 | training loss: 0.000839185377\n",
      "Epoch: 6286 | training loss: 0.000838824257\n",
      "Epoch: 6287 | training loss: 0.000838429900\n",
      "Epoch: 6288 | training loss: 0.000838047010\n",
      "Epoch: 6289 | training loss: 0.000837693980\n",
      "Epoch: 6290 | training loss: 0.000837336585\n",
      "Epoch: 6291 | training loss: 0.000836964289\n",
      "Epoch: 6292 | training loss: 0.000836587278\n",
      "Epoch: 6293 | training loss: 0.000836224353\n",
      "Epoch: 6294 | training loss: 0.000835862244\n",
      "Epoch: 6295 | training loss: 0.000835498970\n",
      "Epoch: 6296 | training loss: 0.000835131737\n",
      "Epoch: 6297 | training loss: 0.000834772829\n",
      "Epoch: 6298 | training loss: 0.000834402337\n",
      "Epoch: 6299 | training loss: 0.000834048958\n",
      "Epoch: 6300 | training loss: 0.000833681959\n",
      "Epoch: 6301 | training loss: 0.000833316473\n",
      "Epoch: 6302 | training loss: 0.000832953723\n",
      "Epoch: 6303 | training loss: 0.000832598016\n",
      "Epoch: 6304 | training loss: 0.000832246733\n",
      "Epoch: 6305 | training loss: 0.000831875601\n",
      "Epoch: 6306 | training loss: 0.000831515645\n",
      "Epoch: 6307 | training loss: 0.000831151672\n",
      "Epoch: 6308 | training loss: 0.000830799807\n",
      "Epoch: 6309 | training loss: 0.000830434554\n",
      "Epoch: 6310 | training loss: 0.000830074248\n",
      "Epoch: 6311 | training loss: 0.000829708471\n",
      "Epoch: 6312 | training loss: 0.000829358702\n",
      "Epoch: 6313 | training loss: 0.000829002820\n",
      "Epoch: 6314 | training loss: 0.000828641350\n",
      "Epoch: 6315 | training loss: 0.000828283955\n",
      "Epoch: 6316 | training loss: 0.000827920565\n",
      "Epoch: 6317 | training loss: 0.000827566953\n",
      "Epoch: 6318 | training loss: 0.000827203563\n",
      "Epoch: 6319 | training loss: 0.000826845178\n",
      "Epoch: 6320 | training loss: 0.000826495234\n",
      "Epoch: 6321 | training loss: 0.000826135394\n",
      "Epoch: 6322 | training loss: 0.000825772644\n",
      "Epoch: 6323 | training loss: 0.000825421303\n",
      "Epoch: 6324 | training loss: 0.000825069670\n",
      "Epoch: 6325 | training loss: 0.000824709190\n",
      "Epoch: 6326 | training loss: 0.000824346207\n",
      "Epoch: 6327 | training loss: 0.000824001094\n",
      "Epoch: 6328 | training loss: 0.000823639624\n",
      "Epoch: 6329 | training loss: 0.000823291193\n",
      "Epoch: 6330 | training loss: 0.000822923495\n",
      "Epoch: 6331 | training loss: 0.000822572503\n",
      "Epoch: 6332 | training loss: 0.000822214759\n",
      "Epoch: 6333 | training loss: 0.000821864931\n",
      "Epoch: 6334 | training loss: 0.000821505964\n",
      "Epoch: 6335 | training loss: 0.000821150083\n",
      "Epoch: 6336 | training loss: 0.000820785121\n",
      "Epoch: 6337 | training loss: 0.000820435351\n",
      "Epoch: 6338 | training loss: 0.000820084126\n",
      "Epoch: 6339 | training loss: 0.000819723005\n",
      "Epoch: 6340 | training loss: 0.000819375331\n",
      "Epoch: 6341 | training loss: 0.000819017121\n",
      "Epoch: 6342 | training loss: 0.000818667526\n",
      "Epoch: 6343 | training loss: 0.000818314613\n",
      "Epoch: 6344 | training loss: 0.000817955472\n",
      "Epoch: 6345 | training loss: 0.000817603373\n",
      "Epoch: 6346 | training loss: 0.000817243592\n",
      "Epoch: 6347 | training loss: 0.000816891668\n",
      "Epoch: 6348 | training loss: 0.000816532702\n",
      "Epoch: 6349 | training loss: 0.000816184562\n",
      "Epoch: 6350 | training loss: 0.000815823325\n",
      "Epoch: 6351 | training loss: 0.000815464649\n",
      "Epoch: 6352 | training loss: 0.000815119827\n",
      "Epoch: 6353 | training loss: 0.000814764411\n",
      "Epoch: 6354 | training loss: 0.000814405619\n",
      "Epoch: 6355 | training loss: 0.000814050436\n",
      "Epoch: 6356 | training loss: 0.000813702703\n",
      "Epoch: 6357 | training loss: 0.000813342049\n",
      "Epoch: 6358 | training loss: 0.000812989078\n",
      "Epoch: 6359 | training loss: 0.000812632730\n",
      "Epoch: 6360 | training loss: 0.000812285114\n",
      "Epoch: 6361 | training loss: 0.000811930862\n",
      "Epoch: 6362 | training loss: 0.000811576087\n",
      "Epoch: 6363 | training loss: 0.000811225153\n",
      "Epoch: 6364 | training loss: 0.000810862519\n",
      "Epoch: 6365 | training loss: 0.000810508733\n",
      "Epoch: 6366 | training loss: 0.000810159836\n",
      "Epoch: 6367 | training loss: 0.000809804245\n",
      "Epoch: 6368 | training loss: 0.000809451391\n",
      "Epoch: 6369 | training loss: 0.000809099001\n",
      "Epoch: 6370 | training loss: 0.000808740151\n",
      "Epoch: 6371 | training loss: 0.000808390032\n",
      "Epoch: 6372 | training loss: 0.000808039331\n",
      "Epoch: 6373 | training loss: 0.000807680131\n",
      "Epoch: 6374 | training loss: 0.000807330711\n",
      "Epoch: 6375 | training loss: 0.000806975644\n",
      "Epoch: 6376 | training loss: 0.000806613592\n",
      "Epoch: 6377 | training loss: 0.000806269760\n",
      "Epoch: 6378 | training loss: 0.000805915915\n",
      "Epoch: 6379 | training loss: 0.000805562828\n",
      "Epoch: 6380 | training loss: 0.000805204210\n",
      "Epoch: 6381 | training loss: 0.000804849085\n",
      "Epoch: 6382 | training loss: 0.000804501120\n",
      "Epoch: 6383 | training loss: 0.000804146694\n",
      "Epoch: 6384 | training loss: 0.000803786621\n",
      "Epoch: 6385 | training loss: 0.000803436444\n",
      "Epoch: 6386 | training loss: 0.000803085335\n",
      "Epoch: 6387 | training loss: 0.000802732422\n",
      "Epoch: 6388 | training loss: 0.000802380790\n",
      "Epoch: 6389 | training loss: 0.000802023744\n",
      "Epoch: 6390 | training loss: 0.000801662798\n",
      "Epoch: 6391 | training loss: 0.000801313494\n",
      "Epoch: 6392 | training loss: 0.000800963026\n",
      "Epoch: 6393 | training loss: 0.000800610753\n",
      "Epoch: 6394 | training loss: 0.000800251379\n",
      "Epoch: 6395 | training loss: 0.000799899222\n",
      "Epoch: 6396 | training loss: 0.000799547532\n",
      "Epoch: 6397 | training loss: 0.000799189031\n",
      "Epoch: 6398 | training loss: 0.000798835652\n",
      "Epoch: 6399 | training loss: 0.000798476860\n",
      "Epoch: 6400 | training loss: 0.000798131048\n",
      "Epoch: 6401 | training loss: 0.000797775108\n",
      "Epoch: 6402 | training loss: 0.000797416375\n",
      "Epoch: 6403 | training loss: 0.000797070621\n",
      "Epoch: 6404 | training loss: 0.000796714681\n",
      "Epoch: 6405 | training loss: 0.000796364620\n",
      "Epoch: 6406 | training loss: 0.000796006061\n",
      "Epoch: 6407 | training loss: 0.000795651460\n",
      "Epoch: 6408 | training loss: 0.000795300002\n",
      "Epoch: 6409 | training loss: 0.000794943189\n",
      "Epoch: 6410 | training loss: 0.000794589927\n",
      "Epoch: 6411 | training loss: 0.000794239168\n",
      "Epoch: 6412 | training loss: 0.000793881831\n",
      "Epoch: 6413 | training loss: 0.000793533924\n",
      "Epoch: 6414 | training loss: 0.000793181011\n",
      "Epoch: 6415 | training loss: 0.000792822102\n",
      "Epoch: 6416 | training loss: 0.000792480190\n",
      "Epoch: 6417 | training loss: 0.000792113366\n",
      "Epoch: 6418 | training loss: 0.000791765284\n",
      "Epoch: 6419 | training loss: 0.000791410508\n",
      "Epoch: 6420 | training loss: 0.000791057420\n",
      "Epoch: 6421 | training loss: 0.000790701655\n",
      "Epoch: 6422 | training loss: 0.000790350372\n",
      "Epoch: 6423 | training loss: 0.000790000136\n",
      "Epoch: 6424 | training loss: 0.000789641577\n",
      "Epoch: 6425 | training loss: 0.000789291342\n",
      "Epoch: 6426 | training loss: 0.000788933656\n",
      "Epoch: 6427 | training loss: 0.000788585108\n",
      "Epoch: 6428 | training loss: 0.000788228062\n",
      "Epoch: 6429 | training loss: 0.000787872297\n",
      "Epoch: 6430 | training loss: 0.000787519384\n",
      "Epoch: 6431 | training loss: 0.000787160534\n",
      "Epoch: 6432 | training loss: 0.000786818506\n",
      "Epoch: 6433 | training loss: 0.000786460005\n",
      "Epoch: 6434 | training loss: 0.000786103483\n",
      "Epoch: 6435 | training loss: 0.000785751035\n",
      "Epoch: 6436 | training loss: 0.000785399112\n",
      "Epoch: 6437 | training loss: 0.000785043987\n",
      "Epoch: 6438 | training loss: 0.000784688105\n",
      "Epoch: 6439 | training loss: 0.000784338452\n",
      "Epoch: 6440 | training loss: 0.000783978845\n",
      "Epoch: 6441 | training loss: 0.000783629541\n",
      "Epoch: 6442 | training loss: 0.000783271505\n",
      "Epoch: 6443 | training loss: 0.000782922143\n",
      "Epoch: 6444 | training loss: 0.000782568473\n",
      "Epoch: 6445 | training loss: 0.000782216783\n",
      "Epoch: 6446 | training loss: 0.000781855371\n",
      "Epoch: 6447 | training loss: 0.000781506242\n",
      "Epoch: 6448 | training loss: 0.000781160779\n",
      "Epoch: 6449 | training loss: 0.000780801114\n",
      "Epoch: 6450 | training loss: 0.000780441507\n",
      "Epoch: 6451 | training loss: 0.000780091970\n",
      "Epoch: 6452 | training loss: 0.000779729162\n",
      "Epoch: 6453 | training loss: 0.000779381429\n",
      "Epoch: 6454 | training loss: 0.000779032067\n",
      "Epoch: 6455 | training loss: 0.000778674672\n",
      "Epoch: 6456 | training loss: 0.000778318907\n",
      "Epoch: 6457 | training loss: 0.000777965761\n",
      "Epoch: 6458 | training loss: 0.000777606387\n",
      "Epoch: 6459 | training loss: 0.000777260226\n",
      "Epoch: 6460 | training loss: 0.000776904111\n",
      "Epoch: 6461 | training loss: 0.000776552130\n",
      "Epoch: 6462 | training loss: 0.000776198169\n",
      "Epoch: 6463 | training loss: 0.000775846769\n",
      "Epoch: 6464 | training loss: 0.000775492750\n",
      "Epoch: 6465 | training loss: 0.000775136054\n",
      "Epoch: 6466 | training loss: 0.000774784479\n",
      "Epoch: 6467 | training loss: 0.000774429063\n",
      "Epoch: 6468 | training loss: 0.000774070504\n",
      "Epoch: 6469 | training loss: 0.000773719163\n",
      "Epoch: 6470 | training loss: 0.000773368112\n",
      "Epoch: 6471 | training loss: 0.000773015432\n",
      "Epoch: 6472 | training loss: 0.000772659318\n",
      "Epoch: 6473 | training loss: 0.000772311701\n",
      "Epoch: 6474 | training loss: 0.000771954597\n",
      "Epoch: 6475 | training loss: 0.000771597261\n",
      "Epoch: 6476 | training loss: 0.000771237595\n",
      "Epoch: 6477 | training loss: 0.000770887767\n",
      "Epoch: 6478 | training loss: 0.000770530489\n",
      "Epoch: 6479 | training loss: 0.000770178682\n",
      "Epoch: 6480 | training loss: 0.000769825419\n",
      "Epoch: 6481 | training loss: 0.000769476348\n",
      "Epoch: 6482 | training loss: 0.000769128441\n",
      "Epoch: 6483 | training loss: 0.000768769823\n",
      "Epoch: 6484 | training loss: 0.000768412952\n",
      "Epoch: 6485 | training loss: 0.000768053811\n",
      "Epoch: 6486 | training loss: 0.000767705846\n",
      "Epoch: 6487 | training loss: 0.000767347345\n",
      "Epoch: 6488 | training loss: 0.000766991056\n",
      "Epoch: 6489 | training loss: 0.000766644487\n",
      "Epoch: 6490 | training loss: 0.000766289886\n",
      "Epoch: 6491 | training loss: 0.000765940174\n",
      "Epoch: 6492 | training loss: 0.000765579462\n",
      "Epoch: 6493 | training loss: 0.000765226898\n",
      "Epoch: 6494 | training loss: 0.000764878991\n",
      "Epoch: 6495 | training loss: 0.000764523400\n",
      "Epoch: 6496 | training loss: 0.000764162338\n",
      "Epoch: 6497 | training loss: 0.000763808494\n",
      "Epoch: 6498 | training loss: 0.000763452379\n",
      "Epoch: 6499 | training loss: 0.000763094926\n",
      "Epoch: 6500 | training loss: 0.000762743759\n",
      "Epoch: 6501 | training loss: 0.000762394746\n",
      "Epoch: 6502 | training loss: 0.000762041251\n",
      "Epoch: 6503 | training loss: 0.000761684671\n",
      "Epoch: 6504 | training loss: 0.000761329022\n",
      "Epoch: 6505 | training loss: 0.000760975759\n",
      "Epoch: 6506 | training loss: 0.000760623603\n",
      "Epoch: 6507 | training loss: 0.000760275580\n",
      "Epoch: 6508 | training loss: 0.000759919116\n",
      "Epoch: 6509 | training loss: 0.000759566843\n",
      "Epoch: 6510 | training loss: 0.000759218179\n",
      "Epoch: 6511 | training loss: 0.000758851238\n",
      "Epoch: 6512 | training loss: 0.000758499198\n",
      "Epoch: 6513 | training loss: 0.000758148904\n",
      "Epoch: 6514 | training loss: 0.000757791917\n",
      "Epoch: 6515 | training loss: 0.000757435686\n",
      "Epoch: 6516 | training loss: 0.000757088303\n",
      "Epoch: 6517 | training loss: 0.000756728346\n",
      "Epoch: 6518 | training loss: 0.000756372232\n",
      "Epoch: 6519 | training loss: 0.000756028690\n",
      "Epoch: 6520 | training loss: 0.000755671179\n",
      "Epoch: 6521 | training loss: 0.000755316752\n",
      "Epoch: 6522 | training loss: 0.000754963025\n",
      "Epoch: 6523 | training loss: 0.000754608656\n",
      "Epoch: 6524 | training loss: 0.000754254870\n",
      "Epoch: 6525 | training loss: 0.000753896544\n",
      "Epoch: 6526 | training loss: 0.000753543340\n",
      "Epoch: 6527 | training loss: 0.000753182219\n",
      "Epoch: 6528 | training loss: 0.000752831751\n",
      "Epoch: 6529 | training loss: 0.000752479769\n",
      "Epoch: 6530 | training loss: 0.000752122840\n",
      "Epoch: 6531 | training loss: 0.000751764048\n",
      "Epoch: 6532 | training loss: 0.000751416432\n",
      "Epoch: 6533 | training loss: 0.000751055777\n",
      "Epoch: 6534 | training loss: 0.000750709791\n",
      "Epoch: 6535 | training loss: 0.000750354724\n",
      "Epoch: 6536 | training loss: 0.000750004081\n",
      "Epoch: 6537 | training loss: 0.000749652390\n",
      "Epoch: 6538 | training loss: 0.000749304367\n",
      "Epoch: 6539 | training loss: 0.000748950406\n",
      "Epoch: 6540 | training loss: 0.000748599297\n",
      "Epoch: 6541 | training loss: 0.000748250866\n",
      "Epoch: 6542 | training loss: 0.000747894286\n",
      "Epoch: 6543 | training loss: 0.000747542595\n",
      "Epoch: 6544 | training loss: 0.000747197890\n",
      "Epoch: 6545 | training loss: 0.000746835372\n",
      "Epoch: 6546 | training loss: 0.000746482634\n",
      "Epoch: 6547 | training loss: 0.000746136415\n",
      "Epoch: 6548 | training loss: 0.000745784782\n",
      "Epoch: 6549 | training loss: 0.000745440309\n",
      "Epoch: 6550 | training loss: 0.000745082274\n",
      "Epoch: 6551 | training loss: 0.000744730467\n",
      "Epoch: 6552 | training loss: 0.000744393736\n",
      "Epoch: 6553 | training loss: 0.000744049903\n",
      "Epoch: 6554 | training loss: 0.000743721379\n",
      "Epoch: 6555 | training loss: 0.000743399374\n",
      "Epoch: 6556 | training loss: 0.000743086915\n",
      "Epoch: 6557 | training loss: 0.000742782897\n",
      "Epoch: 6558 | training loss: 0.000742500240\n",
      "Epoch: 6559 | training loss: 0.000742273463\n",
      "Epoch: 6560 | training loss: 0.000742083299\n",
      "Epoch: 6561 | training loss: 0.000741970609\n",
      "Epoch: 6562 | training loss: 0.000741960830\n",
      "Epoch: 6563 | training loss: 0.000742105418\n",
      "Epoch: 6564 | training loss: 0.000742473174\n",
      "Epoch: 6565 | training loss: 0.000743200420\n",
      "Epoch: 6566 | training loss: 0.000744448917\n",
      "Epoch: 6567 | training loss: 0.000746521808\n",
      "Epoch: 6568 | training loss: 0.000749889761\n",
      "Epoch: 6569 | training loss: 0.000755213550\n",
      "Epoch: 6570 | training loss: 0.000763582997\n",
      "Epoch: 6571 | training loss: 0.000776783912\n",
      "Epoch: 6572 | training loss: 0.000797667424\n",
      "Epoch: 6573 | training loss: 0.000830937817\n",
      "Epoch: 6574 | training loss: 0.000884097768\n",
      "Epoch: 6575 | training loss: 0.000969582819\n",
      "Epoch: 6576 | training loss: 0.001107345102\n",
      "Epoch: 6577 | training loss: 0.001330811996\n",
      "Epoch: 6578 | training loss: 0.001693999278\n",
      "Epoch: 6579 | training loss: 0.002288521035\n",
      "Epoch: 6580 | training loss: 0.003259207821\n",
      "Epoch: 6581 | training loss: 0.004850899801\n",
      "Epoch: 6582 | training loss: 0.007427220233\n",
      "Epoch: 6583 | training loss: 0.011576522142\n",
      "Epoch: 6584 | training loss: 0.018000826240\n",
      "Epoch: 6585 | training loss: 0.027610197663\n",
      "Epoch: 6586 | training loss: 0.040443345904\n",
      "Epoch: 6587 | training loss: 0.055349677801\n",
      "Epoch: 6588 | training loss: 0.066583886743\n",
      "Epoch: 6589 | training loss: 0.067053325474\n",
      "Epoch: 6590 | training loss: 0.050275702029\n",
      "Epoch: 6591 | training loss: 0.023859813809\n",
      "Epoch: 6592 | training loss: 0.004010874778\n",
      "Epoch: 6593 | training loss: 0.002877197228\n",
      "Epoch: 6594 | training loss: 0.015800191090\n",
      "Epoch: 6595 | training loss: 0.026886699721\n",
      "Epoch: 6596 | training loss: 0.024481946602\n",
      "Epoch: 6597 | training loss: 0.011339333840\n",
      "Epoch: 6598 | training loss: 0.001659092028\n",
      "Epoch: 6599 | training loss: 0.003781364998\n",
      "Epoch: 6600 | training loss: 0.011851257645\n",
      "Epoch: 6601 | training loss: 0.014538337477\n",
      "Epoch: 6602 | training loss: 0.008669406176\n",
      "Epoch: 6603 | training loss: 0.002018238418\n",
      "Epoch: 6604 | training loss: 0.001940914197\n",
      "Epoch: 6605 | training loss: 0.006523124408\n",
      "Epoch: 6606 | training loss: 0.008542041294\n",
      "Epoch: 6607 | training loss: 0.005342412740\n",
      "Epoch: 6608 | training loss: 0.001499585574\n",
      "Epoch: 6609 | training loss: 0.001576293609\n",
      "Epoch: 6610 | training loss: 0.004276711959\n",
      "Epoch: 6611 | training loss: 0.005200552288\n",
      "Epoch: 6612 | training loss: 0.003125158139\n",
      "Epoch: 6613 | training loss: 0.001064243028\n",
      "Epoch: 6614 | training loss: 0.001450218260\n",
      "Epoch: 6615 | training loss: 0.003042711411\n",
      "Epoch: 6616 | training loss: 0.003270505462\n",
      "Epoch: 6617 | training loss: 0.001883189427\n",
      "Epoch: 6618 | training loss: 0.000865881913\n",
      "Epoch: 6619 | training loss: 0.001339435228\n",
      "Epoch: 6620 | training loss: 0.002233587205\n",
      "Epoch: 6621 | training loss: 0.002139450051\n",
      "Epoch: 6622 | training loss: 0.001248080865\n",
      "Epoch: 6623 | training loss: 0.000799912377\n",
      "Epoch: 6624 | training loss: 0.001213308540\n",
      "Epoch: 6625 | training loss: 0.001683874289\n",
      "Epoch: 6626 | training loss: 0.001494010212\n",
      "Epoch: 6627 | training loss: 0.000948930567\n",
      "Epoch: 6628 | training loss: 0.000779896509\n",
      "Epoch: 6629 | training loss: 0.001084543066\n",
      "Epoch: 6630 | training loss: 0.001317324815\n",
      "Epoch: 6631 | training loss: 0.001136053121\n",
      "Epoch: 6632 | training loss: 0.000815030769\n",
      "Epoch: 6633 | training loss: 0.000767628662\n",
      "Epoch: 6634 | training loss: 0.000971922185\n",
      "Epoch: 6635 | training loss: 0.001081597991\n",
      "Epoch: 6636 | training loss: 0.000942335231\n",
      "Epoch: 6637 | training loss: 0.000757092610\n",
      "Epoch: 6638 | training loss: 0.000754341716\n",
      "Epoch: 6639 | training loss: 0.000884120935\n",
      "Epoch: 6640 | training loss: 0.000935072429\n",
      "Epoch: 6641 | training loss: 0.000839199172\n",
      "Epoch: 6642 | training loss: 0.000732028857\n",
      "Epoch: 6643 | training loss: 0.000740353309\n",
      "Epoch: 6644 | training loss: 0.000820381101\n",
      "Epoch: 6645 | training loss: 0.000845566799\n",
      "Epoch: 6646 | training loss: 0.000783904572\n",
      "Epoch: 6647 | training loss: 0.000720604206\n",
      "Epoch: 6648 | training loss: 0.000728010782\n",
      "Epoch: 6649 | training loss: 0.000776419241\n",
      "Epoch: 6650 | training loss: 0.000790647638\n",
      "Epoch: 6651 | training loss: 0.000753101194\n",
      "Epoch: 6652 | training loss: 0.000714749331\n",
      "Epoch: 6653 | training loss: 0.000718508614\n",
      "Epoch: 6654 | training loss: 0.000747454003\n",
      "Epoch: 6655 | training loss: 0.000757093308\n",
      "Epoch: 6656 | training loss: 0.000735449081\n",
      "Epoch: 6657 | training loss: 0.000711394183\n",
      "Epoch: 6658 | training loss: 0.000711588014\n",
      "Epoch: 6659 | training loss: 0.000728347455\n",
      "Epoch: 6660 | training loss: 0.000735729234\n",
      "Epoch: 6661 | training loss: 0.000724249345\n",
      "Epoch: 6662 | training loss: 0.000709038228\n",
      "Epoch: 6663 | training loss: 0.000706957304\n",
      "Epoch: 6664 | training loss: 0.000716170471\n",
      "Epoch: 6665 | training loss: 0.000721936172\n",
      "Epoch: 6666 | training loss: 0.000716590032\n",
      "Epoch: 6667 | training loss: 0.000707093976\n",
      "Epoch: 6668 | training loss: 0.000703895930\n",
      "Epoch: 6669 | training loss: 0.000708196079\n",
      "Epoch: 6670 | training loss: 0.000712407986\n",
      "Epoch: 6671 | training loss: 0.000710639986\n",
      "Epoch: 6672 | training loss: 0.000705076032\n",
      "Epoch: 6673 | training loss: 0.000701841898\n",
      "Epoch: 6674 | training loss: 0.000703236903\n",
      "Epoch: 6675 | training loss: 0.000705955841\n",
      "Epoch: 6676 | training loss: 0.000705946120\n",
      "Epoch: 6677 | training loss: 0.000703033758\n",
      "Epoch: 6678 | training loss: 0.000700263074\n",
      "Epoch: 6679 | training loss: 0.000699955737\n",
      "Epoch: 6680 | training loss: 0.000701305864\n",
      "Epoch: 6681 | training loss: 0.000701909827\n",
      "Epoch: 6682 | training loss: 0.000700686534\n",
      "Epoch: 6683 | training loss: 0.000698746589\n",
      "Epoch: 6684 | training loss: 0.000697767478\n",
      "Epoch: 6685 | training loss: 0.000698048447\n",
      "Epoch: 6686 | training loss: 0.000698545831\n",
      "Epoch: 6687 | training loss: 0.000698229764\n",
      "Epoch: 6688 | training loss: 0.000697127951\n",
      "Epoch: 6689 | training loss: 0.000696075382\n",
      "Epoch: 6690 | training loss: 0.000695679162\n",
      "Epoch: 6691 | training loss: 0.000695776660\n",
      "Epoch: 6692 | training loss: 0.000695755007\n",
      "Epoch: 6693 | training loss: 0.000695255934\n",
      "Epoch: 6694 | training loss: 0.000694472808\n",
      "Epoch: 6695 | training loss: 0.000693845213\n",
      "Epoch: 6696 | training loss: 0.000693576410\n",
      "Epoch: 6697 | training loss: 0.000693469425\n",
      "Epoch: 6698 | training loss: 0.000693243521\n",
      "Epoch: 6699 | training loss: 0.000692771049\n",
      "Epoch: 6700 | training loss: 0.000692195084\n",
      "Epoch: 6701 | training loss: 0.000691741181\n",
      "Epoch: 6702 | training loss: 0.000691469468\n",
      "Epoch: 6703 | training loss: 0.000691257592\n",
      "Epoch: 6704 | training loss: 0.000690960267\n",
      "Epoch: 6705 | training loss: 0.000690538378\n",
      "Epoch: 6706 | training loss: 0.000690082845\n",
      "Epoch: 6707 | training loss: 0.000689683249\n",
      "Epoch: 6708 | training loss: 0.000689378299\n",
      "Epoch: 6709 | training loss: 0.000689116598\n",
      "Epoch: 6710 | training loss: 0.000688810833\n",
      "Epoch: 6711 | training loss: 0.000688424567\n",
      "Epoch: 6712 | training loss: 0.000688023341\n",
      "Epoch: 6713 | training loss: 0.000687661581\n",
      "Epoch: 6714 | training loss: 0.000687353255\n",
      "Epoch: 6715 | training loss: 0.000687058899\n",
      "Epoch: 6716 | training loss: 0.000686740386\n",
      "Epoch: 6717 | training loss: 0.000686388405\n",
      "Epoch: 6718 | training loss: 0.000686011510\n",
      "Epoch: 6719 | training loss: 0.000685666455\n",
      "Epoch: 6720 | training loss: 0.000685345556\n",
      "Epoch: 6721 | training loss: 0.000685040315\n",
      "Epoch: 6722 | training loss: 0.000684731291\n",
      "Epoch: 6723 | training loss: 0.000684386236\n",
      "Epoch: 6724 | training loss: 0.000684022321\n",
      "Epoch: 6725 | training loss: 0.000683683960\n",
      "Epoch: 6726 | training loss: 0.000683352409\n",
      "Epoch: 6727 | training loss: 0.000683032675\n",
      "Epoch: 6728 | training loss: 0.000682713813\n",
      "Epoch: 6729 | training loss: 0.000682386511\n",
      "Epoch: 6730 | training loss: 0.000682049897\n",
      "Epoch: 6731 | training loss: 0.000681711012\n",
      "Epoch: 6732 | training loss: 0.000681381673\n",
      "Epoch: 6733 | training loss: 0.000681055302\n",
      "Epoch: 6734 | training loss: 0.000680733763\n",
      "Epoch: 6735 | training loss: 0.000680412864\n",
      "Epoch: 6736 | training loss: 0.000680078578\n",
      "Epoch: 6737 | training loss: 0.000679749995\n",
      "Epoch: 6738 | training loss: 0.000679417863\n",
      "Epoch: 6739 | training loss: 0.000679099234\n",
      "Epoch: 6740 | training loss: 0.000678774435\n",
      "Epoch: 6741 | training loss: 0.000678449986\n",
      "Epoch: 6742 | training loss: 0.000678122800\n",
      "Epoch: 6743 | training loss: 0.000677784556\n",
      "Epoch: 6744 | training loss: 0.000677461794\n",
      "Epoch: 6745 | training loss: 0.000677140604\n",
      "Epoch: 6746 | training loss: 0.000676816213\n",
      "Epoch: 6747 | training loss: 0.000676493277\n",
      "Epoch: 6748 | training loss: 0.000676165742\n",
      "Epoch: 6749 | training loss: 0.000675840478\n",
      "Epoch: 6750 | training loss: 0.000675517134\n",
      "Epoch: 6751 | training loss: 0.000675181102\n",
      "Epoch: 6752 | training loss: 0.000674868235\n",
      "Epoch: 6753 | training loss: 0.000674542447\n",
      "Epoch: 6754 | training loss: 0.000674220501\n",
      "Epoch: 6755 | training loss: 0.000673894247\n",
      "Epoch: 6756 | training loss: 0.000673573115\n",
      "Epoch: 6757 | training loss: 0.000673237781\n",
      "Epoch: 6758 | training loss: 0.000672912574\n",
      "Epoch: 6759 | training loss: 0.000672595284\n",
      "Epoch: 6760 | training loss: 0.000672272174\n",
      "Epoch: 6761 | training loss: 0.000671948132\n",
      "Epoch: 6762 | training loss: 0.000671627524\n",
      "Epoch: 6763 | training loss: 0.000671304471\n",
      "Epoch: 6764 | training loss: 0.000670976471\n",
      "Epoch: 6765 | training loss: 0.000670652080\n",
      "Epoch: 6766 | training loss: 0.000670317677\n",
      "Epoch: 6767 | training loss: 0.000670000038\n",
      "Epoch: 6768 | training loss: 0.000669681467\n",
      "Epoch: 6769 | training loss: 0.000669359753\n",
      "Epoch: 6770 | training loss: 0.000669036352\n",
      "Epoch: 6771 | training loss: 0.000668711960\n",
      "Epoch: 6772 | training loss: 0.000668392517\n",
      "Epoch: 6773 | training loss: 0.000668067718\n",
      "Epoch: 6774 | training loss: 0.000667744083\n",
      "Epoch: 6775 | training loss: 0.000667411194\n",
      "Epoch: 6776 | training loss: 0.000667091866\n",
      "Epoch: 6777 | training loss: 0.000666769163\n",
      "Epoch: 6778 | training loss: 0.000666451990\n",
      "Epoch: 6779 | training loss: 0.000666122418\n",
      "Epoch: 6780 | training loss: 0.000665798842\n",
      "Epoch: 6781 | training loss: 0.000665470783\n",
      "Epoch: 6782 | training loss: 0.000665144704\n",
      "Epoch: 6783 | training loss: 0.000664825609\n",
      "Epoch: 6784 | training loss: 0.000664506981\n",
      "Epoch: 6785 | training loss: 0.000664183055\n",
      "Epoch: 6786 | training loss: 0.000663853833\n",
      "Epoch: 6787 | training loss: 0.000663533167\n",
      "Epoch: 6788 | training loss: 0.000663211395\n",
      "Epoch: 6789 | training loss: 0.000662885606\n",
      "Epoch: 6790 | training loss: 0.000662566570\n",
      "Epoch: 6791 | training loss: 0.000662236125\n",
      "Epoch: 6792 | training loss: 0.000661920756\n",
      "Epoch: 6793 | training loss: 0.000661596074\n",
      "Epoch: 6794 | training loss: 0.000661271275\n",
      "Epoch: 6795 | training loss: 0.000660952763\n",
      "Epoch: 6796 | training loss: 0.000660624471\n",
      "Epoch: 6797 | training loss: 0.000660306832\n",
      "Epoch: 6798 | training loss: 0.000659983489\n",
      "Epoch: 6799 | training loss: 0.000659653684\n",
      "Epoch: 6800 | training loss: 0.000659330632\n",
      "Epoch: 6801 | training loss: 0.000659013109\n",
      "Epoch: 6802 | training loss: 0.000658686797\n",
      "Epoch: 6803 | training loss: 0.000658366014\n",
      "Epoch: 6804 | training loss: 0.000658042030\n",
      "Epoch: 6805 | training loss: 0.000657724042\n",
      "Epoch: 6806 | training loss: 0.000657397613\n",
      "Epoch: 6807 | training loss: 0.000657073862\n",
      "Epoch: 6808 | training loss: 0.000656754943\n",
      "Epoch: 6809 | training loss: 0.000656428980\n",
      "Epoch: 6810 | training loss: 0.000656102609\n",
      "Epoch: 6811 | training loss: 0.000655789452\n",
      "Epoch: 6812 | training loss: 0.000655464188\n",
      "Epoch: 6813 | training loss: 0.000655140379\n",
      "Epoch: 6814 | training loss: 0.000654819945\n",
      "Epoch: 6815 | training loss: 0.000654492993\n",
      "Epoch: 6816 | training loss: 0.000654171337\n",
      "Epoch: 6817 | training loss: 0.000653846073\n",
      "Epoch: 6818 | training loss: 0.000653523603\n",
      "Epoch: 6819 | training loss: 0.000653204392\n",
      "Epoch: 6820 | training loss: 0.000652884482\n",
      "Epoch: 6821 | training loss: 0.000652555726\n",
      "Epoch: 6822 | training loss: 0.000652234361\n",
      "Epoch: 6823 | training loss: 0.000651909679\n",
      "Epoch: 6824 | training loss: 0.000651593029\n",
      "Epoch: 6825 | training loss: 0.000651268638\n",
      "Epoch: 6826 | training loss: 0.000650944072\n",
      "Epoch: 6827 | training loss: 0.000650627888\n",
      "Epoch: 6828 | training loss: 0.000650295988\n",
      "Epoch: 6829 | training loss: 0.000649976660\n",
      "Epoch: 6830 | training loss: 0.000649650756\n",
      "Epoch: 6831 | training loss: 0.000649328926\n",
      "Epoch: 6832 | training loss: 0.000649006513\n",
      "Epoch: 6833 | training loss: 0.000648687710\n",
      "Epoch: 6834 | training loss: 0.000648360641\n",
      "Epoch: 6835 | training loss: 0.000648047833\n",
      "Epoch: 6836 | training loss: 0.000647717738\n",
      "Epoch: 6837 | training loss: 0.000647403998\n",
      "Epoch: 6838 | training loss: 0.000647076929\n",
      "Epoch: 6839 | training loss: 0.000646754575\n",
      "Epoch: 6840 | training loss: 0.000646431174\n",
      "Epoch: 6841 | training loss: 0.000646104978\n",
      "Epoch: 6842 | training loss: 0.000645785476\n",
      "Epoch: 6843 | training loss: 0.000645456836\n",
      "Epoch: 6844 | training loss: 0.000645137741\n",
      "Epoch: 6845 | training loss: 0.000644815562\n",
      "Epoch: 6846 | training loss: 0.000644492393\n",
      "Epoch: 6847 | training loss: 0.000644172484\n",
      "Epoch: 6848 | training loss: 0.000643855950\n",
      "Epoch: 6849 | training loss: 0.000643525331\n",
      "Epoch: 6850 | training loss: 0.000643210253\n",
      "Epoch: 6851 | training loss: 0.000642888132\n",
      "Epoch: 6852 | training loss: 0.000642563100\n",
      "Epoch: 6853 | training loss: 0.000642243831\n",
      "Epoch: 6854 | training loss: 0.000641913502\n",
      "Epoch: 6855 | training loss: 0.000641598832\n",
      "Epoch: 6856 | training loss: 0.000641266350\n",
      "Epoch: 6857 | training loss: 0.000640951679\n",
      "Epoch: 6858 | training loss: 0.000640624494\n",
      "Epoch: 6859 | training loss: 0.000640304410\n",
      "Epoch: 6860 | training loss: 0.000639985607\n",
      "Epoch: 6861 | training loss: 0.000639667618\n",
      "Epoch: 6862 | training loss: 0.000639345730\n",
      "Epoch: 6863 | training loss: 0.000639034610\n",
      "Epoch: 6864 | training loss: 0.000638708298\n",
      "Epoch: 6865 | training loss: 0.000638389611\n",
      "Epoch: 6866 | training loss: 0.000638078141\n",
      "Epoch: 6867 | training loss: 0.000637772493\n",
      "Epoch: 6868 | training loss: 0.000637468009\n",
      "Epoch: 6869 | training loss: 0.000637162651\n",
      "Epoch: 6870 | training loss: 0.000636867131\n",
      "Epoch: 6871 | training loss: 0.000636579003\n",
      "Epoch: 6872 | training loss: 0.000636307232\n",
      "Epoch: 6873 | training loss: 0.000636055018\n",
      "Epoch: 6874 | training loss: 0.000635824399\n",
      "Epoch: 6875 | training loss: 0.000635647564\n",
      "Epoch: 6876 | training loss: 0.000635543722\n",
      "Epoch: 6877 | training loss: 0.000635532895\n",
      "Epoch: 6878 | training loss: 0.000635653909\n",
      "Epoch: 6879 | training loss: 0.000635994715\n",
      "Epoch: 6880 | training loss: 0.000636646058\n",
      "Epoch: 6881 | training loss: 0.000637778197\n",
      "Epoch: 6882 | training loss: 0.000639665523\n",
      "Epoch: 6883 | training loss: 0.000642725034\n",
      "Epoch: 6884 | training loss: 0.000647595210\n",
      "Epoch: 6885 | training loss: 0.000655301439\n",
      "Epoch: 6886 | training loss: 0.000667465560\n",
      "Epoch: 6887 | training loss: 0.000686766347\n",
      "Epoch: 6888 | training loss: 0.000717601040\n",
      "Epoch: 6889 | training loss: 0.000766866724\n",
      "Epoch: 6890 | training loss: 0.000845931587\n",
      "Epoch: 6891 | training loss: 0.000973758171\n",
      "Epoch: 6892 | training loss: 0.001180802356\n",
      "Epoch: 6893 | training loss: 0.001518257428\n",
      "Epoch: 6894 | training loss: 0.002068827394\n",
      "Epoch: 6895 | training loss: 0.002972606570\n",
      "Epoch: 6896 | training loss: 0.004446991254\n",
      "Epoch: 6897 | training loss: 0.006852477789\n",
      "Epoch: 6898 | training loss: 0.010694169439\n",
      "Epoch: 6899 | training loss: 0.016723373905\n",
      "Epoch: 6900 | training loss: 0.025603482500\n",
      "Epoch: 6901 | training loss: 0.037778466940\n",
      "Epoch: 6902 | training loss: 0.051496621221\n",
      "Epoch: 6903 | training loss: 0.062728680670\n",
      "Epoch: 6904 | training loss: 0.063051126897\n",
      "Epoch: 6905 | training loss: 0.048647820950\n",
      "Epoch: 6906 | training loss: 0.024080595002\n",
      "Epoch: 6907 | training loss: 0.005404608790\n",
      "Epoch: 6908 | training loss: 0.003488129703\n",
      "Epoch: 6909 | training loss: 0.014518226497\n",
      "Epoch: 6910 | training loss: 0.024270221591\n",
      "Epoch: 6911 | training loss: 0.021994626150\n",
      "Epoch: 6912 | training loss: 0.010909756646\n",
      "Epoch: 6913 | training loss: 0.002924916102\n",
      "Epoch: 6914 | training loss: 0.004765762947\n",
      "Epoch: 6915 | training loss: 0.010889416561\n",
      "Epoch: 6916 | training loss: 0.012160398066\n",
      "Epoch: 6917 | training loss: 0.007298441138\n",
      "Epoch: 6918 | training loss: 0.002798595466\n",
      "Epoch: 6919 | training loss: 0.003372586099\n",
      "Epoch: 6920 | training loss: 0.006275905296\n",
      "Epoch: 6921 | training loss: 0.006546000484\n",
      "Epoch: 6922 | training loss: 0.003957445733\n",
      "Epoch: 6923 | training loss: 0.002182751196\n",
      "Epoch: 6924 | training loss: 0.002919513267\n",
      "Epoch: 6925 | training loss: 0.004027149174\n",
      "Epoch: 6926 | training loss: 0.003472777782\n",
      "Epoch: 6927 | training loss: 0.002154835965\n",
      "Epoch: 6928 | training loss: 0.001865682192\n",
      "Epoch: 6929 | training loss: 0.002457749331\n",
      "Epoch: 6930 | training loss: 0.002529999008\n",
      "Epoch: 6931 | training loss: 0.001845121733\n",
      "Epoch: 6932 | training loss: 0.001427137060\n",
      "Epoch: 6933 | training loss: 0.001690989360\n",
      "Epoch: 6934 | training loss: 0.001895174151\n",
      "Epoch: 6935 | training loss: 0.001530289068\n",
      "Epoch: 6936 | training loss: 0.001112917904\n",
      "Epoch: 6937 | training loss: 0.001200230327\n",
      "Epoch: 6938 | training loss: 0.001476421719\n",
      "Epoch: 6939 | training loss: 0.001359424787\n",
      "Epoch: 6940 | training loss: 0.000968060805\n",
      "Epoch: 6941 | training loss: 0.000860667205\n",
      "Epoch: 6942 | training loss: 0.001102749258\n",
      "Epoch: 6943 | training loss: 0.001214276766\n",
      "Epoch: 6944 | training loss: 0.000971155008\n",
      "Epoch: 6945 | training loss: 0.000723951380\n",
      "Epoch: 6946 | training loss: 0.000796592794\n",
      "Epoch: 6947 | training loss: 0.000999249052\n",
      "Epoch: 6948 | training loss: 0.000975591072\n",
      "Epoch: 6949 | training loss: 0.000751803513\n",
      "Epoch: 6950 | training loss: 0.000648536952\n",
      "Epoch: 6951 | training loss: 0.000770258019\n",
      "Epoch: 6952 | training loss: 0.000884160399\n",
      "Epoch: 6953 | training loss: 0.000805324293\n",
      "Epoch: 6954 | training loss: 0.000653385418\n",
      "Epoch: 6955 | training loss: 0.000635344419\n",
      "Epoch: 6956 | training loss: 0.000738281757\n",
      "Epoch: 6957 | training loss: 0.000783777621\n",
      "Epoch: 6958 | training loss: 0.000704896054\n",
      "Epoch: 6959 | training loss: 0.000619299710\n",
      "Epoch: 6960 | training loss: 0.000634261407\n",
      "Epoch: 6961 | training loss: 0.000701101206\n",
      "Epoch: 6962 | training loss: 0.000710663560\n",
      "Epoch: 6963 | training loss: 0.000652870920\n",
      "Epoch: 6964 | training loss: 0.000610860356\n",
      "Epoch: 6965 | training loss: 0.000630883034\n",
      "Epoch: 6966 | training loss: 0.000667852233\n",
      "Epoch: 6967 | training loss: 0.000664066698\n",
      "Epoch: 6968 | training loss: 0.000628196110\n",
      "Epoch: 6969 | training loss: 0.000609570881\n",
      "Epoch: 6970 | training loss: 0.000624563545\n",
      "Epoch: 6971 | training loss: 0.000642519444\n",
      "Epoch: 6972 | training loss: 0.000636192563\n",
      "Epoch: 6973 | training loss: 0.000616293866\n",
      "Epoch: 6974 | training loss: 0.000608783797\n",
      "Epoch: 6975 | training loss: 0.000617753540\n",
      "Epoch: 6976 | training loss: 0.000625378045\n",
      "Epoch: 6977 | training loss: 0.000620068342\n",
      "Epoch: 6978 | training loss: 0.000609858660\n",
      "Epoch: 6979 | training loss: 0.000607242691\n",
      "Epoch: 6980 | training loss: 0.000611978350\n",
      "Epoch: 6981 | training loss: 0.000614495599\n",
      "Epoch: 6982 | training loss: 0.000610582822\n",
      "Epoch: 6983 | training loss: 0.000605529174\n",
      "Epoch: 6984 | training loss: 0.000604958623\n",
      "Epoch: 6985 | training loss: 0.000607355440\n",
      "Epoch: 6986 | training loss: 0.000607707596\n",
      "Epoch: 6987 | training loss: 0.000604865607\n",
      "Epoch: 6988 | training loss: 0.000602264539\n",
      "Epoch: 6989 | training loss: 0.000602406508\n",
      "Epoch: 6990 | training loss: 0.000603691442\n",
      "Epoch: 6991 | training loss: 0.000603317341\n",
      "Epoch: 6992 | training loss: 0.000601200387\n",
      "Epoch: 6993 | training loss: 0.000599607127\n",
      "Epoch: 6994 | training loss: 0.000599851366\n",
      "Epoch: 6995 | training loss: 0.000600690604\n",
      "Epoch: 6996 | training loss: 0.000600309228\n",
      "Epoch: 6997 | training loss: 0.000598710962\n",
      "Epoch: 6998 | training loss: 0.000597453618\n",
      "Epoch: 6999 | training loss: 0.000597498030\n",
      "Epoch: 7000 | training loss: 0.000598083891\n",
      "Epoch: 7001 | training loss: 0.000597911479\n",
      "Epoch: 7002 | training loss: 0.000596788886\n",
      "Epoch: 7003 | training loss: 0.000595702091\n",
      "Epoch: 7004 | training loss: 0.000595455174\n",
      "Epoch: 7005 | training loss: 0.000595757156\n",
      "Epoch: 7006 | training loss: 0.000595752324\n",
      "Epoch: 7007 | training loss: 0.000595065765\n",
      "Epoch: 7008 | training loss: 0.000594185607\n",
      "Epoch: 7009 | training loss: 0.000593715755\n",
      "Epoch: 7010 | training loss: 0.000593726290\n",
      "Epoch: 7011 | training loss: 0.000593745324\n",
      "Epoch: 7012 | training loss: 0.000593373901\n",
      "Epoch: 7013 | training loss: 0.000592726108\n",
      "Epoch: 7014 | training loss: 0.000592201366\n",
      "Epoch: 7015 | training loss: 0.000591968885\n",
      "Epoch: 7016 | training loss: 0.000591874355\n",
      "Epoch: 7017 | training loss: 0.000591645017\n",
      "Epoch: 7018 | training loss: 0.000591226097\n",
      "Epoch: 7019 | training loss: 0.000590764743\n",
      "Epoch: 7020 | training loss: 0.000590414391\n",
      "Epoch: 7021 | training loss: 0.000590198149\n",
      "Epoch: 7022 | training loss: 0.000589986914\n",
      "Epoch: 7023 | training loss: 0.000589679985\n",
      "Epoch: 7024 | training loss: 0.000589301460\n",
      "Epoch: 7025 | training loss: 0.000588948838\n",
      "Epoch: 7026 | training loss: 0.000588664494\n",
      "Epoch: 7027 | training loss: 0.000588406518\n",
      "Epoch: 7028 | training loss: 0.000588132651\n",
      "Epoch: 7029 | training loss: 0.000587821065\n",
      "Epoch: 7030 | training loss: 0.000587499933\n",
      "Epoch: 7031 | training loss: 0.000587197545\n",
      "Epoch: 7032 | training loss: 0.000586914481\n",
      "Epoch: 7033 | training loss: 0.000586626935\n",
      "Epoch: 7034 | training loss: 0.000586332870\n",
      "Epoch: 7035 | training loss: 0.000586032402\n",
      "Epoch: 7036 | training loss: 0.000585739152\n",
      "Epoch: 7037 | training loss: 0.000585453992\n",
      "Epoch: 7038 | training loss: 0.000585167843\n",
      "Epoch: 7039 | training loss: 0.000584878551\n",
      "Epoch: 7040 | training loss: 0.000584584777\n",
      "Epoch: 7041 | training loss: 0.000584284426\n",
      "Epoch: 7042 | training loss: 0.000583998277\n",
      "Epoch: 7043 | training loss: 0.000583711662\n",
      "Epoch: 7044 | training loss: 0.000583434827\n",
      "Epoch: 7045 | training loss: 0.000583143032\n",
      "Epoch: 7046 | training loss: 0.000582849607\n",
      "Epoch: 7047 | training loss: 0.000582558336\n",
      "Epoch: 7048 | training loss: 0.000582274632\n",
      "Epoch: 7049 | training loss: 0.000581991277\n",
      "Epoch: 7050 | training loss: 0.000581710949\n",
      "Epoch: 7051 | training loss: 0.000581424334\n",
      "Epoch: 7052 | training loss: 0.000581132655\n",
      "Epoch: 7053 | training loss: 0.000580845168\n",
      "Epoch: 7054 | training loss: 0.000580552616\n",
      "Epoch: 7055 | training loss: 0.000580273045\n",
      "Epoch: 7056 | training loss: 0.000579989050\n",
      "Epoch: 7057 | training loss: 0.000579703774\n",
      "Epoch: 7058 | training loss: 0.000579420186\n",
      "Epoch: 7059 | training loss: 0.000579130952\n",
      "Epoch: 7060 | training loss: 0.000578843639\n",
      "Epoch: 7061 | training loss: 0.000578563835\n",
      "Epoch: 7062 | training loss: 0.000578278094\n",
      "Epoch: 7063 | training loss: 0.000577998173\n",
      "Epoch: 7064 | training loss: 0.000577705330\n",
      "Epoch: 7065 | training loss: 0.000577420695\n",
      "Epoch: 7066 | training loss: 0.000577134197\n",
      "Epoch: 7067 | training loss: 0.000576853054\n",
      "Epoch: 7068 | training loss: 0.000576569466\n",
      "Epoch: 7069 | training loss: 0.000576287857\n",
      "Epoch: 7070 | training loss: 0.000576004328\n",
      "Epoch: 7071 | training loss: 0.000575724116\n",
      "Epoch: 7072 | training loss: 0.000575443089\n",
      "Epoch: 7073 | training loss: 0.000575148151\n",
      "Epoch: 7074 | training loss: 0.000574870384\n",
      "Epoch: 7075 | training loss: 0.000574584119\n",
      "Epoch: 7076 | training loss: 0.000574301346\n",
      "Epoch: 7077 | training loss: 0.000574013509\n",
      "Epoch: 7078 | training loss: 0.000573735684\n",
      "Epoch: 7079 | training loss: 0.000573452213\n",
      "Epoch: 7080 | training loss: 0.000573169324\n",
      "Epoch: 7081 | training loss: 0.000572884979\n",
      "Epoch: 7082 | training loss: 0.000572610064\n",
      "Epoch: 7083 | training loss: 0.000572321238\n",
      "Epoch: 7084 | training loss: 0.000572041608\n",
      "Epoch: 7085 | training loss: 0.000571756857\n",
      "Epoch: 7086 | training loss: 0.000571474491\n",
      "Epoch: 7087 | training loss: 0.000571189332\n",
      "Epoch: 7088 | training loss: 0.000570903474\n",
      "Epoch: 7089 | training loss: 0.000570622215\n",
      "Epoch: 7090 | training loss: 0.000570338627\n",
      "Epoch: 7091 | training loss: 0.000570053700\n",
      "Epoch: 7092 | training loss: 0.000569775293\n",
      "Epoch: 7093 | training loss: 0.000569498283\n",
      "Epoch: 7094 | training loss: 0.000569203112\n",
      "Epoch: 7095 | training loss: 0.000568931166\n",
      "Epoch: 7096 | training loss: 0.000568642630\n",
      "Epoch: 7097 | training loss: 0.000568362244\n",
      "Epoch: 7098 | training loss: 0.000568079296\n",
      "Epoch: 7099 | training loss: 0.000567795360\n",
      "Epoch: 7100 | training loss: 0.000567516137\n",
      "Epoch: 7101 | training loss: 0.000567231851\n",
      "Epoch: 7102 | training loss: 0.000566951174\n",
      "Epoch: 7103 | training loss: 0.000566674338\n",
      "Epoch: 7104 | training loss: 0.000566384755\n",
      "Epoch: 7105 | training loss: 0.000566105184\n",
      "Epoch: 7106 | training loss: 0.000565820781\n",
      "Epoch: 7107 | training loss: 0.000565539929\n",
      "Epoch: 7108 | training loss: 0.000565254304\n",
      "Epoch: 7109 | training loss: 0.000564974966\n",
      "Epoch: 7110 | training loss: 0.000564696500\n",
      "Epoch: 7111 | training loss: 0.000564411224\n",
      "Epoch: 7112 | training loss: 0.000564129557\n",
      "Epoch: 7113 | training loss: 0.000563849928\n",
      "Epoch: 7114 | training loss: 0.000563560636\n",
      "Epoch: 7115 | training loss: 0.000563282229\n",
      "Epoch: 7116 | training loss: 0.000563004985\n",
      "Epoch: 7117 | training loss: 0.000562720117\n",
      "Epoch: 7118 | training loss: 0.000562438334\n",
      "Epoch: 7119 | training loss: 0.000562152767\n",
      "Epoch: 7120 | training loss: 0.000561878551\n",
      "Epoch: 7121 | training loss: 0.000561586930\n",
      "Epoch: 7122 | training loss: 0.000561310793\n",
      "Epoch: 7123 | training loss: 0.000561031979\n",
      "Epoch: 7124 | training loss: 0.000560741755\n",
      "Epoch: 7125 | training loss: 0.000560459448\n",
      "Epoch: 7126 | training loss: 0.000560184417\n",
      "Epoch: 7127 | training loss: 0.000559900654\n",
      "Epoch: 7128 | training loss: 0.000559613574\n",
      "Epoch: 7129 | training loss: 0.000559338194\n",
      "Epoch: 7130 | training loss: 0.000559055363\n",
      "Epoch: 7131 | training loss: 0.000558775093\n",
      "Epoch: 7132 | training loss: 0.000558488246\n",
      "Epoch: 7133 | training loss: 0.000558210071\n",
      "Epoch: 7134 | training loss: 0.000557934924\n",
      "Epoch: 7135 | training loss: 0.000557652384\n",
      "Epoch: 7136 | training loss: 0.000557368097\n",
      "Epoch: 7137 | training loss: 0.000557088701\n",
      "Epoch: 7138 | training loss: 0.000556805520\n",
      "Epoch: 7139 | training loss: 0.000556523737\n",
      "Epoch: 7140 | training loss: 0.000556243293\n",
      "Epoch: 7141 | training loss: 0.000555961393\n",
      "Epoch: 7142 | training loss: 0.000555677107\n",
      "Epoch: 7143 | training loss: 0.000555397244\n",
      "Epoch: 7144 | training loss: 0.000555116276\n",
      "Epoch: 7145 | training loss: 0.000554829021\n",
      "Epoch: 7146 | training loss: 0.000554549333\n",
      "Epoch: 7147 | training loss: 0.000554270810\n",
      "Epoch: 7148 | training loss: 0.000553987222\n",
      "Epoch: 7149 | training loss: 0.000553704333\n",
      "Epoch: 7150 | training loss: 0.000553429360\n",
      "Epoch: 7151 | training loss: 0.000553143793\n",
      "Epoch: 7152 | training loss: 0.000552865968\n",
      "Epoch: 7153 | training loss: 0.000552582089\n",
      "Epoch: 7154 | training loss: 0.000552302401\n",
      "Epoch: 7155 | training loss: 0.000552017184\n",
      "Epoch: 7156 | training loss: 0.000551736564\n",
      "Epoch: 7157 | training loss: 0.000551460660\n",
      "Epoch: 7158 | training loss: 0.000551176956\n",
      "Epoch: 7159 | training loss: 0.000550898432\n",
      "Epoch: 7160 | training loss: 0.000550615427\n",
      "Epoch: 7161 | training loss: 0.000550335622\n",
      "Epoch: 7162 | training loss: 0.000550053373\n",
      "Epoch: 7163 | training loss: 0.000549768331\n",
      "Epoch: 7164 | training loss: 0.000549491437\n",
      "Epoch: 7165 | training loss: 0.000549204997\n",
      "Epoch: 7166 | training loss: 0.000548925309\n",
      "Epoch: 7167 | training loss: 0.000548644690\n",
      "Epoch: 7168 | training loss: 0.000548362499\n",
      "Epoch: 7169 | training loss: 0.000548082229\n",
      "Epoch: 7170 | training loss: 0.000547803007\n",
      "Epoch: 7171 | training loss: 0.000547521457\n",
      "Epoch: 7172 | training loss: 0.000547239557\n",
      "Epoch: 7173 | training loss: 0.000546955329\n",
      "Epoch: 7174 | training loss: 0.000546675059\n",
      "Epoch: 7175 | training loss: 0.000546397525\n",
      "Epoch: 7176 | training loss: 0.000546118419\n",
      "Epoch: 7177 | training loss: 0.000545837858\n",
      "Epoch: 7178 | training loss: 0.000545553281\n",
      "Epoch: 7179 | training loss: 0.000545274350\n",
      "Epoch: 7180 | training loss: 0.000544995943\n",
      "Epoch: 7181 | training loss: 0.000544717710\n",
      "Epoch: 7182 | training loss: 0.000544435869\n",
      "Epoch: 7183 | training loss: 0.000544156821\n",
      "Epoch: 7184 | training loss: 0.000543877017\n",
      "Epoch: 7185 | training loss: 0.000543598435\n",
      "Epoch: 7186 | training loss: 0.000543323928\n",
      "Epoch: 7187 | training loss: 0.000543047965\n",
      "Epoch: 7188 | training loss: 0.000542773516\n",
      "Epoch: 7189 | training loss: 0.000542503083\n",
      "Epoch: 7190 | training loss: 0.000542253896\n",
      "Epoch: 7191 | training loss: 0.000541996676\n",
      "Epoch: 7192 | training loss: 0.000541770249\n",
      "Epoch: 7193 | training loss: 0.000541564776\n",
      "Epoch: 7194 | training loss: 0.000541408604\n",
      "Epoch: 7195 | training loss: 0.000541322457\n",
      "Epoch: 7196 | training loss: 0.000541346089\n",
      "Epoch: 7197 | training loss: 0.000541539746\n",
      "Epoch: 7198 | training loss: 0.000542024616\n",
      "Epoch: 7199 | training loss: 0.000542982540\n",
      "Epoch: 7200 | training loss: 0.000544721668\n",
      "Epoch: 7201 | training loss: 0.000547731644\n",
      "Epoch: 7202 | training loss: 0.000552828191\n",
      "Epoch: 7203 | training loss: 0.000561408524\n",
      "Epoch: 7204 | training loss: 0.000575835700\n",
      "Epoch: 7205 | training loss: 0.000600090716\n",
      "Epoch: 7206 | training loss: 0.000640997605\n",
      "Epoch: 7207 | training loss: 0.000710167689\n",
      "Epoch: 7208 | training loss: 0.000827850192\n",
      "Epoch: 7209 | training loss: 0.001028405735\n",
      "Epoch: 7210 | training loss: 0.001372691244\n",
      "Epoch: 7211 | training loss: 0.001961571164\n",
      "Epoch: 7212 | training loss: 0.002975506708\n",
      "Epoch: 7213 | training loss: 0.004696978256\n",
      "Epoch: 7214 | training loss: 0.007616107352\n",
      "Epoch: 7215 | training loss: 0.012368303724\n",
      "Epoch: 7216 | training loss: 0.019901948050\n",
      "Epoch: 7217 | training loss: 0.030554752797\n",
      "Epoch: 7218 | training loss: 0.044097580016\n",
      "Epoch: 7219 | training loss: 0.056271459907\n",
      "Epoch: 7220 | training loss: 0.062399707735\n",
      "Epoch: 7221 | training loss: 0.055993977934\n",
      "Epoch: 7222 | training loss: 0.040145054460\n",
      "Epoch: 7223 | training loss: 0.022355871275\n",
      "Epoch: 7224 | training loss: 0.011184521951\n",
      "Epoch: 7225 | training loss: 0.009672145359\n",
      "Epoch: 7226 | training loss: 0.014789999463\n",
      "Epoch: 7227 | training loss: 0.020326366648\n",
      "Epoch: 7228 | training loss: 0.019904112443\n",
      "Epoch: 7229 | training loss: 0.012205051258\n",
      "Epoch: 7230 | training loss: 0.004179159179\n",
      "Epoch: 7231 | training loss: 0.004240869079\n",
      "Epoch: 7232 | training loss: 0.010711139068\n",
      "Epoch: 7233 | training loss: 0.013153227046\n",
      "Epoch: 7234 | training loss: 0.007141757291\n",
      "Epoch: 7235 | training loss: 0.001082868665\n",
      "Epoch: 7236 | training loss: 0.002925707260\n",
      "Epoch: 7237 | training loss: 0.007987398654\n",
      "Epoch: 7238 | training loss: 0.007455128711\n",
      "Epoch: 7239 | training loss: 0.002405364532\n",
      "Epoch: 7240 | training loss: 0.000759543967\n",
      "Epoch: 7241 | training loss: 0.003727072384\n",
      "Epoch: 7242 | training loss: 0.005271255970\n",
      "Epoch: 7243 | training loss: 0.003020463977\n",
      "Epoch: 7244 | training loss: 0.001008689404\n",
      "Epoch: 7245 | training loss: 0.001813943032\n",
      "Epoch: 7246 | training loss: 0.003034722526\n",
      "Epoch: 7247 | training loss: 0.002498019719\n",
      "Epoch: 7248 | training loss: 0.001443331945\n",
      "Epoch: 7249 | training loss: 0.001361067989\n",
      "Epoch: 7250 | training loss: 0.001686479663\n",
      "Epoch: 7251 | training loss: 0.001622427953\n",
      "Epoch: 7252 | training loss: 0.001440211083\n",
      "Epoch: 7253 | training loss: 0.001364153228\n",
      "Epoch: 7254 | training loss: 0.001152331708\n",
      "Epoch: 7255 | training loss: 0.000957277603\n",
      "Epoch: 7256 | training loss: 0.001124043367\n",
      "Epoch: 7257 | training loss: 0.001312295673\n",
      "Epoch: 7258 | training loss: 0.001033875858\n",
      "Epoch: 7259 | training loss: 0.000658218749\n",
      "Epoch: 7260 | training loss: 0.000782153627\n",
      "Epoch: 7261 | training loss: 0.001110447105\n",
      "Epoch: 7262 | training loss: 0.001000438700\n",
      "Epoch: 7263 | training loss: 0.000617439393\n",
      "Epoch: 7264 | training loss: 0.000580762164\n",
      "Epoch: 7265 | training loss: 0.000857595354\n",
      "Epoch: 7266 | training loss: 0.000908680027\n",
      "Epoch: 7267 | training loss: 0.000659823650\n",
      "Epoch: 7268 | training loss: 0.000534157269\n",
      "Epoch: 7269 | training loss: 0.000669532456\n",
      "Epoch: 7270 | training loss: 0.000767217425\n",
      "Epoch: 7271 | training loss: 0.000669538626\n",
      "Epoch: 7272 | training loss: 0.000562377274\n",
      "Epoch: 7273 | training loss: 0.000586586073\n",
      "Epoch: 7274 | training loss: 0.000642187893\n",
      "Epoch: 7275 | training loss: 0.000625853776\n",
      "Epoch: 7276 | training loss: 0.000583614688\n",
      "Epoch: 7277 | training loss: 0.000574598496\n",
      "Epoch: 7278 | training loss: 0.000576767256\n",
      "Epoch: 7279 | training loss: 0.000568482908\n",
      "Epoch: 7280 | training loss: 0.000568906835\n",
      "Epoch: 7281 | training loss: 0.000575970043\n",
      "Epoch: 7282 | training loss: 0.000561559340\n",
      "Epoch: 7283 | training loss: 0.000536018866\n",
      "Epoch: 7284 | training loss: 0.000537809916\n",
      "Epoch: 7285 | training loss: 0.000560619927\n",
      "Epoch: 7286 | training loss: 0.000560257700\n",
      "Epoch: 7287 | training loss: 0.000532211270\n",
      "Epoch: 7288 | training loss: 0.000518762274\n",
      "Epoch: 7289 | training loss: 0.000535765546\n",
      "Epoch: 7290 | training loss: 0.000549447606\n",
      "Epoch: 7291 | training loss: 0.000536288717\n",
      "Epoch: 7292 | training loss: 0.000517844048\n",
      "Epoch: 7293 | training loss: 0.000519565772\n",
      "Epoch: 7294 | training loss: 0.000531816739\n",
      "Epoch: 7295 | training loss: 0.000532308652\n",
      "Epoch: 7296 | training loss: 0.000521922135\n",
      "Epoch: 7297 | training loss: 0.000516592234\n",
      "Epoch: 7298 | training loss: 0.000519958499\n",
      "Epoch: 7299 | training loss: 0.000522658695\n",
      "Epoch: 7300 | training loss: 0.000520354719\n",
      "Epoch: 7301 | training loss: 0.000517482578\n",
      "Epoch: 7302 | training loss: 0.000516759523\n",
      "Epoch: 7303 | training loss: 0.000516185537\n",
      "Epoch: 7304 | training loss: 0.000515074935\n",
      "Epoch: 7305 | training loss: 0.000515115855\n",
      "Epoch: 7306 | training loss: 0.000515763357\n",
      "Epoch: 7307 | training loss: 0.000514557003\n",
      "Epoch: 7308 | training loss: 0.000511982071\n",
      "Epoch: 7309 | training loss: 0.000511208316\n",
      "Epoch: 7310 | training loss: 0.000512746221\n",
      "Epoch: 7311 | training loss: 0.000513422478\n",
      "Epoch: 7312 | training loss: 0.000511535909\n",
      "Epoch: 7313 | training loss: 0.000509381178\n",
      "Epoch: 7314 | training loss: 0.000509418256\n",
      "Epoch: 7315 | training loss: 0.000510641257\n",
      "Epoch: 7316 | training loss: 0.000510607206\n",
      "Epoch: 7317 | training loss: 0.000509110047\n",
      "Epoch: 7318 | training loss: 0.000507979537\n",
      "Epoch: 7319 | training loss: 0.000508071797\n",
      "Epoch: 7320 | training loss: 0.000508413301\n",
      "Epoch: 7321 | training loss: 0.000508088851\n",
      "Epoch: 7322 | training loss: 0.000507366378\n",
      "Epoch: 7323 | training loss: 0.000506883196\n",
      "Epoch: 7324 | training loss: 0.000506668002\n",
      "Epoch: 7325 | training loss: 0.000506422832\n",
      "Epoch: 7326 | training loss: 0.000506109209\n",
      "Epoch: 7327 | training loss: 0.000505871256\n",
      "Epoch: 7328 | training loss: 0.000505633012\n",
      "Epoch: 7329 | training loss: 0.000505238364\n",
      "Epoch: 7330 | training loss: 0.000504801399\n",
      "Epoch: 7331 | training loss: 0.000504550873\n",
      "Epoch: 7332 | training loss: 0.000504478230\n",
      "Epoch: 7333 | training loss: 0.000504274911\n",
      "Epoch: 7334 | training loss: 0.000503856689\n",
      "Epoch: 7335 | training loss: 0.000503427465\n",
      "Epoch: 7336 | training loss: 0.000503207324\n",
      "Epoch: 7337 | training loss: 0.000503113144\n",
      "Epoch: 7338 | training loss: 0.000502899755\n",
      "Epoch: 7339 | training loss: 0.000502541079\n",
      "Epoch: 7340 | training loss: 0.000502186420\n",
      "Epoch: 7341 | training loss: 0.000501945673\n",
      "Epoch: 7342 | training loss: 0.000501755800\n",
      "Epoch: 7343 | training loss: 0.000501531526\n",
      "Epoch: 7344 | training loss: 0.000501254748\n",
      "Epoch: 7345 | training loss: 0.000500971219\n",
      "Epoch: 7346 | training loss: 0.000500720867\n",
      "Epoch: 7347 | training loss: 0.000500480819\n",
      "Epoch: 7348 | training loss: 0.000500223890\n",
      "Epoch: 7349 | training loss: 0.000499976333\n",
      "Epoch: 7350 | training loss: 0.000499745307\n",
      "Epoch: 7351 | training loss: 0.000499505724\n",
      "Epoch: 7352 | training loss: 0.000499237620\n",
      "Epoch: 7353 | training loss: 0.000498984591\n",
      "Epoch: 7354 | training loss: 0.000498742913\n",
      "Epoch: 7355 | training loss: 0.000498518348\n",
      "Epoch: 7356 | training loss: 0.000498281559\n",
      "Epoch: 7357 | training loss: 0.000498027133\n",
      "Epoch: 7358 | training loss: 0.000497773988\n",
      "Epoch: 7359 | training loss: 0.000497527479\n",
      "Epoch: 7360 | training loss: 0.000497291505\n",
      "Epoch: 7361 | training loss: 0.000497063273\n",
      "Epoch: 7362 | training loss: 0.000496815890\n",
      "Epoch: 7363 | training loss: 0.000496566412\n",
      "Epoch: 7364 | training loss: 0.000496323395\n",
      "Epoch: 7365 | training loss: 0.000496088993\n",
      "Epoch: 7366 | training loss: 0.000495853252\n",
      "Epoch: 7367 | training loss: 0.000495613494\n",
      "Epoch: 7368 | training loss: 0.000495370943\n",
      "Epoch: 7369 | training loss: 0.000495133805\n",
      "Epoch: 7370 | training loss: 0.000494892127\n",
      "Epoch: 7371 | training loss: 0.000494652952\n",
      "Epoch: 7372 | training loss: 0.000494417152\n",
      "Epoch: 7373 | training loss: 0.000494177744\n",
      "Epoch: 7374 | training loss: 0.000493946776\n",
      "Epoch: 7375 | training loss: 0.000493703003\n",
      "Epoch: 7376 | training loss: 0.000493457890\n",
      "Epoch: 7377 | training loss: 0.000493224128\n",
      "Epoch: 7378 | training loss: 0.000492983439\n",
      "Epoch: 7379 | training loss: 0.000492750318\n",
      "Epoch: 7380 | training loss: 0.000492515333\n",
      "Epoch: 7381 | training loss: 0.000492275110\n",
      "Epoch: 7382 | training loss: 0.000492035993\n",
      "Epoch: 7383 | training loss: 0.000491797866\n",
      "Epoch: 7384 | training loss: 0.000491562765\n",
      "Epoch: 7385 | training loss: 0.000491324987\n",
      "Epoch: 7386 | training loss: 0.000491088955\n",
      "Epoch: 7387 | training loss: 0.000490854902\n",
      "Epoch: 7388 | training loss: 0.000490616541\n",
      "Epoch: 7389 | training loss: 0.000490379171\n",
      "Epoch: 7390 | training loss: 0.000490145176\n",
      "Epoch: 7391 | training loss: 0.000489903614\n",
      "Epoch: 7392 | training loss: 0.000489670550\n",
      "Epoch: 7393 | training loss: 0.000489433238\n",
      "Epoch: 7394 | training loss: 0.000489196798\n",
      "Epoch: 7395 | training loss: 0.000488962629\n",
      "Epoch: 7396 | training loss: 0.000488724501\n",
      "Epoch: 7397 | training loss: 0.000488487887\n",
      "Epoch: 7398 | training loss: 0.000488255348\n",
      "Epoch: 7399 | training loss: 0.000488013320\n",
      "Epoch: 7400 | training loss: 0.000487776124\n",
      "Epoch: 7401 | training loss: 0.000487547688\n",
      "Epoch: 7402 | training loss: 0.000487304758\n",
      "Epoch: 7403 | training loss: 0.000487071491\n",
      "Epoch: 7404 | training loss: 0.000486835022\n",
      "Epoch: 7405 | training loss: 0.000486601086\n",
      "Epoch: 7406 | training loss: 0.000486362376\n",
      "Epoch: 7407 | training loss: 0.000486127188\n",
      "Epoch: 7408 | training loss: 0.000485894096\n",
      "Epoch: 7409 | training loss: 0.000485660275\n",
      "Epoch: 7410 | training loss: 0.000485421566\n",
      "Epoch: 7411 | training loss: 0.000485185359\n",
      "Epoch: 7412 | training loss: 0.000484951510\n",
      "Epoch: 7413 | training loss: 0.000484718941\n",
      "Epoch: 7414 | training loss: 0.000484479911\n",
      "Epoch: 7415 | training loss: 0.000484248798\n",
      "Epoch: 7416 | training loss: 0.000484012620\n",
      "Epoch: 7417 | training loss: 0.000483770971\n",
      "Epoch: 7418 | training loss: 0.000483543263\n",
      "Epoch: 7419 | training loss: 0.000483303214\n",
      "Epoch: 7420 | training loss: 0.000483072043\n",
      "Epoch: 7421 | training loss: 0.000482833770\n",
      "Epoch: 7422 | training loss: 0.000482602045\n",
      "Epoch: 7423 | training loss: 0.000482363568\n",
      "Epoch: 7424 | training loss: 0.000482130621\n",
      "Epoch: 7425 | training loss: 0.000481895404\n",
      "Epoch: 7426 | training loss: 0.000481660187\n",
      "Epoch: 7427 | training loss: 0.000481424708\n",
      "Epoch: 7428 | training loss: 0.000481189694\n",
      "Epoch: 7429 | training loss: 0.000480952556\n",
      "Epoch: 7430 | training loss: 0.000480718591\n",
      "Epoch: 7431 | training loss: 0.000480484567\n",
      "Epoch: 7432 | training loss: 0.000480246847\n",
      "Epoch: 7433 | training loss: 0.000480013259\n",
      "Epoch: 7434 | training loss: 0.000479778973\n",
      "Epoch: 7435 | training loss: 0.000479547161\n",
      "Epoch: 7436 | training loss: 0.000479311449\n",
      "Epoch: 7437 | training loss: 0.000479079084\n",
      "Epoch: 7438 | training loss: 0.000478848640\n",
      "Epoch: 7439 | training loss: 0.000478606671\n",
      "Epoch: 7440 | training loss: 0.000478377857\n",
      "Epoch: 7441 | training loss: 0.000478138827\n",
      "Epoch: 7442 | training loss: 0.000477903959\n",
      "Epoch: 7443 | training loss: 0.000477669179\n",
      "Epoch: 7444 | training loss: 0.000477437337\n",
      "Epoch: 7445 | training loss: 0.000477201247\n",
      "Epoch: 7446 | training loss: 0.000476967252\n",
      "Epoch: 7447 | training loss: 0.000476736051\n",
      "Epoch: 7448 | training loss: 0.000476495305\n",
      "Epoch: 7449 | training loss: 0.000476265384\n",
      "Epoch: 7450 | training loss: 0.000476029643\n",
      "Epoch: 7451 | training loss: 0.000475793349\n",
      "Epoch: 7452 | training loss: 0.000475562178\n",
      "Epoch: 7453 | training loss: 0.000475324283\n",
      "Epoch: 7454 | training loss: 0.000475094188\n",
      "Epoch: 7455 | training loss: 0.000474860892\n",
      "Epoch: 7456 | training loss: 0.000474623143\n",
      "Epoch: 7457 | training loss: 0.000474389526\n",
      "Epoch: 7458 | training loss: 0.000474151864\n",
      "Epoch: 7459 | training loss: 0.000473921828\n",
      "Epoch: 7460 | training loss: 0.000473690510\n",
      "Epoch: 7461 | training loss: 0.000473455177\n",
      "Epoch: 7462 | training loss: 0.000473218155\n",
      "Epoch: 7463 | training loss: 0.000472980260\n",
      "Epoch: 7464 | training loss: 0.000472746004\n",
      "Epoch: 7465 | training loss: 0.000472519634\n",
      "Epoch: 7466 | training loss: 0.000472281594\n",
      "Epoch: 7467 | training loss: 0.000472051674\n",
      "Epoch: 7468 | training loss: 0.000471815350\n",
      "Epoch: 7469 | training loss: 0.000471575418\n",
      "Epoch: 7470 | training loss: 0.000471342413\n",
      "Epoch: 7471 | training loss: 0.000471114297\n",
      "Epoch: 7472 | training loss: 0.000470877014\n",
      "Epoch: 7473 | training loss: 0.000470642699\n",
      "Epoch: 7474 | training loss: 0.000470412109\n",
      "Epoch: 7475 | training loss: 0.000470170984\n",
      "Epoch: 7476 | training loss: 0.000469939259\n",
      "Epoch: 7477 | training loss: 0.000469712482\n",
      "Epoch: 7478 | training loss: 0.000469476276\n",
      "Epoch: 7479 | training loss: 0.000469240942\n",
      "Epoch: 7480 | training loss: 0.000469007588\n",
      "Epoch: 7481 | training loss: 0.000468775863\n",
      "Epoch: 7482 | training loss: 0.000468543381\n",
      "Epoch: 7483 | training loss: 0.000468311016\n",
      "Epoch: 7484 | training loss: 0.000468072423\n",
      "Epoch: 7485 | training loss: 0.000467838516\n",
      "Epoch: 7486 | training loss: 0.000467611826\n",
      "Epoch: 7487 | training loss: 0.000467374048\n",
      "Epoch: 7488 | training loss: 0.000467143371\n",
      "Epoch: 7489 | training loss: 0.000466910598\n",
      "Epoch: 7490 | training loss: 0.000466666999\n",
      "Epoch: 7491 | training loss: 0.000466440921\n",
      "Epoch: 7492 | training loss: 0.000466206431\n",
      "Epoch: 7493 | training loss: 0.000465969497\n",
      "Epoch: 7494 | training loss: 0.000465735968\n",
      "Epoch: 7495 | training loss: 0.000465504650\n",
      "Epoch: 7496 | training loss: 0.000465268473\n",
      "Epoch: 7497 | training loss: 0.000465033751\n",
      "Epoch: 7498 | training loss: 0.000464801211\n",
      "Epoch: 7499 | training loss: 0.000464570825\n",
      "Epoch: 7500 | training loss: 0.000464340206\n",
      "Epoch: 7501 | training loss: 0.000464104465\n",
      "Epoch: 7502 | training loss: 0.000463871285\n",
      "Epoch: 7503 | training loss: 0.000463634904\n",
      "Epoch: 7504 | training loss: 0.000463404023\n",
      "Epoch: 7505 | training loss: 0.000463170523\n",
      "Epoch: 7506 | training loss: 0.000462937576\n",
      "Epoch: 7507 | training loss: 0.000462702388\n",
      "Epoch: 7508 | training loss: 0.000462472904\n",
      "Epoch: 7509 | training loss: 0.000462238706\n",
      "Epoch: 7510 | training loss: 0.000462004682\n",
      "Epoch: 7511 | training loss: 0.000461774645\n",
      "Epoch: 7512 | training loss: 0.000461544318\n",
      "Epoch: 7513 | training loss: 0.000461317308\n",
      "Epoch: 7514 | training loss: 0.000461084885\n",
      "Epoch: 7515 | training loss: 0.000460855605\n",
      "Epoch: 7516 | training loss: 0.000460623618\n",
      "Epoch: 7517 | training loss: 0.000460396899\n",
      "Epoch: 7518 | training loss: 0.000460170035\n",
      "Epoch: 7519 | training loss: 0.000459951145\n",
      "Epoch: 7520 | training loss: 0.000459728588\n",
      "Epoch: 7521 | training loss: 0.000459520961\n",
      "Epoch: 7522 | training loss: 0.000459328585\n",
      "Epoch: 7523 | training loss: 0.000459151692\n",
      "Epoch: 7524 | training loss: 0.000459006900\n",
      "Epoch: 7525 | training loss: 0.000458913448\n",
      "Epoch: 7526 | training loss: 0.000458893715\n",
      "Epoch: 7527 | training loss: 0.000459001923\n",
      "Epoch: 7528 | training loss: 0.000459302188\n",
      "Epoch: 7529 | training loss: 0.000459914212\n",
      "Epoch: 7530 | training loss: 0.000460995710\n",
      "Epoch: 7531 | training loss: 0.000462841534\n",
      "Epoch: 7532 | training loss: 0.000465928053\n",
      "Epoch: 7533 | training loss: 0.000471050094\n",
      "Epoch: 7534 | training loss: 0.000479507609\n",
      "Epoch: 7535 | training loss: 0.000493506377\n",
      "Epoch: 7536 | training loss: 0.000516671920\n",
      "Epoch: 7537 | training loss: 0.000555204344\n",
      "Epoch: 7538 | training loss: 0.000619475730\n",
      "Epoch: 7539 | training loss: 0.000727465551\n",
      "Epoch: 7540 | training loss: 0.000909539580\n",
      "Epoch: 7541 | training loss: 0.001218905672\n",
      "Epoch: 7542 | training loss: 0.001743897563\n",
      "Epoch: 7543 | training loss: 0.002641529776\n",
      "Epoch: 7544 | training loss: 0.004163254052\n",
      "Epoch: 7545 | training loss: 0.006751558743\n",
      "Epoch: 7546 | training loss: 0.011037077755\n",
      "Epoch: 7547 | training loss: 0.018041480333\n",
      "Epoch: 7548 | training loss: 0.028627563268\n",
      "Epoch: 7549 | training loss: 0.043481618166\n",
      "Epoch: 7550 | training loss: 0.059669006616\n",
      "Epoch: 7551 | training loss: 0.071193978190\n",
      "Epoch: 7552 | training loss: 0.065915755928\n",
      "Epoch: 7553 | training loss: 0.042166784406\n",
      "Epoch: 7554 | training loss: 0.012838725001\n",
      "Epoch: 7555 | training loss: 0.000463932229\n",
      "Epoch: 7556 | training loss: 0.010555101559\n",
      "Epoch: 7557 | training loss: 0.026509327814\n",
      "Epoch: 7558 | training loss: 0.028399430215\n",
      "Epoch: 7559 | training loss: 0.013892146759\n",
      "Epoch: 7560 | training loss: 0.001304294448\n",
      "Epoch: 7561 | training loss: 0.004073622636\n",
      "Epoch: 7562 | training loss: 0.014136536047\n",
      "Epoch: 7563 | training loss: 0.015471022576\n",
      "Epoch: 7564 | training loss: 0.006448732689\n",
      "Epoch: 7565 | training loss: 0.000487949525\n",
      "Epoch: 7566 | training loss: 0.004415418021\n",
      "Epoch: 7567 | training loss: 0.009690079838\n",
      "Epoch: 7568 | training loss: 0.007466242649\n",
      "Epoch: 7569 | training loss: 0.001672844868\n",
      "Epoch: 7570 | training loss: 0.001107912976\n",
      "Epoch: 7571 | training loss: 0.005015324336\n",
      "Epoch: 7572 | training loss: 0.005934189074\n",
      "Epoch: 7573 | training loss: 0.002512722742\n",
      "Epoch: 7574 | training loss: 0.000480235321\n",
      "Epoch: 7575 | training loss: 0.002380922902\n",
      "Epoch: 7576 | training loss: 0.004080695566\n",
      "Epoch: 7577 | training loss: 0.002565248869\n",
      "Epoch: 7578 | training loss: 0.000590097159\n",
      "Epoch: 7579 | training loss: 0.001150462893\n",
      "Epoch: 7580 | training loss: 0.002615522128\n",
      "Epoch: 7581 | training loss: 0.002232130850\n",
      "Epoch: 7582 | training loss: 0.000777733396\n",
      "Epoch: 7583 | training loss: 0.000646415225\n",
      "Epoch: 7584 | training loss: 0.001639639959\n",
      "Epoch: 7585 | training loss: 0.001783926506\n",
      "Epoch: 7586 | training loss: 0.000882370165\n",
      "Epoch: 7587 | training loss: 0.000486382341\n",
      "Epoch: 7588 | training loss: 0.001040898496\n",
      "Epoch: 7589 | training loss: 0.001370759332\n",
      "Epoch: 7590 | training loss: 0.000899226172\n",
      "Epoch: 7591 | training loss: 0.000470888626\n",
      "Epoch: 7592 | training loss: 0.000703882077\n",
      "Epoch: 7593 | training loss: 0.001033311128\n",
      "Epoch: 7594 | training loss: 0.000854000624\n",
      "Epoch: 7595 | training loss: 0.000502863608\n",
      "Epoch: 7596 | training loss: 0.000534421182\n",
      "Epoch: 7597 | training loss: 0.000784597360\n",
      "Epoch: 7598 | training loss: 0.000773870735\n",
      "Epoch: 7599 | training loss: 0.000536856125\n",
      "Epoch: 7600 | training loss: 0.000465885445\n",
      "Epoch: 7601 | training loss: 0.000616282981\n",
      "Epoch: 7602 | training loss: 0.000682928017\n",
      "Epoch: 7603 | training loss: 0.000554358994\n",
      "Epoch: 7604 | training loss: 0.000451705680\n",
      "Epoch: 7605 | training loss: 0.000514237676\n",
      "Epoch: 7606 | training loss: 0.000596256927\n",
      "Epoch: 7607 | training loss: 0.000551017816\n",
      "Epoch: 7608 | training loss: 0.000460409996\n",
      "Epoch: 7609 | training loss: 0.000462800614\n",
      "Epoch: 7610 | training loss: 0.000526281190\n",
      "Epoch: 7611 | training loss: 0.000531158759\n",
      "Epoch: 7612 | training loss: 0.000472279789\n",
      "Epoch: 7613 | training loss: 0.000444585166\n",
      "Epoch: 7614 | training loss: 0.000477626250\n",
      "Epoch: 7615 | training loss: 0.000502928160\n",
      "Epoch: 7616 | training loss: 0.000477415335\n",
      "Epoch: 7617 | training loss: 0.000444166537\n",
      "Epoch: 7618 | training loss: 0.000450281572\n",
      "Epoch: 7619 | training loss: 0.000474739616\n",
      "Epoch: 7620 | training loss: 0.000473236141\n",
      "Epoch: 7621 | training loss: 0.000449231477\n",
      "Epoch: 7622 | training loss: 0.000439570053\n",
      "Epoch: 7623 | training loss: 0.000452884648\n",
      "Epoch: 7624 | training loss: 0.000462276308\n",
      "Epoch: 7625 | training loss: 0.000452062057\n",
      "Epoch: 7626 | training loss: 0.000438640011\n",
      "Epoch: 7627 | training loss: 0.000440235453\n",
      "Epoch: 7628 | training loss: 0.000449719606\n",
      "Epoch: 7629 | training loss: 0.000449945917\n",
      "Epoch: 7630 | training loss: 0.000440673553\n",
      "Epoch: 7631 | training loss: 0.000435586844\n",
      "Epoch: 7632 | training loss: 0.000439848343\n",
      "Epoch: 7633 | training loss: 0.000444267411\n",
      "Epoch: 7634 | training loss: 0.000441272598\n",
      "Epoch: 7635 | training loss: 0.000435398601\n",
      "Epoch: 7636 | training loss: 0.000434468937\n",
      "Epoch: 7637 | training loss: 0.000437891285\n",
      "Epoch: 7638 | training loss: 0.000439124880\n",
      "Epoch: 7639 | training loss: 0.000435985392\n",
      "Epoch: 7640 | training loss: 0.000432869594\n",
      "Epoch: 7641 | training loss: 0.000433343812\n",
      "Epoch: 7642 | training loss: 0.000435368711\n",
      "Epoch: 7643 | training loss: 0.000435184105\n",
      "Epoch: 7644 | training loss: 0.000432818313\n",
      "Epoch: 7645 | training loss: 0.000431296387\n",
      "Epoch: 7646 | training loss: 0.000431958819\n",
      "Epoch: 7647 | training loss: 0.000432952307\n",
      "Epoch: 7648 | training loss: 0.000432333443\n",
      "Epoch: 7649 | training loss: 0.000430735585\n",
      "Epoch: 7650 | training loss: 0.000429982319\n",
      "Epoch: 7651 | training loss: 0.000430455490\n",
      "Epoch: 7652 | training loss: 0.000430808286\n",
      "Epoch: 7653 | training loss: 0.000430161221\n",
      "Epoch: 7654 | training loss: 0.000429132895\n",
      "Epoch: 7655 | training loss: 0.000428733329\n",
      "Epoch: 7656 | training loss: 0.000428964675\n",
      "Epoch: 7657 | training loss: 0.000429001113\n",
      "Epoch: 7658 | training loss: 0.000428464788\n",
      "Epoch: 7659 | training loss: 0.000427781168\n",
      "Epoch: 7660 | training loss: 0.000427508116\n",
      "Epoch: 7661 | training loss: 0.000427561521\n",
      "Epoch: 7662 | training loss: 0.000427461491\n",
      "Epoch: 7663 | training loss: 0.000427029881\n",
      "Epoch: 7664 | training loss: 0.000426543556\n",
      "Epoch: 7665 | training loss: 0.000426302344\n",
      "Epoch: 7666 | training loss: 0.000426238694\n",
      "Epoch: 7667 | training loss: 0.000426078914\n",
      "Epoch: 7668 | training loss: 0.000425725651\n",
      "Epoch: 7669 | training loss: 0.000425353617\n",
      "Epoch: 7670 | training loss: 0.000425124483\n",
      "Epoch: 7671 | training loss: 0.000424996950\n",
      "Epoch: 7672 | training loss: 0.000424816622\n",
      "Epoch: 7673 | training loss: 0.000424519909\n",
      "Epoch: 7674 | training loss: 0.000424204860\n",
      "Epoch: 7675 | training loss: 0.000423964491\n",
      "Epoch: 7676 | training loss: 0.000423804420\n",
      "Epoch: 7677 | training loss: 0.000423617428\n",
      "Epoch: 7678 | training loss: 0.000423355435\n",
      "Epoch: 7679 | training loss: 0.000423074292\n",
      "Epoch: 7680 | training loss: 0.000422832789\n",
      "Epoch: 7681 | training loss: 0.000422643090\n",
      "Epoch: 7682 | training loss: 0.000422442215\n",
      "Epoch: 7683 | training loss: 0.000422219106\n",
      "Epoch: 7684 | training loss: 0.000421959732\n",
      "Epoch: 7685 | training loss: 0.000421714911\n",
      "Epoch: 7686 | training loss: 0.000421507517\n",
      "Epoch: 7687 | training loss: 0.000421307544\n",
      "Epoch: 7688 | training loss: 0.000421090925\n",
      "Epoch: 7689 | training loss: 0.000420853466\n",
      "Epoch: 7690 | training loss: 0.000420615019\n",
      "Epoch: 7691 | training loss: 0.000420401135\n",
      "Epoch: 7692 | training loss: 0.000420190016\n",
      "Epoch: 7693 | training loss: 0.000419978955\n",
      "Epoch: 7694 | training loss: 0.000419750635\n",
      "Epoch: 7695 | training loss: 0.000419524935\n",
      "Epoch: 7696 | training loss: 0.000419302931\n",
      "Epoch: 7697 | training loss: 0.000419093092\n",
      "Epoch: 7698 | training loss: 0.000418880780\n",
      "Epoch: 7699 | training loss: 0.000418663840\n",
      "Epoch: 7700 | training loss: 0.000418437878\n",
      "Epoch: 7701 | training loss: 0.000418210955\n",
      "Epoch: 7702 | training loss: 0.000418003619\n",
      "Epoch: 7703 | training loss: 0.000417794276\n",
      "Epoch: 7704 | training loss: 0.000417573727\n",
      "Epoch: 7705 | training loss: 0.000417352538\n",
      "Epoch: 7706 | training loss: 0.000417130068\n",
      "Epoch: 7707 | training loss: 0.000416916213\n",
      "Epoch: 7708 | training loss: 0.000416706869\n",
      "Epoch: 7709 | training loss: 0.000416490628\n",
      "Epoch: 7710 | training loss: 0.000416268886\n",
      "Epoch: 7711 | training loss: 0.000416050229\n",
      "Epoch: 7712 | training loss: 0.000415836752\n",
      "Epoch: 7713 | training loss: 0.000415626302\n",
      "Epoch: 7714 | training loss: 0.000415404385\n",
      "Epoch: 7715 | training loss: 0.000415192597\n",
      "Epoch: 7716 | training loss: 0.000414976064\n",
      "Epoch: 7717 | training loss: 0.000414763985\n",
      "Epoch: 7718 | training loss: 0.000414546812\n",
      "Epoch: 7719 | training loss: 0.000414331415\n",
      "Epoch: 7720 | training loss: 0.000414122653\n",
      "Epoch: 7721 | training loss: 0.000413901958\n",
      "Epoch: 7722 | training loss: 0.000413681730\n",
      "Epoch: 7723 | training loss: 0.000413469126\n",
      "Epoch: 7724 | training loss: 0.000413256697\n",
      "Epoch: 7725 | training loss: 0.000413045549\n",
      "Epoch: 7726 | training loss: 0.000412831840\n",
      "Epoch: 7727 | training loss: 0.000412616937\n",
      "Epoch: 7728 | training loss: 0.000412404072\n",
      "Epoch: 7729 | training loss: 0.000412184425\n",
      "Epoch: 7730 | training loss: 0.000411970454\n",
      "Epoch: 7731 | training loss: 0.000411759946\n",
      "Epoch: 7732 | training loss: 0.000411545538\n",
      "Epoch: 7733 | training loss: 0.000411333574\n",
      "Epoch: 7734 | training loss: 0.000411114946\n",
      "Epoch: 7735 | training loss: 0.000410906971\n",
      "Epoch: 7736 | training loss: 0.000410691253\n",
      "Epoch: 7737 | training loss: 0.000410477514\n",
      "Epoch: 7738 | training loss: 0.000410263368\n",
      "Epoch: 7739 | training loss: 0.000410047971\n",
      "Epoch: 7740 | training loss: 0.000409836473\n",
      "Epoch: 7741 | training loss: 0.000409622968\n",
      "Epoch: 7742 | training loss: 0.000409411412\n",
      "Epoch: 7743 | training loss: 0.000409196888\n",
      "Epoch: 7744 | training loss: 0.000408978900\n",
      "Epoch: 7745 | training loss: 0.000408770400\n",
      "Epoch: 7746 | training loss: 0.000408557797\n",
      "Epoch: 7747 | training loss: 0.000408342836\n",
      "Epoch: 7748 | training loss: 0.000408131513\n",
      "Epoch: 7749 | training loss: 0.000407916144\n",
      "Epoch: 7750 | training loss: 0.000407701824\n",
      "Epoch: 7751 | training loss: 0.000407492742\n",
      "Epoch: 7752 | training loss: 0.000407284650\n",
      "Epoch: 7753 | training loss: 0.000407067826\n",
      "Epoch: 7754 | training loss: 0.000406851701\n",
      "Epoch: 7755 | training loss: 0.000406636333\n",
      "Epoch: 7756 | training loss: 0.000406426669\n",
      "Epoch: 7757 | training loss: 0.000406217732\n",
      "Epoch: 7758 | training loss: 0.000406004954\n",
      "Epoch: 7759 | training loss: 0.000405787374\n",
      "Epoch: 7760 | training loss: 0.000405580155\n",
      "Epoch: 7761 | training loss: 0.000405366096\n",
      "Epoch: 7762 | training loss: 0.000405153085\n",
      "Epoch: 7763 | training loss: 0.000404942926\n",
      "Epoch: 7764 | training loss: 0.000404729217\n",
      "Epoch: 7765 | training loss: 0.000404516730\n",
      "Epoch: 7766 | training loss: 0.000404305407\n",
      "Epoch: 7767 | training loss: 0.000404091319\n",
      "Epoch: 7768 | training loss: 0.000403878395\n",
      "Epoch: 7769 | training loss: 0.000403668935\n",
      "Epoch: 7770 | training loss: 0.000403453974\n",
      "Epoch: 7771 | training loss: 0.000403245620\n",
      "Epoch: 7772 | training loss: 0.000403028622\n",
      "Epoch: 7773 | training loss: 0.000402816513\n",
      "Epoch: 7774 | training loss: 0.000402607315\n",
      "Epoch: 7775 | training loss: 0.000402397243\n",
      "Epoch: 7776 | training loss: 0.000402179663\n",
      "Epoch: 7777 | training loss: 0.000401971076\n",
      "Epoch: 7778 | training loss: 0.000401758589\n",
      "Epoch: 7779 | training loss: 0.000401547062\n",
      "Epoch: 7780 | training loss: 0.000401335681\n",
      "Epoch: 7781 | training loss: 0.000401118770\n",
      "Epoch: 7782 | training loss: 0.000400911667\n",
      "Epoch: 7783 | training loss: 0.000400699035\n",
      "Epoch: 7784 | training loss: 0.000400486111\n",
      "Epoch: 7785 | training loss: 0.000400278484\n",
      "Epoch: 7786 | training loss: 0.000400065561\n",
      "Epoch: 7787 | training loss: 0.000399852084\n",
      "Epoch: 7788 | training loss: 0.000399639190\n",
      "Epoch: 7789 | training loss: 0.000399430108\n",
      "Epoch: 7790 | training loss: 0.000399215438\n",
      "Epoch: 7791 | training loss: 0.000399007724\n",
      "Epoch: 7792 | training loss: 0.000398794189\n",
      "Epoch: 7793 | training loss: 0.000398583245\n",
      "Epoch: 7794 | training loss: 0.000398368516\n",
      "Epoch: 7795 | training loss: 0.000398158882\n",
      "Epoch: 7796 | training loss: 0.000397951721\n",
      "Epoch: 7797 | training loss: 0.000397734140\n",
      "Epoch: 7798 | training loss: 0.000397525553\n",
      "Epoch: 7799 | training loss: 0.000397312309\n",
      "Epoch: 7800 | training loss: 0.000397103489\n",
      "Epoch: 7801 | training loss: 0.000396890653\n",
      "Epoch: 7802 | training loss: 0.000396680087\n",
      "Epoch: 7803 | training loss: 0.000396470656\n",
      "Epoch: 7804 | training loss: 0.000396259944\n",
      "Epoch: 7805 | training loss: 0.000396050687\n",
      "Epoch: 7806 | training loss: 0.000395840267\n",
      "Epoch: 7807 | training loss: 0.000395629148\n",
      "Epoch: 7808 | training loss: 0.000395416864\n",
      "Epoch: 7809 | training loss: 0.000395206152\n",
      "Epoch: 7810 | training loss: 0.000394994218\n",
      "Epoch: 7811 | training loss: 0.000394785631\n",
      "Epoch: 7812 | training loss: 0.000394573493\n",
      "Epoch: 7813 | training loss: 0.000394360686\n",
      "Epoch: 7814 | training loss: 0.000394151662\n",
      "Epoch: 7815 | training loss: 0.000393940107\n",
      "Epoch: 7816 | training loss: 0.000393730210\n",
      "Epoch: 7817 | training loss: 0.000393516180\n",
      "Epoch: 7818 | training loss: 0.000393306749\n",
      "Epoch: 7819 | training loss: 0.000393100025\n",
      "Epoch: 7820 | training loss: 0.000392887276\n",
      "Epoch: 7821 | training loss: 0.000392678921\n",
      "Epoch: 7822 | training loss: 0.000392471586\n",
      "Epoch: 7823 | training loss: 0.000392257352\n",
      "Epoch: 7824 | training loss: 0.000392045418\n",
      "Epoch: 7825 | training loss: 0.000391834765\n",
      "Epoch: 7826 | training loss: 0.000391624082\n",
      "Epoch: 7827 | training loss: 0.000391412352\n",
      "Epoch: 7828 | training loss: 0.000391203503\n",
      "Epoch: 7829 | training loss: 0.000390995643\n",
      "Epoch: 7830 | training loss: 0.000390783040\n",
      "Epoch: 7831 | training loss: 0.000390572473\n",
      "Epoch: 7832 | training loss: 0.000390365894\n",
      "Epoch: 7833 | training loss: 0.000390155765\n",
      "Epoch: 7834 | training loss: 0.000389944063\n",
      "Epoch: 7835 | training loss: 0.000389737776\n",
      "Epoch: 7836 | training loss: 0.000389524066\n",
      "Epoch: 7837 | training loss: 0.000389316439\n",
      "Epoch: 7838 | training loss: 0.000389099238\n",
      "Epoch: 7839 | training loss: 0.000388890621\n",
      "Epoch: 7840 | training loss: 0.000388681365\n",
      "Epoch: 7841 | training loss: 0.000388472807\n",
      "Epoch: 7842 | training loss: 0.000388261513\n",
      "Epoch: 7843 | training loss: 0.000388051732\n",
      "Epoch: 7844 | training loss: 0.000387845241\n",
      "Epoch: 7845 | training loss: 0.000387630047\n",
      "Epoch: 7846 | training loss: 0.000387422566\n",
      "Epoch: 7847 | training loss: 0.000387214473\n",
      "Epoch: 7848 | training loss: 0.000387000968\n",
      "Epoch: 7849 | training loss: 0.000386793690\n",
      "Epoch: 7850 | training loss: 0.000386583328\n",
      "Epoch: 7851 | training loss: 0.000386371859\n",
      "Epoch: 7852 | training loss: 0.000386160944\n",
      "Epoch: 7853 | training loss: 0.000385948166\n",
      "Epoch: 7854 | training loss: 0.000385740626\n",
      "Epoch: 7855 | training loss: 0.000385532127\n",
      "Epoch: 7856 | training loss: 0.000385324558\n",
      "Epoch: 7857 | training loss: 0.000385114428\n",
      "Epoch: 7858 | training loss: 0.000384902582\n",
      "Epoch: 7859 | training loss: 0.000384695537\n",
      "Epoch: 7860 | training loss: 0.000384487386\n",
      "Epoch: 7861 | training loss: 0.000384275161\n",
      "Epoch: 7862 | training loss: 0.000384066079\n",
      "Epoch: 7863 | training loss: 0.000383855106\n",
      "Epoch: 7864 | training loss: 0.000383646053\n",
      "Epoch: 7865 | training loss: 0.000383440667\n",
      "Epoch: 7866 | training loss: 0.000383229868\n",
      "Epoch: 7867 | training loss: 0.000383020408\n",
      "Epoch: 7868 | training loss: 0.000382813276\n",
      "Epoch: 7869 | training loss: 0.000382603146\n",
      "Epoch: 7870 | training loss: 0.000382396509\n",
      "Epoch: 7871 | training loss: 0.000382184371\n",
      "Epoch: 7872 | training loss: 0.000381977268\n",
      "Epoch: 7873 | training loss: 0.000381765654\n",
      "Epoch: 7874 | training loss: 0.000381554622\n",
      "Epoch: 7875 | training loss: 0.000381346967\n",
      "Epoch: 7876 | training loss: 0.000381136197\n",
      "Epoch: 7877 | training loss: 0.000380931713\n",
      "Epoch: 7878 | training loss: 0.000380723330\n",
      "Epoch: 7879 | training loss: 0.000380517507\n",
      "Epoch: 7880 | training loss: 0.000380315090\n",
      "Epoch: 7881 | training loss: 0.000380109210\n",
      "Epoch: 7882 | training loss: 0.000379909150\n",
      "Epoch: 7883 | training loss: 0.000379707111\n",
      "Epoch: 7884 | training loss: 0.000379518635\n",
      "Epoch: 7885 | training loss: 0.000379335426\n",
      "Epoch: 7886 | training loss: 0.000379165576\n",
      "Epoch: 7887 | training loss: 0.000379028352\n",
      "Epoch: 7888 | training loss: 0.000378931931\n",
      "Epoch: 7889 | training loss: 0.000378902856\n",
      "Epoch: 7890 | training loss: 0.000378992583\n",
      "Epoch: 7891 | training loss: 0.000379273028\n",
      "Epoch: 7892 | training loss: 0.000379883946\n",
      "Epoch: 7893 | training loss: 0.000381034741\n",
      "Epoch: 7894 | training loss: 0.000383123959\n",
      "Epoch: 7895 | training loss: 0.000386796368\n",
      "Epoch: 7896 | training loss: 0.000393180380\n",
      "Epoch: 7897 | training loss: 0.000404285383\n",
      "Epoch: 7898 | training loss: 0.000423545076\n",
      "Epoch: 7899 | training loss: 0.000456986483\n",
      "Epoch: 7900 | training loss: 0.000515310676\n",
      "Epoch: 7901 | training loss: 0.000617314072\n",
      "Epoch: 7902 | training loss: 0.000796326669\n",
      "Epoch: 7903 | training loss: 0.001109917415\n",
      "Epoch: 7904 | training loss: 0.001658814494\n",
      "Epoch: 7905 | training loss: 0.002606621478\n",
      "Epoch: 7906 | training loss: 0.004211605061\n",
      "Epoch: 7907 | training loss: 0.006793250330\n",
      "Epoch: 7908 | training loss: 0.010630686767\n",
      "Epoch: 7909 | training loss: 0.015415239148\n",
      "Epoch: 7910 | training loss: 0.019697384909\n",
      "Epoch: 7911 | training loss: 0.020490555093\n",
      "Epoch: 7912 | training loss: 0.015919251367\n",
      "Epoch: 7913 | training loss: 0.008374752477\n",
      "Epoch: 7914 | training loss: 0.003846137552\n",
      "Epoch: 7915 | training loss: 0.005695956759\n",
      "Epoch: 7916 | training loss: 0.011177781969\n",
      "Epoch: 7917 | training loss: 0.014309971593\n",
      "Epoch: 7918 | training loss: 0.011833797209\n",
      "Epoch: 7919 | training loss: 0.006860603578\n",
      "Epoch: 7920 | training loss: 0.005043729208\n",
      "Epoch: 7921 | training loss: 0.007122170180\n",
      "Epoch: 7922 | training loss: 0.008563855663\n",
      "Epoch: 7923 | training loss: 0.006135786418\n",
      "Epoch: 7924 | training loss: 0.002153855748\n",
      "Epoch: 7925 | training loss: 0.000888717477\n",
      "Epoch: 7926 | training loss: 0.002510190243\n",
      "Epoch: 7927 | training loss: 0.003607027465\n",
      "Epoch: 7928 | training loss: 0.002465563593\n",
      "Epoch: 7929 | training loss: 0.001104936935\n",
      "Epoch: 7930 | training loss: 0.001623747870\n",
      "Epoch: 7931 | training loss: 0.003045029473\n",
      "Epoch: 7932 | training loss: 0.003145336173\n",
      "Epoch: 7933 | training loss: 0.001831068890\n",
      "Epoch: 7934 | training loss: 0.000935101358\n",
      "Epoch: 7935 | training loss: 0.001263978542\n",
      "Epoch: 7936 | training loss: 0.001682886621\n",
      "Epoch: 7937 | training loss: 0.001218636055\n",
      "Epoch: 7938 | training loss: 0.000503546267\n",
      "Epoch: 7939 | training loss: 0.000550346682\n",
      "Epoch: 7940 | training loss: 0.001149168820\n",
      "Epoch: 7941 | training loss: 0.001358022215\n",
      "Epoch: 7942 | training loss: 0.000979806529\n",
      "Epoch: 7943 | training loss: 0.000679982128\n",
      "Epoch: 7944 | training loss: 0.000825799478\n",
      "Epoch: 7945 | training loss: 0.001008272404\n",
      "Epoch: 7946 | training loss: 0.000815892534\n",
      "Epoch: 7947 | training loss: 0.000469687948\n",
      "Epoch: 7948 | training loss: 0.000399799465\n",
      "Epoch: 7949 | training loss: 0.000585204631\n",
      "Epoch: 7950 | training loss: 0.000672389404\n",
      "Epoch: 7951 | training loss: 0.000548777170\n",
      "Epoch: 7952 | training loss: 0.000448379258\n",
      "Epoch: 7953 | training loss: 0.000528470555\n",
      "Epoch: 7954 | training loss: 0.000644400716\n",
      "Epoch: 7955 | training loss: 0.000612109085\n",
      "Epoch: 7956 | training loss: 0.000482687727\n",
      "Epoch: 7957 | training loss: 0.000426652667\n",
      "Epoch: 7958 | training loss: 0.000470909814\n",
      "Epoch: 7959 | training loss: 0.000494055974\n",
      "Epoch: 7960 | training loss: 0.000434599293\n",
      "Epoch: 7961 | training loss: 0.000370292051\n",
      "Epoch: 7962 | training loss: 0.000380675134\n",
      "Epoch: 7963 | training loss: 0.000432060682\n",
      "Epoch: 7964 | training loss: 0.000444861595\n",
      "Epoch: 7965 | training loss: 0.000412272813\n",
      "Epoch: 7966 | training loss: 0.000392939022\n",
      "Epoch: 7967 | training loss: 0.000413733622\n",
      "Epoch: 7968 | training loss: 0.000437274139\n",
      "Epoch: 7969 | training loss: 0.000426391372\n",
      "Epoch: 7970 | training loss: 0.000396146992\n",
      "Epoch: 7971 | training loss: 0.000383661303\n",
      "Epoch: 7972 | training loss: 0.000393416965\n",
      "Epoch: 7973 | training loss: 0.000398391334\n",
      "Epoch: 7974 | training loss: 0.000384129467\n",
      "Epoch: 7975 | training loss: 0.000366571650\n",
      "Epoch: 7976 | training loss: 0.000364636187\n",
      "Epoch: 7977 | training loss: 0.000373964838\n",
      "Epoch: 7978 | training loss: 0.000377206336\n",
      "Epoch: 7979 | training loss: 0.000369716901\n",
      "Epoch: 7980 | training loss: 0.000363039551\n",
      "Epoch: 7981 | training loss: 0.000366013031\n",
      "Epoch: 7982 | training loss: 0.000373229646\n",
      "Epoch: 7983 | training loss: 0.000374842610\n",
      "Epoch: 7984 | training loss: 0.000370333815\n",
      "Epoch: 7985 | training loss: 0.000367270841\n",
      "Epoch: 7986 | training loss: 0.000369779940\n",
      "Epoch: 7987 | training loss: 0.000373765593\n",
      "Epoch: 7988 | training loss: 0.000373880786\n",
      "Epoch: 7989 | training loss: 0.000370819878\n",
      "Epoch: 7990 | training loss: 0.000369174668\n",
      "Epoch: 7991 | training loss: 0.000370838330\n",
      "Epoch: 7992 | training loss: 0.000373181683\n",
      "Epoch: 7993 | training loss: 0.000373329385\n",
      "Epoch: 7994 | training loss: 0.000371891190\n",
      "Epoch: 7995 | training loss: 0.000371578324\n",
      "Epoch: 7996 | training loss: 0.000373423682\n",
      "Epoch: 7997 | training loss: 0.000375946664\n",
      "Epoch: 7998 | training loss: 0.000377512712\n",
      "Epoch: 7999 | training loss: 0.000378613011\n",
      "Epoch: 8000 | training loss: 0.000380950223\n",
      "Epoch: 8001 | training loss: 0.000385296007\n",
      "Epoch: 8002 | training loss: 0.000390969275\n",
      "Epoch: 8003 | training loss: 0.000397454365\n",
      "Epoch: 8004 | training loss: 0.000405531900\n",
      "Epoch: 8005 | training loss: 0.000416846829\n",
      "Epoch: 8006 | training loss: 0.000432816625\n",
      "Epoch: 8007 | training loss: 0.000454501045\n",
      "Epoch: 8008 | training loss: 0.000483299547\n",
      "Epoch: 8009 | training loss: 0.000522309623\n",
      "Epoch: 8010 | training loss: 0.000575991231\n",
      "Epoch: 8011 | training loss: 0.000651043607\n",
      "Epoch: 8012 | training loss: 0.000755642774\n",
      "Epoch: 8013 | training loss: 0.000902396161\n",
      "Epoch: 8014 | training loss: 0.001108267345\n",
      "Epoch: 8015 | training loss: 0.001400905778\n",
      "Epoch: 8016 | training loss: 0.001814478310\n",
      "Epoch: 8017 | training loss: 0.002404381987\n",
      "Epoch: 8018 | training loss: 0.003233345924\n",
      "Epoch: 8019 | training loss: 0.004406143911\n",
      "Epoch: 8020 | training loss: 0.006014467683\n",
      "Epoch: 8021 | training loss: 0.008211535402\n",
      "Epoch: 8022 | training loss: 0.011029674672\n",
      "Epoch: 8023 | training loss: 0.014525806531\n",
      "Epoch: 8024 | training loss: 0.018285123631\n",
      "Epoch: 8025 | training loss: 0.021821329370\n",
      "Epoch: 8026 | training loss: 0.023736763746\n",
      "Epoch: 8027 | training loss: 0.023091586307\n",
      "Epoch: 8028 | training loss: 0.018898379058\n",
      "Epoch: 8029 | training loss: 0.012303249910\n",
      "Epoch: 8030 | training loss: 0.005465081427\n",
      "Epoch: 8031 | training loss: 0.001137306681\n",
      "Epoch: 8032 | training loss: 0.000582745299\n",
      "Epoch: 8033 | training loss: 0.003022290301\n",
      "Epoch: 8034 | training loss: 0.006256107241\n",
      "Epoch: 8035 | training loss: 0.007971969433\n",
      "Epoch: 8036 | training loss: 0.007151804864\n",
      "Epoch: 8037 | training loss: 0.004397825804\n",
      "Epoch: 8038 | training loss: 0.001594261266\n",
      "Epoch: 8039 | training loss: 0.000363723666\n",
      "Epoch: 8040 | training loss: 0.001044828212\n",
      "Epoch: 8041 | training loss: 0.002639798215\n",
      "Epoch: 8042 | training loss: 0.003713279730\n",
      "Epoch: 8043 | training loss: 0.003474873723\n",
      "Epoch: 8044 | training loss: 0.002185208024\n",
      "Epoch: 8045 | training loss: 0.000857061648\n",
      "Epoch: 8046 | training loss: 0.000353114272\n",
      "Epoch: 8047 | training loss: 0.000791747472\n",
      "Epoch: 8048 | training loss: 0.001588307088\n",
      "Epoch: 8049 | training loss: 0.002010493539\n",
      "Epoch: 8050 | training loss: 0.001751766773\n",
      "Epoch: 8051 | training loss: 0.001063583652\n",
      "Epoch: 8052 | training loss: 0.000483965501\n",
      "Epoch: 8053 | training loss: 0.000371990900\n",
      "Epoch: 8054 | training loss: 0.000677243632\n",
      "Epoch: 8055 | training loss: 0.001053968328\n",
      "Epoch: 8056 | training loss: 0.001172598219\n",
      "Epoch: 8057 | training loss: 0.000963485567\n",
      "Epoch: 8058 | training loss: 0.000611438416\n",
      "Epoch: 8059 | training loss: 0.000376223004\n",
      "Epoch: 8060 | training loss: 0.000383820152\n",
      "Epoch: 8061 | training loss: 0.000561126042\n",
      "Epoch: 8062 | training loss: 0.000728208572\n",
      "Epoch: 8063 | training loss: 0.000749100698\n",
      "Epoch: 8064 | training loss: 0.000622004911\n",
      "Epoch: 8065 | training loss: 0.000451206171\n",
      "Epoch: 8066 | training loss: 0.000353804702\n",
      "Epoch: 8067 | training loss: 0.000373268384\n",
      "Epoch: 8068 | training loss: 0.000463600387\n",
      "Epoch: 8069 | training loss: 0.000539534143\n",
      "Epoch: 8070 | training loss: 0.000543344999\n",
      "Epoch: 8071 | training loss: 0.000479343405\n",
      "Epoch: 8072 | training loss: 0.000397082797\n",
      "Epoch: 8073 | training loss: 0.000349579117\n",
      "Epoch: 8074 | training loss: 0.000356602366\n",
      "Epoch: 8075 | training loss: 0.000398298958\n",
      "Epoch: 8076 | training loss: 0.000436353614\n",
      "Epoch: 8077 | training loss: 0.000442893011\n",
      "Epoch: 8078 | training loss: 0.000416460098\n",
      "Epoch: 8079 | training loss: 0.000377090182\n",
      "Epoch: 8080 | training loss: 0.000349367299\n",
      "Epoch: 8081 | training loss: 0.000345924869\n",
      "Epoch: 8082 | training loss: 0.000362031889\n",
      "Epoch: 8083 | training loss: 0.000381976773\n",
      "Epoch: 8084 | training loss: 0.000391109788\n",
      "Epoch: 8085 | training loss: 0.000384416024\n",
      "Epoch: 8086 | training loss: 0.000367379369\n",
      "Epoch: 8087 | training loss: 0.000350684975\n",
      "Epoch: 8088 | training loss: 0.000342806627\n",
      "Epoch: 8089 | training loss: 0.000345476437\n",
      "Epoch: 8090 | training loss: 0.000354105927\n",
      "Epoch: 8091 | training loss: 0.000361783867\n",
      "Epoch: 8092 | training loss: 0.000363591942\n",
      "Epoch: 8093 | training loss: 0.000358909165\n",
      "Epoch: 8094 | training loss: 0.000350916322\n",
      "Epoch: 8095 | training loss: 0.000343942520\n",
      "Epoch: 8096 | training loss: 0.000340929138\n",
      "Epoch: 8097 | training loss: 0.000342177285\n",
      "Epoch: 8098 | training loss: 0.000345771201\n",
      "Epoch: 8099 | training loss: 0.000349031441\n",
      "Epoch: 8100 | training loss: 0.000349972397\n",
      "Epoch: 8101 | training loss: 0.000348223897\n",
      "Epoch: 8102 | training loss: 0.000344849424\n",
      "Epoch: 8103 | training loss: 0.000341516279\n",
      "Epoch: 8104 | training loss: 0.000339520979\n",
      "Epoch: 8105 | training loss: 0.000339282560\n",
      "Epoch: 8106 | training loss: 0.000340346014\n",
      "Epoch: 8107 | training loss: 0.000341765903\n",
      "Epoch: 8108 | training loss: 0.000342633895\n",
      "Epoch: 8109 | training loss: 0.000342495594\n",
      "Epoch: 8110 | training loss: 0.000341420295\n",
      "Epoch: 8111 | training loss: 0.000339887250\n",
      "Epoch: 8112 | training loss: 0.000338494981\n",
      "Epoch: 8113 | training loss: 0.000337659381\n",
      "Epoch: 8114 | training loss: 0.000337472651\n",
      "Epoch: 8115 | training loss: 0.000337754318\n",
      "Epoch: 8116 | training loss: 0.000338198734\n",
      "Epoch: 8117 | training loss: 0.000338494050\n",
      "Epoch: 8118 | training loss: 0.000338436221\n",
      "Epoch: 8119 | training loss: 0.000338024751\n",
      "Epoch: 8120 | training loss: 0.000337386038\n",
      "Epoch: 8121 | training loss: 0.000336705794\n",
      "Epoch: 8122 | training loss: 0.000336148194\n",
      "Epoch: 8123 | training loss: 0.000335784425\n",
      "Epoch: 8124 | training loss: 0.000335619203\n",
      "Epoch: 8125 | training loss: 0.000335600111\n",
      "Epoch: 8126 | training loss: 0.000335631019\n",
      "Epoch: 8127 | training loss: 0.000335631543\n",
      "Epoch: 8128 | training loss: 0.000335542572\n",
      "Epoch: 8129 | training loss: 0.000335341960\n",
      "Epoch: 8130 | training loss: 0.000335056509\n",
      "Epoch: 8131 | training loss: 0.000334710290\n",
      "Epoch: 8132 | training loss: 0.000334369659\n",
      "Epoch: 8133 | training loss: 0.000334075012\n",
      "Epoch: 8134 | training loss: 0.000333843229\n",
      "Epoch: 8135 | training loss: 0.000333674921\n",
      "Epoch: 8136 | training loss: 0.000333551987\n",
      "Epoch: 8137 | training loss: 0.000333455449\n",
      "Epoch: 8138 | training loss: 0.000333352888\n",
      "Epoch: 8139 | training loss: 0.000333232543\n",
      "Epoch: 8140 | training loss: 0.000333069853\n",
      "Epoch: 8141 | training loss: 0.000332880009\n",
      "Epoch: 8142 | training loss: 0.000332660304\n",
      "Epoch: 8143 | training loss: 0.000332428841\n",
      "Epoch: 8144 | training loss: 0.000332203927\n",
      "Epoch: 8145 | training loss: 0.000331989169\n",
      "Epoch: 8146 | training loss: 0.000331786461\n",
      "Epoch: 8147 | training loss: 0.000331603020\n",
      "Epoch: 8148 | training loss: 0.000331434072\n",
      "Epoch: 8149 | training loss: 0.000331272226\n",
      "Epoch: 8150 | training loss: 0.000331118703\n",
      "Epoch: 8151 | training loss: 0.000330961484\n",
      "Epoch: 8152 | training loss: 0.000330803654\n",
      "Epoch: 8153 | training loss: 0.000330639828\n",
      "Epoch: 8154 | training loss: 0.000330468640\n",
      "Epoch: 8155 | training loss: 0.000330293464\n",
      "Epoch: 8156 | training loss: 0.000330116134\n",
      "Epoch: 8157 | training loss: 0.000329939474\n",
      "Epoch: 8158 | training loss: 0.000329764327\n",
      "Epoch: 8159 | training loss: 0.000329582777\n",
      "Epoch: 8160 | training loss: 0.000329410483\n",
      "Epoch: 8161 | training loss: 0.000329231407\n",
      "Epoch: 8162 | training loss: 0.000329061295\n",
      "Epoch: 8163 | training loss: 0.000328889146\n",
      "Epoch: 8164 | training loss: 0.000328723952\n",
      "Epoch: 8165 | training loss: 0.000328550144\n",
      "Epoch: 8166 | training loss: 0.000328382128\n",
      "Epoch: 8167 | training loss: 0.000328214956\n",
      "Epoch: 8168 | training loss: 0.000328049908\n",
      "Epoch: 8169 | training loss: 0.000327877671\n",
      "Epoch: 8170 | training loss: 0.000327709015\n",
      "Epoch: 8171 | training loss: 0.000327541609\n",
      "Epoch: 8172 | training loss: 0.000327376387\n",
      "Epoch: 8173 | training loss: 0.000327206566\n",
      "Epoch: 8174 | training loss: 0.000327043410\n",
      "Epoch: 8175 | training loss: 0.000326872163\n",
      "Epoch: 8176 | training loss: 0.000326706562\n",
      "Epoch: 8177 | training loss: 0.000326540146\n",
      "Epoch: 8178 | training loss: 0.000326369889\n",
      "Epoch: 8179 | training loss: 0.000326204608\n",
      "Epoch: 8180 | training loss: 0.000326039153\n",
      "Epoch: 8181 | training loss: 0.000325872912\n",
      "Epoch: 8182 | training loss: 0.000325703761\n",
      "Epoch: 8183 | training loss: 0.000325537432\n",
      "Epoch: 8184 | training loss: 0.000325374800\n",
      "Epoch: 8185 | training loss: 0.000325205969\n",
      "Epoch: 8186 | training loss: 0.000325044268\n",
      "Epoch: 8187 | training loss: 0.000324879278\n",
      "Epoch: 8188 | training loss: 0.000324717141\n",
      "Epoch: 8189 | training loss: 0.000324551831\n",
      "Epoch: 8190 | training loss: 0.000324395543\n",
      "Epoch: 8191 | training loss: 0.000324237626\n",
      "Epoch: 8192 | training loss: 0.000324082008\n",
      "Epoch: 8193 | training loss: 0.000323928922\n",
      "Epoch: 8194 | training loss: 0.000323784421\n",
      "Epoch: 8195 | training loss: 0.000323654269\n",
      "Epoch: 8196 | training loss: 0.000323533721\n",
      "Epoch: 8197 | training loss: 0.000323429704\n",
      "Epoch: 8198 | training loss: 0.000323360611\n",
      "Epoch: 8199 | training loss: 0.000323344924\n",
      "Epoch: 8200 | training loss: 0.000323390443\n",
      "Epoch: 8201 | training loss: 0.000323545421\n",
      "Epoch: 8202 | training loss: 0.000323862274\n",
      "Epoch: 8203 | training loss: 0.000324422552\n",
      "Epoch: 8204 | training loss: 0.000325367000\n",
      "Epoch: 8205 | training loss: 0.000326900947\n",
      "Epoch: 8206 | training loss: 0.000329362723\n",
      "Epoch: 8207 | training loss: 0.000333262666\n",
      "Epoch: 8208 | training loss: 0.000339410733\n",
      "Epoch: 8209 | training loss: 0.000349230279\n",
      "Epoch: 8210 | training loss: 0.000364961685\n",
      "Epoch: 8211 | training loss: 0.000390236295\n",
      "Epoch: 8212 | training loss: 0.000430960557\n",
      "Epoch: 8213 | training loss: 0.000497004134\n",
      "Epoch: 8214 | training loss: 0.000604362984\n",
      "Epoch: 8215 | training loss: 0.000780474278\n",
      "Epoch: 8216 | training loss: 0.001069792314\n",
      "Epoch: 8217 | training loss: 0.001548827626\n",
      "Epoch: 8218 | training loss: 0.002338577062\n",
      "Epoch: 8219 | training loss: 0.003648522776\n",
      "Epoch: 8220 | training loss: 0.005785715766\n",
      "Epoch: 8221 | training loss: 0.009264649823\n",
      "Epoch: 8222 | training loss: 0.014668460935\n",
      "Epoch: 8223 | training loss: 0.022789254785\n",
      "Epoch: 8224 | training loss: 0.033486854285\n",
      "Epoch: 8225 | training loss: 0.045624550432\n",
      "Epoch: 8226 | training loss: 0.053638186306\n",
      "Epoch: 8227 | training loss: 0.051656745374\n",
      "Epoch: 8228 | training loss: 0.035307452083\n",
      "Epoch: 8229 | training loss: 0.013580317609\n",
      "Epoch: 8230 | training loss: 0.000894658966\n",
      "Epoch: 8231 | training loss: 0.004473221488\n",
      "Epoch: 8232 | training loss: 0.016374154016\n",
      "Epoch: 8233 | training loss: 0.021989000961\n",
      "Epoch: 8234 | training loss: 0.015234099701\n",
      "Epoch: 8235 | training loss: 0.003927506506\n",
      "Epoch: 8236 | training loss: 0.000611846393\n",
      "Epoch: 8237 | training loss: 0.006553195417\n",
      "Epoch: 8238 | training loss: 0.011775283143\n",
      "Epoch: 8239 | training loss: 0.009045613930\n",
      "Epoch: 8240 | training loss: 0.002414524555\n",
      "Epoch: 8241 | training loss: 0.000587660703\n",
      "Epoch: 8242 | training loss: 0.004396447912\n",
      "Epoch: 8243 | training loss: 0.006993426010\n",
      "Epoch: 8244 | training loss: 0.004524521995\n",
      "Epoch: 8245 | training loss: 0.000862937653\n",
      "Epoch: 8246 | training loss: 0.000963370083\n",
      "Epoch: 8247 | training loss: 0.003566072090\n",
      "Epoch: 8248 | training loss: 0.004098784644\n",
      "Epoch: 8249 | training loss: 0.001876207185\n",
      "Epoch: 8250 | training loss: 0.000355881930\n",
      "Epoch: 8251 | training loss: 0.001371569117\n",
      "Epoch: 8252 | training loss: 0.002716595540\n",
      "Epoch: 8253 | training loss: 0.002132575493\n",
      "Epoch: 8254 | training loss: 0.000668082095\n",
      "Epoch: 8255 | training loss: 0.000487385754\n",
      "Epoch: 8256 | training loss: 0.001460405765\n",
      "Epoch: 8257 | training loss: 0.001784531167\n",
      "Epoch: 8258 | training loss: 0.000975205796\n",
      "Epoch: 8259 | training loss: 0.000346706714\n",
      "Epoch: 8260 | training loss: 0.000700065691\n",
      "Epoch: 8261 | training loss: 0.001230257563\n",
      "Epoch: 8262 | training loss: 0.001034158980\n",
      "Epoch: 8263 | training loss: 0.000468999613\n",
      "Epoch: 8264 | training loss: 0.000382263970\n",
      "Epoch: 8265 | training loss: 0.000752088963\n",
      "Epoch: 8266 | training loss: 0.000889144547\n",
      "Epoch: 8267 | training loss: 0.000588319381\n",
      "Epoch: 8268 | training loss: 0.000334647135\n",
      "Epoch: 8269 | training loss: 0.000454746943\n",
      "Epoch: 8270 | training loss: 0.000665686093\n",
      "Epoch: 8271 | training loss: 0.000608971459\n",
      "Epoch: 8272 | training loss: 0.000390931789\n",
      "Epoch: 8273 | training loss: 0.000335306220\n",
      "Epoch: 8274 | training loss: 0.000469371444\n",
      "Epoch: 8275 | training loss: 0.000541595044\n",
      "Epoch: 8276 | training loss: 0.000440476404\n",
      "Epoch: 8277 | training loss: 0.000328603463\n",
      "Epoch: 8278 | training loss: 0.000353791052\n",
      "Epoch: 8279 | training loss: 0.000440021046\n",
      "Epoch: 8280 | training loss: 0.000440087519\n",
      "Epoch: 8281 | training loss: 0.000359152327\n",
      "Epoch: 8282 | training loss: 0.000317757134\n",
      "Epoch: 8283 | training loss: 0.000357810903\n",
      "Epoch: 8284 | training loss: 0.000398981705\n",
      "Epoch: 8285 | training loss: 0.000375782198\n",
      "Epoch: 8286 | training loss: 0.000326278445\n",
      "Epoch: 8287 | training loss: 0.000318531878\n",
      "Epoch: 8288 | training loss: 0.000349782029\n",
      "Epoch: 8289 | training loss: 0.000364301464\n",
      "Epoch: 8290 | training loss: 0.000340765167\n",
      "Epoch: 8291 | training loss: 0.000314721197\n",
      "Epoch: 8292 | training loss: 0.000318597100\n",
      "Epoch: 8293 | training loss: 0.000338024634\n",
      "Epoch: 8294 | training loss: 0.000340612751\n",
      "Epoch: 8295 | training loss: 0.000323318731\n",
      "Epoch: 8296 | training loss: 0.000310667063\n",
      "Epoch: 8297 | training loss: 0.000316436868\n",
      "Epoch: 8298 | training loss: 0.000327290501\n",
      "Epoch: 8299 | training loss: 0.000325865229\n",
      "Epoch: 8300 | training loss: 0.000314651668\n",
      "Epoch: 8301 | training loss: 0.000308728864\n",
      "Epoch: 8302 | training loss: 0.000313422206\n",
      "Epoch: 8303 | training loss: 0.000319253479\n",
      "Epoch: 8304 | training loss: 0.000317165483\n",
      "Epoch: 8305 | training loss: 0.000310212723\n",
      "Epoch: 8306 | training loss: 0.000307284674\n",
      "Epoch: 8307 | training loss: 0.000310415024\n",
      "Epoch: 8308 | training loss: 0.000313548633\n",
      "Epoch: 8309 | training loss: 0.000311863725\n",
      "Epoch: 8310 | training loss: 0.000307636597\n",
      "Epoch: 8311 | training loss: 0.000305959373\n",
      "Epoch: 8312 | training loss: 0.000307786831\n",
      "Epoch: 8313 | training loss: 0.000309523748\n",
      "Epoch: 8314 | training loss: 0.000308439252\n",
      "Epoch: 8315 | training loss: 0.000305866852\n",
      "Epoch: 8316 | training loss: 0.000304768560\n",
      "Epoch: 8317 | training loss: 0.000305739057\n",
      "Epoch: 8318 | training loss: 0.000306743430\n",
      "Epoch: 8319 | training loss: 0.000306122354\n",
      "Epoch: 8320 | training loss: 0.000304532645\n",
      "Epoch: 8321 | training loss: 0.000303674606\n",
      "Epoch: 8322 | training loss: 0.000304067857\n",
      "Epoch: 8323 | training loss: 0.000304658461\n",
      "Epoch: 8324 | training loss: 0.000304372632\n",
      "Epoch: 8325 | training loss: 0.000303393579\n",
      "Epoch: 8326 | training loss: 0.000302698027\n",
      "Epoch: 8327 | training loss: 0.000302760716\n",
      "Epoch: 8328 | training loss: 0.000303088658\n",
      "Epoch: 8329 | training loss: 0.000302968983\n",
      "Epoch: 8330 | training loss: 0.000302356406\n",
      "Epoch: 8331 | training loss: 0.000301793509\n",
      "Epoch: 8332 | training loss: 0.000301660155\n",
      "Epoch: 8333 | training loss: 0.000301778666\n",
      "Epoch: 8334 | training loss: 0.000301729946\n",
      "Epoch: 8335 | training loss: 0.000301366876\n",
      "Epoch: 8336 | training loss: 0.000300931337\n",
      "Epoch: 8337 | training loss: 0.000300702057\n",
      "Epoch: 8338 | training loss: 0.000300675281\n",
      "Epoch: 8339 | training loss: 0.000300626329\n",
      "Epoch: 8340 | training loss: 0.000300405576\n",
      "Epoch: 8341 | training loss: 0.000300081621\n",
      "Epoch: 8342 | training loss: 0.000299839390\n",
      "Epoch: 8343 | training loss: 0.000299723324\n",
      "Epoch: 8344 | training loss: 0.000299643521\n",
      "Epoch: 8345 | training loss: 0.000299493578\n",
      "Epoch: 8346 | training loss: 0.000299252657\n",
      "Epoch: 8347 | training loss: 0.000299014151\n",
      "Epoch: 8348 | training loss: 0.000298843777\n",
      "Epoch: 8349 | training loss: 0.000298728672\n",
      "Epoch: 8350 | training loss: 0.000298601168\n",
      "Epoch: 8351 | training loss: 0.000298418250\n",
      "Epoch: 8352 | training loss: 0.000298204657\n",
      "Epoch: 8353 | training loss: 0.000298013125\n",
      "Epoch: 8354 | training loss: 0.000297870807\n",
      "Epoch: 8355 | training loss: 0.000297744642\n",
      "Epoch: 8356 | training loss: 0.000297589577\n",
      "Epoch: 8357 | training loss: 0.000297399703\n",
      "Epoch: 8358 | training loss: 0.000297208084\n",
      "Epoch: 8359 | training loss: 0.000297047023\n",
      "Epoch: 8360 | training loss: 0.000296902610\n",
      "Epoch: 8361 | training loss: 0.000296756276\n",
      "Epoch: 8362 | training loss: 0.000296589453\n",
      "Epoch: 8363 | training loss: 0.000296417769\n",
      "Epoch: 8364 | training loss: 0.000296245009\n",
      "Epoch: 8365 | training loss: 0.000296087965\n",
      "Epoch: 8366 | training loss: 0.000295941631\n",
      "Epoch: 8367 | training loss: 0.000295789912\n",
      "Epoch: 8368 | training loss: 0.000295629550\n",
      "Epoch: 8369 | training loss: 0.000295458914\n",
      "Epoch: 8370 | training loss: 0.000295286038\n",
      "Epoch: 8371 | training loss: 0.000295129954\n",
      "Epoch: 8372 | training loss: 0.000294984027\n",
      "Epoch: 8373 | training loss: 0.000294830883\n",
      "Epoch: 8374 | training loss: 0.000294670404\n",
      "Epoch: 8375 | training loss: 0.000294505997\n",
      "Epoch: 8376 | training loss: 0.000294343627\n",
      "Epoch: 8377 | training loss: 0.000294185244\n",
      "Epoch: 8378 | training loss: 0.000294031954\n",
      "Epoch: 8379 | training loss: 0.000293876335\n",
      "Epoch: 8380 | training loss: 0.000293721037\n",
      "Epoch: 8381 | training loss: 0.000293556746\n",
      "Epoch: 8382 | training loss: 0.000293397083\n",
      "Epoch: 8383 | training loss: 0.000293246849\n",
      "Epoch: 8384 | training loss: 0.000293087389\n",
      "Epoch: 8385 | training loss: 0.000292932062\n",
      "Epoch: 8386 | training loss: 0.000292777491\n",
      "Epoch: 8387 | training loss: 0.000292618468\n",
      "Epoch: 8388 | training loss: 0.000292460230\n",
      "Epoch: 8389 | training loss: 0.000292304496\n",
      "Epoch: 8390 | training loss: 0.000292145618\n",
      "Epoch: 8391 | training loss: 0.000291991048\n",
      "Epoch: 8392 | training loss: 0.000291834178\n",
      "Epoch: 8393 | training loss: 0.000291679113\n",
      "Epoch: 8394 | training loss: 0.000291523407\n",
      "Epoch: 8395 | training loss: 0.000291368517\n",
      "Epoch: 8396 | training loss: 0.000291212404\n",
      "Epoch: 8397 | training loss: 0.000291055476\n",
      "Epoch: 8398 | training loss: 0.000290902652\n",
      "Epoch: 8399 | training loss: 0.000290744472\n",
      "Epoch: 8400 | training loss: 0.000290590018\n",
      "Epoch: 8401 | training loss: 0.000290433876\n",
      "Epoch: 8402 | training loss: 0.000290278840\n",
      "Epoch: 8403 | training loss: 0.000290123862\n",
      "Epoch: 8404 | training loss: 0.000289967662\n",
      "Epoch: 8405 | training loss: 0.000289812626\n",
      "Epoch: 8406 | training loss: 0.000289657444\n",
      "Epoch: 8407 | training loss: 0.000289503310\n",
      "Epoch: 8408 | training loss: 0.000289347488\n",
      "Epoch: 8409 | training loss: 0.000289191667\n",
      "Epoch: 8410 | training loss: 0.000289036718\n",
      "Epoch: 8411 | training loss: 0.000288883195\n",
      "Epoch: 8412 | training loss: 0.000288726646\n",
      "Epoch: 8413 | training loss: 0.000288573996\n",
      "Epoch: 8414 | training loss: 0.000288419076\n",
      "Epoch: 8415 | training loss: 0.000288264157\n",
      "Epoch: 8416 | training loss: 0.000288106326\n",
      "Epoch: 8417 | training loss: 0.000287953590\n",
      "Epoch: 8418 | training loss: 0.000287800940\n",
      "Epoch: 8419 | training loss: 0.000287645118\n",
      "Epoch: 8420 | training loss: 0.000287490548\n",
      "Epoch: 8421 | training loss: 0.000287336705\n",
      "Epoch: 8422 | training loss: 0.000287181640\n",
      "Epoch: 8423 | training loss: 0.000287028844\n",
      "Epoch: 8424 | training loss: 0.000286876340\n",
      "Epoch: 8425 | training loss: 0.000286720344\n",
      "Epoch: 8426 | training loss: 0.000286565104\n",
      "Epoch: 8427 | training loss: 0.000286411203\n",
      "Epoch: 8428 | training loss: 0.000286254508\n",
      "Epoch: 8429 | training loss: 0.000286100170\n",
      "Epoch: 8430 | training loss: 0.000285945658\n",
      "Epoch: 8431 | training loss: 0.000285791437\n",
      "Epoch: 8432 | training loss: 0.000285636401\n",
      "Epoch: 8433 | training loss: 0.000285487593\n",
      "Epoch: 8434 | training loss: 0.000285332644\n",
      "Epoch: 8435 | training loss: 0.000285178772\n",
      "Epoch: 8436 | training loss: 0.000285023998\n",
      "Epoch: 8437 | training loss: 0.000284867478\n",
      "Epoch: 8438 | training loss: 0.000284716516\n",
      "Epoch: 8439 | training loss: 0.000284562499\n",
      "Epoch: 8440 | training loss: 0.000284410635\n",
      "Epoch: 8441 | training loss: 0.000284258422\n",
      "Epoch: 8442 | training loss: 0.000284101901\n",
      "Epoch: 8443 | training loss: 0.000283945614\n",
      "Epoch: 8444 | training loss: 0.000283795787\n",
      "Epoch: 8445 | training loss: 0.000283637492\n",
      "Epoch: 8446 | training loss: 0.000283486617\n",
      "Epoch: 8447 | training loss: 0.000283333968\n",
      "Epoch: 8448 | training loss: 0.000283180707\n",
      "Epoch: 8449 | training loss: 0.000283027301\n",
      "Epoch: 8450 | training loss: 0.000282875204\n",
      "Epoch: 8451 | training loss: 0.000282719644\n",
      "Epoch: 8452 | training loss: 0.000282565888\n",
      "Epoch: 8453 | training loss: 0.000282414636\n",
      "Epoch: 8454 | training loss: 0.000282257854\n",
      "Epoch: 8455 | training loss: 0.000282105932\n",
      "Epoch: 8456 | training loss: 0.000281955581\n",
      "Epoch: 8457 | training loss: 0.000281800836\n",
      "Epoch: 8458 | training loss: 0.000281649001\n",
      "Epoch: 8459 | training loss: 0.000281498855\n",
      "Epoch: 8460 | training loss: 0.000281342946\n",
      "Epoch: 8461 | training loss: 0.000281193556\n",
      "Epoch: 8462 | training loss: 0.000281038199\n",
      "Epoch: 8463 | training loss: 0.000280885171\n",
      "Epoch: 8464 | training loss: 0.000280728302\n",
      "Epoch: 8465 | training loss: 0.000280575827\n",
      "Epoch: 8466 | training loss: 0.000280422682\n",
      "Epoch: 8467 | training loss: 0.000280272478\n",
      "Epoch: 8468 | training loss: 0.000280116132\n",
      "Epoch: 8469 | training loss: 0.000279966916\n",
      "Epoch: 8470 | training loss: 0.000279813830\n",
      "Epoch: 8471 | training loss: 0.000279660919\n",
      "Epoch: 8472 | training loss: 0.000279511441\n",
      "Epoch: 8473 | training loss: 0.000279357715\n",
      "Epoch: 8474 | training loss: 0.000279203465\n",
      "Epoch: 8475 | training loss: 0.000279052299\n",
      "Epoch: 8476 | training loss: 0.000278899621\n",
      "Epoch: 8477 | training loss: 0.000278745661\n",
      "Epoch: 8478 | training loss: 0.000278594322\n",
      "Epoch: 8479 | training loss: 0.000278438616\n",
      "Epoch: 8480 | training loss: 0.000278287160\n",
      "Epoch: 8481 | training loss: 0.000278130785\n",
      "Epoch: 8482 | training loss: 0.000277982210\n",
      "Epoch: 8483 | training loss: 0.000277831830\n",
      "Epoch: 8484 | training loss: 0.000277679821\n",
      "Epoch: 8485 | training loss: 0.000277527899\n",
      "Epoch: 8486 | training loss: 0.000277373270\n",
      "Epoch: 8487 | training loss: 0.000277220417\n",
      "Epoch: 8488 | training loss: 0.000277075625\n",
      "Epoch: 8489 | training loss: 0.000276917999\n",
      "Epoch: 8490 | training loss: 0.000276765670\n",
      "Epoch: 8491 | training loss: 0.000276615727\n",
      "Epoch: 8492 | training loss: 0.000276461767\n",
      "Epoch: 8493 | training loss: 0.000276309554\n",
      "Epoch: 8494 | training loss: 0.000276154518\n",
      "Epoch: 8495 | training loss: 0.000276005041\n",
      "Epoch: 8496 | training loss: 0.000275852741\n",
      "Epoch: 8497 | training loss: 0.000275700906\n",
      "Epoch: 8498 | training loss: 0.000275550701\n",
      "Epoch: 8499 | training loss: 0.000275393191\n",
      "Epoch: 8500 | training loss: 0.000275244616\n",
      "Epoch: 8501 | training loss: 0.000275092432\n",
      "Epoch: 8502 | training loss: 0.000274944934\n",
      "Epoch: 8503 | training loss: 0.000274788268\n",
      "Epoch: 8504 | training loss: 0.000274640304\n",
      "Epoch: 8505 | training loss: 0.000274485850\n",
      "Epoch: 8506 | training loss: 0.000274336344\n",
      "Epoch: 8507 | training loss: 0.000274187681\n",
      "Epoch: 8508 | training loss: 0.000274032674\n",
      "Epoch: 8509 | training loss: 0.000273884303\n",
      "Epoch: 8510 | training loss: 0.000273729500\n",
      "Epoch: 8511 | training loss: 0.000273582817\n",
      "Epoch: 8512 | training loss: 0.000273428101\n",
      "Epoch: 8513 | training loss: 0.000273275829\n",
      "Epoch: 8514 | training loss: 0.000273126789\n",
      "Epoch: 8515 | training loss: 0.000272972859\n",
      "Epoch: 8516 | training loss: 0.000272824342\n",
      "Epoch: 8517 | training loss: 0.000272670819\n",
      "Epoch: 8518 | training loss: 0.000272522448\n",
      "Epoch: 8519 | training loss: 0.000272371050\n",
      "Epoch: 8520 | training loss: 0.000272222271\n",
      "Epoch: 8521 | training loss: 0.000272069592\n",
      "Epoch: 8522 | training loss: 0.000271919387\n",
      "Epoch: 8523 | training loss: 0.000271769793\n",
      "Epoch: 8524 | training loss: 0.000271618483\n",
      "Epoch: 8525 | training loss: 0.000271472323\n",
      "Epoch: 8526 | training loss: 0.000271321507\n",
      "Epoch: 8527 | training loss: 0.000271176279\n",
      "Epoch: 8528 | training loss: 0.000271027267\n",
      "Epoch: 8529 | training loss: 0.000270879420\n",
      "Epoch: 8530 | training loss: 0.000270739256\n",
      "Epoch: 8531 | training loss: 0.000270595832\n",
      "Epoch: 8532 | training loss: 0.000270456600\n",
      "Epoch: 8533 | training loss: 0.000270318793\n",
      "Epoch: 8534 | training loss: 0.000270187098\n",
      "Epoch: 8535 | training loss: 0.000270057004\n",
      "Epoch: 8536 | training loss: 0.000269941229\n",
      "Epoch: 8537 | training loss: 0.000269836775\n",
      "Epoch: 8538 | training loss: 0.000269756507\n",
      "Epoch: 8539 | training loss: 0.000269710901\n",
      "Epoch: 8540 | training loss: 0.000269714859\n",
      "Epoch: 8541 | training loss: 0.000269805489\n",
      "Epoch: 8542 | training loss: 0.000270009128\n",
      "Epoch: 8543 | training loss: 0.000270399498\n",
      "Epoch: 8544 | training loss: 0.000271052588\n",
      "Epoch: 8545 | training loss: 0.000272120757\n",
      "Epoch: 8546 | training loss: 0.000273857760\n",
      "Epoch: 8547 | training loss: 0.000276636740\n",
      "Epoch: 8548 | training loss: 0.000281043816\n",
      "Epoch: 8549 | training loss: 0.000288077892\n",
      "Epoch: 8550 | training loss: 0.000299325009\n",
      "Epoch: 8551 | training loss: 0.000317295780\n",
      "Epoch: 8552 | training loss: 0.000346212124\n",
      "Epoch: 8553 | training loss: 0.000392985705\n",
      "Epoch: 8554 | training loss: 0.000469119579\n",
      "Epoch: 8555 | training loss: 0.000593469129\n",
      "Epoch: 8556 | training loss: 0.000797859742\n",
      "Epoch: 8557 | training loss: 0.001134401071\n",
      "Epoch: 8558 | training loss: 0.001692375052\n",
      "Epoch: 8559 | training loss: 0.002613552846\n",
      "Epoch: 8560 | training loss: 0.004137639888\n",
      "Epoch: 8561 | training loss: 0.006615296472\n",
      "Epoch: 8562 | training loss: 0.010596770793\n",
      "Epoch: 8563 | training loss: 0.016662197188\n",
      "Epoch: 8564 | training loss: 0.025398340076\n",
      "Epoch: 8565 | training loss: 0.036094650626\n",
      "Epoch: 8566 | training loss: 0.046422954649\n",
      "Epoch: 8567 | training loss: 0.050033546984\n",
      "Epoch: 8568 | training loss: 0.042279608548\n",
      "Epoch: 8569 | training loss: 0.023783849552\n",
      "Epoch: 8570 | training loss: 0.006722216029\n",
      "Epoch: 8571 | training loss: 0.002267938806\n",
      "Epoch: 8572 | training loss: 0.010096106678\n",
      "Epoch: 8573 | training loss: 0.018632918596\n",
      "Epoch: 8574 | training loss: 0.017318369821\n",
      "Epoch: 8575 | training loss: 0.007910475135\n",
      "Epoch: 8576 | training loss: 0.001500796876\n",
      "Epoch: 8577 | training loss: 0.004221499898\n",
      "Epoch: 8578 | training loss: 0.009916848503\n",
      "Epoch: 8579 | training loss: 0.009675226174\n",
      "Epoch: 8580 | training loss: 0.003882180434\n",
      "Epoch: 8581 | training loss: 0.000615124125\n",
      "Epoch: 8582 | training loss: 0.003316274378\n",
      "Epoch: 8583 | training loss: 0.006545527373\n",
      "Epoch: 8584 | training loss: 0.005027777515\n",
      "Epoch: 8585 | training loss: 0.001216561301\n",
      "Epoch: 8586 | training loss: 0.000613450538\n",
      "Epoch: 8587 | training loss: 0.003123414004\n",
      "Epoch: 8588 | training loss: 0.004109091591\n",
      "Epoch: 8589 | training loss: 0.002067398978\n",
      "Epoch: 8590 | training loss: 0.000325170229\n",
      "Epoch: 8591 | training loss: 0.001174047706\n",
      "Epoch: 8592 | training loss: 0.002576819388\n",
      "Epoch: 8593 | training loss: 0.002077612560\n",
      "Epoch: 8594 | training loss: 0.000635546981\n",
      "Epoch: 8595 | training loss: 0.000487056619\n",
      "Epoch: 8596 | training loss: 0.001441896195\n",
      "Epoch: 8597 | training loss: 0.001648844918\n",
      "Epoch: 8598 | training loss: 0.000802035676\n",
      "Epoch: 8599 | training loss: 0.000321847561\n",
      "Epoch: 8600 | training loss: 0.000793634797\n",
      "Epoch: 8601 | training loss: 0.001203912543\n",
      "Epoch: 8602 | training loss: 0.000829408877\n",
      "Epoch: 8603 | training loss: 0.000322272303\n",
      "Epoch: 8604 | training loss: 0.000435568189\n",
      "Epoch: 8605 | training loss: 0.000822563423\n",
      "Epoch: 8606 | training loss: 0.000777414476\n",
      "Epoch: 8607 | training loss: 0.000397851923\n",
      "Epoch: 8608 | training loss: 0.000285852642\n",
      "Epoch: 8609 | training loss: 0.000522935763\n",
      "Epoch: 8610 | training loss: 0.000644813408\n",
      "Epoch: 8611 | training loss: 0.000457377580\n",
      "Epoch: 8612 | training loss: 0.000278038991\n",
      "Epoch: 8613 | training loss: 0.000350812043\n",
      "Epoch: 8614 | training loss: 0.000490018982\n",
      "Epoch: 8615 | training loss: 0.000449611049\n",
      "Epoch: 8616 | training loss: 0.000308421673\n",
      "Epoch: 8617 | training loss: 0.000284381211\n",
      "Epoch: 8618 | training loss: 0.000375454198\n",
      "Epoch: 8619 | training loss: 0.000406179839\n",
      "Epoch: 8620 | training loss: 0.000327886490\n",
      "Epoch: 8621 | training loss: 0.000267684809\n",
      "Epoch: 8622 | training loss: 0.000303290377\n",
      "Epoch: 8623 | training loss: 0.000354966731\n",
      "Epoch: 8624 | training loss: 0.000333817385\n",
      "Epoch: 8625 | training loss: 0.000276200415\n",
      "Epoch: 8626 | training loss: 0.000266666524\n",
      "Epoch: 8627 | training loss: 0.000304233283\n",
      "Epoch: 8628 | training loss: 0.000319998624\n",
      "Epoch: 8629 | training loss: 0.000290299911\n",
      "Epoch: 8630 | training loss: 0.000262081157\n",
      "Epoch: 8631 | training loss: 0.000271170982\n",
      "Epoch: 8632 | training loss: 0.000293206773\n",
      "Epoch: 8633 | training loss: 0.000290678407\n",
      "Epoch: 8634 | training loss: 0.000269129174\n",
      "Epoch: 8635 | training loss: 0.000260364090\n",
      "Epoch: 8636 | training loss: 0.000271739729\n",
      "Epoch: 8637 | training loss: 0.000280606095\n",
      "Epoch: 8638 | training loss: 0.000272532343\n",
      "Epoch: 8639 | training loss: 0.000260207977\n",
      "Epoch: 8640 | training loss: 0.000260146917\n",
      "Epoch: 8641 | training loss: 0.000268605159\n",
      "Epoch: 8642 | training loss: 0.000270658697\n",
      "Epoch: 8643 | training loss: 0.000263164809\n",
      "Epoch: 8644 | training loss: 0.000256897183\n",
      "Epoch: 8645 | training loss: 0.000259077235\n",
      "Epoch: 8646 | training loss: 0.000264119706\n",
      "Epoch: 8647 | training loss: 0.000263709255\n",
      "Epoch: 8648 | training loss: 0.000258596468\n",
      "Epoch: 8649 | training loss: 0.000255696679\n",
      "Epoch: 8650 | training loss: 0.000257633743\n",
      "Epoch: 8651 | training loss: 0.000260117988\n",
      "Epoch: 8652 | training loss: 0.000259065651\n",
      "Epoch: 8653 | training loss: 0.000255996623\n",
      "Epoch: 8654 | training loss: 0.000254847342\n",
      "Epoch: 8655 | training loss: 0.000256217492\n",
      "Epoch: 8656 | training loss: 0.000257275824\n",
      "Epoch: 8657 | training loss: 0.000256200961\n",
      "Epoch: 8658 | training loss: 0.000254345825\n",
      "Epoch: 8659 | training loss: 0.000253863080\n",
      "Epoch: 8660 | training loss: 0.000254750921\n",
      "Epoch: 8661 | training loss: 0.000255253894\n",
      "Epoch: 8662 | training loss: 0.000254446815\n",
      "Epoch: 8663 | training loss: 0.000253250793\n",
      "Epoch: 8664 | training loss: 0.000252912199\n",
      "Epoch: 8665 | training loss: 0.000253381440\n",
      "Epoch: 8666 | training loss: 0.000253637932\n",
      "Epoch: 8667 | training loss: 0.000253131555\n",
      "Epoch: 8668 | training loss: 0.000252370432\n",
      "Epoch: 8669 | training loss: 0.000252088124\n",
      "Epoch: 8670 | training loss: 0.000252278434\n",
      "Epoch: 8671 | training loss: 0.000252369558\n",
      "Epoch: 8672 | training loss: 0.000252028520\n",
      "Epoch: 8673 | training loss: 0.000251523859\n",
      "Epoch: 8674 | training loss: 0.000251286518\n",
      "Epoch: 8675 | training loss: 0.000251331716\n",
      "Epoch: 8676 | training loss: 0.000251340622\n",
      "Epoch: 8677 | training loss: 0.000251092337\n",
      "Epoch: 8678 | training loss: 0.000250728190\n",
      "Epoch: 8679 | training loss: 0.000250501384\n",
      "Epoch: 8680 | training loss: 0.000250458455\n",
      "Epoch: 8681 | training loss: 0.000250423967\n",
      "Epoch: 8682 | training loss: 0.000250253332\n",
      "Epoch: 8683 | training loss: 0.000249985140\n",
      "Epoch: 8684 | training loss: 0.000249764358\n",
      "Epoch: 8685 | training loss: 0.000249660749\n",
      "Epoch: 8686 | training loss: 0.000249584991\n",
      "Epoch: 8687 | training loss: 0.000249451463\n",
      "Epoch: 8688 | training loss: 0.000249247911\n",
      "Epoch: 8689 | training loss: 0.000249046541\n",
      "Epoch: 8690 | training loss: 0.000248907309\n",
      "Epoch: 8691 | training loss: 0.000248808821\n",
      "Epoch: 8692 | training loss: 0.000248683093\n",
      "Epoch: 8693 | training loss: 0.000248513883\n",
      "Epoch: 8694 | training loss: 0.000248337223\n",
      "Epoch: 8695 | training loss: 0.000248182972\n",
      "Epoch: 8696 | training loss: 0.000248061551\n",
      "Epoch: 8697 | training loss: 0.000247946213\n",
      "Epoch: 8698 | training loss: 0.000247799937\n",
      "Epoch: 8699 | training loss: 0.000247638673\n",
      "Epoch: 8700 | training loss: 0.000247483433\n",
      "Epoch: 8701 | training loss: 0.000247342337\n",
      "Epoch: 8702 | training loss: 0.000247211545\n",
      "Epoch: 8703 | training loss: 0.000247080607\n",
      "Epoch: 8704 | training loss: 0.000246935233\n",
      "Epoch: 8705 | training loss: 0.000246783486\n",
      "Epoch: 8706 | training loss: 0.000246642623\n",
      "Epoch: 8707 | training loss: 0.000246504176\n",
      "Epoch: 8708 | training loss: 0.000246370211\n",
      "Epoch: 8709 | training loss: 0.000246235024\n",
      "Epoch: 8710 | training loss: 0.000246088719\n",
      "Epoch: 8711 | training loss: 0.000245945528\n",
      "Epoch: 8712 | training loss: 0.000245809031\n",
      "Epoch: 8713 | training loss: 0.000245676027\n",
      "Epoch: 8714 | training loss: 0.000245541596\n",
      "Epoch: 8715 | training loss: 0.000245401228\n",
      "Epoch: 8716 | training loss: 0.000245257339\n",
      "Epoch: 8717 | training loss: 0.000245118281\n",
      "Epoch: 8718 | training loss: 0.000244984665\n",
      "Epoch: 8719 | training loss: 0.000244849041\n",
      "Epoch: 8720 | training loss: 0.000244708703\n",
      "Epoch: 8721 | training loss: 0.000244575029\n",
      "Epoch: 8722 | training loss: 0.000244429393\n",
      "Epoch: 8723 | training loss: 0.000244293973\n",
      "Epoch: 8724 | training loss: 0.000244156137\n",
      "Epoch: 8725 | training loss: 0.000244021925\n",
      "Epoch: 8726 | training loss: 0.000243881979\n",
      "Epoch: 8727 | training loss: 0.000243747840\n",
      "Epoch: 8728 | training loss: 0.000243608374\n",
      "Epoch: 8729 | training loss: 0.000243472430\n",
      "Epoch: 8730 | training loss: 0.000243335395\n",
      "Epoch: 8731 | training loss: 0.000243199669\n",
      "Epoch: 8732 | training loss: 0.000243062983\n",
      "Epoch: 8733 | training loss: 0.000242926209\n",
      "Epoch: 8734 | training loss: 0.000242787472\n",
      "Epoch: 8735 | training loss: 0.000242653885\n",
      "Epoch: 8736 | training loss: 0.000242515525\n",
      "Epoch: 8737 | training loss: 0.000242377850\n",
      "Epoch: 8738 | training loss: 0.000242241498\n",
      "Epoch: 8739 | training loss: 0.000242107504\n",
      "Epoch: 8740 | training loss: 0.000241970323\n",
      "Epoch: 8741 | training loss: 0.000241836373\n",
      "Epoch: 8742 | training loss: 0.000241700574\n",
      "Epoch: 8743 | training loss: 0.000241562113\n",
      "Epoch: 8744 | training loss: 0.000241427799\n",
      "Epoch: 8745 | training loss: 0.000241290982\n",
      "Epoch: 8746 | training loss: 0.000241154950\n",
      "Epoch: 8747 | training loss: 0.000241018075\n",
      "Epoch: 8748 | training loss: 0.000240884925\n",
      "Epoch: 8749 | training loss: 0.000240749781\n",
      "Epoch: 8750 | training loss: 0.000240616035\n",
      "Epoch: 8751 | training loss: 0.000240480556\n",
      "Epoch: 8752 | training loss: 0.000240346999\n",
      "Epoch: 8753 | training loss: 0.000240211200\n",
      "Epoch: 8754 | training loss: 0.000240071473\n",
      "Epoch: 8755 | training loss: 0.000239938207\n",
      "Epoch: 8756 | training loss: 0.000239804329\n",
      "Epoch: 8757 | training loss: 0.000239664165\n",
      "Epoch: 8758 | training loss: 0.000239530898\n",
      "Epoch: 8759 | training loss: 0.000239395886\n",
      "Epoch: 8760 | training loss: 0.000239257890\n",
      "Epoch: 8761 | training loss: 0.000239122426\n",
      "Epoch: 8762 | training loss: 0.000238988257\n",
      "Epoch: 8763 | training loss: 0.000238855369\n",
      "Epoch: 8764 | training loss: 0.000238716224\n",
      "Epoch: 8765 | training loss: 0.000238581822\n",
      "Epoch: 8766 | training loss: 0.000238448556\n",
      "Epoch: 8767 | training loss: 0.000238313645\n",
      "Epoch: 8768 | training loss: 0.000238180670\n",
      "Epoch: 8769 | training loss: 0.000238042820\n",
      "Epoch: 8770 | training loss: 0.000237906206\n",
      "Epoch: 8771 | training loss: 0.000237777815\n",
      "Epoch: 8772 | training loss: 0.000237641769\n",
      "Epoch: 8773 | training loss: 0.000237505956\n",
      "Epoch: 8774 | training loss: 0.000237371016\n",
      "Epoch: 8775 | training loss: 0.000237234621\n",
      "Epoch: 8776 | training loss: 0.000237102649\n",
      "Epoch: 8777 | training loss: 0.000236969776\n",
      "Epoch: 8778 | training loss: 0.000236832420\n",
      "Epoch: 8779 | training loss: 0.000236697553\n",
      "Epoch: 8780 | training loss: 0.000236564199\n",
      "Epoch: 8781 | training loss: 0.000236432417\n",
      "Epoch: 8782 | training loss: 0.000236295018\n",
      "Epoch: 8783 | training loss: 0.000236164036\n",
      "Epoch: 8784 | training loss: 0.000236030872\n",
      "Epoch: 8785 | training loss: 0.000235898799\n",
      "Epoch: 8786 | training loss: 0.000235768239\n",
      "Epoch: 8787 | training loss: 0.000235638436\n",
      "Epoch: 8788 | training loss: 0.000235512940\n",
      "Epoch: 8789 | training loss: 0.000235385756\n",
      "Epoch: 8790 | training loss: 0.000235266023\n",
      "Epoch: 8791 | training loss: 0.000235156069\n",
      "Epoch: 8792 | training loss: 0.000235051994\n",
      "Epoch: 8793 | training loss: 0.000234958075\n",
      "Epoch: 8794 | training loss: 0.000234884064\n",
      "Epoch: 8795 | training loss: 0.000234839812\n",
      "Epoch: 8796 | training loss: 0.000234837586\n",
      "Epoch: 8797 | training loss: 0.000234903768\n",
      "Epoch: 8798 | training loss: 0.000235068786\n",
      "Epoch: 8799 | training loss: 0.000235386571\n",
      "Epoch: 8800 | training loss: 0.000235940257\n",
      "Epoch: 8801 | training loss: 0.000236853957\n",
      "Epoch: 8802 | training loss: 0.000238342647\n",
      "Epoch: 8803 | training loss: 0.000240727502\n",
      "Epoch: 8804 | training loss: 0.000244543364\n",
      "Epoch: 8805 | training loss: 0.000250647427\n",
      "Epoch: 8806 | training loss: 0.000260403467\n",
      "Epoch: 8807 | training loss: 0.000276067643\n",
      "Epoch: 8808 | training loss: 0.000301279215\n",
      "Epoch: 8809 | training loss: 0.000342079496\n",
      "Epoch: 8810 | training loss: 0.000408374006\n",
      "Epoch: 8811 | training loss: 0.000516621396\n",
      "Epoch: 8812 | training loss: 0.000693393347\n",
      "Epoch: 8813 | training loss: 0.000982626923\n",
      "Epoch: 8814 | training loss: 0.001452439465\n",
      "Epoch: 8815 | training loss: 0.002210828010\n",
      "Epoch: 8816 | training loss: 0.003403040813\n",
      "Epoch: 8817 | training loss: 0.005215863697\n",
      "Epoch: 8818 | training loss: 0.007759956643\n",
      "Epoch: 8819 | training loss: 0.010939326137\n",
      "Epoch: 8820 | training loss: 0.013995506801\n",
      "Epoch: 8821 | training loss: 0.015549411066\n",
      "Epoch: 8822 | training loss: 0.014021583833\n",
      "Epoch: 8823 | training loss: 0.009517917410\n",
      "Epoch: 8824 | training loss: 0.004441957455\n",
      "Epoch: 8825 | training loss: 0.001713443082\n",
      "Epoch: 8826 | training loss: 0.002165549668\n",
      "Epoch: 8827 | training loss: 0.004087592475\n",
      "Epoch: 8828 | training loss: 0.005116339307\n",
      "Epoch: 8829 | training loss: 0.004258444533\n",
      "Epoch: 8830 | training loss: 0.002602499444\n",
      "Epoch: 8831 | training loss: 0.001844614395\n",
      "Epoch: 8832 | training loss: 0.002383453306\n",
      "Epoch: 8833 | training loss: 0.003086151788\n",
      "Epoch: 8834 | training loss: 0.002756171860\n",
      "Epoch: 8835 | training loss: 0.001505182823\n",
      "Epoch: 8836 | training loss: 0.000551121077\n",
      "Epoch: 8837 | training loss: 0.000766107696\n",
      "Epoch: 8838 | training loss: 0.001715465682\n",
      "Epoch: 8839 | training loss: 0.002247107448\n",
      "Epoch: 8840 | training loss: 0.001768462243\n",
      "Epoch: 8841 | training loss: 0.000798424473\n",
      "Epoch: 8842 | training loss: 0.000268349890\n",
      "Epoch: 8843 | training loss: 0.000505058910\n",
      "Epoch: 8844 | training loss: 0.001019535586\n",
      "Epoch: 8845 | training loss: 0.001181949629\n",
      "Epoch: 8846 | training loss: 0.000888917479\n",
      "Epoch: 8847 | training loss: 0.000532875536\n",
      "Epoch: 8848 | training loss: 0.000460570678\n",
      "Epoch: 8849 | training loss: 0.000609996961\n",
      "Epoch: 8850 | training loss: 0.000684790779\n",
      "Epoch: 8851 | training loss: 0.000550294179\n",
      "Epoch: 8852 | training loss: 0.000362567022\n",
      "Epoch: 8853 | training loss: 0.000332824857\n",
      "Epoch: 8854 | training loss: 0.000467691891\n",
      "Epoch: 8855 | training loss: 0.000582223409\n",
      "Epoch: 8856 | training loss: 0.000529919518\n",
      "Epoch: 8857 | training loss: 0.000359190919\n",
      "Epoch: 8858 | training loss: 0.000236234366\n",
      "Epoch: 8859 | training loss: 0.000255499443\n",
      "Epoch: 8860 | training loss: 0.000356786128\n",
      "Epoch: 8861 | training loss: 0.000415602204\n",
      "Epoch: 8862 | training loss: 0.000379773730\n",
      "Epoch: 8863 | training loss: 0.000302654516\n",
      "Epoch: 8864 | training loss: 0.000262994872\n",
      "Epoch: 8865 | training loss: 0.000279088796\n",
      "Epoch: 8866 | training loss: 0.000306381786\n",
      "Epoch: 8867 | training loss: 0.000300884189\n",
      "Epoch: 8868 | training loss: 0.000267025025\n",
      "Epoch: 8869 | training loss: 0.000243108298\n",
      "Epoch: 8870 | training loss: 0.000252487778\n",
      "Epoch: 8871 | training loss: 0.000279508065\n",
      "Epoch: 8872 | training loss: 0.000291045639\n",
      "Epoch: 8873 | training loss: 0.000273157784\n",
      "Epoch: 8874 | training loss: 0.000242429465\n",
      "Epoch: 8875 | training loss: 0.000225270735\n",
      "Epoch: 8876 | training loss: 0.000231179496\n",
      "Epoch: 8877 | training loss: 0.000247376040\n",
      "Epoch: 8878 | training loss: 0.000255415915\n",
      "Epoch: 8879 | training loss: 0.000249353034\n",
      "Epoch: 8880 | training loss: 0.000237880129\n",
      "Epoch: 8881 | training loss: 0.000232271515\n",
      "Epoch: 8882 | training loss: 0.000234898325\n",
      "Epoch: 8883 | training loss: 0.000239085755\n",
      "Epoch: 8884 | training loss: 0.000237853004\n",
      "Epoch: 8885 | training loss: 0.000231145852\n",
      "Epoch: 8886 | training loss: 0.000224877207\n",
      "Epoch: 8887 | training loss: 0.000224181102\n",
      "Epoch: 8888 | training loss: 0.000228521443\n",
      "Epoch: 8889 | training loss: 0.000233032566\n",
      "Epoch: 8890 | training loss: 0.000233482133\n",
      "Epoch: 8891 | training loss: 0.000229830577\n",
      "Epoch: 8892 | training loss: 0.000225422642\n",
      "Epoch: 8893 | training loss: 0.000223417766\n",
      "Epoch: 8894 | training loss: 0.000224143820\n",
      "Epoch: 8895 | training loss: 0.000225567070\n",
      "Epoch: 8896 | training loss: 0.000225649710\n",
      "Epoch: 8897 | training loss: 0.000224195013\n",
      "Epoch: 8898 | training loss: 0.000222573275\n",
      "Epoch: 8899 | training loss: 0.000222143892\n",
      "Epoch: 8900 | training loss: 0.000223008275\n",
      "Epoch: 8901 | training loss: 0.000224099640\n",
      "Epoch: 8902 | training loss: 0.000224291318\n",
      "Epoch: 8903 | training loss: 0.000223333336\n",
      "Epoch: 8904 | training loss: 0.000221902359\n",
      "Epoch: 8905 | training loss: 0.000220901187\n",
      "Epoch: 8906 | training loss: 0.000220713307\n",
      "Epoch: 8907 | training loss: 0.000221018505\n",
      "Epoch: 8908 | training loss: 0.000221214679\n",
      "Epoch: 8909 | training loss: 0.000220988222\n",
      "Epoch: 8910 | training loss: 0.000220471644\n",
      "Epoch: 8911 | training loss: 0.000220035843\n",
      "Epoch: 8912 | training loss: 0.000219924521\n",
      "Epoch: 8913 | training loss: 0.000220097485\n",
      "Epoch: 8914 | training loss: 0.000220288886\n",
      "Epoch: 8915 | training loss: 0.000220247224\n",
      "Epoch: 8916 | training loss: 0.000219941139\n",
      "Epoch: 8917 | training loss: 0.000219518566\n",
      "Epoch: 8918 | training loss: 0.000219178008\n",
      "Epoch: 8919 | training loss: 0.000219003850\n",
      "Epoch: 8920 | training loss: 0.000218950008\n",
      "Epoch: 8921 | training loss: 0.000218891510\n",
      "Epoch: 8922 | training loss: 0.000218738831\n",
      "Epoch: 8923 | training loss: 0.000218496265\n",
      "Epoch: 8924 | training loss: 0.000218241374\n",
      "Epoch: 8925 | training loss: 0.000218053421\n",
      "Epoch: 8926 | training loss: 0.000217959401\n",
      "Epoch: 8927 | training loss: 0.000217919878\n",
      "Epoch: 8928 | training loss: 0.000217862398\n",
      "Epoch: 8929 | training loss: 0.000217756024\n",
      "Epoch: 8930 | training loss: 0.000217602472\n",
      "Epoch: 8931 | training loss: 0.000217439403\n",
      "Epoch: 8932 | training loss: 0.000217295586\n",
      "Epoch: 8933 | training loss: 0.000217191788\n",
      "Epoch: 8934 | training loss: 0.000217107678\n",
      "Epoch: 8935 | training loss: 0.000217018940\n",
      "Epoch: 8936 | training loss: 0.000216906134\n",
      "Epoch: 8937 | training loss: 0.000216770946\n",
      "Epoch: 8938 | training loss: 0.000216628556\n",
      "Epoch: 8939 | training loss: 0.000216491011\n",
      "Epoch: 8940 | training loss: 0.000216372398\n",
      "Epoch: 8941 | training loss: 0.000216269371\n",
      "Epoch: 8942 | training loss: 0.000216163928\n",
      "Epoch: 8943 | training loss: 0.000216059954\n",
      "Epoch: 8944 | training loss: 0.000215948734\n",
      "Epoch: 8945 | training loss: 0.000215832755\n",
      "Epoch: 8946 | training loss: 0.000215716034\n",
      "Epoch: 8947 | training loss: 0.000215615524\n",
      "Epoch: 8948 | training loss: 0.000215523105\n",
      "Epoch: 8949 | training loss: 0.000215446838\n",
      "Epoch: 8950 | training loss: 0.000215377979\n",
      "Epoch: 8951 | training loss: 0.000215318592\n",
      "Epoch: 8952 | training loss: 0.000215269320\n",
      "Epoch: 8953 | training loss: 0.000215246284\n",
      "Epoch: 8954 | training loss: 0.000215259162\n",
      "Epoch: 8955 | training loss: 0.000215330729\n",
      "Epoch: 8956 | training loss: 0.000215472959\n",
      "Epoch: 8957 | training loss: 0.000215712193\n",
      "Epoch: 8958 | training loss: 0.000216088578\n",
      "Epoch: 8959 | training loss: 0.000216661108\n",
      "Epoch: 8960 | training loss: 0.000217511260\n",
      "Epoch: 8961 | training loss: 0.000218797504\n",
      "Epoch: 8962 | training loss: 0.000220698174\n",
      "Epoch: 8963 | training loss: 0.000223549345\n",
      "Epoch: 8964 | training loss: 0.000227804238\n",
      "Epoch: 8965 | training loss: 0.000234191306\n",
      "Epoch: 8966 | training loss: 0.000243813862\n",
      "Epoch: 8967 | training loss: 0.000258369924\n",
      "Epoch: 8968 | training loss: 0.000280494743\n",
      "Epoch: 8969 | training loss: 0.000314406701\n",
      "Epoch: 8970 | training loss: 0.000366478402\n",
      "Epoch: 8971 | training loss: 0.000447270577\n",
      "Epoch: 8972 | training loss: 0.000573004829\n",
      "Epoch: 8973 | training loss: 0.000770488987\n",
      "Epoch: 8974 | training loss: 0.001080130576\n",
      "Epoch: 8975 | training loss: 0.001570239780\n",
      "Epoch: 8976 | training loss: 0.002339875791\n",
      "Epoch: 8977 | training loss: 0.003556392156\n",
      "Epoch: 8978 | training loss: 0.005434555002\n",
      "Epoch: 8979 | training loss: 0.008327077143\n",
      "Epoch: 8980 | training loss: 0.012532228604\n",
      "Epoch: 8981 | training loss: 0.018434308469\n",
      "Epoch: 8982 | training loss: 0.025549521670\n",
      "Epoch: 8983 | training loss: 0.032859724015\n",
      "Epoch: 8984 | training loss: 0.036707490683\n",
      "Epoch: 8985 | training loss: 0.034173142165\n",
      "Epoch: 8986 | training loss: 0.023407729343\n",
      "Epoch: 8987 | training loss: 0.009779988788\n",
      "Epoch: 8988 | training loss: 0.001062440104\n",
      "Epoch: 8989 | training loss: 0.001729990589\n",
      "Epoch: 8990 | training loss: 0.008537710644\n",
      "Epoch: 8991 | training loss: 0.013765229844\n",
      "Epoch: 8992 | training loss: 0.012283881195\n",
      "Epoch: 8993 | training loss: 0.005642222706\n",
      "Epoch: 8994 | training loss: 0.000626528228\n",
      "Epoch: 8995 | training loss: 0.001375075895\n",
      "Epoch: 8996 | training loss: 0.005437409505\n",
      "Epoch: 8997 | training loss: 0.007339064032\n",
      "Epoch: 8998 | training loss: 0.004854425788\n",
      "Epoch: 8999 | training loss: 0.001191987540\n",
      "Epoch: 9000 | training loss: 0.000384815270\n",
      "Epoch: 9001 | training loss: 0.002491448075\n",
      "Epoch: 9002 | training loss: 0.004164633807\n",
      "Epoch: 9003 | training loss: 0.003185783280\n",
      "Epoch: 9004 | training loss: 0.000995590817\n",
      "Epoch: 9005 | training loss: 0.000266281364\n",
      "Epoch: 9006 | training loss: 0.001419035019\n",
      "Epoch: 9007 | training loss: 0.002481711796\n",
      "Epoch: 9008 | training loss: 0.001971446443\n",
      "Epoch: 9009 | training loss: 0.000683571445\n",
      "Epoch: 9010 | training loss: 0.000250228739\n",
      "Epoch: 9011 | training loss: 0.000938061625\n",
      "Epoch: 9012 | training loss: 0.001553109963\n",
      "Epoch: 9013 | training loss: 0.001224945881\n",
      "Epoch: 9014 | training loss: 0.000466451689\n",
      "Epoch: 9015 | training loss: 0.000245971343\n",
      "Epoch: 9016 | training loss: 0.000670730777\n",
      "Epoch: 9017 | training loss: 0.001012456487\n",
      "Epoch: 9018 | training loss: 0.000791496364\n",
      "Epoch: 9019 | training loss: 0.000346164015\n",
      "Epoch: 9020 | training loss: 0.000239666711\n",
      "Epoch: 9021 | training loss: 0.000500629365\n",
      "Epoch: 9022 | training loss: 0.000690290821\n",
      "Epoch: 9023 | training loss: 0.000547482341\n",
      "Epoch: 9024 | training loss: 0.000285733462\n",
      "Epoch: 9025 | training loss: 0.000230452686\n",
      "Epoch: 9026 | training loss: 0.000387048640\n",
      "Epoch: 9027 | training loss: 0.000496168446\n",
      "Epoch: 9028 | training loss: 0.000410242123\n",
      "Epoch: 9029 | training loss: 0.000255630555\n",
      "Epoch: 9030 | training loss: 0.000221269132\n",
      "Epoch: 9031 | training loss: 0.000312331249\n",
      "Epoch: 9032 | training loss: 0.000379135017\n",
      "Epoch: 9033 | training loss: 0.000332076248\n",
      "Epoch: 9034 | training loss: 0.000239987057\n",
      "Epoch: 9035 | training loss: 0.000214145592\n",
      "Epoch: 9036 | training loss: 0.000264892471\n",
      "Epoch: 9037 | training loss: 0.000308511197\n",
      "Epoch: 9038 | training loss: 0.000286597584\n",
      "Epoch: 9039 | training loss: 0.000231639744\n",
      "Epoch: 9040 | training loss: 0.000209655758\n",
      "Epoch: 9041 | training loss: 0.000235248735\n",
      "Epoch: 9042 | training loss: 0.000264525443\n",
      "Epoch: 9043 | training loss: 0.000258068467\n",
      "Epoch: 9044 | training loss: 0.000226360862\n",
      "Epoch: 9045 | training loss: 0.000207769161\n",
      "Epoch: 9046 | training loss: 0.000218276342\n",
      "Epoch: 9047 | training loss: 0.000237402070\n",
      "Epoch: 9048 | training loss: 0.000238971246\n",
      "Epoch: 9049 | training loss: 0.000222185365\n",
      "Epoch: 9050 | training loss: 0.000207531877\n",
      "Epoch: 9051 | training loss: 0.000209261139\n",
      "Epoch: 9052 | training loss: 0.000220553979\n",
      "Epoch: 9053 | training loss: 0.000225387281\n",
      "Epoch: 9054 | training loss: 0.000218164831\n",
      "Epoch: 9055 | training loss: 0.000207884339\n",
      "Epoch: 9056 | training loss: 0.000205292730\n",
      "Epoch: 9057 | training loss: 0.000210673054\n",
      "Epoch: 9058 | training loss: 0.000215648877\n",
      "Epoch: 9059 | training loss: 0.000214018102\n",
      "Epoch: 9060 | training loss: 0.000207976918\n",
      "Epoch: 9061 | training loss: 0.000204096927\n",
      "Epoch: 9062 | training loss: 0.000205404751\n",
      "Epoch: 9063 | training loss: 0.000208900514\n",
      "Epoch: 9064 | training loss: 0.000209912629\n",
      "Epoch: 9065 | training loss: 0.000207311910\n",
      "Epoch: 9066 | training loss: 0.000204010881\n",
      "Epoch: 9067 | training loss: 0.000203112577\n",
      "Epoch: 9068 | training loss: 0.000204687094\n",
      "Epoch: 9069 | training loss: 0.000206286350\n",
      "Epoch: 9070 | training loss: 0.000205900345\n",
      "Epoch: 9071 | training loss: 0.000203960270\n",
      "Epoch: 9072 | training loss: 0.000202400624\n",
      "Epoch: 9073 | training loss: 0.000202405237\n",
      "Epoch: 9074 | training loss: 0.000203382049\n",
      "Epoch: 9075 | training loss: 0.000203920528\n",
      "Epoch: 9076 | training loss: 0.000203324802\n",
      "Epoch: 9077 | training loss: 0.000202163399\n",
      "Epoch: 9078 | training loss: 0.000201451403\n",
      "Epoch: 9079 | training loss: 0.000201589763\n",
      "Epoch: 9080 | training loss: 0.000202082651\n",
      "Epoch: 9081 | training loss: 0.000202206618\n",
      "Epoch: 9082 | training loss: 0.000201724717\n",
      "Epoch: 9083 | training loss: 0.000201039351\n",
      "Epoch: 9084 | training loss: 0.000200664290\n",
      "Epoch: 9085 | training loss: 0.000200722745\n",
      "Epoch: 9086 | training loss: 0.000200917711\n",
      "Epoch: 9087 | training loss: 0.000200901297\n",
      "Epoch: 9088 | training loss: 0.000200577051\n",
      "Epoch: 9089 | training loss: 0.000200166935\n",
      "Epoch: 9090 | training loss: 0.000199924965\n",
      "Epoch: 9091 | training loss: 0.000199903327\n",
      "Epoch: 9092 | training loss: 0.000199955874\n",
      "Epoch: 9093 | training loss: 0.000199890506\n",
      "Epoch: 9094 | training loss: 0.000199676520\n",
      "Epoch: 9095 | training loss: 0.000199406568\n",
      "Epoch: 9096 | training loss: 0.000199212838\n",
      "Epoch: 9097 | training loss: 0.000199129077\n",
      "Epoch: 9098 | training loss: 0.000199095753\n",
      "Epoch: 9099 | training loss: 0.000199029804\n",
      "Epoch: 9100 | training loss: 0.000198885784\n",
      "Epoch: 9101 | training loss: 0.000198693800\n",
      "Epoch: 9102 | training loss: 0.000198524125\n",
      "Epoch: 9103 | training loss: 0.000198405018\n",
      "Epoch: 9104 | training loss: 0.000198330556\n",
      "Epoch: 9105 | training loss: 0.000198249545\n",
      "Epoch: 9106 | training loss: 0.000198136739\n",
      "Epoch: 9107 | training loss: 0.000197995061\n",
      "Epoch: 9108 | training loss: 0.000197844362\n",
      "Epoch: 9109 | training loss: 0.000197715737\n",
      "Epoch: 9110 | training loss: 0.000197614951\n",
      "Epoch: 9111 | training loss: 0.000197526999\n",
      "Epoch: 9112 | training loss: 0.000197432906\n",
      "Epoch: 9113 | training loss: 0.000197313391\n",
      "Epoch: 9114 | training loss: 0.000197176982\n",
      "Epoch: 9115 | training loss: 0.000197046553\n",
      "Epoch: 9116 | training loss: 0.000196931389\n",
      "Epoch: 9117 | training loss: 0.000196832669\n",
      "Epoch: 9118 | training loss: 0.000196732668\n",
      "Epoch: 9119 | training loss: 0.000196623791\n",
      "Epoch: 9120 | training loss: 0.000196505280\n",
      "Epoch: 9121 | training loss: 0.000196382971\n",
      "Epoch: 9122 | training loss: 0.000196263747\n",
      "Epoch: 9123 | training loss: 0.000196153778\n",
      "Epoch: 9124 | training loss: 0.000196045017\n",
      "Epoch: 9125 | training loss: 0.000195941961\n",
      "Epoch: 9126 | training loss: 0.000195835033\n",
      "Epoch: 9127 | training loss: 0.000195720961\n",
      "Epoch: 9128 | training loss: 0.000195604720\n",
      "Epoch: 9129 | training loss: 0.000195489250\n",
      "Epoch: 9130 | training loss: 0.000195380271\n",
      "Epoch: 9131 | training loss: 0.000195271074\n",
      "Epoch: 9132 | training loss: 0.000195162036\n",
      "Epoch: 9133 | training loss: 0.000195053231\n",
      "Epoch: 9134 | training loss: 0.000194944936\n",
      "Epoch: 9135 | training loss: 0.000194833206\n",
      "Epoch: 9136 | training loss: 0.000194722947\n",
      "Epoch: 9137 | training loss: 0.000194613283\n",
      "Epoch: 9138 | training loss: 0.000194502747\n",
      "Epoch: 9139 | training loss: 0.000194393942\n",
      "Epoch: 9140 | training loss: 0.000194282096\n",
      "Epoch: 9141 | training loss: 0.000194174776\n",
      "Epoch: 9142 | training loss: 0.000194066000\n",
      "Epoch: 9143 | training loss: 0.000193959277\n",
      "Epoch: 9144 | training loss: 0.000193844258\n",
      "Epoch: 9145 | training loss: 0.000193736414\n",
      "Epoch: 9146 | training loss: 0.000193628570\n",
      "Epoch: 9147 | training loss: 0.000193518717\n",
      "Epoch: 9148 | training loss: 0.000193409462\n",
      "Epoch: 9149 | training loss: 0.000193302840\n",
      "Epoch: 9150 | training loss: 0.000193193046\n",
      "Epoch: 9151 | training loss: 0.000193086176\n",
      "Epoch: 9152 | training loss: 0.000192976964\n",
      "Epoch: 9153 | training loss: 0.000192868043\n",
      "Epoch: 9154 | training loss: 0.000192758656\n",
      "Epoch: 9155 | training loss: 0.000192650477\n",
      "Epoch: 9156 | training loss: 0.000192537860\n",
      "Epoch: 9157 | training loss: 0.000192431733\n",
      "Epoch: 9158 | training loss: 0.000192325082\n",
      "Epoch: 9159 | training loss: 0.000192218053\n",
      "Epoch: 9160 | training loss: 0.000192106672\n",
      "Epoch: 9161 | training loss: 0.000191998799\n",
      "Epoch: 9162 | training loss: 0.000191891100\n",
      "Epoch: 9163 | training loss: 0.000191782077\n",
      "Epoch: 9164 | training loss: 0.000191675936\n",
      "Epoch: 9165 | training loss: 0.000191564090\n",
      "Epoch: 9166 | training loss: 0.000191458108\n",
      "Epoch: 9167 | training loss: 0.000191349478\n",
      "Epoch: 9168 | training loss: 0.000191243627\n",
      "Epoch: 9169 | training loss: 0.000191134211\n",
      "Epoch: 9170 | training loss: 0.000191026877\n",
      "Epoch: 9171 | training loss: 0.000190917912\n",
      "Epoch: 9172 | training loss: 0.000190809951\n",
      "Epoch: 9173 | training loss: 0.000190703009\n",
      "Epoch: 9174 | training loss: 0.000190596926\n",
      "Epoch: 9175 | training loss: 0.000190487073\n",
      "Epoch: 9176 | training loss: 0.000190378647\n",
      "Epoch: 9177 | training loss: 0.000190269900\n",
      "Epoch: 9178 | training loss: 0.000190161838\n",
      "Epoch: 9179 | training loss: 0.000190057297\n",
      "Epoch: 9180 | training loss: 0.000189948711\n",
      "Epoch: 9181 | training loss: 0.000189840706\n",
      "Epoch: 9182 | training loss: 0.000189734579\n",
      "Epoch: 9183 | training loss: 0.000189629936\n",
      "Epoch: 9184 | training loss: 0.000189519924\n",
      "Epoch: 9185 | training loss: 0.000189413520\n",
      "Epoch: 9186 | training loss: 0.000189305167\n",
      "Epoch: 9187 | training loss: 0.000189198807\n",
      "Epoch: 9188 | training loss: 0.000189088925\n",
      "Epoch: 9189 | training loss: 0.000188983584\n",
      "Epoch: 9190 | training loss: 0.000188875099\n",
      "Epoch: 9191 | training loss: 0.000188768798\n",
      "Epoch: 9192 | training loss: 0.000188658145\n",
      "Epoch: 9193 | training loss: 0.000188555088\n",
      "Epoch: 9194 | training loss: 0.000188448321\n",
      "Epoch: 9195 | training loss: 0.000188342136\n",
      "Epoch: 9196 | training loss: 0.000188233927\n",
      "Epoch: 9197 | training loss: 0.000188129139\n",
      "Epoch: 9198 | training loss: 0.000188021368\n",
      "Epoch: 9199 | training loss: 0.000187914062\n",
      "Epoch: 9200 | training loss: 0.000187805577\n",
      "Epoch: 9201 | training loss: 0.000187698635\n",
      "Epoch: 9202 | training loss: 0.000187589379\n",
      "Epoch: 9203 | training loss: 0.000187487691\n",
      "Epoch: 9204 | training loss: 0.000187380138\n",
      "Epoch: 9205 | training loss: 0.000187274127\n",
      "Epoch: 9206 | training loss: 0.000187166355\n",
      "Epoch: 9207 | training loss: 0.000187061771\n",
      "Epoch: 9208 | training loss: 0.000186958088\n",
      "Epoch: 9209 | training loss: 0.000186850753\n",
      "Epoch: 9210 | training loss: 0.000186746212\n",
      "Epoch: 9211 | training loss: 0.000186639416\n",
      "Epoch: 9212 | training loss: 0.000186538731\n",
      "Epoch: 9213 | training loss: 0.000186433987\n",
      "Epoch: 9214 | training loss: 0.000186332516\n",
      "Epoch: 9215 | training loss: 0.000186230784\n",
      "Epoch: 9216 | training loss: 0.000186131016\n",
      "Epoch: 9217 | training loss: 0.000186028381\n",
      "Epoch: 9218 | training loss: 0.000185932819\n",
      "Epoch: 9219 | training loss: 0.000185835845\n",
      "Epoch: 9220 | training loss: 0.000185737998\n",
      "Epoch: 9221 | training loss: 0.000185644662\n",
      "Epoch: 9222 | training loss: 0.000185556171\n",
      "Epoch: 9223 | training loss: 0.000185472221\n",
      "Epoch: 9224 | training loss: 0.000185399404\n",
      "Epoch: 9225 | training loss: 0.000185339071\n",
      "Epoch: 9226 | training loss: 0.000185292825\n",
      "Epoch: 9227 | training loss: 0.000185272744\n",
      "Epoch: 9228 | training loss: 0.000185292345\n",
      "Epoch: 9229 | training loss: 0.000185371478\n",
      "Epoch: 9230 | training loss: 0.000185532466\n",
      "Epoch: 9231 | training loss: 0.000185811135\n",
      "Epoch: 9232 | training loss: 0.000186266843\n",
      "Epoch: 9233 | training loss: 0.000186984398\n",
      "Epoch: 9234 | training loss: 0.000188106089\n",
      "Epoch: 9235 | training loss: 0.000189842205\n",
      "Epoch: 9236 | training loss: 0.000192519248\n",
      "Epoch: 9237 | training loss: 0.000196637004\n",
      "Epoch: 9238 | training loss: 0.000203009724\n",
      "Epoch: 9239 | training loss: 0.000212882645\n",
      "Epoch: 9240 | training loss: 0.000228286779\n",
      "Epoch: 9241 | training loss: 0.000252445141\n",
      "Epoch: 9242 | training loss: 0.000290583179\n",
      "Epoch: 9243 | training loss: 0.000350985094\n",
      "Epoch: 9244 | training loss: 0.000447234052\n",
      "Epoch: 9245 | training loss: 0.000600627158\n",
      "Epoch: 9246 | training loss: 0.000845804811\n",
      "Epoch: 9247 | training loss: 0.001234652125\n",
      "Epoch: 9248 | training loss: 0.001848456217\n",
      "Epoch: 9249 | training loss: 0.002792669460\n",
      "Epoch: 9250 | training loss: 0.004205663688\n",
      "Epoch: 9251 | training loss: 0.006171016488\n",
      "Epoch: 9252 | training loss: 0.008650652133\n",
      "Epoch: 9253 | training loss: 0.011137910187\n",
      "Epoch: 9254 | training loss: 0.012682879344\n",
      "Epoch: 9255 | training loss: 0.011941579171\n",
      "Epoch: 9256 | training loss: 0.008611868136\n",
      "Epoch: 9257 | training loss: 0.004110668320\n",
      "Epoch: 9258 | training loss: 0.001063704141\n",
      "Epoch: 9259 | training loss: 0.001001001569\n",
      "Epoch: 9260 | training loss: 0.003177054925\n",
      "Epoch: 9261 | training loss: 0.005369564984\n",
      "Epoch: 9262 | training loss: 0.005624976009\n",
      "Epoch: 9263 | training loss: 0.003799094353\n",
      "Epoch: 9264 | training loss: 0.001545264735\n",
      "Epoch: 9265 | training loss: 0.000719641335\n",
      "Epoch: 9266 | training loss: 0.001591885462\n",
      "Epoch: 9267 | training loss: 0.002837417647\n",
      "Epoch: 9268 | training loss: 0.003014967544\n",
      "Epoch: 9269 | training loss: 0.001934881089\n",
      "Epoch: 9270 | training loss: 0.000674741517\n",
      "Epoch: 9271 | training loss: 0.000326919777\n",
      "Epoch: 9272 | training loss: 0.000907120120\n",
      "Epoch: 9273 | training loss: 0.001531753922\n",
      "Epoch: 9274 | training loss: 0.001452141441\n",
      "Epoch: 9275 | training loss: 0.000786133343\n",
      "Epoch: 9276 | training loss: 0.000240951485\n",
      "Epoch: 9277 | training loss: 0.000289666525\n",
      "Epoch: 9278 | training loss: 0.000723687815\n",
      "Epoch: 9279 | training loss: 0.000990384957\n",
      "Epoch: 9280 | training loss: 0.000818148023\n",
      "Epoch: 9281 | training loss: 0.000436382688\n",
      "Epoch: 9282 | training loss: 0.000249926001\n",
      "Epoch: 9283 | training loss: 0.000387060718\n",
      "Epoch: 9284 | training loss: 0.000620779057\n",
      "Epoch: 9285 | training loss: 0.000668420165\n",
      "Epoch: 9286 | training loss: 0.000490370847\n",
      "Epoch: 9287 | training loss: 0.000284613925\n",
      "Epoch: 9288 | training loss: 0.000240149704\n",
      "Epoch: 9289 | training loss: 0.000347190362\n",
      "Epoch: 9290 | training loss: 0.000446264894\n",
      "Epoch: 9291 | training loss: 0.000417991570\n",
      "Epoch: 9292 | training loss: 0.000295858423\n",
      "Epoch: 9293 | training loss: 0.000202567375\n",
      "Epoch: 9294 | training loss: 0.000210546699\n",
      "Epoch: 9295 | training loss: 0.000279472501\n",
      "Epoch: 9296 | training loss: 0.000318271632\n",
      "Epoch: 9297 | training loss: 0.000284692622\n",
      "Epoch: 9298 | training loss: 0.000216767556\n",
      "Epoch: 9299 | training loss: 0.000179884708\n",
      "Epoch: 9300 | training loss: 0.000197616086\n",
      "Epoch: 9301 | training loss: 0.000237001397\n",
      "Epoch: 9302 | training loss: 0.000251423509\n",
      "Epoch: 9303 | training loss: 0.000227797049\n",
      "Epoch: 9304 | training loss: 0.000192296633\n",
      "Epoch: 9305 | training loss: 0.000177637514\n",
      "Epoch: 9306 | training loss: 0.000190950173\n",
      "Epoch: 9307 | training loss: 0.000212138897\n",
      "Epoch: 9308 | training loss: 0.000217967303\n",
      "Epoch: 9309 | training loss: 0.000204216165\n",
      "Epoch: 9310 | training loss: 0.000185750352\n",
      "Epoch: 9311 | training loss: 0.000179071212\n",
      "Epoch: 9312 | training loss: 0.000187002588\n",
      "Epoch: 9313 | training loss: 0.000198897149\n",
      "Epoch: 9314 | training loss: 0.000202936892\n",
      "Epoch: 9315 | training loss: 0.000197005691\n",
      "Epoch: 9316 | training loss: 0.000188672464\n",
      "Epoch: 9317 | training loss: 0.000186721882\n",
      "Epoch: 9318 | training loss: 0.000193373955\n",
      "Epoch: 9319 | training loss: 0.000203949079\n",
      "Epoch: 9320 | training loss: 0.000212707964\n",
      "Epoch: 9321 | training loss: 0.000218526009\n",
      "Epoch: 9322 | training loss: 0.000225778218\n",
      "Epoch: 9323 | training loss: 0.000240590642\n",
      "Epoch: 9324 | training loss: 0.000267322554\n",
      "Epoch: 9325 | training loss: 0.000307787035\n",
      "Epoch: 9326 | training loss: 0.000364377745\n",
      "Epoch: 9327 | training loss: 0.000443476660\n",
      "Epoch: 9328 | training loss: 0.000558814208\n",
      "Epoch: 9329 | training loss: 0.000731293694\n",
      "Epoch: 9330 | training loss: 0.000992298010\n",
      "Epoch: 9331 | training loss: 0.001382541726\n",
      "Epoch: 9332 | training loss: 0.001968497410\n",
      "Epoch: 9333 | training loss: 0.002833724255\n",
      "Epoch: 9334 | training loss: 0.004120475147\n",
      "Epoch: 9335 | training loss: 0.005971256178\n",
      "Epoch: 9336 | training loss: 0.008612978272\n",
      "Epoch: 9337 | training loss: 0.012101901695\n",
      "Epoch: 9338 | training loss: 0.016476649791\n",
      "Epoch: 9339 | training loss: 0.020973237231\n",
      "Epoch: 9340 | training loss: 0.024585099891\n",
      "Epoch: 9341 | training loss: 0.025012912229\n",
      "Epoch: 9342 | training loss: 0.021254101768\n",
      "Epoch: 9343 | training loss: 0.013487458229\n",
      "Epoch: 9344 | training loss: 0.005305842962\n",
      "Epoch: 9345 | training loss: 0.000587550050\n",
      "Epoch: 9346 | training loss: 0.001075573615\n",
      "Epoch: 9347 | training loss: 0.004962705541\n",
      "Epoch: 9348 | training loss: 0.008447051980\n",
      "Epoch: 9349 | training loss: 0.008651532233\n",
      "Epoch: 9350 | training loss: 0.005451600999\n",
      "Epoch: 9351 | training loss: 0.001666291500\n",
      "Epoch: 9352 | training loss: 0.000183380762\n",
      "Epoch: 9353 | training loss: 0.001539985766\n",
      "Epoch: 9354 | training loss: 0.003746949602\n",
      "Epoch: 9355 | training loss: 0.004410567693\n",
      "Epoch: 9356 | training loss: 0.002996514319\n",
      "Epoch: 9357 | training loss: 0.000970108202\n",
      "Epoch: 9358 | training loss: 0.000182821866\n",
      "Epoch: 9359 | training loss: 0.000983578037\n",
      "Epoch: 9360 | training loss: 0.002149594948\n",
      "Epoch: 9361 | training loss: 0.002350690542\n",
      "Epoch: 9362 | training loss: 0.001438358217\n",
      "Epoch: 9363 | training loss: 0.000419407792\n",
      "Epoch: 9364 | training loss: 0.000225250813\n",
      "Epoch: 9365 | training loss: 0.000805450371\n",
      "Epoch: 9366 | training loss: 0.001354762120\n",
      "Epoch: 9367 | training loss: 0.001253078342\n",
      "Epoch: 9368 | training loss: 0.000664309773\n",
      "Epoch: 9369 | training loss: 0.000214128610\n",
      "Epoch: 9370 | training loss: 0.000279583066\n",
      "Epoch: 9371 | training loss: 0.000650692557\n",
      "Epoch: 9372 | training loss: 0.000853033445\n",
      "Epoch: 9373 | training loss: 0.000678525539\n",
      "Epoch: 9374 | training loss: 0.000338909682\n",
      "Epoch: 9375 | training loss: 0.000176672242\n",
      "Epoch: 9376 | training loss: 0.000292845158\n",
      "Epoch: 9377 | training loss: 0.000493671105\n",
      "Epoch: 9378 | training loss: 0.000541575835\n",
      "Epoch: 9379 | training loss: 0.000397697906\n",
      "Epoch: 9380 | training loss: 0.000223350784\n",
      "Epoch: 9381 | training loss: 0.000179710885\n",
      "Epoch: 9382 | training loss: 0.000270206016\n",
      "Epoch: 9383 | training loss: 0.000367947912\n",
      "Epoch: 9384 | training loss: 0.000364658365\n",
      "Epoch: 9385 | training loss: 0.000272553298\n",
      "Epoch: 9386 | training loss: 0.000187676575\n",
      "Epoch: 9387 | training loss: 0.000181549971\n",
      "Epoch: 9388 | training loss: 0.000237397675\n",
      "Epoch: 9389 | training loss: 0.000283577974\n",
      "Epoch: 9390 | training loss: 0.000272223842\n",
      "Epoch: 9391 | training loss: 0.000219351787\n",
      "Epoch: 9392 | training loss: 0.000177218230\n",
      "Epoch: 9393 | training loss: 0.000178364455\n",
      "Epoch: 9394 | training loss: 0.000209470483\n",
      "Epoch: 9395 | training loss: 0.000232739767\n",
      "Epoch: 9396 | training loss: 0.000225191237\n",
      "Epoch: 9397 | training loss: 0.000196608366\n",
      "Epoch: 9398 | training loss: 0.000173927940\n",
      "Epoch: 9399 | training loss: 0.000173812572\n",
      "Epoch: 9400 | training loss: 0.000189813320\n",
      "Epoch: 9401 | training loss: 0.000202842086\n",
      "Epoch: 9402 | training loss: 0.000200333045\n",
      "Epoch: 9403 | training loss: 0.000185798577\n",
      "Epoch: 9404 | training loss: 0.000172643209\n",
      "Epoch: 9405 | training loss: 0.000170549596\n",
      "Epoch: 9406 | training loss: 0.000178015442\n",
      "Epoch: 9407 | training loss: 0.000185873054\n",
      "Epoch: 9408 | training loss: 0.000186487625\n",
      "Epoch: 9409 | training loss: 0.000179874623\n",
      "Epoch: 9410 | training loss: 0.000172063388\n",
      "Epoch: 9411 | training loss: 0.000168925879\n",
      "Epoch: 9412 | training loss: 0.000171433465\n",
      "Epoch: 9413 | training loss: 0.000175911337\n",
      "Epoch: 9414 | training loss: 0.000177879585\n",
      "Epoch: 9415 | training loss: 0.000175762063\n",
      "Epoch: 9416 | training loss: 0.000171548221\n",
      "Epoch: 9417 | training loss: 0.000168496015\n",
      "Epoch: 9418 | training loss: 0.000168369952\n",
      "Epoch: 9419 | training loss: 0.000170376938\n",
      "Epoch: 9420 | training loss: 0.000172275206\n",
      "Epoch: 9421 | training loss: 0.000172358501\n",
      "Epoch: 9422 | training loss: 0.000170628438\n",
      "Epoch: 9423 | training loss: 0.000168470171\n",
      "Epoch: 9424 | training loss: 0.000167307473\n",
      "Epoch: 9425 | training loss: 0.000167580118\n",
      "Epoch: 9426 | training loss: 0.000168622777\n",
      "Epoch: 9427 | training loss: 0.000169354375\n",
      "Epoch: 9428 | training loss: 0.000169159728\n",
      "Epoch: 9429 | training loss: 0.000168172992\n",
      "Epoch: 9430 | training loss: 0.000167080871\n",
      "Epoch: 9431 | training loss: 0.000166512604\n",
      "Epoch: 9432 | training loss: 0.000166623780\n",
      "Epoch: 9433 | training loss: 0.000167087914\n",
      "Epoch: 9434 | training loss: 0.000167406004\n",
      "Epoch: 9435 | training loss: 0.000167280377\n",
      "Epoch: 9436 | training loss: 0.000166775004\n",
      "Epoch: 9437 | training loss: 0.000166186161\n",
      "Epoch: 9438 | training loss: 0.000165805599\n",
      "Epoch: 9439 | training loss: 0.000165749443\n",
      "Epoch: 9440 | training loss: 0.000165899677\n",
      "Epoch: 9441 | training loss: 0.000166041413\n",
      "Epoch: 9442 | training loss: 0.000166012658\n",
      "Epoch: 9443 | training loss: 0.000165779566\n",
      "Epoch: 9444 | training loss: 0.000165446836\n",
      "Epoch: 9445 | training loss: 0.000165159770\n",
      "Epoch: 9446 | training loss: 0.000165007339\n",
      "Epoch: 9447 | training loss: 0.000164986413\n",
      "Epoch: 9448 | training loss: 0.000165010802\n",
      "Epoch: 9449 | training loss: 0.000165005200\n",
      "Epoch: 9450 | training loss: 0.000164907848\n",
      "Epoch: 9451 | training loss: 0.000164731391\n",
      "Epoch: 9452 | training loss: 0.000164531841\n",
      "Epoch: 9453 | training loss: 0.000164364305\n",
      "Epoch: 9454 | training loss: 0.000164254423\n",
      "Epoch: 9455 | training loss: 0.000164205849\n",
      "Epoch: 9456 | training loss: 0.000164169716\n",
      "Epoch: 9457 | training loss: 0.000164114928\n",
      "Epoch: 9458 | training loss: 0.000164017431\n",
      "Epoch: 9459 | training loss: 0.000163888748\n",
      "Epoch: 9460 | training loss: 0.000163751160\n",
      "Epoch: 9461 | training loss: 0.000163623321\n",
      "Epoch: 9462 | training loss: 0.000163515171\n",
      "Epoch: 9463 | training loss: 0.000163433142\n",
      "Epoch: 9464 | training loss: 0.000163360208\n",
      "Epoch: 9465 | training loss: 0.000163291159\n",
      "Epoch: 9466 | training loss: 0.000163210017\n",
      "Epoch: 9467 | training loss: 0.000163109333\n",
      "Epoch: 9468 | training loss: 0.000163002187\n",
      "Epoch: 9469 | training loss: 0.000162892495\n",
      "Epoch: 9470 | training loss: 0.000162793163\n",
      "Epoch: 9471 | training loss: 0.000162702898\n",
      "Epoch: 9472 | training loss: 0.000162620810\n",
      "Epoch: 9473 | training loss: 0.000162534387\n",
      "Epoch: 9474 | training loss: 0.000162455661\n",
      "Epoch: 9475 | training loss: 0.000162363693\n",
      "Epoch: 9476 | training loss: 0.000162269556\n",
      "Epoch: 9477 | training loss: 0.000162175726\n",
      "Epoch: 9478 | training loss: 0.000162079494\n",
      "Epoch: 9479 | training loss: 0.000161987060\n",
      "Epoch: 9480 | training loss: 0.000161898322\n",
      "Epoch: 9481 | training loss: 0.000161811360\n",
      "Epoch: 9482 | training loss: 0.000161726784\n",
      "Epoch: 9483 | training loss: 0.000161642107\n",
      "Epoch: 9484 | training loss: 0.000161557036\n",
      "Epoch: 9485 | training loss: 0.000161467717\n",
      "Epoch: 9486 | training loss: 0.000161376302\n",
      "Epoch: 9487 | training loss: 0.000161285556\n",
      "Epoch: 9488 | training loss: 0.000161191434\n",
      "Epoch: 9489 | training loss: 0.000161099422\n",
      "Epoch: 9490 | training loss: 0.000161010452\n",
      "Epoch: 9491 | training loss: 0.000160922151\n",
      "Epoch: 9492 | training loss: 0.000160834752\n",
      "Epoch: 9493 | training loss: 0.000160746582\n",
      "Epoch: 9494 | training loss: 0.000160660915\n",
      "Epoch: 9495 | training loss: 0.000160572978\n",
      "Epoch: 9496 | training loss: 0.000160485986\n",
      "Epoch: 9497 | training loss: 0.000160392752\n",
      "Epoch: 9498 | training loss: 0.000160302123\n",
      "Epoch: 9499 | training loss: 0.000160213807\n",
      "Epoch: 9500 | training loss: 0.000160125550\n",
      "Epoch: 9501 | training loss: 0.000160036347\n",
      "Epoch: 9502 | training loss: 0.000159950956\n",
      "Epoch: 9503 | training loss: 0.000159863121\n",
      "Epoch: 9504 | training loss: 0.000159773044\n",
      "Epoch: 9505 | training loss: 0.000159689531\n",
      "Epoch: 9506 | training loss: 0.000159601754\n",
      "Epoch: 9507 | training loss: 0.000159515475\n",
      "Epoch: 9508 | training loss: 0.000159427713\n",
      "Epoch: 9509 | training loss: 0.000159338379\n",
      "Epoch: 9510 | training loss: 0.000159251533\n",
      "Epoch: 9511 | training loss: 0.000159161340\n",
      "Epoch: 9512 | training loss: 0.000159074742\n",
      "Epoch: 9513 | training loss: 0.000158987154\n",
      "Epoch: 9514 | training loss: 0.000158896073\n",
      "Epoch: 9515 | training loss: 0.000158807961\n",
      "Epoch: 9516 | training loss: 0.000158719689\n",
      "Epoch: 9517 | training loss: 0.000158632887\n",
      "Epoch: 9518 | training loss: 0.000158545066\n",
      "Epoch: 9519 | training loss: 0.000158455485\n",
      "Epoch: 9520 | training loss: 0.000158370356\n",
      "Epoch: 9521 | training loss: 0.000158283758\n",
      "Epoch: 9522 | training loss: 0.000158196301\n",
      "Epoch: 9523 | training loss: 0.000158108072\n",
      "Epoch: 9524 | training loss: 0.000158022493\n",
      "Epoch: 9525 | training loss: 0.000157934992\n",
      "Epoch: 9526 | training loss: 0.000157849514\n",
      "Epoch: 9527 | training loss: 0.000157763949\n",
      "Epoch: 9528 | training loss: 0.000157674236\n",
      "Epoch: 9529 | training loss: 0.000157585950\n",
      "Epoch: 9530 | training loss: 0.000157499016\n",
      "Epoch: 9531 | training loss: 0.000157411559\n",
      "Epoch: 9532 | training loss: 0.000157324859\n",
      "Epoch: 9533 | training loss: 0.000157238421\n",
      "Epoch: 9534 | training loss: 0.000157151677\n",
      "Epoch: 9535 | training loss: 0.000157066053\n",
      "Epoch: 9536 | training loss: 0.000156980313\n",
      "Epoch: 9537 | training loss: 0.000156893278\n",
      "Epoch: 9538 | training loss: 0.000156811613\n",
      "Epoch: 9539 | training loss: 0.000156724767\n",
      "Epoch: 9540 | training loss: 0.000156639726\n",
      "Epoch: 9541 | training loss: 0.000156556023\n",
      "Epoch: 9542 | training loss: 0.000156470560\n",
      "Epoch: 9543 | training loss: 0.000156386770\n",
      "Epoch: 9544 | training loss: 0.000156302514\n",
      "Epoch: 9545 | training loss: 0.000156223250\n",
      "Epoch: 9546 | training loss: 0.000156141497\n",
      "Epoch: 9547 | training loss: 0.000156064489\n",
      "Epoch: 9548 | training loss: 0.000155990332\n",
      "Epoch: 9549 | training loss: 0.000155920075\n",
      "Epoch: 9550 | training loss: 0.000155859554\n",
      "Epoch: 9551 | training loss: 0.000155807502\n",
      "Epoch: 9552 | training loss: 0.000155764501\n",
      "Epoch: 9553 | training loss: 0.000155742455\n",
      "Epoch: 9554 | training loss: 0.000155744361\n",
      "Epoch: 9555 | training loss: 0.000155778282\n",
      "Epoch: 9556 | training loss: 0.000155860005\n",
      "Epoch: 9557 | training loss: 0.000156023118\n",
      "Epoch: 9558 | training loss: 0.000156297407\n",
      "Epoch: 9559 | training loss: 0.000156737457\n",
      "Epoch: 9560 | training loss: 0.000157430943\n",
      "Epoch: 9561 | training loss: 0.000158511975\n",
      "Epoch: 9562 | training loss: 0.000160190859\n",
      "Epoch: 9563 | training loss: 0.000162800716\n",
      "Epoch: 9564 | training loss: 0.000166801969\n",
      "Epoch: 9565 | training loss: 0.000172980261\n",
      "Epoch: 9566 | training loss: 0.000182600575\n",
      "Epoch: 9567 | training loss: 0.000197652960\n",
      "Epoch: 9568 | training loss: 0.000221278460\n",
      "Epoch: 9569 | training loss: 0.000258714776\n",
      "Epoch: 9570 | training loss: 0.000318338454\n",
      "Epoch: 9571 | training loss: 0.000414019043\n",
      "Epoch: 9572 | training loss: 0.000567506533\n",
      "Epoch: 9573 | training loss: 0.000815860054\n",
      "Epoch: 9574 | training loss: 0.001217081561\n",
      "Epoch: 9575 | training loss: 0.001870583510\n",
      "Epoch: 9576 | training loss: 0.002922948683\n",
      "Epoch: 9577 | training loss: 0.004623402841\n",
      "Epoch: 9578 | training loss: 0.007283747662\n",
      "Epoch: 9579 | training loss: 0.011387701146\n",
      "Epoch: 9580 | training loss: 0.017203383148\n",
      "Epoch: 9581 | training loss: 0.024855611846\n",
      "Epoch: 9582 | training loss: 0.032618902624\n",
      "Epoch: 9583 | training loss: 0.037743505090\n",
      "Epoch: 9584 | training loss: 0.034964840859\n",
      "Epoch: 9585 | training loss: 0.023735333234\n",
      "Epoch: 9586 | training loss: 0.008968291804\n",
      "Epoch: 9587 | training loss: 0.000593123608\n",
      "Epoch: 9588 | training loss: 0.002866048366\n",
      "Epoch: 9589 | training loss: 0.010681772605\n",
      "Epoch: 9590 | training loss: 0.014831504785\n",
      "Epoch: 9591 | training loss: 0.010688782670\n",
      "Epoch: 9592 | training loss: 0.003186181886\n",
      "Epoch: 9593 | training loss: 0.000275802799\n",
      "Epoch: 9594 | training loss: 0.003675011219\n",
      "Epoch: 9595 | training loss: 0.007558288518\n",
      "Epoch: 9596 | training loss: 0.006516597234\n",
      "Epoch: 9597 | training loss: 0.002230082406\n",
      "Epoch: 9598 | training loss: 0.000228982652\n",
      "Epoch: 9599 | training loss: 0.002233293606\n",
      "Epoch: 9600 | training loss: 0.004463986028\n",
      "Epoch: 9601 | training loss: 0.003536518663\n",
      "Epoch: 9602 | training loss: 0.000976322859\n",
      "Epoch: 9603 | training loss: 0.000292936253\n",
      "Epoch: 9604 | training loss: 0.001818342367\n",
      "Epoch: 9605 | training loss: 0.002787241479\n",
      "Epoch: 9606 | training loss: 0.001729426906\n",
      "Epoch: 9607 | training loss: 0.000325383036\n",
      "Epoch: 9608 | training loss: 0.000458787195\n",
      "Epoch: 9609 | training loss: 0.001498169731\n",
      "Epoch: 9610 | training loss: 0.001655100612\n",
      "Epoch: 9611 | training loss: 0.000744168530\n",
      "Epoch: 9612 | training loss: 0.000162943659\n",
      "Epoch: 9613 | training loss: 0.000584554218\n",
      "Epoch: 9614 | training loss: 0.001120522269\n",
      "Epoch: 9615 | training loss: 0.000898436294\n",
      "Epoch: 9616 | training loss: 0.000309054827\n",
      "Epoch: 9617 | training loss: 0.000207459918\n",
      "Epoch: 9618 | training loss: 0.000587995513\n",
      "Epoch: 9619 | training loss: 0.000755443296\n",
      "Epoch: 9620 | training loss: 0.000462952390\n",
      "Epoch: 9621 | training loss: 0.000176219459\n",
      "Epoch: 9622 | training loss: 0.000268327072\n",
      "Epoch: 9623 | training loss: 0.000499753281\n",
      "Epoch: 9624 | training loss: 0.000479620736\n",
      "Epoch: 9625 | training loss: 0.000256936532\n",
      "Epoch: 9626 | training loss: 0.000162451470\n",
      "Epoch: 9627 | training loss: 0.000284135924\n",
      "Epoch: 9628 | training loss: 0.000387434324\n",
      "Epoch: 9629 | training loss: 0.000310128205\n",
      "Epoch: 9630 | training loss: 0.000178932998\n",
      "Epoch: 9631 | training loss: 0.000173615306\n",
      "Epoch: 9632 | training loss: 0.000264821778\n",
      "Epoch: 9633 | training loss: 0.000294521888\n",
      "Epoch: 9634 | training loss: 0.000221678114\n",
      "Epoch: 9635 | training loss: 0.000156904920\n",
      "Epoch: 9636 | training loss: 0.000179157854\n",
      "Epoch: 9637 | training loss: 0.000233261555\n",
      "Epoch: 9638 | training loss: 0.000231517115\n",
      "Epoch: 9639 | training loss: 0.000180448595\n",
      "Epoch: 9640 | training loss: 0.000153125860\n",
      "Epoch: 9641 | training loss: 0.000176618545\n",
      "Epoch: 9642 | training loss: 0.000204538053\n",
      "Epoch: 9643 | training loss: 0.000193992819\n",
      "Epoch: 9644 | training loss: 0.000162626340\n",
      "Epoch: 9645 | training loss: 0.000152786452\n",
      "Epoch: 9646 | training loss: 0.000170275118\n",
      "Epoch: 9647 | training loss: 0.000183678232\n",
      "Epoch: 9648 | training loss: 0.000173297944\n",
      "Epoch: 9649 | training loss: 0.000154952591\n",
      "Epoch: 9650 | training loss: 0.000152024382\n",
      "Epoch: 9651 | training loss: 0.000163482080\n",
      "Epoch: 9652 | training loss: 0.000169849402\n",
      "Epoch: 9653 | training loss: 0.000162185330\n",
      "Epoch: 9654 | training loss: 0.000151524146\n",
      "Epoch: 9655 | training loss: 0.000150751584\n",
      "Epoch: 9656 | training loss: 0.000157785093\n",
      "Epoch: 9657 | training loss: 0.000161094635\n",
      "Epoch: 9658 | training loss: 0.000156188471\n",
      "Epoch: 9659 | training loss: 0.000149865606\n",
      "Epoch: 9660 | training loss: 0.000149464322\n",
      "Epoch: 9661 | training loss: 0.000153588393\n",
      "Epoch: 9662 | training loss: 0.000155552232\n",
      "Epoch: 9663 | training loss: 0.000152702894\n",
      "Epoch: 9664 | training loss: 0.000148865234\n",
      "Epoch: 9665 | training loss: 0.000148368286\n",
      "Epoch: 9666 | training loss: 0.000150655571\n",
      "Epoch: 9667 | training loss: 0.000151920365\n",
      "Epoch: 9668 | training loss: 0.000150386200\n",
      "Epoch: 9669 | training loss: 0.000148045423\n",
      "Epoch: 9670 | training loss: 0.000147492538\n",
      "Epoch: 9671 | training loss: 0.000148701307\n",
      "Epoch: 9672 | training loss: 0.000149587169\n",
      "Epoch: 9673 | training loss: 0.000148867359\n",
      "Epoch: 9674 | training loss: 0.000147398823\n",
      "Epoch: 9675 | training loss: 0.000146786711\n",
      "Epoch: 9676 | training loss: 0.000147329585\n",
      "Epoch: 9677 | training loss: 0.000147954910\n",
      "Epoch: 9678 | training loss: 0.000147709026\n",
      "Epoch: 9679 | training loss: 0.000146833336\n",
      "Epoch: 9680 | training loss: 0.000146249164\n",
      "Epoch: 9681 | training loss: 0.000146374907\n",
      "Epoch: 9682 | training loss: 0.000146757011\n",
      "Epoch: 9683 | training loss: 0.000146746926\n",
      "Epoch: 9684 | training loss: 0.000146275197\n",
      "Epoch: 9685 | training loss: 0.000145802143\n",
      "Epoch: 9686 | training loss: 0.000145701837\n",
      "Epoch: 9687 | training loss: 0.000145869271\n",
      "Epoch: 9688 | training loss: 0.000145935541\n",
      "Epoch: 9689 | training loss: 0.000145717728\n",
      "Epoch: 9690 | training loss: 0.000145376165\n",
      "Epoch: 9691 | training loss: 0.000145179642\n",
      "Epoch: 9692 | training loss: 0.000145194877\n",
      "Epoch: 9693 | training loss: 0.000145256097\n",
      "Epoch: 9694 | training loss: 0.000145176338\n",
      "Epoch: 9695 | training loss: 0.000144954305\n",
      "Epoch: 9696 | training loss: 0.000144745456\n",
      "Epoch: 9697 | training loss: 0.000144652760\n",
      "Epoch: 9698 | training loss: 0.000144653546\n",
      "Epoch: 9699 | training loss: 0.000144630147\n",
      "Epoch: 9700 | training loss: 0.000144515245\n",
      "Epoch: 9701 | training loss: 0.000144339909\n",
      "Epoch: 9702 | training loss: 0.000144202451\n",
      "Epoch: 9703 | training loss: 0.000144131991\n",
      "Epoch: 9704 | training loss: 0.000144095422\n",
      "Epoch: 9705 | training loss: 0.000144032310\n",
      "Epoch: 9706 | training loss: 0.000143917074\n",
      "Epoch: 9707 | training loss: 0.000143786747\n",
      "Epoch: 9708 | training loss: 0.000143687590\n",
      "Epoch: 9709 | training loss: 0.000143624027\n",
      "Epoch: 9710 | training loss: 0.000143571262\n",
      "Epoch: 9711 | training loss: 0.000143492085\n",
      "Epoch: 9712 | training loss: 0.000143383630\n",
      "Epoch: 9713 | training loss: 0.000143273835\n",
      "Epoch: 9714 | training loss: 0.000143180048\n",
      "Epoch: 9715 | training loss: 0.000143108788\n",
      "Epoch: 9716 | training loss: 0.000143039622\n",
      "Epoch: 9717 | training loss: 0.000142957710\n",
      "Epoch: 9718 | training loss: 0.000142864446\n",
      "Epoch: 9719 | training loss: 0.000142767705\n",
      "Epoch: 9720 | training loss: 0.000142681820\n",
      "Epoch: 9721 | training loss: 0.000142606135\n",
      "Epoch: 9722 | training loss: 0.000142528108\n",
      "Epoch: 9723 | training loss: 0.000142449397\n",
      "Epoch: 9724 | training loss: 0.000142361212\n",
      "Epoch: 9725 | training loss: 0.000142270175\n",
      "Epoch: 9726 | training loss: 0.000142187084\n",
      "Epoch: 9727 | training loss: 0.000142107368\n",
      "Epoch: 9728 | training loss: 0.000142030127\n",
      "Epoch: 9729 | training loss: 0.000141950673\n",
      "Epoch: 9730 | training loss: 0.000141865108\n",
      "Epoch: 9731 | training loss: 0.000141779150\n",
      "Epoch: 9732 | training loss: 0.000141692450\n",
      "Epoch: 9733 | training loss: 0.000141613928\n",
      "Epoch: 9734 | training loss: 0.000141538490\n",
      "Epoch: 9735 | training loss: 0.000141458004\n",
      "Epoch: 9736 | training loss: 0.000141375276\n",
      "Epoch: 9737 | training loss: 0.000141289245\n",
      "Epoch: 9738 | training loss: 0.000141206518\n",
      "Epoch: 9739 | training loss: 0.000141126569\n",
      "Epoch: 9740 | training loss: 0.000141044176\n",
      "Epoch: 9741 | training loss: 0.000140966760\n",
      "Epoch: 9742 | training loss: 0.000140884775\n",
      "Epoch: 9743 | training loss: 0.000140802062\n",
      "Epoch: 9744 | training loss: 0.000140721066\n",
      "Epoch: 9745 | training loss: 0.000140641423\n",
      "Epoch: 9746 | training loss: 0.000140560107\n",
      "Epoch: 9747 | training loss: 0.000140478878\n",
      "Epoch: 9748 | training loss: 0.000140397256\n",
      "Epoch: 9749 | training loss: 0.000140317978\n",
      "Epoch: 9750 | training loss: 0.000140235905\n",
      "Epoch: 9751 | training loss: 0.000140156844\n",
      "Epoch: 9752 | training loss: 0.000140075703\n",
      "Epoch: 9753 | training loss: 0.000139993746\n",
      "Epoch: 9754 | training loss: 0.000139915734\n",
      "Epoch: 9755 | training loss: 0.000139833472\n",
      "Epoch: 9756 | training loss: 0.000139755284\n",
      "Epoch: 9757 | training loss: 0.000139673881\n",
      "Epoch: 9758 | training loss: 0.000139592725\n",
      "Epoch: 9759 | training loss: 0.000139514363\n",
      "Epoch: 9760 | training loss: 0.000139435549\n",
      "Epoch: 9761 | training loss: 0.000139355310\n",
      "Epoch: 9762 | training loss: 0.000139274067\n",
      "Epoch: 9763 | training loss: 0.000139193551\n",
      "Epoch: 9764 | training loss: 0.000139113428\n",
      "Epoch: 9765 | training loss: 0.000139033771\n",
      "Epoch: 9766 | training loss: 0.000138953270\n",
      "Epoch: 9767 | training loss: 0.000138872565\n",
      "Epoch: 9768 | training loss: 0.000138792559\n",
      "Epoch: 9769 | training loss: 0.000138713702\n",
      "Epoch: 9770 | training loss: 0.000138634554\n",
      "Epoch: 9771 | training loss: 0.000138555566\n",
      "Epoch: 9772 | training loss: 0.000138475152\n",
      "Epoch: 9773 | training loss: 0.000138395786\n",
      "Epoch: 9774 | training loss: 0.000138314994\n",
      "Epoch: 9775 | training loss: 0.000138236966\n",
      "Epoch: 9776 | training loss: 0.000138157950\n",
      "Epoch: 9777 | training loss: 0.000138077972\n",
      "Epoch: 9778 | training loss: 0.000137997951\n",
      "Epoch: 9779 | training loss: 0.000137920724\n",
      "Epoch: 9780 | training loss: 0.000137838972\n",
      "Epoch: 9781 | training loss: 0.000137759853\n",
      "Epoch: 9782 | training loss: 0.000137679774\n",
      "Epoch: 9783 | training loss: 0.000137600378\n",
      "Epoch: 9784 | training loss: 0.000137522919\n",
      "Epoch: 9785 | training loss: 0.000137442024\n",
      "Epoch: 9786 | training loss: 0.000137364259\n",
      "Epoch: 9787 | training loss: 0.000137286377\n",
      "Epoch: 9788 | training loss: 0.000137209805\n",
      "Epoch: 9789 | training loss: 0.000137129580\n",
      "Epoch: 9790 | training loss: 0.000137050607\n",
      "Epoch: 9791 | training loss: 0.000136973205\n",
      "Epoch: 9792 | training loss: 0.000136896415\n",
      "Epoch: 9793 | training loss: 0.000136817252\n",
      "Epoch: 9794 | training loss: 0.000136741626\n",
      "Epoch: 9795 | training loss: 0.000136668445\n",
      "Epoch: 9796 | training loss: 0.000136593240\n",
      "Epoch: 9797 | training loss: 0.000136521820\n",
      "Epoch: 9798 | training loss: 0.000136453251\n",
      "Epoch: 9799 | training loss: 0.000136387273\n",
      "Epoch: 9800 | training loss: 0.000136329181\n",
      "Epoch: 9801 | training loss: 0.000136277871\n",
      "Epoch: 9802 | training loss: 0.000136237693\n",
      "Epoch: 9803 | training loss: 0.000136213770\n",
      "Epoch: 9804 | training loss: 0.000136217204\n",
      "Epoch: 9805 | training loss: 0.000136256262\n",
      "Epoch: 9806 | training loss: 0.000136357092\n",
      "Epoch: 9807 | training loss: 0.000136548493\n",
      "Epoch: 9808 | training loss: 0.000136880757\n",
      "Epoch: 9809 | training loss: 0.000137428811\n",
      "Epoch: 9810 | training loss: 0.000138310832\n",
      "Epoch: 9811 | training loss: 0.000139719865\n",
      "Epoch: 9812 | training loss: 0.000141966782\n",
      "Epoch: 9813 | training loss: 0.000145558603\n",
      "Epoch: 9814 | training loss: 0.000151299493\n",
      "Epoch: 9815 | training loss: 0.000160521595\n",
      "Epoch: 9816 | training loss: 0.000175391615\n",
      "Epoch: 9817 | training loss: 0.000199463509\n",
      "Epoch: 9818 | training loss: 0.000238700813\n",
      "Epoch: 9819 | training loss: 0.000302808272\n",
      "Epoch: 9820 | training loss: 0.000408074062\n",
      "Epoch: 9821 | training loss: 0.000580891443\n",
      "Epoch: 9822 | training loss: 0.000865319977\n",
      "Epoch: 9823 | training loss: 0.001329609426\n",
      "Epoch: 9824 | training loss: 0.002082327381\n",
      "Epoch: 9825 | training loss: 0.003266524756\n",
      "Epoch: 9826 | training loss: 0.005062845536\n",
      "Epoch: 9827 | training loss: 0.007549828384\n",
      "Epoch: 9828 | training loss: 0.010563118383\n",
      "Epoch: 9829 | training loss: 0.013193177059\n",
      "Epoch: 9830 | training loss: 0.013967463747\n",
      "Epoch: 9831 | training loss: 0.011424777098\n",
      "Epoch: 9832 | training loss: 0.006305137649\n",
      "Epoch: 9833 | training loss: 0.001582545694\n",
      "Epoch: 9834 | training loss: 0.000184709148\n",
      "Epoch: 9835 | training loss: 0.002184452722\n",
      "Epoch: 9836 | training loss: 0.005008392967\n",
      "Epoch: 9837 | training loss: 0.005824105348\n",
      "Epoch: 9838 | training loss: 0.003883192781\n",
      "Epoch: 9839 | training loss: 0.001161025255\n",
      "Epoch: 9840 | training loss: 0.000153545028\n",
      "Epoch: 9841 | training loss: 0.001302130055\n",
      "Epoch: 9842 | training loss: 0.002850278281\n",
      "Epoch: 9843 | training loss: 0.002953909338\n",
      "Epoch: 9844 | training loss: 0.001589220599\n",
      "Epoch: 9845 | training loss: 0.000303765351\n",
      "Epoch: 9846 | training loss: 0.000320365885\n",
      "Epoch: 9847 | training loss: 0.001249386230\n",
      "Epoch: 9848 | training loss: 0.001796263386\n",
      "Epoch: 9849 | training loss: 0.001330050291\n",
      "Epoch: 9850 | training loss: 0.000457175658\n",
      "Epoch: 9851 | training loss: 0.000140929915\n",
      "Epoch: 9852 | training loss: 0.000552709680\n",
      "Epoch: 9853 | training loss: 0.001026222715\n",
      "Epoch: 9854 | training loss: 0.000958410092\n",
      "Epoch: 9855 | training loss: 0.000470824656\n",
      "Epoch: 9856 | training loss: 0.000143254540\n",
      "Epoch: 9857 | training loss: 0.000270114077\n",
      "Epoch: 9858 | training loss: 0.000581905013\n",
      "Epoch: 9859 | training loss: 0.000656327233\n",
      "Epoch: 9860 | training loss: 0.000421073928\n",
      "Epoch: 9861 | training loss: 0.000170513071\n",
      "Epoch: 9862 | training loss: 0.000162945682\n",
      "Epoch: 9863 | training loss: 0.000337174919\n",
      "Epoch: 9864 | training loss: 0.000442648627\n",
      "Epoch: 9865 | training loss: 0.000355365337\n",
      "Epoch: 9866 | training loss: 0.000192154068\n",
      "Epoch: 9867 | training loss: 0.000133441019\n",
      "Epoch: 9868 | training loss: 0.000210518949\n",
      "Epoch: 9869 | training loss: 0.000299856474\n",
      "Epoch: 9870 | training loss: 0.000289442367\n",
      "Epoch: 9871 | training loss: 0.000199628761\n",
      "Epoch: 9872 | training loss: 0.000134597387\n",
      "Epoch: 9873 | training loss: 0.000152371736\n",
      "Epoch: 9874 | training loss: 0.000210654398\n",
      "Epoch: 9875 | training loss: 0.000232232938\n",
      "Epoch: 9876 | training loss: 0.000194989843\n",
      "Epoch: 9877 | training loss: 0.000144614984\n",
      "Epoch: 9878 | training loss: 0.000132482615\n",
      "Epoch: 9879 | training loss: 0.000159861243\n",
      "Epoch: 9880 | training loss: 0.000186432095\n",
      "Epoch: 9881 | training loss: 0.000180929143\n",
      "Epoch: 9882 | training loss: 0.000152073597\n",
      "Epoch: 9883 | training loss: 0.000131567795\n",
      "Epoch: 9884 | training loss: 0.000136513743\n",
      "Epoch: 9885 | training loss: 0.000154684487\n",
      "Epoch: 9886 | training loss: 0.000162867887\n",
      "Epoch: 9887 | training loss: 0.000152727531\n",
      "Epoch: 9888 | training loss: 0.000136364804\n",
      "Epoch: 9889 | training loss: 0.000129949447\n",
      "Epoch: 9890 | training loss: 0.000136643357\n",
      "Epoch: 9891 | training loss: 0.000146034596\n",
      "Epoch: 9892 | training loss: 0.000147192841\n",
      "Epoch: 9893 | training loss: 0.000139533557\n",
      "Epoch: 9894 | training loss: 0.000131337569\n",
      "Epoch: 9895 | training loss: 0.000129906548\n",
      "Epoch: 9896 | training loss: 0.000134586226\n",
      "Epoch: 9897 | training loss: 0.000139006544\n",
      "Epoch: 9898 | training loss: 0.000138361822\n",
      "Epoch: 9899 | training loss: 0.000133672540\n",
      "Epoch: 9900 | training loss: 0.000129638691\n",
      "Epoch: 9901 | training loss: 0.000129476161\n",
      "Epoch: 9902 | training loss: 0.000132182933\n",
      "Epoch: 9903 | training loss: 0.000134316666\n",
      "Epoch: 9904 | training loss: 0.000133662921\n",
      "Epoch: 9905 | training loss: 0.000131030713\n",
      "Epoch: 9906 | training loss: 0.000128908636\n",
      "Epoch: 9907 | training loss: 0.000128856249\n",
      "Epoch: 9908 | training loss: 0.000130269094\n",
      "Epoch: 9909 | training loss: 0.000131370325\n",
      "Epoch: 9910 | training loss: 0.000131025590\n",
      "Epoch: 9911 | training loss: 0.000129625128\n",
      "Epoch: 9912 | training loss: 0.000128415995\n",
      "Epoch: 9913 | training loss: 0.000128245461\n",
      "Epoch: 9914 | training loss: 0.000128902713\n",
      "Epoch: 9915 | training loss: 0.000129516629\n",
      "Epoch: 9916 | training loss: 0.000129438777\n",
      "Epoch: 9917 | training loss: 0.000128736734\n",
      "Epoch: 9918 | training loss: 0.000128001484\n",
      "Epoch: 9919 | training loss: 0.000127734806\n",
      "Epoch: 9920 | training loss: 0.000127969601\n",
      "Epoch: 9921 | training loss: 0.000128314423\n",
      "Epoch: 9922 | training loss: 0.000128372631\n",
      "Epoch: 9923 | training loss: 0.000128060434\n",
      "Epoch: 9924 | training loss: 0.000127610692\n",
      "Epoch: 9925 | training loss: 0.000127325562\n",
      "Epoch: 9926 | training loss: 0.000127318039\n",
      "Epoch: 9927 | training loss: 0.000127463674\n",
      "Epoch: 9928 | training loss: 0.000127540086\n",
      "Epoch: 9929 | training loss: 0.000127432169\n",
      "Epoch: 9930 | training loss: 0.000127196065\n",
      "Epoch: 9931 | training loss: 0.000126961997\n",
      "Epoch: 9932 | training loss: 0.000126848070\n",
      "Epoch: 9933 | training loss: 0.000126849161\n",
      "Epoch: 9934 | training loss: 0.000126884974\n",
      "Epoch: 9935 | training loss: 0.000126862811\n",
      "Epoch: 9936 | training loss: 0.000126754356\n",
      "Epoch: 9937 | training loss: 0.000126599174\n",
      "Epoch: 9938 | training loss: 0.000126463448\n",
      "Epoch: 9939 | training loss: 0.000126390398\n",
      "Epoch: 9940 | training loss: 0.000126368395\n",
      "Epoch: 9941 | training loss: 0.000126353334\n",
      "Epoch: 9942 | training loss: 0.000126306797\n",
      "Epoch: 9943 | training loss: 0.000126223022\n",
      "Epoch: 9944 | training loss: 0.000126123239\n",
      "Epoch: 9945 | training loss: 0.000126036699\n",
      "Epoch: 9946 | training loss: 0.000125982740\n",
      "Epoch: 9947 | training loss: 0.000125957187\n",
      "Epoch: 9948 | training loss: 0.000125944644\n",
      "Epoch: 9949 | training loss: 0.000125930441\n",
      "Epoch: 9950 | training loss: 0.000125907100\n",
      "Epoch: 9951 | training loss: 0.000125893101\n",
      "Epoch: 9952 | training loss: 0.000125908176\n",
      "Epoch: 9953 | training loss: 0.000125975843\n",
      "Epoch: 9954 | training loss: 0.000126108411\n",
      "Epoch: 9955 | training loss: 0.000126336905\n",
      "Epoch: 9956 | training loss: 0.000126688465\n",
      "Epoch: 9957 | training loss: 0.000127201303\n",
      "Epoch: 9958 | training loss: 0.000127989857\n",
      "Epoch: 9959 | training loss: 0.000129193504\n",
      "Epoch: 9960 | training loss: 0.000131016073\n",
      "Epoch: 9961 | training loss: 0.000133767884\n",
      "Epoch: 9962 | training loss: 0.000137935145\n",
      "Epoch: 9963 | training loss: 0.000144289719\n",
      "Epoch: 9964 | training loss: 0.000154033070\n",
      "Epoch: 9965 | training loss: 0.000169060353\n",
      "Epoch: 9966 | training loss: 0.000192485895\n",
      "Epoch: 9967 | training loss: 0.000229156314\n",
      "Epoch: 9968 | training loss: 0.000287056493\n",
      "Epoch: 9969 | training loss: 0.000378711149\n",
      "Epoch: 9970 | training loss: 0.000525020121\n",
      "Epoch: 9971 | training loss: 0.000758544367\n",
      "Epoch: 9972 | training loss: 0.001134438091\n",
      "Epoch: 9973 | training loss: 0.001735512982\n",
      "Epoch: 9974 | training loss: 0.002702720696\n",
      "Epoch: 9975 | training loss: 0.004228489473\n",
      "Epoch: 9976 | training loss: 0.006627405528\n",
      "Epoch: 9977 | training loss: 0.010205443949\n",
      "Epoch: 9978 | training loss: 0.015354941599\n",
      "Epoch: 9979 | training loss: 0.021777439862\n",
      "Epoch: 9980 | training loss: 0.028607202694\n",
      "Epoch: 9981 | training loss: 0.032525304705\n",
      "Epoch: 9982 | training loss: 0.030554179102\n",
      "Epoch: 9983 | training loss: 0.020769063383\n",
      "Epoch: 9984 | training loss: 0.008278116584\n",
      "Epoch: 9985 | training loss: 0.000672823866\n",
      "Epoch: 9986 | training loss: 0.002001147484\n",
      "Epoch: 9987 | training loss: 0.008541512303\n",
      "Epoch: 9988 | training loss: 0.012604890391\n",
      "Epoch: 9989 | training loss: 0.010005983524\n",
      "Epoch: 9990 | training loss: 0.003596158233\n",
      "Epoch: 9991 | training loss: 0.000192416366\n",
      "Epoch: 9992 | training loss: 0.002305632457\n",
      "Epoch: 9993 | training loss: 0.005949730985\n",
      "Epoch: 9994 | training loss: 0.006155123003\n",
      "Epoch: 9995 | training loss: 0.002820246387\n",
      "Epoch: 9996 | training loss: 0.000280965498\n",
      "Epoch: 9997 | training loss: 0.001144415466\n",
      "Epoch: 9998 | training loss: 0.003336911090\n",
      "Epoch: 9999 | training loss: 0.003524298780\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    \n",
    "    # Model\n",
    "    input_size        = 2\n",
    "    hidden_size       = 5\n",
    "    output_size       = 1\n",
    "    nof_hidden_layers = 5\n",
    "\n",
    "    pinn = vanilla_PINN(input_size, hidden_size, output_size, nof_hidden_layers)\n",
    "    pinn = pinn.to(device)\n",
    "    \n",
    "    # Problem params (nylon string)\n",
    "    T     = 50        #Newton\n",
    "    mu    = 8e-4      #kg/meter\n",
    "    ro    = 950       #kg/m³ (volumic mass)\n",
    "    d     = 1e-3      #meter (diameter of the string) \n",
    "    L     = 1e-1      #meter (length of the string)\n",
    "    u_max = 1e-3      #meter\n",
    "    t_max = 10        #seconds\n",
    "    lbda  = 2 * L / 5 #meter (wave length at t=0)\n",
    "\n",
    "    nof_pts = 10000\n",
    "\n",
    "    # Setting first BC (u(x=0, t) = 0 for all t)\n",
    "    x_bc1_pts = torch.zeros((nof_pts, 1))\n",
    "    t_bc1_pts = torch.linspace(0, t_max, nof_pts)\n",
    "    t_bc1_pts = t_bc1_pts.reshape(t_bc1_pts.shape[0], 1)\n",
    "    u_bc1_pts = torch.zeros((nof_pts, 1))\n",
    "\n",
    "    #Setting second BC (u(x=L, t) = 0 for all t)\n",
    "    x_bc2_pts = L + torch.zeros((nof_pts, 1))\n",
    "    t_bc2_pts = torch.linspace(0, t_max, nof_pts)\n",
    "    t_bc2_pts = t_bc2_pts.reshape(t_bc2_pts.shape[0], 1)\n",
    "    u_bc2_pts = torch.zeros((nof_pts, 1))\n",
    "\n",
    "    #Setting third BC (u(x, t=0) = u_max * cos(2 * pi * x / lambda) for all x and where lambda is the wave length)\n",
    "    x_bc3_pts = torch.linspace(0, L, nof_pts)\n",
    "    x_bc3_pts = x_bc3_pts.reshape(x_bc3_pts.shape[0], 1)\n",
    "    t_bc3_pts = torch.zeros((nof_pts, 1))\n",
    "    u_bc3_pts = u_max * torch.cos(2 * pi * x_bc3_pts / lbda)\n",
    "    u_bc3_pts = u_bc3_pts.reshape(u_bc3_pts.shape[0], 1)\n",
    "\n",
    "    # Training\n",
    "    model                = pinn\n",
    "    pb_params_list       = [T, mu]\n",
    "    nof_collocations_pts = 10000\n",
    "    learning_rate        = 1e-3 #1e-3\n",
    "    optimizer            = torch.optim.Adam(pinn.parameters(), lr=learning_rate)\n",
    "    nof_iterations       = int(1e4) #1e4\n",
    "    weight_loss1 = 10000\n",
    "    weight_loss2 = 10000 #1e5\n",
    "    weight_loss3 = 1\n",
    "\n",
    "    training_loss = train(pinn, nof_collocations_pts,\n",
    "                          x_bc1_pts, t_bc1_pts, u_bc1_pts,\n",
    "                          x_bc2_pts, t_bc2_pts, u_bc2_pts,\n",
    "                          x_bc3_pts, t_bc3_pts, u_bc3_pts,\n",
    "                          pb_params_list,\n",
    "                          optimizer, weight_loss1, weight_loss2, weight_loss3,\n",
    "                          nof_iterations, \n",
    "                          device)\n",
    "\n",
    "    #Plot training loss\n",
    "    plt.plot(training_loss)\n",
    "  \n",
    "    #Save the model\n",
    "    torch.save(pinn.state_dict(), 'pinn_model_stringvib.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAasAAAGOCAYAAAApcCzoAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAA9hAAAPYQGoP6dpAAEAAElEQVR4nOz9eZQk53neif6+2HJfKmvtvQuNBrob3QAIgAS7JVDy0YjUMl7G9hHnWsNzLFm65tE9I0uUjm1qsXw1czSWxpYpiZJlWbJlaSSS9tA05TE1JqU7ogCiCRBA72j0vndVdVXlvsX2ffePyIjMqq7qrqqu6i4A8ZyDg+rMyIgvIzPjifd9n/d5hVJKESNGjBgxYmxiaI96ATFixIgRI8b9EJNVjBgxYsTY9IjJKkaMGDFibHrEZBUjRowYMTY9YrKKESNGjBibHjFZxYgRI0aMTY+YrGLEiBEjxqZHTFYxYsSIEWPTIyarGDFixIix6RGTVYwYMWLE2PSIySpGjBgxYmx6xGQVI0aMGDE2PWKyihEjRowYmx4xWcWIESNGjE2PmKxixIgRI8amR0xWMWLEiBFj0yMmqxgxYsSIsekRk1WMGDFixNj0iMkqRowYMWJsesRkFSNGjBgxNj1isooRI0aMGJseMVnFiBEjRoxNj5isYsSIESPGpkdMVjFixIgRY9MjJqsYMWLEiLHpEZNVjBgxYsTY9IjJKkaMGDFibHrEZBUjRowYMTY9YrKKESNGjBibHjFZxYgRI0aMTY+YrGLEiBEjxqZHTFYxYsSIEWPTIyarGDFixIix6RGTVYwYMWLE2PSIySpGjBgxYmx6xGQVI0aMGDE2PYxHvYAY7y8opfB9H9u20XU9+k/T4vumGDFiLI+YrGI8NCilcF0Xz/OwbTt6XNM0DMPAMIyYvGLEiLEkhFJKPepFxHjvw/d9XNdFSokQAsdx0DQNpRRKKaSUKKUQQiCEiMkrRowYCxCTVYwNhVIKz/PwPA8AIUQUYQkhltw+JK4Q4XaWZWGaJoZhLPnaGDFivHcRpwFjbBiklFE0BURRU0hEYSQ1CCEEuq5H/w7J65vf/CZPPvkkxWIRTdPQdX1B9BWTV4wY723EZBVj3RESjOu6C1J7i7dZCcGE5BX+X9f1aN+O4yCEiMjLNM1om5i8YsR4byEmqxjrijDFd/r0acbGxhgZGVkX4gj3sVzktZi8Fte8YvKKEePdjZisYqwbQtLwfZ9Go8HQ0NC6kcRg+nDx4yF5hc9LKXEcB9u2Y/KKEeM9gpisYjwwwt4pz/OQUqJp2rLkspEICSgmrxgx3nuIySrGAyFM+/m+DxARVShLXy+shfyWIq/wP9u2cRwnWnNMXjFibG7EZBVjzQgjlsFoahDLkcujIoJBoYeu63eR12DkFUrkwx6vmLxixHi0iMkqxqoRpv1Ctd9SF/P7RUKrvfhvRFrxXuTV7XajbWLyihHj0SMmqxirgpQSz/PuSvstxqOoWT0oVkJe3W4Xz/MYGRmJyStGjIeImKxirAgr6Z0axHqT1aMSbCwmr0qlQqVSIZvNAgFZL655xeQVI8b6IyarGPfFUpZJ97sYvxsjq/shfN9CCEzTXOBraNs23W43Jq8YMTYIMVnFuCcGe6fC+s1K8F6IrO6HxaQdkpfv+9EYlMGaV2gRtRKyjxEjxkLEZBVjSSzXO7VSbAS5bDayWoyQhEJCHyQvz/Oi55fyNYzJK0aMeyMmqxh3YbneqdVgIyKrdxuWIy/P8yLX+ZC8Bn0N43EoMWLcjZisYizA/XqnVor3cmT1IOckJq8YMdaGmKxiACvrnVoN4sjq/rgfeUE8RTlGjBAxWcVYl7TfYryXI6uNwnLkNegoH09RjvF+RUxW73P4vs+NGzdIpVIUCoUNd0l/kP2937AUeYXqTNd1abVadDodtmzZEpFXPEU5xnsVMVm9TzHYO3Xz5k3Gx8cpFovrtv+V2C2tlsze65HV/bB4ller1WJ6epqRkZElI694inKM9xJisnofYvG4+Y1IIwkhov2v1/42A5RSeFduUm24pJ+YxBoZeqTrCckpXFs8RTnGexUxWb2PMHgxG1T7aZq2rsQC608um6EpuHX2AnP/8vdo/bdXOdnqIEyT7f+fH2Tnp34YPZ166OsJba9CxFOUY7yXEZPV+wT3ElFslKP5ehPgo0Tz7AVu/Oq/ofV/vwKOS3LnOKm9k0z/0Repf/MNDvzBv8QsFR/6uu7nzxgPoozxXkEsI3ofILxAhS4Ki9V+G0Es76XIqnP9Fld+7leo/uWb4ASScum42Fevktn3GK0z5znzgz+OtJ2Huq7FkdW9sJx7hlIKx3FotVo0Gg3q9TqtVgvbtvE875FHszFihIjJ6j2MUEThOA6+7y8rSV/vqb7w3oms7KkZrv5//3dAJ/34DpIvPIU2MYw1VsSZmaV57DTZQ0/SOnWOK7/4aw91bQ/ymd2LvGzbpt1ux+QVY1MhTgO+R7Ga3qmNSgOu9/4e9oXS73a59I9/AWeqQevMheBBXcM8uAfDSkTbNU++Tf7Ic8z80ZcY/t6XKH77hx/K+lYTWd0Pg6NQVjJFOSS52FE+xsNCTFbvQYROFCu1TNoogcVy+2y1Wpw7d45EIkGpVKJYLEaKts0CpRTn//6PI30LLZ0isW0c+9YMyef2455+h7auk/vg0zS+dRKA5rFTlL7nRaZ+/dfIv/g8mmk+lHVuFFHEU5RjbDZsritEjAfCYO/UaiyTHmZkNTU1xenTp5mYmEBKyYULF+h2u+RyOYaGhhgaGqJQKCxQtW3UGu+FKz/zT3BmqrTeuR48oGkM/9XvoPbaseDfvk/79Duk9u6mc+EqxSPPItpl7Kkp5v7Df2TsB//Ohq9xPSOr+2El5FWpVEin0xQKhZi8Yqw7YrJ6j2Cp3qnVFN83IrIaJBff93nnnXeYnp7m6aefZmhoKIr8ut1uNIH37bffxvM8CoVCRF65XA54eE3Bt3/nd6n+5et0b9Wix3LPH6Dx6lGMsRG8ZAI1W0a5LqrTofCR53GvnAYgfeggM//+Dyl9//dhrGOT9VJ4lPWjpcjr9u3bDA8Pk0gkom0W18Ri8oqxVsRk9S7HasfNLwVN06La1nphkKxarRbHjx9H0zQOHz5MKpWKjFoBkskkW7ZsYcuWLSilaLfbEXldv34dpRS6ruP7PsPDw2QymQ274JW/+lXufOFLSNvDKCZJTe5CLxRonz4NSiFnZtFHh1GZNH6rjTU+TLJo0uq93rl1EyUls//h/2TL//tHNmSNIR5mZHU/DBJXPEU5xkYgJqt3MRaLKNY6xG+j0oBKKaampjhz5gzbt2/niSeeuK/yUAhBJpMhk8mwfft2lFI0Gg3eeecd2u02b7zxBoZhRFHX0NAQqdT6NOS2zr7Djf/913Bm6yg3OKdevYZfmyP7zH4ab5wAqVCz82QOPYVbaUJzivaZ61g7d+Bcv4FfqZJ+5mnm/9OXGPvB/xd6JrMua1sOm+lCL6Vc0LsXT1GOsZ6IyepdisFx8w96d7oRAouQZM6cOcOhQ4cYHx9f036EEOTzebLZLOl0mp07d1Kr1ahUKkxNTUVCjZC4SqUSlmWt+jjO3DxX/vHPIm2fxPgQxtgWOtdvoadMnNtzNI+dIPf80zS+dQIAd3aW7MHdtI9PA6An+upA58ZNpG0z95/+M+Of+ME1ve+VYLPJyO8V6cVTlGM8KGKyepfhQcfNL4X1jqxarRZXrlzB8zyOHDlCOp1el/2GopGQmAA8z6NWq1Eul7l+/Tpvv/02mUxmQeR1P6WhdF2u/OSncGYq+C0bALdcJ/eBA7jVerRd8/hJzINP4l69SWI4iXvtKlomg2y16F6+TGrfPjrvvINXLpM+dIju2TMoz0NskNJxM6UBgej7uBKsZhDl4rRhjPcnYrJ6F2Ej5k7B+goswrRfoVBASrluRLXc+zQMg+HhYYaHhwFwXTeqd128eHFFSsMrn/oU3VuzCENH6AIlIfvMflonTyESCTJP7aN15h0A/Bs3sQ7sxL92BYDUoadpHQ+iLeX163B60kJePU3r9VfJHvnIupyDxdhsZPUg64mnKMe4H2KyepdASsnMzAwzMzPs27dvXS9S6xFZ+b7PuXPnuH37NocOHYrmZK0nVrJG0zQZGxtjbGwM4L5Kw9pv/Satsxdxqw0AtJRJ/ts+TP0brwbHtG3smzewtk7gTM2Q2LsT4fdtlTpvn8EcH8edmcG+coXk3scRmkCbu4I5uYfG176yYWQFm6tmFUa+64F4inKMxYjJapNjcNy8bdtUKpV1v0A9qN1Su93m+PHjCCGitN/U1NSmGL54L6Xhnc/9Meaffx2/3Iq2T03uoPGNb5B+Yi/dm7eRjSay1cYslcgffg7v/JlguwNP0X77DPg+1vgY7swMAEYhj1G7Db6P0HXsd97GuXENa8euBz8Ji7DZIqtBgcV6YznyCh3lISav9zpistrEkFLieV6U9tN1fUP89h4kDTg9Pc3p06fZtm0bTz75ZHRxuFe0tpai+XpEf4NKw9z1a9z65mu4UqIXkwjLRCroXLuOADrnL5DY8xjddht8SWJiFDNl4PX25cxMISwL5Th0zr2DMTKC0DTMzixaIY9/p4N75RJ6aZjGn/0pwz/0yQda+1LYbGS1npHV/bAUeQ1OUQai5vh8Ph9PUX4PIL7t2IQIo6nQgHZwiN5GkNVaIispJW+//TanT5/m0KFD7N+/f8GFajPMn1oO3SuXmfrVf4FTriM7Nn7HAdtGlz6p/U9G29mXLiMnd6HtfQz38lk677yNGBkFwK9USO3bF2zoeSR37SA7loVWA6M0EjyuJOaWLbRefXlBPWu9sNnIaiMjq/thKTHG3Nwc77zzDp1Oh2azSa1Wo9ls0u12Y1PedyFistpkGHRKX9zkuxESc1h9ZNVut/nmN79JtVrlyJEjS8rS15us1mt/brPB1Z/5x3htG7OQwcgk0FMJhKEjbQf3/Dmyzz0bbZ8SkMhZCADPw7P6yQjn5g2EaSKSSSzDA7cTHOP6FUQiCYA3dQvZbtJ+61sPvPalsJnIajOR52CTckheEAhwBh3lm81m7Cj/LkFMVpsI4dypwQLy4rlTG/GDWk1kNT09zauvvsrQ0BAf/vCHl1X7bcbISknJ9X/4U3jzNfx6E6/aQLkuiXwCJftr7bx9htQTe7G2bcXyWyScLqJnTGvOTONv2QKAX63i79hBYu8u1PQNzB27g+N0O1iTewCQtSqJ3Xto/uX/b/3fz2Y7vw8xDbgSDErpByOvUE0Y1rziWV7vDsQ1q02AlfZObWRkdb8fppSSd955h9u3b3Pw4EEmJiYeaJ+PomZ18xd+Dvv6bfx2N9wpVj6NU2ujp5OI0nAglPB98GxSY3m8m9fx2i1STx2iffw4AIauEa4kO1rEuH0ZgPbtm+hCIJTCq1b6azcNOsffwq/X0POFB3oPg9hMkQw82jTgUggb5pdCSF4QT1F+tyAmq0eM1fROPao0YKj2A1bc5LvZIquZ3/4tmidP4bc7CF1gZpLomRT2XGBW67e7mIUhRDqNUAorm8TMZSNBhX3hHHqxiF+tIqZuY+15jMTwEOL2RYzJx3EvX8BsNVDbdsDN68g7UziFIaxaBefqZbThEZzTb5E68lfW7T1tNrLazJHVvTCYMoSYvDYrYrJ6hAh/DI9y7lS43+WIZTm13/1wP7JaLZE9iGKx+n9/heqffQ230oge0y0dt1InMZzDng8ed6emSO3fj2aAf/0y/swtEnsex750EeU4JB9/gna1CkByywRcDWTsyL4JsJVIEnZhpcfH8WoVpGHAeInKK1/lUn40soVaD0/DzXKxDKXkm2U9sDpHjUHci7xs276nVH4zvf/3GmKyegQY7J1azdypkFTW+6KwFBFIKTl37hy3bt1aUdpvqX2ud2S1lv21TrzFzL/9N8iuTXIkj/J8MAzcehMAt9oks32U1s1ZAMx0At3S6PReL9tN0ARIhX3hPORyMDIMN86ij43j35nBu34FfWwC/850IK7I5VGNOmr6NiKVprh3N3oqgX/7Bo70mJ6e5vz58ws8DYeGhqLRGht5PjYK4Vo2W2S12K1kLRgkr6WmKA+SVzxFeeMQk9VDxoNYJoUXgvX6EQ7ud/DCt5a032JshBpwtXCmppj5zL/Ar7dQno/s2phpEyFVQB6dwAewMzVH+uBT6EkL7/JZZDaHls0hmw28mRlSTx6gc/YMynXQn3iC1PwNhFIYQ8P4d3rNwEPD+Hemgybh7Tuxz55GdjrkPvgC4uYFZF1DZDJsqd9h97d/FN/3qVar0RiUtXgabqZIJvysN8t6IPidmBswsXklgygHHeXjcSjrg5isHiJWO25+McLtN2qcB/TTflu3bmXfvn1rvlN+1JGVtLvc/Cf/CHu+irBMlOejpxMoFNJ2EYaONZTD6aUG9YSJqM8Hr202SOw7QOdMkObzynOgCcyJrVidMug6SB/32mVEKo3qtHFvXQPdAN/Dr5RBCLJPH8TQFT6AlOijW3FPvk7i2z+KruvLehpeunSJdrtNPp+/p6fhZiKrMDLfLOuBtacBV4uYvB4OYrJ6CBgcNw9rN6AdjKzWE0IIfN/n7Nmza077LbXPRxVZKaW4+U/+Me5sGdUNoicjbWKmTZxaO9jG88F10NMJrG078G9cILFrEm/uDgD2uXcwJibwpqfxZu+QeeYDaPVpaDVwtu7EvHkN5diYe/fjnHsb1W5hPbYX5+J5/DvTZD/4Ivr0RWTdAsMEz0XZHeTUDWRlDm1oZMGa7+VpePbsWVzXJZ/PUyqVFkxP3iwXu82aBnwU67kXeV2/fp1Go8Hjjz8eT1FeJWKy2mAsHjf/IPN5NoqsHMfB8zwqlcq6jfR4lJHVzL/8Z3QvX+1L1DUNI5XAbXawhrI41RYohW+7pLaMIhvzICXOlUskn9xP99zZwH1iaAhvehotm8VMCPxWLwpr1ECIYHJwtdw/cC+1m3r6GayUHkRUroO+/TH865eQM7cgW8A98xaJb//oPd/DYk/DTqdDuVxeMD05TN8ODQ1t6PTklWCzpgHXM12+Vgz+5sMWlVAsFU9RXjlistogDHqVrefcqfUc5wEwMzPDyZMnAfjwhz+8rq7Z691ntRLMf+4PaL35BkL5JIpplJJgGHiNIKJy6y1SY0U6MxWMVALlOiT2PE737VMAeLMzCNNEuS72hfNYO3aSKKaR1y9i7JwMxBSNGsbuPbhXLuLPzmBs3YF3+wbu9SuknnkOq3Id5eYiQiPsylIKfWQc98yb9yWrxe89nU6TTqej6cnNZpMzZ85E05N1XV8wgDKZTD7UC13YY7WZLq6PKrK6F3zfv8tg915TlGPy6mNzfZLvEQy6Qa9G7bcSrJd8XUrJ2bNnOXXqFE888cQCU9D1wsOOrJqvfp36V7+CV2sguzZeu4tuWWA76Kn+9ODuXJXU1hHMXArl2DiXL6KXgtqRX62QeKLn+Scg/dhOmL0d/HPgLn3w49SyWQASe54gWQxsllSrgTaxAwA5dQOs3uNOF//aRWSjtubzIIQgl8uRSqXYsWMHH/nIRzh06BCZTIbp6Wm++c1vcvToUc6ePcv09DS2ba/5WCvFZqqfhdjMZDWIpYgpTM3btk2r1eL7v//7+fznP/+IVr05EEdW6wwpJdPTwajzUqm0IeM8HpSs2u02J06cQCnF4cOH0TSNs2fPrusF50HHjizG/dbVvXiO2X/zW3iNNkY+g9doYw3lozlVQoFVzOFUGwhNQ1ceytSRXYLa085d+OVAYOHevB4MXTywH3X5bbTCELJWwbt+BZkfQqtXAol6voiqV/FuXsXcs5eEV4Vm/ycV+gPie+g7duJf66UCk2nct4+RePE7H+ichJ+XpmkUi0WKxSKTk5MLlIY3bty4S2lYLBbXXSW32dwr4N1DVouxlKP89PT0fdWh73W8v9/9OmLQMun27dskEolI6bWeeFCympmZ4dSpU2zdupUnn3wSXdcjxdJ63x2vZxrwXvtzK/PM/tqv4FWbKN/Hd1ysXArc/pBE5Uv8ro2eTpIq5fCbTYxcFk9ooCTOxfOY23fi3ryObNTJfdu3oS4E6VFjYgtOrRKk9IpDUK+AlJhbtuHUq+ilETLjJdTNCmp+BjE0iqrMIu/cBi1QDhKuXUr0sa1450+tC1kthZUoDXO5XCTWWEppuJa1bDZi2KxkZVnW/TccgBCCdru9blO3362IyWodsLh3Stf16O/1xlqFC4NNvk899RRbemaswII7uPVcZ7jPpYgpJPaV/nCXIzfpusz84qfp3p5B9c65kU3j2zbKl1jFLE69BVIhHY/sthJeMxi26DWaJA8coHvmNABaryk3fehptPlb+LoOvo9/61o0u0qbu4PSNISUyPlZtOFRUhkdofzIL1ArDOFXZqHbRt+yA//WtSCi6qkCEYoEXZTTRfTSg2vBSm8u7qc0dBwnmp5cKpXI5XKrvshvxjTgvbwBHxXWIvpQStFqtSIF6PsVMVk9IJbqndJ1PZKprzfWElmFaT8pJYcPHyaTySx4PrzIrKd66l5kVS6XOX78OK7rrvgiuRxJ3/nlf4IzX0EzDYRloBkCabsoPzhHbr1FqpSlM9cgvWUYv9XCKhVw5qsA+LdvIJJJVLeLc+USmRc+iDZ1EaUU5q7HcS+fQ3XamI/tw7lwFtFt441vw5i6gXIdsk/uQUxfR83cRKSyqE4TVZ3rL9DskbHroG+bxL9zm2Q2idZpwI0LsOfQms/xWgniXkrDGzduoJSiWCxGn8tKlIZxGnBlWCuBtlqtu3637zfEZLVGDPZOLRZRbJSH31r2Hab9tmzZwr59+5Yko42OrEIopbh69SoXL15k79695PN5arVadJEEKBaLlEqlyDvvXhfA8u/+Bp3zF/F71knC0NHNBEIHI5PEawXpzW65SXbHKF69jhACt1pHz6TxW21ks0HyyafonD6FtWs3CVPhhWu22/21t+r99yZ9RCpNZucERsKKmn61sS341y6g6hXE6ARqdho5Ow2aBlKCrpPeswe9XUGNbENdfQfxAGQ1eJ4f5PVLKQ3DyOvKlStomrbAWWOpzyVOA64MK6lZLYUwdft+RkxWa8DicfOL1X4bNdE3PNZK9n2vtN9iDEZW64XFZOW6LqdPn6ZWq/GhD32ITCaD67pks1m2bduGUopGo0G5XGZ2dpaLFy9immZUV/F9fwHx1f/rf6Jx9BugJNZQFq/Zxkwn8KLeKg+rmMGptrCKOWSnjZHN4LfaoBRGj6wA3GuXsXY/RlK08W9eRhsZQ87dwZ+5jbFtJ96t6/gzt9EntuJP30avzJM5+BTa3A2k24nqUsruROvTMnn82WnotNDGtyPv3CJRyGI0Aw9CoeuoG+dR0kdoa4tmN8IbMFQa5nI5du7ciZSSer1OpVK5p6dhHFmtDGshq3DGXbanOn2/IiarVWCwd2rxFN9BPOrIqtPpcPz48WXTfouxETZOg/us1+scP36cdDrNkSNHsCzrrjSpEIJ8Pk8+n2f37t34vk+tVqNcLnP9+nWazSa6rnPx4kXyUzdwvvKf8euB0s8HEsU8yh2QaEsZOKWPFhHKB0+hofB7vU/O3Dzmzt2416+iJZNkdk3gXwjslfTCELLnZKGl+kVtPV/AuzONPrkLM5NEzgHdDtqWXchbV1F3biNyRVSjiqrN999bMkVy31MBUZUmYO42VGfB6cLUNdj22JrO8cOoE61UaZhMJqPfxkb48a0W4W91s5HVWlLtzWaQOYjJKsaKsFhEca8GSE3TNlRgcS+yunPnDqdOnWJiYmLZtN9S+9woe6Tbt29z/vx5Jicn2bNnz4ovrrquR+lAgKtXrzI7O4t3+zrtP/wdZK0/7sMs5iIXdatUxKnUAvWdpmElBG4r+Cy8jk1iYgx7aqa3SBDpNKmxIdTUtaC+5Dp4Ny4jMllUq4l382rk/+ffuoa+9wlynTloDEjUBy6IWmkUv1FF1cqI4XHU/AxGLoPZuNNbbK+GZXdgaAx17R3EGskKHr5jxHJKw9u3b+M4Di+//PK6Kw3Xgs1o/wRrq1k1m80oXft+RkxWK0B4xxh+0e53gXgUaUApJefPn+fGjRscPHjwnmm/pbDezhjhvi5evMgHPvABRkZG7vOKe8MwDCzfp/B//UccTyJ0A8000FIWfr0VbedW6yRLOexai0Qxh9dukyhmsXuGtV61hjANlOvhTd0m+4FnUVffQQHG5BN4l86B52HsfBz34lnwXIzJJ3EvvE1y7xNITUIHVHUuIiM5cxPMBLg2qjuQCswX0MfGSDTvoIqjUL0D9b49k0imUdfOwZHvW9M52QwKvFBpGNZwDx06FIk11ktpuBaE37/NSFarJe9Qtr7Z3svDRkxW98BKx80vxkZGVks12w6m/Y4cObIm1dB6NvG2222OHTsGwPPPP0+xWHzwnUqf4hf/HfbUHZTbMwS2TPxGG2EaaIaB3wqIwq62yGwZwq0F0ZZTb6FnM/jNFtJ2SGydoHtrisyBfeheN5oGrKrlvudfeTb6WzWqJA4cwqjexhkaj5YksnnU/Ax4Ltq2SeSNy6jZKUS2gGrW0NNpEq1eRJXs3RV3mlAcDdKArRrUy6jqLKI4uupTstnmWWmaRiKRuEtpWKlUKJfL3Lx5EynlqpWGa8F7iayazeYj937cDIjJahk8yNyphxlZrSXttxTWK7K6c+cOJ0+eZOvWrTQajVUPFFwW/8e/RvVUfJoGsmsjO52ATFwvaNLNpXAbHVJbhvGaTbRUAtmxA0++pIXf669yZudIP/sBxO1L+IA2thV55zayMoe+fTf+jSvI6jz6tl34N69iDJWwEqAaYFZn8a0kutMNiCr0/1PhuVNow+OI8S0BUWWL0KxCu68mJJUNyKpRgVwRrp0LCGyV2AyRVYilBBaDSsNQRLMWpeFa1wObi6zCOtpqf6OxbD1ATFZLYLXj5hfjYQgsBtN+Tz31FFu3bn3g/T7InbpSigsXLnDt2rUoDRm6gz8oql/4feTlS9Dq4APKMtETJnrC6I/88CXS8UhPDOE3eu7omkAKQIFbrmIMD+HNV9DGR3E8m7AdV0tnCD+twfqTME3Myccx27OIocdQgFASNz+MPncL2g3E6FbUnVuoO7ejVKBIJkh0er1WmQI0q4hGBRUSV6fZf3PZIur6OcQz376mc7NZyGol0vV7KQ1nZmY4f/48lmVF9a61TE8OsV7m0euJ8JoQR1ZrQ0xWA1jruPnF2Giysm2b11577YHSfovxIAIL27Y5ceIEtm1z+PDhSLV0r32u9Ly2/p8/pfX1P0P25lIJTWBYGn7PIio5VqJ7J6gDWaUCuuYTJmC9jo01PoozHcjFNRTJHVswnBaqIZG6geZ7eDcu46UyGJ0W/q2rkeef8D2shELYCjU/DT1rJs3p9t9HMhU4V/RSgUJKEm4NkUgHfVoDvVpRlFWfh3QW2k2E56Dmp1F2t+8luEJs9sjqfliJ0jCdTkfktRpPw82oBBzM0qwG7XY7jqyIySrC/XqnVoONrFmFVjlbt25l//796+o4sRaCrVQqHD9+nKGhIZ577rkFZpsPqjDsnj1J47/8n6huB10DLCswp+25TwC4lSqp0Ty+p1DdNh4EFkvVIHrx6w2EoaM8H83QsdIm0gFhd7B2P4F3+RxCKSiNwq0WSEkrmSGh6aSFgza0DdVqQKeFmNiJmrqG1axAPvAIVOV+KlBLJLHcGkJKGN4KM9eCdF8yDd12IFUPURiBdhMqM5AbQszdhG2Pr+r8bLaa1YMS50o9DcN6172Uhu8lsgojq/c73vdktdLeqdVgI2pWUkouXLhAuVxmeHiYgwcPruv+V5sGVEpx7do1Lly4wBNPPMHOnTuXrFms9YLqTt+m9nu/gTs/H42DMlIWbm9MvW6YeL3+E+krErkEbi2QSnitLlomjWy1kV2bxMQ4vm1j6BJNiCjlJ8uzoAmQCqtZQ/acJtIaJHJJNN+mXZ4l1dvelzKaqaMNjSDrlSA6GtsGvocl7CCN6EuQA31khRHoXg/UgIlUIFv3e88nUujbdqLN38BfA1ltpshqvclhsaehbdsrVhpuRl/AUFyx2s+s1Wq973us4H1OVovHza/X8Lj1TgN2Oh1OnDiB7/ts3bp1Q0YFrIZYPM/j9OnTVKtVPvjBDy6r9rvXubzXc363Q/mzv4S0bYxCIYhUlcTv9VYpx0UqMNIJlATD1PDbXbSEibRdUAozl8HuOVT4nTaJQho6HfxaDW18G3LmFqpeQd8+iX/9MqpRQ98xiZybIZ1PomXyqJkbpLtNvOwQerOCmL2NJ3QM5eNUy9GPR8vmsFQHze2ghrfC7C2o3AHDAs+B8PulVCBhn7kePJ/OYUxsAdNAzN4IhBobNJRyo/EwiHM5pWGlUrlLabjZ6lWwdu/NdrsdkxXvY7Ia7J1a78GDIVmtxw84VPuNj4+zf/9+Ll26hOu667TSPlZKsI1Gg2PHjpFKpSI3iuWwlmnBUkpqv/G/4s0MSNRzGVSni5bLIBuBok+5LsJKYWaNQPEHGAkTxw7OjTtXxiwN4Xc6JDIWRjaD1wmk7SKZio4nVP89CwSpbVvRmmVEoRS5qJulEWSzgqYk9tA4Rnkao1Ghm8igKYmFh/B640hE73skfRjZBnduBKk+3QiiqfB8aDrarsfRmnOotgKng6jNoopjy56vxdhMkdXD9ga8n9KwUqkgpeT06dPrrjRcK2IT2wfD+46sQhHF1atX0TSNrVu3rvsXOLx7ehAX8zDtd/369QVqv40Sb6wksrp9+zZnzpxh9+7dPP744/c9b2tJA9b/zb+ge+Vq4LeHh55J4rV7EvVWm8TYCPadOdA0zFwKDRml9byOjTU2jHMnsDrSTB0rNwSdNn6tFhnKyptXEPkhVL2Cf/s6ojiMatYwswk0JyA0NXsrcrRQ1VkgkBUOfprprTvQcTG9Nu1ElrTdwC9PoQmBUApJbxS39GF0WxBxVe+AmUDfMYmwArGAcLrITBExe33VZLVZ8Ki9ARcrDaenp7l69SqZTGbdlYZrxVpNbJvN5vvexBbeZ2Q12DvVaDQ2LFUQ3j2tlazCtJ/neQvUdeG+N4qsltuvlDIakf7ss88yOrqynqDVklXzTz5H98wplB30RhnZFFoqhbSdqG7lzM2TGCmiCYVstZCAmU/j1oOUn2x3QNdAgZky0UwTvwPK9TBGRvBmZ4O+q9FxvHowUFEfGcPYugW9Oo3Y9ljgsO65iK27UTevQLOOGN2Cmr2NWZvFNyx0TcPMZ9DLUwCkiiWYaWBIn06mSLJZQc5PgxBoSuFJiQEI34fHn0Jvl1GdgXOTSAWpwL0vrPh8bbbIarOsBYL1WJbF5OTkuisN14q1klWr1Vq1I817Ee8bslrcO7WRAxIHi7yr/QHMzs5y8uTJKO23+Mu9UWS1nMAidMdQSnH48OFV+ZOthqw6r/0Fra9/LWrcFboGvo9XqaInk0ilUJ1ATaclE+i+E0VUfsdGJBIoO2gUToyNBIMUm7UoegHwm82+Q8WdqSjS0tNJ9PItAFSj2l+/60apQJFKB31WUuIOT5Aq5dGr06hMHtGqw8DrktkcNCtBbSs3jFWfCyYHA/bYNkylMAFht1HpAqJdQzjdQNZudwIRxirO8WbAZhsRsljwsZTSsFqtUi6X71IahuS13p6GD1Kzer/7AsL7gKyW653SdR3Hce6/gzVgLSM3lkv7LbXvh5UGDIlzYmKC/fv3r/pitFKyci6fo/GlPwbXwRzK9+o+Cq8auD7IbhctlUQlTKxcGtWo45tGMGbD91G+xCzlcWaCfiojYaI6oXS9jpbLIRsNVLeLtm038uYVVKuBvuMx9HQKs3wTxrajpm8E3n3DW2B+CjV3G9I5aDdQ83dAaChALxQwOpVg8dkitOqITgNVGEHU5hb4/1npLNTnMJSPnDxA3q1j230vw6YPOQisl0wLMXcDte2JFZ3fzRTNrOfgzvXA/dSJpmkyOjoaZQls245sod55550N8TR8kMgqFli8x8nqXpZJGz16fjXy9W63y/Hjx5dM+y3GRkZW4X6VUly8eJGrV69y4MABtm3btqZ9roSsvGqZxr//LH6lGogPujZmJoXveJhDBdxKDQDZ6WKMDaM6ocDCw8ikcHuCC3duHj2bwRgagnYdozSEN9erXVlmFIUxsB49l8csXw8X2193L4pCKbThMWS7AZ0mYnwbMpGg4NVRZgLh2kEkFCKVhdocottCFUagNgfNgNS03fswshmo1ElIB5nMonWbhCsTKOrKQF4+QydRolgs3lf1uZlqVps9srofEokEExMTTExM3FdpODQ0RDabXfWNwoMILGKyeg+T1VLj5gexkWQFKyeV+6X9ltrvRg3dU0rhOA4nTpyg0+nw4Q9/+IEKu/cjK991aHz2f8Wdm0dYFsq2MUtDeNWAoPxGk8TEOPb0DGYug+i2MfM53F7E5bU6GKUiXrkKSmGVAgNZhEA2m1Gaz6/VIJ2Gdhs5FQgq9NIwZuUWZAvQrAXih1QGOq3AraL3WtXtR0HG2Dipym2AQII+exPqc6hUBtFpQbs/toRUJiCrdgMeP4Rh11D2wIUqmYVuk4TbRhkWwnNIJBPo7TInz5+na9vk8/loTMpSd/abLbLaLGuBB+v72ihPwweZEhyT1XuQrBb3Ti0nothIl4mV7H8w7bea6GUjBRatVotXX32VQqHAkSNHHrif635k1f69X8WdnQXfB9/HymcAGQrvAHDn5khsm4BmHaTCqzcwsim8Zi+ikRKEwBwZhk4LPZtFtloox8UolfDm5oKR80MjyPb1QLixbQdm5TYCBYWRgOCkRAxPoG5eCqKlse0wfR3mZyA/hDG+BaNTRSEQqMD1gt5S88GxRaOCyhQQrVrk/yd2PYGWToFdC1KFiQzCbkVSd4FCZocQ1RksuwXK58j+x2gnC3e5lYcXxlKpFNUwNgtBbMbIaj3dXVbiaRh+NsspDdeyJqVULF3v4T1FVmHvVHgxv1eT70ZHVvdKA3a7XU6cOIHruvdN+y3GRpBVmPa4c+cOTzzxBLt3716Xi+C9yKr+B5/FvnAeZQcXbSOfxW8FUYyRTeN3HZTroVkmmu9CMhEo/QDpy37UVG+S2DqBcLoIQLMMZC8Ykt2+vZGqzoOmoY1uwXQaQdZPAbV5Inbs9CMjoeuRuELfvhuzMQ1AN10g2a5CdRalmwjfDRp/Q2SL0KohGmWY3I/hNVFe/8Kl0jmE3YJWpf/63rkWntOTsN8g9cQEqVSKrVu3Rnf25XKZubk5Ll26hGmaKKWYnZ1lYmLinv1uDwObKcqDjbVbWqmnYUheodLQ9/013QDG0vUA7wmyGrRMWqnb8kaO8YDlSSVM+42NjXHgwIFV32mtN1mFbhTNZpNt27YxOTm5bvtejqzq//ULdE69iaYpjGIWFPjd/kh62e5g5nN47S5WNhmYvKaSkTWSsh2s0RGcmVm0bBbD1JB2cE78Wh2RTqPabVS7jRidQM1OQ6eN8fhTWHYF0a7D2I4gcmrVA+f02VtQme17/s1NgW6i7ZjEEv0mbGn0eqN8DzW8BeZuBcRlJhFuF5ze+9i2By2bhWoTWtX+8+GkaaWQ2SKiNoto1aKIDSsJ/sKm78E7+127dkUXxxMnTjA1NcXFixfJZDJRynAjlGz3w2ZMA26E08tSWE5puNjTUEpJNptddTowrlkFeNeT1VrnTj2MmtXg/qWUXLx4kWvXrj2QaGE9yarZbHLs2DESiQTj4+Pr3iS5FFl1Tr1B55U/C2pKgEQFKTgNtEwav2eR5DWaJLdvxZ8PRm2oTndBvcqtVNHzOaxcGjpt9GIhEGkAeiqJ1+65nWu9qCWbx8omEM2QFAfWZfajEpEvoeoVcB20vQcx23OIJsieRN3qNgIJO4HjBQQuGGpoBO7chNos7NiLJruR/58AZKaAqHYRzQpKMxDSi9wuhGcjs0PQLKMyebDr4NrByJEloOs6Q0NDADz77LMIISIZ9rlz57Bt+6FP592MacBHtZ7llIaXL19mfn6ev/zLv4w+n6GhIfL5/LJrVUrFrus9vKvJarXj5gfxMNOAD5L2W4z1kq5PTU1x+vRpdu7cyd69ezl79uy6CzcWk5V97RKNP/7X+PXeaHnfR+gGMnSN8LuYw0O48xWSwwVkZT5Q9ZUDRZ3faqNls8hmEwEkJ4ZRlZ6E3HUJ03l+vQFWIohy5u4gSqMkC1m0OzdRkaDiNqQz0G7B3HTfDqkR7E9s2YmRstDCCR+ZIrTqGJ5NJ1Ug1alBbQ4ltMCyqWfbJMZ3IHJ5tLqNalX7qb5Q9ackMj/ci6iqEfFhJZFb9iC8biC4qM+ihrev6BxblrXA8LXdbkf1rhs3bgAsqHdthO3QZoysNgt5hkrDmZkZhoeHKZVKK1YadrtdfN+P04DA5vg0V4lQROE4zpqICh5eGnB2dpZvfOMbpNNpPvzhDz9wOP+gasDQjeLMmTM888wzPPnkk9H5W+/zMUhWfqtB4w9/I2jMlQqkwsyk0cTAe1EKr1YnsX1bVG/y63W0ZCJ6Xk8H6cDktgmo1xC9aFB1uxilYrCd7yFGerZFuoG1dRzNbgIKURju7UtCqTei3rURYz1iaFQQu/dhGT5a9Q5K76X9BuZSRalAz4FwzH11DsZ3oOkqSOfRi7gyQQQURFS9n5sIUkDCtVGZYM0qnUWoIP2nEmlEffae5zY8r0t970MV26FDh3jppZd49tlnyeVyzM7O8tprr/Hqq69y9uxZZmZm1q3X8P1Us1orwppV+PkcPHiQb//2b+e5555jaGiISqXCW2+9xSuvvMKpU6f4D//hP3Ds2DEAPve5zzE5OUkymeT555/n5Zdfvuexvv71r/P888+TTCZ57LHH+O3f/u27tvniF7/IgQMHSCQSHDhwgC996Ut3bfNbv/Vb9zzuf/pP/4mPfexjjIyMIITg+PHjd+3Dtm3+5//5f2ZkZIRMJsNf+2t/jZs3b67izAXYXJ/mChCm/dZjQOJGRlZCCKampjh+/Dj79u3j0KFD65JDf5A0YLfb5fXXX6dSqXDkyJHoTjzc70ZFVr7rUP2t/xVnagrVU2nq+Rx+p4t0XIxCLhIZWIVc0NMUOn9IuWAooV+ukNq9A9FuBOSV6Xf2Kwa+B40qaDrW9m0Y7XqUDqRe7vdUDUrNQ0Pb0hhmMYemJEL6kVefaJRR6XywRqcvZxchcRVHEMUSGgrRrKLCY4TiCemjsj3i6tT7SUgriT++GyEdlBGkI4VS0Jjvr2kJrPSzEkKQz+fZvXs3H/jAB/jIRz7Cvn37ME2Ta9eu8corr/D6669z8eJF5ufn1/ybiNOA98dSfVZhPXLnzp0888wzvPTSSzz99NNks1n+8A//kL/6V/8qpmny0z/903zXd30X/+2//Tdeeuklvvd7v5fr168veZwrV67wfd/3fbz00kscO3aMn/mZn+HHf/zH+eIXvxhtc/ToUT7+8Y/ziU98ghMnTvCJT3yCH/iBH+C1116LtvnCF77AT/zET/CzP/uzHDt2bMnjtlotvu3bvo1/9s/+2bLv+yd+4if40pe+xOc//3leeeUVms0m//1//9+v+rsm1GbqLLwP7tc7tRp0u13+4i/+go997GPrfkfY7Xb5xje+gRCCD33oQ+taHG02mxw9epTv/u7vXtXr5ubmOHHixLL9XOfOncP3fQ4cOLBua/3Wt77Fli1byPzXP8S5crFXm9IQyutLz3vQCjmkJ9F9FwHo+TxetRo9r5dKeHPzWDt3YOAhe6Pr0TTQdFQvQpC5IlTmQQiMvfuw6ncAUCPbgpoSwPCWQFABkB8OoiJNh6FRzLSBMC1ET3quShOI+cD/T45sR9wJfqgqW0LU51GJFKQyaOkk5IfRqnd66xhGa8wHBOTZgaCiMIZWmQmeT+fRWlX8iccQBAQuUwW0VgWl6SB95OMfgmxpyXPrui4vv/wyH/nIRx7oJshxnChlWC6XcV33rnrXSn4fr7/+OpOTkyv2jdxovPXWW9E4kc2Cb37zm+zduzcSYqwEb7zxBt/93d/NoUOHME2TN998k9/93d/lV37lV/gbf+Nv8L/9b//bXa/5R//oH/Enf/InnD17Nnrsk5/8JCdOnODo0aMAfPzjH6der/Onf/qn0Tbf8z3fw9DQEJ/73OcAePHFF3nuuef4V//qX0Xb7N+/f8njXr16lcnJSY4dO8azzz4bPV6r1RgdHeUP//AP+fjHPw4Ehtg7duzgK1/5Ch/72MdWfC42163HMgijqUFvv/UYkAise3Q1NzfHq6++imEY7NixY91VPKuNrJRSXLp0iWPHjvHkk09y8ODBJZVIGyGJF0Kg/8WXcc6/jWw0AuNZJfHbXfRUAmEOTBV2vaDxt/dvv15HKxSi52Wng7l1Aq1VQ7ZaaGEOX0q0fL6/n974D2PXJIbWvw9b8H0xBvwas73XpjPoWybQPAfRaQa9U9CTqAfrFM6gW0Wv4J3MIIZHg0isXe9Hd1ov1ec5/VRgt9l/vZVCju0CPJTei6jC9KH0IZm7byrwrve1BliWFd3AHDlyhA996EOMjo5Sr9c5fvw4L7/8MqdOneLWrVt0Op1l9xNHVvfHWpqCw5rVz//8z/P6668zPz/P3/ybf5OPfvSjvPrqq0u+5ujRo3z0ox9d8NjHPvYx3njjjWi80HLbhPt0HIc333zzrm3uddyl8Oabb+K67oL9bN26lYMHD65qP/AuEFis57j5QQyS1Xqk5wbVfvv376c6EBWsJ8J03UpqBI7jcOrUKZrNJi+++CL5gYv6YjzoCPqlUDz7BuLkt4IalaahWwayd8GTtoNIWIjeuRe6QNVqaIUCshY4WCjbQRhGkDrMZDHTCeRA7SiE32pFfVdifgYxuRerW0U5ApVIIewOzN2OHCqYnw4Iy3ODPqtkGnN0JCCJEFZAekL6yNIWxPxtqM/jGQkMz4ZOA5UbQstlA+PZdg3h2oHqr1VFDPRsEab37DYqnQ9ILZFC4AVKwUQa0XbAbqF640XQjUBksfXJJc/tRrmYhM4N27dvR0pJo9GgXC5Hza+JRCKSyA8NDUVGzbHA4v5YS1Pw9HTQ3zc+HtRGw0Gn4+Pj0XNLvSbcPsT4+Die5zE3N8eWLVuW3Sbc59zcHL7v33Obla4/bJh+kP3AJiarjRg3P4hwX+sRWYVqP8dxIouier2+ITWxlY4fqdVqHDt2jHw+z5EjR+7r/r7eAgv73Amy77yJ8Nwg+kkmEJaJRlCngoCM9FQSkcmgemk91e2CYQTTdR0bfWwcZXcwlINqyb5requFyGZRzSbC7qJGt8LMLcTEdsxsErphL1MhICslEUNjqM4V8BzE+E7U1FVwbfTJvWj1WZTbQVmpIIJqVgYk6r1zBLipHEbDDohxbCua3UANGNNiBvU14XQiYhoUZ6hEGpnOo4QPWqAUDAdACumjkjno1MHtButwOhFxDuJeAov1gqZpFAoFCoUCk5OTeJ4X9Q9duXKF06dPk8vlKJVKeJ63qbwKNyNZrcUbsN1rwVit1dbi55b6viy1zeLHVrLNWrCW/WxKslJKUa/XaTQaDA8PrztRwerNZpfD3NwcJ0+eZGRkhOeffz6K0jZKGh+eh+UuDEopbty4wblz59izZw+Tk5MrOnfrKbBwb16l8YV/g+j1UqEFHUmyN/7DKBaDepSmoSUtNOXj9UhIOQ76yCj+bK/W5NhYmSSq3QbXRS8W8XtydT+RRguPYbdRE9tJeC1o0yeadou+Q8VAGk56YJgY23ehWYGiMKgrDSNmbwZmtPlhqM8H/n+ajpA+mvRxzSRWaQSSabAbwbbJLKLbRDgDxGSlArLqNlGJLMJuoqwECj9YUSKDaFeDiKq3SgZSjspMBNHVyM5lz/XDjGYMw2BkZISRkRFgoVO567qcPHmSYrEYRV5rMXtdL2w2sgpvvtdiAgDcFYXcuXPnrqgnxMTExJLbG4YR1cuW2ybc58jICLqur+q4y60lrIsORld37tzhyJEjK94PbMKaVTh3qlKpcP78+Q0bkAgPRihKKS5cuMCxY8d44oknePrppxekEzfSHR2WHj/i+z6nTp3i4sWLPPfcczz22GMrPnfrlQb0Oy0a/8dng9lSqTQin0PPZCIBBAT1KLNUwBwZRnVt/HYHo9T/Ivvzc2j5AiKVwkwYC8bQy1arr7CrzqPSQU1QWAmMtIUmQOu2AqEFILotGJ4IXlybg1C6Xr6DtnsvercOjXKk3hOD1klhKtBzoacKNDwbOTyOJp0F26pUbx09/z8AnO6C52VpK0oD9DDKXVijAsAdeI21vIT9YURW90PYP3TgwAEMw+DAgQOMjIxQq9UiCfbp06e5ffv2PetdG4FHNbJEee6Sj4fXmbXUrPL5PF/72tcWPP61r31t2Yv94cOH79r+q1/9Ki+88EKUYVlum3CflmXx/PPPr+q4S+H555/HNM0F+wl7PFdLVpsmsgrnTnmeh5Qy8tLaSKyVrLrdLidPnsS27WWdyTdKGr8cWbVaLY4dO4Zpmhw5coRkMrnUy++53wclVykljX/3L/Bmp4OhhoAw0shWGyOfw2s0oxEdumWBlD0dHPiVClo2h2wGknQSCay0hXBsVM1FJJNBitB1EcOjqLk7CKVQ+SIkkliGQiUSEGbkBm4cmo5HKHMRmTyqXkbf9ThaOg3tchDF5Eehdgdq80Gfk90OZkz1TxDKSqKGxlBWEtwmtGsoI4Hw7LuIS9gtRLsW2Swpw0JpvRgvmYUwoopqVGHvVRdlJQOvQ+kHKUHpR4KN6BibrK8JIJPJMD4+zo4dOxaYvU5NTXHu3DmSyeQCS6iNnMz7KCIrWa8gcoWln+v9tlZLVq1WiyeeeILf/d3f5YUXXuDw4cP8zu/8DtevX+eTn/wkAJ/+9Ke5desWf/AHfwAEyr/PfvazfOpTn+JHf/RHOXr0KL/3e78XqfwA/sE/+Ad85CMf4Zd/+Zf563/9r/PlL3+ZP/uzP+OVV16JtvnUpz7FJz7xiWWPC1Aul7l+/Tq3bwfTCM6dOwcQjVspFAr8vb/39/ipn/qpqCH6p3/6pzl06BD/3X/3363qXGwKslrKMskwjE1JVoNpv+eee25ZccZGNR2HKdHBfU9PT3P69Gm2b9/OE088saYf6XpEVo1/+7/jXLkUpbS0bCZK/fnNJkaxgFerkRgfQ9ZrCF2PJvwC/am+uo6ZMNBMHeUEI+5FKhWQFSAdp99R1UsTam4XVa+gDBPhueiN/t/pTh0pNDQlceam8Md2UOhW8Q3R30/vwilQyHwJMdsOoqTsEKJZgXYdMbGDhNsh/MYIVGDDVJuF1iBxub3n6dWn8ihdBPZKqv99E9KHRA669YVRmJUOJgd3m/i5UWS3jZ5eeEO0mepDcLfAYrHZa1jvKpfLXL58ecFk3lKpRKFQWFdyedhkJWdvQWkcIZY+pu/7aypntNttnnzySX7oh36IX/zFX2RqaoqDBw/yla98hV27dgFBpDLY+zQ5OclXvvIVfvInf5Lf/M3fZOvWrfz6r/86f+tv/a1omyNHjvD5z3+en/u5n+Pnf/7n2bNnD1/4whd48cUXo20+/vGPMz8/v+xxAf7kT/6EH/qhH4r+/T/+j/8jAL/wC7/AP/2n/xSAf/kv/yWGYfADP/ADdDodvuu7vovf//3fXzVxP3KyWjxufnA4YjjmY6OwmmhicCDh/v372bZt2z2/eBuVBhzct5SS8+fPc/PmTQ4ePMjExMSa9/mgAovqn/wBzuXzCCWDPqmhIXzZd0cH8Gs1EqPDyEbg76d8Hz2dwuuRlaxW0Ce2YOhBKk8Y+ah5VtXrKCuBcGy0Rg2/NIbotEhkEqhMDipBJCKLY4jyNEJKnNIWrLmbgWv72HaYuY6xZQeGoUHHQWtWsDWThHRRtTnoRTliMJWTyqI6DcTwRGDh5HZIet2+zZIMR4WoQAlYuwPtKkq3EL6DMkyUriFQqGQmiJQGIiql60Ftze2izBTC7USqRJkbxknl0brNJclqM0VW91vPUvWusLfrzJkzeJ63wBIqk8ms+f2F9aGHRVby1iXExC6EvvzldK1OO81mk2w2y4/92I/xYz/2Y0tu8/u///t3PfYd3/EdvPXWW/fc99/+23+bv/23//Y9t7nXcQH+7t/9u/zdv/t377mPZDLJb/zGb/Abv/Eb99zufnhkZLXcuPkQYdSzkT/KlUZWK0n7LcZGk1W32+XMmTOR3+CDGl0+iMBi9s++jHz9ZbQe6WjpNH49ICRhmgjDQLZaGEPFwBYpn8WrBeo/2e6gj43j3wmaZY10Eq3nz6eadUS+gKrXAgVgcQT9TtDMK3QDa6SEbreR7kDKc6AXSrgD9SclETufIOk2kPkt0KkFJJErQW0GzXOoa0nyfgdqc0gjgebZgZvG1kk0t43s1cc0ZDCDqjGPaFdRQkcon6gGpRQyV0QpibRMhJRBanPQzSKRhW5jYY3KTCDcDthNvPwYtp5AaBp+p8lSCbPNQlZhK8VqJ/OGTbvhzKZQrHHlypXIrDeUyK8mrR1+jzecrJREXj4DO564J1HBg420Xyz7fr/ikZDVSpzSw/TaRhZKV0JW8/PznDhx4r5pv8XYaDunEydOMDo6ygsvvLAu52ctkZVSiuuv/DnJl/8UA4UoFEATUeoPCCToQsMcKgQXaSnxGy30bAa/t51qNcA0MUdH0BoVxNAIaj5QAwql+nOlavNBg64AM2Wh9cxhtVYNP5NHa9XR2g287BBGs4LVmEXlhgJz2kQKozfqQzTKUWRkqX70ni4UodxBoKgJkzwO3WSWhJJo9MQT9FR7ob+f9JHZYURjLjCm1TSElCjDQhrB3ySzQUTVXUL153YjubyQPSeLTAknVQS3i3KdwM3C6aJZ/Qv2ZkoDPqjYQwhBNpslm81G9a5arUalUuHWrVu88847pFKpiLiGhobu+TsMv8cbSla+jzx3DPY8FalJ77352siq0+mwY8eOtazwPYdHQlbhl/p+wxFh7R/ySnAvslpt2m+pfW/EkMQrV67gui6Tk5M88cQT63Z3vdrIynVdTr92lK0v/xf0ThsFKCmDNJom0Ap5/Fq9t3OBnkhEDcEAynEDUYHvg21j7dqNVu2NA2k1+mPlWw1UcQRRnUO4DnJsG0bCxOzW8YqjaOHYeSsJrd74kGQBo9lzT88VIFdA91uo4hbE3O2gebcwiqjOIJoVZCqH6DTQW9UoPZfPZpGpMbK41JSgQEAsLUwyuD1/v94MKq0fMclMCaV8ZCIRyOOVCogRENILIiq7GYwACc+FmQx6qrot/PwoXS2IqIBAXGFYyE7jLrLaTJEVrB85DI6Mf+yxx6L5UOVymUuXLtHpdMjn81HktXjExoaTlesi3/4mPPEBtMTdPXBLYa033c1mM5oK/X7HI0sD3u/iGH7RPM/bsCmoy0U/tm1z4sSJVaX9ltr3epKV67qcOnWKRqNBMpmMXI7XC6sRWDSbTY699k32vfEVRK0aPKhpaKYZEZJqNNBKJWS1ipVLI1tNtFQKv1eHVJ6HPjqGPzONPjwcpP4SSbCDAYaiWEKVe+RlmtGkez2TxmzOA6C36lGEpDWq/WipMduvKWk6umyjCVADBLFAXZctQKeBcLqo/AiqNovIFzF9GyTkkwloBe/LFTooF+E5tLUEadkNxBUDwxN9Q+8RUwa6TXA694yoCCOqbAk7WQwiKq+/VqEb+J0mRmGh795mIavBydwbgcXzobrdLuVymUqlwqlTp6IRG6HScPBmeN1hdwKi2vscWmrlVmpraQiGePDiIB65wGI5CCE2XBG4VPQTpv2Gh4dXlfZbjPVMA9brdY4dO0Y2m+Xw4cO8/vrrG+Ljt5J9Tk9Pc+rkSZ69eBS924nGXuipBGpR256qVbG2jCMrZSDw9zPyObx6r141N4u+ZSu63QqEGdkc0g5qOKrTt1DSyjPITA5VKJFpl/FyQ+iNCsK18QvD6NVZhO/SLW4jWbmJ7nawi1sxhY+FDblS4GLeKKOSGUS3hWjMR4Qm7IEeIDMB2x9H99r99F6nHhFLWpOEckBhJaHbRfNdGhiYug6GiR5uoPWIyXchkQG7taBGhZkMamzdFn5ulK6e6F9gpQzW4trgeyjPQfleVBvZjJHVw1pPMplk69atbN26FaUUzWaTSqXC/Pw8ly5diiKY6elpSqXS+g0V7TSQp1+Dfc+hZZeWqC+HB6lZxWQVYNOSFTycAYnh/kPD1ytXrqwp7bfUvteDUG7evMnZs2d57LHHoibfjRBv3C/SlVJy4cIFbty4wXOVK4ipm6ieKaZeKOD3/Pz0fL43s0pijQ6jalW0TNBrBcEARZHOoNotRCaLkbACoQGgqvOQTEG3A7aNNzyBMXs7SC2ObSHdDiIqBn/0g35+A+dEWgksw0GTHrKXPhMQ2C91WwjPRRbGENXpIBXYc6AglUbzQvLqnw+VyiGcDpbv4GgWlnRI0q93WfkhMAOiarqKrCnwOo1IGKEMK+i9crt3jbhXmSG6UY1qIKLS9CC96jmg6fidBkZvzMhmrFk9CseIcMRGOGbD932mp6cjlezZs2fJZDJRyrBYLK7tBrRVQ57+Jjz5PFpuaSf8e2GtZBVPCe7jkZHVZhg9r+s6rutGab9ut7vmtN9iPCih+L7P22+/zezsLM8999yCsQIbNShxuX06jhOdnxe6M6gLp8DQ0ZJJQEZEBSCbTYxCEWEZqEbvcc9DGDrK8wNVXjIBSmEmTUS9Avkiql4FpdAyWWQ3IAut13uktu4i6TQjyyO9UUZaSTSni9as4iWzGN0myfo0XiKD0kxSlkSggVwoqFgQRQ1eXLNFVGkM3e8i03lEuxY09YbpRL9PTI4ekJWw20GDL6DlipHVUjpXgG4dE0nbg7QBdqtBWN0IVH9dsFvI7AgdPdXvz1ESYSZQro0aOKYwg7oVA2S1WSKrjU4Drga6rpPNZjFNkw9+8IO4rhtN5b1w4ULkCBGmDHO53P1Jtj4fENW+D6IVR9a0rrXUrEKVZDwlOMCmjqwMw9jQXitd16nVanzjG9944LTfYjxIGrDdbnPs2DF0XV/SjeJhRlahIW6hUOCFoRT2K6+h2oGoQUskkc02RiaF7/qRpZJmaghDJ1yhcl20bDYYNw+oZhNz61ZEtRcpDQxNVLUyMpFCsztojSr+jsdJ2tXARLYwglntOVdk8uB0EYCdGcHoNhEo3MIEKdPDkjayOIEoTwWCivwIonYnUOz1BBWDqUCSKXS/l6Izel6B0kdmSz2Jei0aUW/I/ndSpvJI0wyiotB5wu9L5lP5IrSrJIXElhoJIWk36mR1cMw0XrIIntOLqHp1r57SEN8NrJl8F6RCOq1AxNL7rDYDOcDmHg9imiZjY2PRoNFOpxPVu8KR8oMS+XQ6vfC8Vu4gz3wTnnwBrTS21OFWhLhm9eDY1GS1kZGVUipqSjxw4ADbt29f1x9/mAZc7UVlZmaGU6dOsW3btmjk/GJs5FTfQdy6dYu3336bPXv2sF126Hzxd6Ohh1oqiWwFxrDSdtBME5XNohk6qtNGtRWiWET1RqXIZhMxVELVaxgjpSCyCGUTjRoijK6kRA2Pwsx1VGkcI2mg9TJj2kCKTBuQgZvtKgrwrTRGKonl9EjQ7zf3DvbBqEw+IKteKlCZJprXc1x3uwvnToXFehQyW0TUZkn4QSoPoSEzOQhd1Y1kQKA9eyXhOQvWYGXy0K6S0RVNLY1KFmg1GuRSQaQpNR1deqgBshOGifLdKD0ou627GoQfNd5N40FSqRTbtm1j27ZtUb2rXC4zOzvLxYsXMU0zIq4SXYyLx2Hv82ijWx9oTXHN6sHxvkwD2rbNyZMnaTQaFIvFDeljCH8sKyWrsCZ0/fp1Dh48eM8JpxsVWYX7lFLyzjvvMDU1xQc+8AGGkiatf/cr4HtoCQuRSiGkRHoeyg8ITrkueiqFMHVkmL6rNwKbpJ5CULgO2vgEWrsnaR8ahspcbwH9H7Jem0UOjZKwFKJRxk+m0btt9HYDL1PAaNXQ7Dad3ATJxjSWXacztB0rZZFx5nGSBaxuLUj/9eZZifqgoGKgDyyVRfM7QRo0lQ3Iyu0iU3m0Tr2XCuz594VCAkBmivhWIhBIhNHPQNoOMwWeA047ishUz91DpYto6RGU2yWXTgbfEaDV7pJPGuB7eAgM1EAdTiGsFI7rkWJzpQE3Y2S1EmIYrHft2rUL3/ep1WpB5HXuJP7sJSqlXSTqXUrGPMVicc1tNL7vr9oLMUwDxjWrAJs+slrvNOD8/DwnT56kVCrx+OOPr3oA2EoxaDh7vx9yWDNzHIfDhw/f905qo6b6KqXodrscP34cKSWHDx8moet0/vCfI2cDhwl0HRwH6TggRNTcK5KJoElXav3+KVQwor03/kMvFtEsAxVO0ei2oueoVfDzJfR6GWUm0EdG0Gq9EfGpHHq39yKzr+ySvYm/UjNQhRGS3WD8vGeksaj1oqGhgKx8t99b1aqh0nlIptE0H1SwhgU2S2YSOnWE7yHTRUSrgmhVkUJDCq2XhuytyUpDp7aAmEI5ejAKJA3tGnRbyEyJtpFBC4lGKbRejSqXToEM1mA7HoalI91uMLtSgK2n6TiC5AqHbz4sbKa1wNp9AXVdD2pZ3SpKa+G+8J0YqRLlcplz585h2zaFQmFBvWul73stNat2u41SKq5Z9bCpyWo9peuDar99+/axfft2ZmZmNizNuNJJxOVymRMnTlAqlVZcM9sosvJ9n6NHjzI8PMxTTz2Fpmm0/+Bf4M9Oo2WzQZrQ8yLjWZRCttsYpVLAOe0WuC5GvoDXk6vTaSNGxtBME73bAM9AhcMV7W4QXfX6qUikkckuZiGHGEj56a0qShMIqdCbVXzNRJcuqcYMvpHEG91O1i0jhY6mfCy72p9nNWi/NGAyqgojaNIOSKxHRgtc1AfmUkW9UUrSMrNomSym2+kZ08rgvxAhMdmtSBASPq9Seex0CVwb6fVrVEJoge6w1wCM55BNp1CegxACV0JLJlCejqZJrl2/EfYhbwq8m9KA98WNs8ir52DHfqzd+xgnmGqrlKLT6USWUKFx7KCfYSqVWvY8rCUN2GoFGYA4DRjgfZEGDNN+nU5nwXj3jbRECt/fcqSilOLq1atcvHiRJ598kh07dqz4B7/eZKWUYmoqiEr27NkTraXzlT/Cu3kZXBcFaNkcyvPQ0in8didcTCCqM8xI6C3rtWBIYq9eJQwDw++Rj+ehFUrI+WBOk+r59wlAb1RgbAtGpwZuF7dnm6R5Dl5+GKM6h/A9OvntZGvXEdKju+0ARSfYVzs9Sro1g+F2kPlhRH0+EFSEgxEb80Gklx1CJCxEpyeo6KUgBUEUJxp2oPRLZAK5ebeBQoBu4GcKJIxeSjDZ8/dzWgPE1U8VqkTfuFami3SWjKi6yIHxIkIzUDiRXB3po+dG0fxgjUr6SCW4dvVyNL8svNNPpVbmprDe2IxpwDWt59pp1PULsPVxtN37FjwlhCCdTpNOp6N6V6PRoFwuc+fOHS5cuEAikVgg1hg0M1iLwKLVaqHr+vr1ib3Lsakjq/UgqzDtNzQ0xAc+8IEFkctGjfEAon6opdbvui6nT5+mVqvxwQ9+kGKxuOp9r9e6fd/nzJkzzM0F0U1IVN2Xv4J78jUIe6nSafxmT83ngj5Uwq9WMMfGULUKaBoilUZ1gohEOS4YJuQLGE4bCkWo9qKtRi14znMRrQb+8Ba06ixidAxSqSClBlFEAyzoodJ7xNcZfxxL7z8uBgUier8+oDL5gKx8Dzm2E03IYGpvb26V6NT7dakBpV/wfCtQE2aHkOkcCSlRyu/5A4YbqmB4YuSo3lMY9tajkjm6YUS1wEUjbADuR1RqoG9MGBZdPU3b79+RWwkL3TDYt38/ly9dIpfLMTMzw/nz5xfMi7qff9564j0RWV05ibx5GSYm0fYcuu/mQgjy+Tz5fJ7du3fj+35kCXXt2jXOnDlDNpuNPg/P89YUWWUymU11I/AosanJyjAMbNu+/4ZLQCnF5cuXuXz5cpT2W/yDehh9XItJpdFocOzYMdLpNEeOHFmTldR6RVaDEvkPfvCDfOMb3whGj1w8hfvG/wMoRDqDlk71U389yFoFc7xHVABSIjQRTNxVCuwu2thWDLsZXNPrtb6dkvQRQ6Oo2V69UPmwZQeWU0cqN0r5GY0K0gxmVenNCnYiT8Kuk2rN0tyyn6JfRnXBNdOYbptEt4yvJ9B9OxBXhGm2nlpPZQqQzSGaAWkGkVO7lwosIFrVBRL10LVdaQZ+fgScNjrQxQgagu02dyGMuMKIKpWnbQ308iiFMBIoz0YOzL0Suo7yQHluoFz0PWw9S1sGFzjT0HE9H88LFKYagcPL7t272b1794J5UYP+eaVSieHh4VXVV1aLzRZZrTqKuXwSdfsyjG5D2/vsmo6p6zrDw8NRP6TjOJFE/uzZs9i2zbVr17Btm6GhoRV9HuF4kBgB3pNpwOXSfuu1/5ViMamEUvDJyUn27Nmz5ovHekjXZ2dnOXnyJFu2bGHfvn3ReXDLd/C+9h+jmVN6KoWsBISk5XLIdht8H2NsAtWoohUKyF5TsOq00UbGAjFGJovhdxHJJHTavYt4OiArArNa1bsoa7kcugY4gTzdzY1g1mYRSiIzBbRq0E/lpIZI2HVao3vQk0loBRd6J1HAdNtoSgapwOYUwnOQ+VFEfRbRriGHxiGZ6EVRPVXggP9evy6lgqbgxjyi20AmMvj5kX40BUihBeYW0ofQmNZp98Uivc9VJbJ0UsPgu0h3gJg0EdWoNMMKUn4DQiJhJOiaRVrS6B1zwElDKYze93bw+7N4XlTYT1Qul7lx4wZAdJdfKpVWPUn6XnjXCiyUgssnUFNXoTSBtu+D67YGy7KiablKKY4ePUqxWKRWq3H16tUFZr3LpXBjJeBCbOrIai1kEgoWlkr7rcf+V4OQrHzf5+zZs8zMzPDss89GhpwPsl/Xde+/4RIYjDgPHDjAtm3bgOAHbjkdvP/4W8hGDZFMIXQ9cpMAUO02WiqNsExUMyAo2WigFYrInqGtqsxDYQjD1NE8G5VOB2QFQXSVTEO3jeZ06QyNYySSpO06Tr5/TjQ58N7sbiSWSHbmaQ9PUtCaOK6MHjecvhTddwfUo71alEpkUMURtHZvTlao7us2+9L2TqNvRtu7wVBCwy9tRbkd8PzIJsliIKoddEcPjWvtNjKZo5XIo+lGIGtXMlL9yUHVoW4E6b+BBmBbS9OSwfc2iKg8PG8gPaiB8u49yG9xP1G9XqdcLkcj5sORG2HK8EEmG7wr04BSwqXjqJlrkBtFHPjwhq0nVNqOj49TLBaRUkb1rtAaKplMLqh3maZJu92+u0n5fYxHGrvf70NYjXQ9VPu9+eab7Nmzh2eeeea+OfswQtnIIYntdpvXXnuNRqPBkSNHHpiowv2uZc2e53H8+HFu3LjBhz70oYioQhy49haqUUdIGdRcPAc9m0OY/VSlsMyg1DLw2clOO6hBBYvDKJWCwYWAaNQgE0pvFX6mGL3OSGVIuUEdzGzMI3vOEXqrhtdrfDW7DTqZcQCczDB6JrgDtbw23WTg0ZZwG4FbOZB15gOpOASCimQGVRxe4CqhBtauEkGaRXhOIGeHKBXoDW9bsG0omzeQYPXuhJ1B+6YeOZopOulhQCBdpx8Xhb1k0kcYwTlVA03DmmFgp0o0pRGd3mhSMmAYPYWpF3z25grmKEHwOysUCkxOTvL888/z0ksvsWfPHpRSXLhwgb/8y7/krbfe4urVqzQajVVH7ZstDXhfspISLh9D3bkJuWHEM9++4WsaVANqmnbX57F37150XefKlSt89atf5cUXX+QLX/gC5XKZ3bt3k0wmef7553n55ZfveZyvf/3rPP/88ySTSR577DF++7d/+65tvvjFL3LgwAESiQQHDhzgS1/60l3b/NZv/RaTk5PLHlcpxT/9p/+UrVu3kkql+M7v/E7OnDmzYJvv/M7vjMZAhf+FY+/Xgs3zDVsCK5Wu27bNG2+8wa1bt3jxxRfZuXPnitOMsLxi70EhpeTs2bMUi0VefPHFdVNrrYWsms0mR48exfM8jhw5QqHQd41WSuF++XdJN8rgBXOmhK6D5yObDRAgsjlEMhmMvuh20DIDM3Y8D4ZGAmXg8AhabQ414EotBnTWWnUW30rhju0g7VTx8kHaSiiJn+tPRLX1/rlSukWnsI2s6Qapw/Cwep9E2yJIlwhAhYSoacixHQjpIuxWn8R6qUAISCqCEdnO4o3uQCk/SO/13NNZsG3v2L4b9FlBMAokkaGVKPQFHkqi9baVnhuRjwiJzfd6xxV0tTRtGbzO7BGTOxhR9f4vlQIUqfTaUkSGYTA6OsqTTz7J4cOHefHFFxkbG6Ner/PWW2/xyiuvcObMGaamplZUM35XRVa+D5feRM3ehlQOnt54ogrXtFz0GqZw9+7dy4svvsiRI0f44R/+Yc6fP8+NGze4ffs2zz//POl0mu/93u+NZPOLceXKFb7v+76Pl156iWPHjvEzP/Mz/PiP/zhf/OIXo22OHj3Kxz/+cT7xiU9w4sQJPvGJT/ADP/ADvPbaa9E2X/jCF/iJn/gJfvZnf5Zjx47x0ksv3XXcX/mVX+FXf/VX+exnP8u3vvUtJiYm+O7v/m4aPYebED/6oz/K1NRU9N+//tf/es3ncFOT1UrSdOVymVdffRXTNDly5Miy9anl9g+seypQKcX58+fpdDps27aNAwcOrOud52rJamZmhqNHjzI+Ps4LL7xwl6jD/m+fw79xCUwTkc2iZdKRpBwIjGh9Hz2XjdSBqtVCHzzX83fQdkyiR+4QA3fnrQayR15C+vhjO0j6wXZioEfJ6KXiAJLdOn4kKZckkjq6UKS7ZXwRkEfargT1IyBp16LXCqeL0k3UyLYFikJlpqI1qHRvPd1G//FuEyUE3sgOlDFwjsIoyu3ihpQxSFw9YlKGRScdkK8aSPWJgYhKWyKiErpJN1WiKc1I2i5l//yFGQJvwfdUkUyuz1C+dDrN9u3befrpp3nppZc4dOgQqVSKW7du8eqrr/Laa69x4cIF5ufnl/ytvGsiK8+FS28hZqeDz/TZjyzovdsohNmblaZah4aG+Pt//+/TbDaZnJzkxIkTfPzjH6dUKrFt2zb+1b/6V0u+7rd/+7fZuXMnn/nMZ9i/fz8/8iM/wg//8A/zz//5P4+2+cxnPsN3f/d38+lPf5p9+/bx6U9/mu/6ru/iM5/5TLTNr/7qr/L3/t7f40d+5EfYv38/n/nMZ9ixY0d0XKUUn/nMZ/jZn/1Z/ubf/JscPHiQf//v/z3tdps//uM/XrCmdDod1e4mJiYW3CSvFps+DXivSb6rTfstdfywGXa94DgOb7zxBjMzM5G0db2xUul6SJonT57k0KFDS04Wds98C//sW9Bpo3luYJLabCI0DS3snNc09Ewaul3EQOpJtpqQ7qXRxrege3Y/cmg3Id+PlPweIXiFURJ+K7qAG80yXio4juZ0AtUdgTy9nd2CnR4mk9aweyk/Tfm0k4HiSpcuzUSwfUq1aFg9R+xuEzkxifDthVGUO1B/G0wFpnoRimvjju5GKRko/Qac0KPzFZZ5XbvvpuF2UYkMzUSxT1zSR/T+vqtGRRhRGYAI5Om9iCq8oHm+ZPGvQ6l+KlAhaHkJ2vb6ekRqmkaxWOSxxx7jhRde4Nu//duZnJzE933eeecdXn75ZY4dO8b169dpNpvRhXizR1bKdeDyCUR5Cmkm4Nnv7N9EbDDC68tqCN1xHG7fvs3OnTvZv38/P/7jP86Xv/xlvud7vodXX311ydccPXqUj370owse+9jHPsYbb7wR1biX2ybcp+M4vPnmm3dt89GPfjTa5sqVK0xPTy/YJpFI8B3f8R13re2P/uiPGBkZ4amnnuKnf/qn74q8VoNNL7BYqmblOA4nT56k3W7fU+13P9yrF2otqFQqHD9+PBJ3nDhxYkNSjCuJrAbHeixn4eSefRPnz/9PVE+hRzqN6vVSBSPlm0FaTwV/AwgrEVz8wz4iy0Lkt2F2AvWgKg5Dz01dDUizzdoM7ugOEtjono1bGMOsBNL1oHk2OK6D2f9SCoGZS6Hjoqt+JKOrgXEdAz1ILgZKaMixncFcLLcVOKenC4GgYrDRt9OIequE76EAb2RnILjoBIIIrHSg9AuJS0m0wYjRTIFro3SDVmoYpEQO9GkJXQ9MaKWPZpgoz1049kMz6SQLdHwDTRNIqXopvgCGqeO6Pq7nI3qmv2G5sOklcZWi1lGkExtHFIOu5aGLQ7lcZn5+nitXrixoWnUcZ8Omeq8Gi8lKdjuIaycRlTmksOADf2WBsfFGI7y+rEbEMj8/j1KKUmnh7Kzx8fFlLeKmp6cZHx+/a3vP85ibm2PLli3LbhPuc25uDt/377lN+P+ltrl27Vr07x/8wR9kcnKSiYkJTp8+zac//WlOnDjB1772tZWehgXY1GQV1qwGpbGDar8jR448cOPjejQGK6W4du0aFy5cYO/evezatWvDhiTC/aXrtVqN48ePk8vlOHz48JLnyCvP4H7jv4LyEbqGyGSDER+pJCp0dgA00wjUTD2yUo6Nls1G0nZhmmgJC3pBi+i2+yTQbWMXxknUZgJniFwOvd4bLzIwV0pvliNronTrDo6RQWiCVEpD6jp4Lmmngm1kSHgtUnYFx0hjeW2G/DlcYWEqh6xfpZYZpuDbdJs+oTh78K5fmcmArKSH6vVW0anjjexCKj+ImPonOnxVIFHvNrDw+yazvouy0jQTQ2i6iZI2yveDv/2FxBTUvYLHhGGC52EbGTpeL8LUNRzp4/kSXQhkz/8vhGnquK6HLyVdlcFRBtCi1oGJwsORjg+6OGzfvh0pJbVajStXrtBsNnnllVcWNMIWCoUHUhmuFYNk5TtdtCsn0GpzSHR47q8E5/8hImwzWEuqdHGd+35tAoufW2qK81LbLH5sPbb50R/90ejvgwcPsnfvXl544QXeeustnnvuuWXfw3J4pGS1kjQg9L98oeR6tfZE9zvGg0RWnudx+vRpKpUKL7zwAkND/dTXRtk53YsEw16uwcnCd6253cT7L/8O1XM8F4kkstmMHBe0XB7ZbKAPj0Cj5zqeyfYJq9lEFYYQuoHudRE2KE0LVIR2F5UfglrQeKspH5lIo+fzWM3ZfpNvt4GbG8ZszKP5Lu3cVtL1Wwgl6eTGSVmSJF3q5jgJLyA4x8qR8FoIoEKJcdro+FTNLZSc23ilbeQyFrRrJPHoopPERzargIaOXOAVqHpzo/zSdmQiHVgneQ7KTAUpQ6dDNMYk+tGDg4Gh3CAVlywBaqHzhG5EZBX87S2MqHSTjpGn7RkYGvhSLahR6YaOdD08T0atW0H7lqAt06jez1boSXwJLRuy69c2tWKEvULVapVkMsnjjz8eeeedPXsW13UpFosMDw9TKpUemgw7vF747TbiWkhUGnzgOxHmw7cuWouJ7fDw8JIlijt37twV0YSYmJi4K+q6c+cOhmFEzcrLbRPuc2RkBF3X77nNxMQEEERYg9Mh7rU2gOeeew7TNLlw4cKayGrzVEWXQPgBdzod3nzzTW7evMmHPvShFav9VnqMtRJKo9Hg6NGjuK7LkSNHFhBVuO+HlQaUUvL222/zzjvv8Oyzzy7bdCylj/vl30E2a4hMLpgj5XnRxRiCWpQ+sQXavblOSgXF6YEITTMs9OASgLC7MDQgye+0o5qQ3m3ij2/H8DoIJfEGpOuD4+lDqbuvWxjpFCmC6C7pNqLEW8Lrz5lKqb57hKY8qoU9pLTuglEjVk9dqAvoar2Bik6bbnix79Rxh7biKxn0SYUI77ylHxjTQs9hPZxtBcpK00gUoxqe8r3o2HIRcUXP63er/gZrVEuJK0JVoO9LuipN2zUWKAqTJlTbj3bEfXhHbVkW4+Pj7N+/nyNHjvDBD36Q4eFhyuUy3/rWt3j11VejfkPHce6/4zVCSonudBFXT6DXykHb3DPfiUg8Gu/EtfgCWpZFoVC4S/n3ta99jSNHjiz5msOHD9+VYvvqV7/KCy+8EI0nWW6bcJ+WZfH888/ftc3gccPU3uA2juPw9a9/fdm1AZw5cwbXde85/uhe2NRpwPADfv3116O032pnwtwPayWr27dvc+bMGXbt2sXevXuXJIaNTAMO7jcc6+H7PocPHyadXl4l5n7t86jpW4GEV9dB0xFKoWXSgTmtUmj5AqJRRRRLyEo4yNBDy+aQ1SoqncMQHuSKUAlMZKlXUbqB8D0016Y7tBWrOo0aHkcfkK7r7WqUJjQa83hmGsNtBw2/qRHI5sn7ZdrWEGmnguW3aVnDZJx5LK9FVStRlGXyskpTL5L1qzjJPPlE7zMcrEW5/XRmOpWCVhBVKTMJbpN5ZSFcRc4UKKcNQkcoHwaVkCH5Kdkzrg3so5rJgAjVgPhCN0x8x48iKu6KqAw6Ro6Ob6BrAl8q/IHP0TA0HHdhKlDKgAhaMoPo/R66LoFFldARQKMbRGf6I7JiX0rQIIQgk8mQyWTYsWPHgllRoXdeLpdbkDJcL0WhkDbZuSvobieYrPzMd0BqbTL/9cBaBy9u3bqV119/nX/7b/8thw8f5nd+53e4fv06n/zkJwH49Kc/za1bt/iDP/gDAD75yU/y2c9+lk996lP86I/+KEePHuX3fu/3+NznPhft8x/8g3/ARz7yEX75l3+Zv/7X/zpf/vKX+bM/+zNeeeWVaJtPfepTfOITn+CFF15Y8rhCCH7iJ36CX/qlX2Lv3r3s3buXX/qlXyKdTvN3/s7fAeDSpUv80R/9Ed/3fd/HyMgIb7/9Nj/1Uz/FBz7wAb7t275tTedx05KVUoorV64AsH379mUJ4UGxWkIZHEz4zDPPROOy12PfK8WgGjAUdYRjPe71o3D+8j/jv3M8ICqhBQKA3h2u6tqQSgc1KOmCUqhaGa1YQvYMaFWzgRrbgqE8NLeLalajibjCd/FyQxi1gNyMbgNvbAdpt47yWniJHIbdQHdt3PwoZi0YT99Nj5CtXUcKDae4hZIXzLDytH6hXg6Y0tpaitBAoiVy2NkhRpNtqtoICWkvqEUJu42yMginBQM2S0lN4Re3khMCV0+A10EA1Y5DMamDZyN1E813YYDwAGx07Mwwhmnhuw7ScwP5s5ILUoGabiJDstINkD5dLUPHDy7Iuq7hSx/PVxgRcd2dCpRS0VFpOq5OwoCwJUBTLlLodN0gNVnvwNAjuh6vxG4pmhXVEwyE3nnlcpkzZ87g+z7FYjHyMrzXuI17wW/W2NmdQ0ehNAHPfAQy66/IXdWa1khW2WyW/+l/+p/4xV/8Raampjh48CBf+cpX2LVrFwBTU1MLIq/JyUm+8pWv8JM/+ZP85m/+Jlu3buXXf/3X+Vt/629F2xw5coTPf/7z/NzP/Rw///M/z549e/jCF77Aiy++GG3z8Y9/nPn5+WWPC/AP/+E/pNPp8GM/9mNUKhVefPFFvvrVr0aztyzL4s///M/5tV/7NZrNJjt27OD7v//7+YVf+IU11zGFWu/56KuA7/v3VPu1Wi08z+P5559ftTP5SvHGG28wPj6+omnBnU6H48ePo5Ti2WefvWcEA3D+/Hkcx+HgwYPrtVygLzJ57LHHOH/+PE888cR9U6POyVfxj/5poPwTGloqjZI+qtHopwAtCy2TgW6nP/VWCDAslN1FGQaiNIputxC95/3CCFovulKGifIlmu/ibn0MYRgkm8Fz3dwYiVqQB/cyQxg9M1nXyqLbTdrDj5HWumjSQ0Pia2YwHkP5+MJASR8DH5sEprTRUNxOPsZEJqhndfQcKb/nCp8pojUDayWZLaE1gtqczBTRmmX84hY8MwWeHUROvhcMauzV0wCqts9QIiAWVxiY0kFZaepGFk030EwL2YvAdDMRTUdG0wJT356NEoBmpWiToiMNDL0XBekCvzdlOWHqOD2bqDDiMnQNJSVNP4Om6bR7wZ6pKzwJQjkoERB6usfrk6OPJqt//vx5NE3j8ccfX9Prw4m4ocqwVqthWdYCO6iVZFS8ehlx7TSV61fIpdIknvsrUBxZ05rWE9PT09y6dYvnn39+Va87fPgw/8v/8r/wP/wP/8MGrezdhU0XWYWRQrFY5MiRI5HrwkZhpWnA0Ph1YmKCffv2rejuYCNHkLiuy6VLl3j++efvkrcuhjNzA/+1/xYJJLR0GlmvAiASiSCV5clA7t1uQToT/L8nT1eJBMp10IbH0Nt1ZH4I0YugtPo80kyguTbCc7GHtiEMg4xdw9H6DYBWaz6KVoxWhW6iSNKuYjpNqmP7KPlzIKGRGCVnz6JLN/i7ewddecxq44zKKRLYzOljaIkEE+kOdVUgT42U30CZiWBoY6fRN6sdHL6owC9M4GlG0CPl2f26lN1Cc20QOiifYjYdRVUdx8NFYaeyuJ4ipRMRFbBAtacZJtKxg1SgpoOSdLR0pPrTtSCi8n0VEZPnD6QSe89LpWirDF1Pxxr4lZp68FEpYYHvgG7hSXA8cDyFZTz8VOBaBASDEEKQzWbJZrPs3LlzwbiNK1eucPr06chBvlQqkc/n70oZuvN30G+9g95tIW0b5+mXSGwCooK1jSwJCTyeEtzHpiGrMO136dKlBZHCek4LXgr3I6vBCcODxq8rwUakAdvtNmfOnEEpxZEjR+7rnu3P3Ub+l38LdheRSoNlRaazEEjRMS20oSL05Oi0W4jCEKrXL0WzDlt3odd66sFGDZlIotndoDaUK0A5SN8JyyLlBfuxOjWcVBGrU0WTPnZ+DKvai64SObCr1Et7SFhaJH3XBkxiNTWQVhMDNTozx5ZUExDY9NVdKpFBuHbPoaJnVtuzQBJ2C2km8HruFww4SCyUqKcCM1rXjlKcuWyGpp5F1zTsAaf2ruuRNA1kb6Ivqm+CC4EIpU2Kjqf3iWkw1ddLBQaRVBBp+VKiaYKGm8XQA+JxPLCM4P+u31MnAigXsHA8SBow19TZWtyYm6N7Yb1d1xeP27BtO0oZnjp1CillZPpaKpUwu020qbPo3Saq63Ijs4Unh5ZPzz9srDUN2Gq14hEhA9gU0nXHcTh16hTNZpMPfehDCyw5HoYz+nL7H2w+/vCHP7zqu5z1lq7Pzc1x4sQJRkZG6HQ69ycq18X72hfA7gTj1S0L1aijZfMo14magfXiUGCJZCUQTnAxVrVKIKBoVGF8O0arijIthOsEPn5WGq33elEv41kp/OwwKbeKkxsnWZ8J9jPQ06IPjIpPte9QKz7GkNZA2i1cLUjvpe0ytpYkIbuknQodkSKlOgzJeboiSccaYjRt4wsDA58cDSQCDRVEVUtAmSlkIoMndEhmwG4F/VRGL7pyun3H9cGsuJlECUHLzEcS9GTCii7O+oDysNW1ySQs/LCGhaKjZeh4YY2qV5fye8QkFf5ARKVpOn6vObntZ7B9DV+pvrO8Bg7g+pAwwPGBXl3P0KDrmcy3DbYUbB6COnwBHmiM/AqQSCTYsmULW7ZsQSlFs9lkfn6eO3fuMHX5HSb1Dgmng+lJjAMv0rhwbVPZP62VrNrtdjwiZACP/BOtVCq8+uqraJp2l8EqrM55fS1Yjgyr1Sqvvvoquq5z+PDhNYXj65UGDMd6HDt2jCeffJK9e/fe1xlb+h7+f/191OytoP6UTEfNvqrdDOo0hSH00jDUK4HaTxPB2PfwuJ6PHJ3AaFd70Ur/Lk9vVlGpntWS9PGGt5MSHTQh0N12JK+2WuXIasnoNumkgzvednYLRjao+WlIOolAXSdQdK1i9HdNK0XbzJnbyWXA1HyqKtgmKWzaoueWbrcinz8GbJaUYQSpv8VX8dBNXvpBRAULbJaUVLQSJSR9fz9NCNzeR2rq/Z9PeOMggHqzTdVL0va06JALxBNhw2qvdgV96XrdzSJ6/V+eFCR7t5POwE8gUv1pJikTGnaCum3g+lBuPfyf9MOcZyWEIJfLsXv3bg7smODZgk4eH9W1uSRyvHz+KlJKbt26Ra1We+C5b+uBtZCVlDKOrBbhkUZW5XKZN954454CgYcxzXdwNpRSihs3bnDu3Dkef/xxdu/e/UBDEh+UrDzP49SpU9RqtSjq7Ha7qJ7DwXJrc7/2x6jZm4hkCmWYAdloAsJTKSVaItEXUgA4NlomhwrTgdlcsE23Z7XUqCBTWbROIN/2klmMThMnO0JKtVCaAdLFtJvYmWESrXmEUnipPHrPl0/qJo3cDnJGB1v1I0NL9lV3Cb8/nyqtgr9r1ijJjBlNghdioC9MM6L3pRJBQ69QEpUuooSGK3QI61lRv5Ra+N5DZ3UUJDIoz6Vl5QLCkz7SX+qGSaGZCaQbCD5CZjIL49gqiChbzTrpTL4nppCAhucvJC7fD+yU2iqD42sMPB1lKD0JSTOQrXfdQDIvpEeHLN1ePSyXlEzXdYazDzcV+CiMbLszNzGnL2J4NkoprEOHeWr74+xutXjttdfodDqcOHECYEHKcL0mH6wGa6nptVrB9z6uWfXxSMmqVCrdN732MGpW3W5wofQ8jzNnzlAul1ckXLgfHjQN2Gw2OXbsGMlkkiNHjkS+ayFBLUdW7olXUFfPgeuidD0wYHDsYNtcPphZlS9AoxZct0NBBUCrgSqNoXwfw24hnDYykUKzA3m3Mo2ovqRXZ3GGtmNpLobn0MmOoTd6ne8DZKJ3m5HgQSFIJQWagpRToatnSfpNkm6dtlkg7dZIug2qFClSJa9qTBk7GM755ESdlp8iIzoMUcVWJgnhkqXZ761y+qQnjQSe0Hp5NDNI/amAjPqpQCtQ7TmdKOWmhKCVKiER6IaJdPzA3083kL6H3pv0u3iKr2YmaJGi6xvoIhjlkcsVoqiq226TTGeDfysfoel4XlCjqjlpTCOMuBYSU4j+iBDA61JzCyihY2gKTwp8CY2uRtcVJM2HF1E8bCPb9tQNzNnLGG4nyBbs3AfbAyVi+Bs5cOAAuq7TaDSYn5+PhhwODp0sFosPbNe2Evi+v+r+0JCs4jRgH4+8ZnW/O4eHEVn5vk+z2eT48ePRqJHQnPNB973WyGpmZoZTp06xY8cO9u7du+DONfx7qVqBe+pV/Ne+2hvlIRCGgQpnEikViCfGtyLqZQIWI2iCNa1+M6xS6IaG5vRqOFYKel5+WrOGzBbQmjX8ZBZRKGDUA4Ky7L4KL9Gu4CYymHYLw+3QTI8jpE8mKWknS1id4DWulSXZCSI330yBG0wg7ooUqCpNo4ifLmCIQOzRJEeGDppQzHTy7EzNo0kXlcpDu4Zw2igrjTITuLoJqN4gyWUmK5vJgKykD1YaJT1aZg6pguhLDXx+oidz10SPxDwnGlePCJwpum7P66/X4Bs26/pSkcnmIuJyXRcrEdwotfwUGBq+26fAMIKUSpC2FB0nIC5NgKFDuZNCiuACmLZ86l2dtqORNCS3azqPjWxc6nwxHmYasDNzg8TMJQzfRnW7sG0v7NofPR/+3jQtmKQcTj6YnJzE87zIDurChQt0u10KhUJEXrlcbkPeh+/7960vL0a73cY0zXW5Dr1XsGnUgMtho2tW4TTfo0ePsnPnzruI4UH3vVqyCqe3Xrt2jUOHDkU+XIv3C3cPjXTv3EAd/8vg4qyJYEKv7wby9JCwEgm0TjC+I1L7+T6kghlWKplBEz5kCtEUXL1ZxU/n0dpBetDVkxiWg5bPYbZm8YwkhtdFdzt0siMkG4Ey0E/mMHvzrTxhkUu76HhYXl9OnnRrkbghYdfwex5+Q1SoiSJWPktRtJGq95a0ThTMJPSB74Xop1lkKo+r6YBChM7pntOXti9IBQ6QmGHR0ov4aoCMQgn64lSg6CsINTNBWwVEFYoCF9So9H7Tr6ELPF+RSKQQAjoMI0QwWkUqgXSaaFaWjqMin/VwVwpImTDdSATnSdlIkaDrab1nBZYBM3WdXSUP/SFl5jZaYBGifesKZvkahteFTge2TMJjC3sYwyhvqfWEQyfDad2hg3y5XOb69esIIRakDFdLMMthLTWrZrNJJpPZVKNXHjUeeWR1PxiGsaJppWuBlJKZmRmazSbPPvvsPU0Y14LVktWg+nC5sR7hfmEhWXmNCvLPvhCIJQBSaWj1Z8eIbA5ld9EsCxwbGlVELyUIQKeNN7wFU7nobhdVm8NPZdF7Uc/gZ2W2K/gTu0i1A7LzskWMXsOvPjCU0GhV8ISOZ2bJpCS+sDB8j4TXoGMWSblVTL9LyyoFdkrKZlYbYVTewRcG7ewEWa0G+FRkgZKokRUt5roZRhItRq06NgkS2NANojqVzOJaySiSUtFIRoKU33KpQKVoG1n8JQrykY2S9HE9iWlo+L33KYTA1jJ03eAzMQ0Nx5ULnSkGilCaJsBXCE2j7afw0dH0VEidmJaFT0BSbqeKmSrSdcEUgRij0rHwZU9hiIskgeMJsgmfpq3TtINhIjN1na3FjctIDOJhRFbtqSuYs9exfBe6bdTEbnj82bu2Ww1xplIptm3bxrZt25BSRinD27dvc+7cOdLp9IKU4Vp7ydZSswrJKkYf74rIaiPSgKGfXrfbJZPJrDtRwepqVvV6nWPHjkVjPe6V4w6HRoZk5dtd5Fd+H9FpQSaHMk3oNf1G6LbRRsagZ50UPNaBRBLsLko3gppJmNUCxKBxbbuBnR3FbJXxh7cGY+97sNoVpKahSYnVreOkipidKoZ0mM/sIp/0sHCoW0MkOoF83TeS0bFcNWBoq2m0VQZVGEKj/7g38FVtyRQjtNAEKCsDdtBb5edHcTWjZ7o7IEvvpSYXjP8YnEtlpWhpKXylgqZezw0k6D0KGbwpkFGPk0I3E7RUhq6roxHIzAesAu/qo/J8hedJdE2j3E2TNHvGtWGqzwUfCz3gM1LpHJ4KPo1abR7HmAChk9Qdur6FSwrRo+OQkqUS5JM+5ZbGloL/UGTsGy2w6Fw7j16bCiJyu4sqTsDepV271xrlaZpGoVCgUCjw2GOP4bpulDI8d+4ctm1HdlClUolsNrtigl6LkW273V7VMd4PeORkJYS4p7x0I9KA8/PznDhxgtHRUSYnJ7lw4cK67j/ESmtWoSnuvcZ6LMZg1Ob/+eehXu6Z0yYDotJ0RDKJagfkIIpDiFoZCkOokLCkRCRSSNdFDI9hdBvY6QJ6z7lBa9Xws0X0ZhUIhAfe2E5Sbh3VbOFZGQynheY7dDKjpHrpv47IYFLF1VMY6RSWDCKwpFuPUn4pu4wvdHTlk/PK2JgkcEnTop0dp6h3MKjhKh1T+BS1Op4UGJpiJNnqjcwAS/S8DZNZvFQ+kJ5D4Mfn2QSpwFQQRfluMM/K7fbHf+gGbSuH7wU3FYPnXjfNwP/P99A0PbB86s0SE5pGV8tENSrT0HA8ietLNBGIILwlIipN02n5SaTSeqk+tSDVB4G4ouWApwL3CiGgldwCPV/Bbqcd9MyhkxAdbJWiZWskTUnX1ZBKcKNsUMoothY3vna1kQKLzq3L6OUpEr4N3Q5qaByeWt7Ze71SkssNnSyXy1y9ehVd1xekDO9VW1pLGrDVat3Xzu39hkdOVvfDekZWYb/S5cuX2b9/P9u3b2d+fn7DBBz3SwNKKTl37hy3b9/m2WefjXLpq9m3+/J/hpsXg8m+iSSEKVPpo7o+ZPOQTCMaPYKqV6AwBLVeurDTwt+yk2TPPy/RruFlh9B73npiwEVCpXPhDEEECj+Zw3CCmpTh9VV42e4dOkYOCsMUVZmOmSfl1rFkl3ZimLQ9h648auYIBecOOj7z2jgFWcHNjeCQBjoY+MzKEqN6GVN43LILbE9Vyeg2d7o5xqwGmt1CZoZwrFRgjxQWjQbSkX3ZAgGJuQTbpHI09TR+j0SU9BeMoB+8h9J6qUBdE7i+wjeL2J6+4BghTEPDdiW+VJiDEZWuMddNRRGVIoio2j3xRGil5AxOK9Fgqmah0MglfRpdHWEVUMoBYWG7RL9i32mT0E1uVjIMpX0u3jEfClltVGTVvHYOqzpDwu8GNddMEQ4cvudrNqJ+ttTQyXq9Trlc5ubNm5w9e/aeQyfXWrOKe6wWYtOT1XpJ1wddMl588UXy+aCRdCP9+8KJvkv9gGzb5vjx43ied9+xHktBCIF+6mXU5dPBBTiVDFI+mh40/YZIJgNRQU8kACCaNVQqHdSpRreRaFfxrSR6T/atDcjOtU6TTn4CLZki69XoWsPRc2ZrHl8z0aWLaTdopUbIdOYAQbu4ixEVRFqOSJKiN/Z+8A58wMHB0jya6a0UjS6+6t+lmtrAbKgFvVWBRFkl0ji54SCiUqo3ij6IovqpwE4/FRiSmG7QsQrRd0szjKAutWwqMNhOSkWbNEoGFx/LEDiewvEUWi9LMCiu0DQNfB9N12l6SZTS6DoKTahAUBFtKrAMhecELhVJIzhX0w0rqmdJ2Sc5Q9l4wkLpKTKmT9vVcX2otgzQoN5y8UWCG7Me20f0DU0nbURk1b5+HnN+ioRyAiVqpgjPfMfdjd1LrGWjxR6aplEsFikWi1HKMIy6BodOhuS11sgqrlktxCMnq5WkAR+UrAbHvC+eibWR0vjFk45DhGa9pVKJgwcPrqlwW3RqGDfOgN2LJlw7EE5AMNW324FsAdGsBxe7dBbRbkRjZ5XQ8Ee2kOgGUnGVSENIVu0GXq6E3ovG/FSGrBtEWon2PG4ii2k30aRPd6C3ysNACp1OaQcZrROMXAIyXhVfaOhKknIqkbVSXpZpkSKBg0rnMfTge5ATDVp+kozWJU+dpmuRNR1GE01sXyeh+xSNJr6RwsmUgnRmdGYGvktGmAoEkUgHzc2+i0pkaJs5vIHPffCGZelUoI9mWNQdC2kkgppesHV0TDOaR9WvUbleUFyf7aRJDUZUpqLlQNcVmLrC9QV2MJkFIYJm4Js1C9AoJH0aXUHL0UgYEtvT8EWSsMlYCEHKlEy1i5RyHuUW+CKJpWqcuqG4ceFoNK13aGho3XuL1lNgIaWke+0cojpLEje40Uhm4OmX7ktU4esfdoOyaZqMj48zPj6OUop2ux2R1+XLl5FScu3aNcbGxiiVSlEv2L0Qk9XdeORkdT88CJkopbh58ybvvPMOe/bsYXJy8q4f1UaS1WLV3qA7xt69e9m1a9eafuTezYvsnz6N8JwgqjJM6PRdH2i3oFhCeG50ERftJuRL0HNL9xPpQBnYyxoarSpupojZqgZrVz4KgTO0lZxXo5MqkeqUEYBvpTHtIHoznWZUh8rZs1SGHmOYGkhoGkNkvQqGcmlYo+TsO2hIytoII3I6GBwo8rRSGUpWlztekSxdBNARWTJ00YSi5mfImg6mJplyCmzVq3gYqOJY4DzuOZHpLE4wAoVFggqlehIEzaCbLEZO57ph4ntuQEa6jvJ9/IFUoIzG2WvYWg5laAiCiMr1JI4nByKfQWeKgKw0XaflBhFVZyCi8gaCeUsPoilPClKWQinB7XoCQwvTgv3vSMJQ2B4oYWLRxiHVawYObnhqnX6TsG7lcFzB6M5nUe2bXLp0iU6nQ6FQiMhrPYr465UG9H2fzo0LGJUZkniB6i+RDSIqbWU3dGsRM6wnFg+dlFLyF3/xF1iWxY0bN3j77bdXNHQytlq6G5uerAzDWJPAwvd9zpw5w9zcHM8991zk4LwYuq4vm6p7UAySle/7vP3228zOzj6QO4bfbaFe/1M01YsmUoGaj3wB1WwEtatkOnBy0LT+RRygXkFmiyghMP0uotXFT2TQe71QAUEFsYLWaVIfeYy8rAUGkgMXtERrHt9IoHs2utOmmhinaE/TLE6iWcmosdcX/QhWk30CsPxABCERiHSeUjI4flbrDGzTir6daWNADq9Jmn4SPx9ECMrvSchDJwoGU4Fe0PTbE1QoI0E7MYSvxMI6VrhGvZdyVqpPYp6Hphu0RBZfBr1bwenov9oygxqV6ytMTeBJhesrdF1nrp0iZfUjqqQZ1KhsL0j7OZ6g60FUWVOCqUbQ35NL+tQ6Oh1XI2v5tB2Npq2hC4WvBBKdpAHTdYtiKrjh8qWglPEotww6rsZQ2uN2e5iP7E0HfV09ocD8/HwkFAgHHpZKpTVN4l6PNKDnutg3LqDX50jiILpdlJkIhieuIhJ8FJHVSjA5OUkikbjn0MlSqUQ6nQ4mQ8eR1V145J/qSiaM+r6/KkPKVqvFN7/5TdrtNkeOHFmWqGD5Btv1QNic2Or5lTWbTY4cObJmopK+j/p/vgDlO2i+FxjLtluBCrDdRCSTkCsiDA3hewjXCar2Az9eT+houha4lCuFGrgQ6N0WbrY3yTU7QtIMhBQAqW4FJ9kzjFUSN12MXmcIRTn3GHmtQdqvEyblcl4Zp0dYGa9KSwQ/vrxoUteKzCR2M5xqU5fBHWRasyl7gaNJ3rSZt4M63pDRou4Gdayk5tBMDmMZCjVgraQWOFQMKiP06P/d9DAeAiUluh687yCKEtH5jdD7XgqhYet5HF8PeqZ6aUVnIDQajKi0Xieupuk0vRQKjY7TrwN6cqBfrfex+FKQMsEyBbcbCSw92HfHCZt9FzpaZJISlMT1VNTfVe1opM3gdfVedAXQdjR8H67NB+837C16+umneemll3jqqadIJBJcu3aNl19+mTfeeIPLly+vygT2QSMr13HoXj+PXp0jJW2E00WZFjzzV4KswSqw2cgqvK6EqX7r/8/enwdbdl33ffhn7zPe+Y3dr18P6MZIgABBAiCBBoiQsjgoCcNKfnHEKpbgKFHR1E8u0dDwE+PYqVI5KUpUXHSVrcROlaOyI6dC1c8Ko/hnkgYlm5MxEAS7AQJoDE2i5zfe9+58xr3374997rn3NXpGg2g5WFUPfXDvPsM959699lrru75f32dpaYm77rqLRx55hPvvv5/Z2Vna7TbPPvss/9v/9r/x2GOPsbKywksvvcShQ4cIw5D777+f733ve5c813e+8x3uv/9+wjDk5ptv5h//43/8pjF/+qd/yl133UUQBNx111187Wtfe9OY//l//p8veV5jDL/7u7/L8vIylUqFj370o7z00ks7xiRJwq//+q+zsLBArVbj05/+NGfOnLmqe3e+3ThP9SI2Xfe5EltdXeWpp2yO/kMf+tBlu9DHx387KZ2OHDlCq9XiwQcfvOaueGMM6lt/jGmvQaVOXm1YZzRteY7wfZsaLEwkMaZunYzyK/iuQNcmFFfuqIeqz+04RhK28DxJmPaIKhNHr9zJtXvRxCnpoEboF6q3JqbvTpjSh85MuU9fV4vPAoPKLhbrBSejmOTwe/Hk2lMxOd9AhYxyjyicpZcUDshoSwUFFlDhFcCMNJowWmQxSIdRZR411bclpiY017Pn1Foji8lRZRlIyUhaR1VaMYEbYyMqwEZRY8LZIqJaG4YFOMNGVJVizh1HVABRZt+1/5Ws9QNAUCl4/VIlaASFA0okvmu341SQxhHdtD5FeisIvIlDbBaRlivtxR49FZCdl6CQUjI7O8stt9zChz70IR555BH27t3LaDTi+eef53vf+x4vvvgi586du2Rj/luJrLIkIT39Ok5/i5C44HZ04H0fAf/qqYZuNGdVAngucE1j0ckDBw7w/ve/n0cffZS7776bZrPJt7/9bb7zne8A8F/9V/8Vt9xyC//hf/gf7pCxn7Y33niD/+g/+o949NFHOXLkCP/tf/vf8oUvfIE//dM/Lcc89dRTfOYzn+Gxxx7j+eef57HHHuMXf/EXeeaZZ8oxf/Inf8Ljjz/O3/7bf5sjR47w6KOPvum8f/AHf8BXvvIV/vAP/5Bnn32WpaUlPv7xj9PvT0gIHn/8cb72ta/x1a9+le9///sMBgM+9alPvaV59h2VtQdLHnupD6C15oknnuDnfu7nLtnLoLXmtdde48yZM9x9990XpCm6mP3rf/2vefTRR69rX8NYTPK1117jlltu4bbbbntLx8uf+xbmpWdsaisIMbEVPsTzJ1DtxgxiNLDy8kLsIHWN6guErsHNrHZTHlRwx5x8QQ0ZDxEYMq9KOreXRlTI0QczhLEFWhgEyvFxCwb1briEkh5z/ogtdxdzuUX/9Zw5mrmtjY1kjWohNz8yFUIiVtwDzNYVDjmOMCTaxS22o9wlEBmONIyUTyhipIB2Wiev1GnWBN3YYSa0M+8gyakHhYPzK4gipSn8GiYZgHSIqotkelJ7MkYXwB7rbh3XRRWpZtfzUGliIyq3yWjcR+VArg1GKxAW1DCuWwEERSrQdV36aUCUSQTWiWkjilqTndBrBVwdoOobUuWwPvCpeJo4l7jSoLS93zVfM0ztRNcKFcPEQjuGwyEJVk6nVbHcgGCo+ppR6iCFoRkqVrZdjAHPNRxcyPnATVfGBmOMKeHZ7Xabfr9PtVot04UzMzPlBPxv/+2/5cEHH7zq308axySnX8cbdghMikyGGByb+qtcG9v4iRMnGA6HvPe9772m/a+3jUYjnnnmGX7u537uqvbbv38/t956K1/4whd44oknyrTtf/qf/qf83u/93pvGf/GLX+T//r//b44dO1a+9qu/+qs8//zzPPXUUwB85jOfodfr8Y1vfKMc8wu/8AvMzs7yf/wf/wcADz74IPfddx//6B/9o3LMnXfeWZ7XGMPy8jKPP/44X/ziFwEbRe3evZsvf/nLfP7zn6fb7bK4uMgf//Ef85nPfAawvaT79+/n61//Op/85Cev6l6M7R1fglxuRTZma7iUQ4vjmGeffZbNzU0OHz58VY4Krj/IIs9zjh49yqlTp/B9/5JpyCs63o+fxLz8A+uoXN8yNozXGFlq61QLyxZEARZYIcREz0lK/EqIHEPXMaU+E4CTDMlaiygngMYMgUnKRFqYdIiDmXK/vDLRGzNeyKxXSBmoDmqc/lPbxEVUVNVDOtpOPFURsRYeYncjxhcZPWMjvkDmrEZ2TMXNWSm2q07KVt4kVi55dYbEWKfUDJWVIwFqgVvCv9PRoExdaZXZiKq6gHam0J+u/dzGGNxxFJXnhWAitjlYSIaySW6mUqRFHs7yBNqU4xiuDjaicl2X9WEwYcVHEE5FVMFURGXTq4bcSNYHNrIcR1y5FjRC6wSH6SS9N8okxgi2hi45IeOobBr+7hWPtRZookSQKUGuBbVA8/qax/YV6l0JIWi1Whw6dIgHHniAD3/4wxw8eJAsy3j55Zf53ve+xwsvvMCZM2euKQ0YD4ekp4/jjrpUdILMYtAC7nn0mh0V3JiR1dWifdM0pdvt8sEPfpC/9tf+Gv/8n/9zvvGNb/CJT3yCJ5988oL7PPXUU3ziE5/Y8donP/lJfvjDH5YSSBcbMz5mmqY899xzbxozfd433niD1dXVHWOCIOAjH/lIOea5554jy7IdY5aXl7n77rsvev1XYjfOU72ICSEu6Uy2trZ48sknqVQqPPTQQ9dUlLyezmpcL8uyrITJv5V6WH7yGLz8lJ0kKzWbGqlWJ0UMgNYMcrCFaUwciUgTqNYxgJpbwo+6qPps+b4b9cnrM5NjpBlZa5FAR/hpn6iyUL5lpif7URclJJ1gD013yNCx5/RMSt+xTllgiN1muU9c1KrOyv3IYJLym2Z4mL5Fyky+lon26AZLNGqGblRIpACiSPkJwAnsit53nVK2Pk9j1nVIZiR5npfOd8ezmIaou5MJJXZapMopnJF9LVOGC+UgxrIeQjr00hCQRBlW3wqL8ivPUXwsbQQV39a1VnsBlcIZDWJZ1rayKQSg6xhcachyMRFzJKDm2gi3Hzu0QnuibuQwX8tY2XLY6Lu0qvb1zb5LM9Q889OQa/k6juHZd955Z1lrabVarK/baPpHP/oRr732Gpubm5f9LSWjEemZ13GH21RMgkgjjNKYez4MteYl972cXQsP39tp13I97bbNSpxPAbd7925WV1cvuM/q6uoFx+d5zubm5iXHjI85fnaXGjP+93JjfN9ndnb2omOuxW54ZwUXplwas1E899xz3Hrrrdxzzz3X3D9yveTn19fXeeqpp1hYWOCBBx7A9/23JMCYD3vw7BMQDTFZaiOrUd+CKhzHMlbMLiJHfavllIxss+/4c40GZMu3EMQ2Def22uThZNUqVW6pfoREV1uYsVou4JgJYCGMtsgKBV5PJ3TrB2lULa3QdB3LYfKMvKxfTu6zYpt1dx+7WhlN2SctuADn3D6j3D6zPbVBmXLbUxuQKEmqHUylRiW0B9rdzMpjmh1iiBMv4gUVEJKssUQwNfGNRoX4o1Klk7JOrKi7aY0QkoFooMXke+S59n2lLVzd3jivRBLm2iIH14cVCtFfjBFUiox1kgv8on8sLiIqgSHTDptD63y94n1lBPWiRhVlknqgiv0kSlvgxDR4ItVe+dkzZfsVW6Gi3XcmTO16cn+iTNCPJMdWrh7xN23jWstNN93EvffeC8DNN9+MMYbXXnuN7373uxw5coRTp04xGAx2ADXi4ZDszHH8uE/F2BqV0Rruehgasxc75RXbvw+R1djOF4q8XD/b+e+N7/v06xcac/5r12vM+fZW+/He8ad6JRd/fuSTZVn5Y/jQhz50UZXhK7W3ymIxlvV4/vnnee9738t73vOe8gdzrY5QDzrwxB9DNLCIqFqjbPoFQCnrrKZ+mEJri/ArfhzZ/DJB1EE744jE7BjvJCPixh7S+QNUdR9PJeXkHaQ9onC23C8tpOYH3hxe6OAIe7/qqlOSzDZUh5G0UVRNxLSN3b8tFtAV+7orNJtpwR4iDOsj61xdaViLLCrQk5qzoxZtdzdzDcWZLXv99VCjxoq+eVoixUwaTaToVcaoMk+GRE5NErUp3bTBoGD4mJpEjTFEokGqHJL8whGfKBGCogRXGFx6ifVMk/SeBWiW97m4DEtYC0Y4rPV9qr4d1E9k6YDibLpJXuA7mlEiSkc4DZ7ItMdcETkNU8l8LeNM2zq0xYZ9vRc77Cq2o1QyX8954aTHyvb1iT7Gv5uFhQXuuOMOHn74YR588EEWFxfZ3t7mhz/8IU8++STHjh1j5cwpkjOv48YDKqSIJIZMYd7zELTeWqp8+npuNGd1tdczRgufD2pZX1+/KOH20tLSm6KW9fV1XNctyxAXGzM+5sLCAo7jXHLMuMRyuTFpmrK9vX3F138lduM81UvYNOVSr9fjqaeeQmvNww8/TKvVuszel7e3kgYc53lXVlZ46KGH2LNnz5uOfS2aVubJP0MkIxsEBAEiHiIarRIhpcIqTp4ghh301IpU5CnUm2RzewjSHlJlqPrkHrlRn6w2GZ8HVSq5jbz8bMgonEIGTtW1/GibkT+HW/Op626Z/nNMTs+d7DPUExBMhsca88y3dn7+YIpCqepNIqSKN56EJZHXoFW3k3Y4havZQdfkTFKKwg9BSKJgBu3aHfTURGG0LiOqacHPKI7J8pytJCQucnbnI/3GGcI0N5iCVl1rg+N4bIxC3Kn0XrW4pHi6RpUWEZUwxLnD9sg62fF+xtiaEkCSS5pFvSpTkOWCUeqwPXII3Ak0XRSR7yCxYIqZimJl2ytTjd1IlhHb5sChHihaFcXZtkM90Hz7WMAoeeusExdavY859O69914effRR7rzzTjwHRPssaXuVpH2W0XabNI7Rt90Hc9dP8eBGdFZXG1n5vo/rurzwwgs7Xv/Wt77Fww9fmMT38OHDfOtb39rx2hNPPMEDDzxQ9s5dbMz4mL7vc//9979pzPR5Dx06xNLS0o4xaZryne98pxxz//3343nejjErKyu8+OKLF73+K7Eb56lewsbO5MyZMzzzzDPs3buX+++//4poS67m+FdrY8cphODw4cMXVD2+ljSg/sHXYf2MZWao1i07hTG2l0ormFnElcJGUoAYdMjDSa1OOQFiqjbk9jfJq5OUmFQZBsFo7iZaapu4NiHQdfTEeYTxFqlnj5tLj7i5m0DY95WcHD9ksgKsmyHKjMEIgkojQEqYdXsMc7vPnD9gu4hGdlVGbMV2e6k6pB0HnM53cdNiTjeyX8+FWlpGfNLocttMk9VqTVSZJzWS6Xbf6ejKGUdiU3WEarVK4s6CG9pm4WLyjeIJknIMrjCGkuhW49ItPsMoE5OIaprtaSqiqvigjMvm0KM2jqjiiUMZpbI8Rq4EoafpDOXks045tFwLPCywJVWC2UrO6bZLkkvm6vbYaS5pFRGYNoJqoGn3BGku2BrYVOJfvBiSv8Xs9/h+XcxBOI5D4EoWTMqMMCxUJKEjSUcxL8UB33/1DV566SVWVlZI0/SCx7ga+/fBWYF1/n/2Z3/GH/3RH3Hs2DF+4zd+g1OnTvGrv/qrAPytv/W3+Gt/7a+V43/1V3+VkydP8pu/+ZscO3aMP/qjP+J//V//V377t3+7HPM3/+bf5IknnuDLX/4yr7zyCl/+8pf58z//cx5//PFyzG/+5m/yT/7JP7noeYUQPP7443zpS1/ia1/7Gi+++CK//Mu/TLVa5bOf/SwArVaLX/mVX+G3fuu3+Iu/+AuOHDnCL/3SL3HPPffwsY997FpuI/CXgMEC7A/h1KlTDIdDPvCBD7CwsHD5na7y+FfrrMayHocOHeKWW265aBryao+tnv5XcOoYeD7GLyTXp/jncFyEzjGVOqJfMKNTOCDHRYV1PBVDnKEdH6lS69DOS/91529lJreFVycblZRJYdYnCmaoJB0Ehr5s0XA0qj6LbyZOqVY0/Pomo6r7DGSDuu5TEQmraoHQM7QaDhtpk4q7jRTQVTVqBRtFJ60wG9jjddOQuTBBGcGKXuSWXdYhbvQ9WpUE17GRk0kjJAZZbFuGigDyjMhrkGHh23meTwhop+79NEBCSIkwhoFpIF0XMo10PLyC+gjpo/IMx/WI4gwpnSIyE6SZYpA0qAeGVFlnUPcnXH9jmHqUFkALAVHm0o3c4jtRXA+Cqm9ZKjIlaFXsNtheqiR3SHKoh4pRYqOriqeJMklKE09khL7kdNsrIOuSjb5TsrNvDlzm6zkYOL3hsNhUrHVdci1QWtCP4N+8FPKxe+IdeJ2rsfFC7GLf/1F3m/TcSSr5iKqJIMsJXYfgA49yz+6bSsHDafbyMTz+YlREl7ueG8lZXQvAQilFlmX89//9f8/f/bt/l5WVFe6++26+/vWvc9NNNwE2UpnufTp06BBf//rX+Y3f+A3+p//pf2J5eZl/8A/+Af/5f/6fl2MefvhhvvrVr/J3/s7f4b/77/47brnlFv7kT/6EBx98sBzzmc98hna7fdHzAvzO7/wOURTxa7/2a2xvb/Pggw/yxBNP7Fis//2///dxXZdf/MVfJIoifv7nf55/+k//6VsCv7zjfVbGmEuuqEajEU8++SSu6/Lggw++qeh4PezIkSPMzs5y8ODBy44dy3qcPXuWe++997KyHkePHqXZbHLzzTdf9tjqpy/Cc09YfjsvsLB0o8FxbUd/NMLUG8i4EDCsNnAG3XL/bGY3bhaV0VFan8MrpD8AsmoLb9Ahau7B8SR+NixX83FlnnBkx0bhHJWo2JY1ssYCDWydZ+Q0qOb2nB1ngdnc9mOdyxdZlmsAnFbLzM8ZPGnYzmrMujbN2M0qtFx77d0koOlF1omlAXU34US6hF9xWajb6GWzP9kWXoApuP6EH5a6VSKoErlVEuPguS55AdP1XJe8iIKmI2cBYDTScempmtXLckTJFRh4giQr0o+eIM2KaCaNcP0K/cGQzJlBOAFO0Q81buSNC0xKzTcMUzt51wNNJ/IZJC6NUDFILCuFpVqSONKiDLURhK4FrWz0Haq+KXqnYLY6cWKzVcX2yAFjqDpD1gZ2gpirKbaGdkwjVAxiG5Ut1jPWe5K46NXa1cxZ71mn2apqQlfjOoaPvy8pU4hXY6PRiB/84Ad89KMffdN7W+ub6PZZ6vmQqo4gSTBZijlwJ+y/403jp6mI2u02WuuShmhubu6KfvtHjhxh9+7dLC8vX/2HeRvsWvq+Op0OBw4coN1uXzPbzb+PduMsQS5g6+vrPPnkkwRBwN69e98WRwVXngZMkoRnn32WdrvN4cOHr0h/6kprVur06xNH5biWkWEsO6tyRDxCLSyVjgosO7qu2slKeyGuycmrk/qUP9hCTf2/VIphdRehTAjUiFF1cv3T3H1hvE3kNFDCJa3Nk8kJwlBNsU34akKeu+BskxmHnmgxMyvIjE25zXpDupl9bi0vYn1UbAcJ65FNMTa8hNfiZfYsCubripWOnUwXGvkERJElZEWOzWSJ7SETgtitkRToQjV1n6dXYOMeKrANwEJKerqGKJg+skKCHmxdamzTZLO1Wg0hPXJ/N6bQ7lJaQGbvQZTtZKYQWLj5IPEZlIwb5RWVLBVqqqfKkTYyy7WkNwVH3x45JTJweyRpBAqd9lkbNJip2IXJ1tCxURQWyr7QVCzUc05tulQ8Uy5KNvsOczV7LN/RDGLBuS2H/99zYelsr8Yuxl5x8mSbbOMMLTWyjiqNIUsw+99zQUcFO6mIPvzhD3PffffRaDRYW1vj6aef5umnn+b111+/pAbdjRZZXUsacFQIpr5LZLvTbsg0oNaa119/nVOnTnH33Xezvb39tmlOwZU5q06nU0Zg999//xXD5K8kDai6bfjxtxEzC1CpYbwQts5Zxd+CUkm35nG7m+j6DLJQ7hUAeYqu1DFBBTcbIYxGOx5SjfXpp3gBpY+ut5CJjZrcPCrTf37S25H+GzpNstocTTliOLWmqeXbpMLHNylVIrb0DHOygy9yTppl5uuawFGsJE2qjr32oa7QwkLHbd3KbsfaQxv4SbRUkN7a8VZ91068wvFK3r+0EDPEWPXf2K2QGFmSHWutcYtnOZ0KVFMwdwP0TYNMOwRTc4jrWAJaYybRVa4MfiH1kWqXTmIdp+PXChF7YdkvimNEwz5O0LT8fb5mexQwyqyjGSQO/ULNN8kl/XhCSBullqj2XNel6o2vUnChlIcj7TuDvIkQkBtZqg3344mEiFYwKrK220OH3a2cta6LNnbcUivj1GaB4gw1/Vjw/32qwsfuSdgze+W/tfPhyJsDybmVTfbkZ5glwlcRpCnECXrPLXDgPVd0XCEEjUaDRqPBwYMHyfOc7e1t2u02r7zySqkZNU4Zjglg/31wVsPhkCAIrruUy192e8fvxvmrsiRJeP7550mShMOHD1Ov1+n1etdd2n7aLuVQ3qqsx2XVgpWC4z9C1uoYpTBhaDv4K7dAcwu6HfsDLJR75bBrHc44/acUam4vQd+m4KRKSWtz+H3bWOiOumT1eVSa4gYuQselgwryIVF1gcpoo7jYgt0BgfFDQmlXeDUzZODMUlfbSDQD0WLO2ONnBVHtkBqyViEoUn51ZwJQaMohyoAjYKk2KuXpd1WGHI92s2+3IEozMmVpjW5anLC/TxPUeuPCihAkXo1Ev/k5iELsECx1Up6lBVuFi9KaXl7BFLyBSWbKamCqzOSc0zy4UgAe7VFAPdAMUzDCpeoZRhkoEeA7hlQJnKCBMQqjMlZ6Bu1WLJH6FBQ9cCHJbdpvXKPyXUOUSIyRDFOYreZsj1x6scNMRdGJHHqxw3wtpx9Jzmx5hGKbhFkGsWRXI2e975LmFppe05ozbYfQ06XU/VrXZWkmZ61ja1or2w7zdUV74NCPJa5j2N3M+caPfA4saB66I6MeXr5CMGavWOtKfrruUlGr7FUnmRMxnoogiyDN0Ms3wy33XvZ4FzPXdVlcXGRxcXGHZlS73eYnP/kJvu8zNzdHmqZXRXr9dptS6pI0cReywWBgI/m3UTDzL6PdOEsQrCjhOO03dlRw/dSCL2YXi6zGMiPHjx/nvvvu4+DBg1f9BbpUGtBoBa8+hVQJVJqY3TfZmXLYAekgwir5re9HL920M6017KHDGgZbpwoGGyT1CejEG26RT1HWKByo1nBNRpAPGYaTnpad6b8tBrLBdv0m5r0+XTkFcZ8iga2JCR3TnNxmWzXJazPsrgzo54W8hRvRTm2ar+5nrAzts6y4OSsju30mnrMUT0DFN6x2rOMTaKvyC6ALiinAdwQZkqQyR6Rk+SymFzK5Uhdmq0Aw0DUy7ZQpP7BRFNim38CdpALH9ZtUu2yN7PmndaUmgEOBXyz5DJJ6KElMA+3N4gvrsIeJA4UsSi+a9FQNE0ndU5zddhhlEzRgkk/Y1u05Tdlv1SnoklJTxxUFO8XAOiApDLkCpcY9W5LQMzgFK8ZGz2GplbHasYCOzkiyp5C9bwSa1Y5D6MN6T/DP/m3IE0d8frLqXDA9mOVwblvy/JkaZ/L38exPfCr5KvvNGeZkgpdHVromTjBze+CW97/5INdoY82o/fv3lwSwd9xxB47jkKYpL730Ej/60Y84ceIE/X7/HXVe1xLpjZ3Vu7bT3vHIamxvvPEGx48f5/bbb39Tk+/bKZA4Pv75DiWKIo4cOYIQgocffvia2dIvGVmdeBGxtQpxH72wz06A1Sb02xAPUa1dZAhUfQ69fDuNc69NFGrzlGh2mXpiIywvGaAcD0dZwUVT/ECUEyB8n8ytQmIjKNv8a48TpD2ScJYg3kZgyWn3uDaKq5ioHNfU2yQiIDAJgR6xpWeZk9vkuHTCJfZ69jr6qkrDtZN0P5bMFyUufd666Phokb17HE5uTF7bv9uBIpKS0pk46CLi08YwkhUoJOUd6ZAXKb5xKrCMogqCZEda4qNuXiHXxUQ/VZdSO/zZBHXpOQJlPLZGfkEma2HfQo/AqTBKKUUOoxSsQ4Fu5JHkhRxE4Ns+biGo+IJI2ag1i7YQwRxCx4xUAEjibAKSGKVyx/ZCLWez7zCInRIgofFo+jHdxEEbS8PUCBSr2w5CGGZriu2hQ2fksNBQ9EYC39Wc3PTYM2sjK6UFKx2X/fMZm11JkgmSzP7uFhqKOIenXnXpRz6BZ9gzZ+hHAtcxBB7kWqKUQDgZB2onWM5XmJMxThZDEtva4sI+eM+HLvz9v07mOA7z8/PMz8+zvr7OrbfeSp7ntNttTp48WWp2jf+uV8vLldi11qzejazebO+4s9Jac+TIkZK4cWZm5k1jLkS3dD3tfGfYbrc5evQou3fv5q677npLOfCLOVqztWLrUkEFXZ8FncKwC5UmhHWMXyVxKyRJRCgM7L8VpRPc1ZMApI0FXGfyZZZ5SlKbwxnYepQX9Ygbu9CuT1UPQTtT6b8Bo2CeamJTebpghdgMD7AYDMiMi0dOlSFdMcOM6SAxxG6LILNccEY6xNpnWFukKibPpukO0UYghWGpOiJVlm5oT21IlDtUXEVkQmZaHqDZN58ziIVNOeUJY5VfnSWAtFFWnqK0oUuIG9bKNN0OQMUOXPoUvYzj0stCMuVQ8SHODNrYKCrJTdn0mytId6QFPTqxjfS0mTqeMWP5RULPMEgmNEmrvYBMSRqhYpg4DIoaVZxJYh3iOZpMSZxgFtSAjXgGoROQOQiXQUxZfxok9h4GrmF7KEmL6Gaz71AL7PE7cchCI6cbOeRT0aAxgii11zRIJP1IMFtTnN2yk+bKtsOulqIzkLSqmpPrDlLC3jnF1kCAsPRNJzfGiwJDqwZn2g65hr1ztsY1WzckKdzUWGOf6TDnxsh4hMhSTJpCc/Ftd1Tnm9aaSqVCs9lk7969aK3pdrtsbW1x6tSpUql37NwajcbbWuO61prVu5HVm+0dd1ZSSubn53nve9970RXPzyoNaIzhxIkTHD9+nPe85z3s37//LR/7QpGVUQpefxZGXUxrN8b3ESKE/pZ9rbnASAaYNLITpQChMvSBO9HDHkr6VFJbG0rr8/gD63T84TaZX8NLhxghyMIW9dQ6ryAfMAjmqRfgCsHkfoZxm7XwEIuhhadvinkWjA15jJAlii3IB6UjqjNg09/DriABcrbTKrPeiJqbcq5fZW99SOgqTnYb3NTq40nDmX4DLT327vU4fs7htj0aR0KtGVoNqqKHSieRVesNQnQyQmtFhwputYkx4LoOea4si4QUaGN2fD/yPLfCl0IwyKslKWy+MytYmltI0Bus1Eecu2xFftG7JEqkX5oLtKwgjAIxTo8ZAhc6kUemzp/0BIGriTPrQKq+phtB6BkSbCrUyICGP2KQuaTKwc3b5O48aS5ZqKasdj3iTLLYyNnoS7QRBfOFBiRxKghlzkbf/pSXWjmrXZc4k1QDzWwtZxgLTrdd5uuKXiTJlGC967A8k9nPZCwg4+yWQ6uqqfgGKSFwFbmCetX2fc3XFYEn6EaSmbpGabi5epL5ZIV5oRFJAlmKSRJMcx5z9yNX8hO5rnZ+X9NYs2us25UkSQmPf+GFFzDGMDs7WwI1rjWDcjG7Fmc1GAzeRQJewN5xZwVw8ODBS4IQ3u40oJSWmfv555+n0+lcNMK71mO/6bOdfQVcH9NYQDsSoh6EDag2IEsYyIA4zakHDrXANgabPEFWGvTf+3M0Xv5ueSg3GaClh9SZXZV7PiYd0m8eoJlvMQoXqMYF+k+nZVqvknZJghZB0mXL24NXDUFbZ1VlKv1nOsQiJDQxITFbzNIwXbb9XaSEULBXjHKfWc/WZbIp4IM7Ra000j4H9xYItCnpI2GmIiQ9zQGZ28Zdp0FYb5Z9UxaKXojaOW7JKjFOBY5f72UVcm1rSmlu6yyeax3TDnBFPo6VIFUu3Xi8aJp8Dt+xzgohcYhR2GPXfcO5no/ScoL6i62sR5xLepHEdzSpspL0FU9zruPjSlOmEWNVKbeNN2tJhHXCqQ0PKRKgwkbfZaaq6IwcupFDQBvcFkkmcIRdQGgjWOvatN9m38ERZofgYnvgUAs0jYrGd0yJBpyra3zXjh0mku7IOt2qrwkDwdktC8BYmtEoAwtNhRCaA/4pFvI1ZLqF8KqItOijqrWso3oH0liXqxEFQcCePXvYs2cPxpiyKXllZYVXX331oppdb+V6rjUN+K7ttBvCWV3O3m5nlec5vV6vpE26WvTOpex8pKHJUzj5Eng+uj4HnocYZjDqQX2WrlvHxRBWq6ASdBrhBBXIE2KnzlDWyPe/n8U3rKCazFOSxgJBz0ZC/qhLZ+5WZpQVTHRMVjqeMB8wDOaoJfa92ATE3i5alQxjOmQ4eCiqjOiIWWbMtk3/OQ3CordII9lw97BQzeikkx/hfDAsUX57GyNGmUPVU+ypj+jEHu2szoF9Lps9wULTsHtGoZA4RmPyFOlaiLrJUnA822emMkZuE+1W0blCCkuHNJ0SthGxnRenqX9GqlpKyE9PmY6g5IYPPFGmBUNPMMo8eiPPNvjmYkddapSKMkWnjI/BUPGhE7moohY2mZttfSjOC5VgX5NFtkk6y8dRnmC+ltMeumRKsFBXtIcOykgW6oozm3W0EFT9mEHRM98fpgjhYXBRJsQ3Gd3IRgJjaLpBsD2ULM1krG47pLkFWYxrWIPYpge1thFenAm2BpKFpmKUShoVzVwdpFNErRrqQU5uJO2Bw945hTaKW8I32K02CFWPOE3AkTaiqtYx7/sPdrRM/KzMGHNV2lpCCJrNJs1mk0OHDpFlWQmPP3bsGFmW7Yi6rkWc9VqIbN9NA17Y/lI4q+kV8/W29fV1XnvtNRzH4YMf/OB1z1+/Cbxx6mWoNFCVGiYZIqhC2MAMt+nkkswReI6wMG0ZFEwNAl2bY9uEhK4kmt3HaHsv1c5ZAPxBm9yv4KYRveZ+fKnHgQdB1mcYzFMr6lPTE7fWBrdeQZoRoOiIeebNmPFiSvY9HaCLmThyamWD6owfsxlXWQhHhI7iRKfGwdYAVxpOD2vcPNNHCjg1bHFov0QKbMG/aZ/lKEpoFOqEqdKMhSuSLMMThiRoIf0a4/DAdV2yLMMYg+d5ZFmG1nY7zzOUUriuTzcLyZTEk4Zc26hqOooaO7exnpYQkCiPXlGjcuTYvVuOvkEibO9UoBkmAiNc6oHmXNdHG0nNVwxTG1GNa1TdyMrQZ0VE5TuatZ6PI00ZaXUip9zeGlqoeeAaTm961ANNN3IYpLYutdl3UaJC3RuQZIJRYlOAnsjIjFdC01c7Ls3QAi1mqpr1nkOcCdLcIv/yIt0HVidrz6zCkZq1jnWaUWqbhhPlEmeCqqdZnDEIbdg7l+O6GQfc0yyZNg2Zkiax1U5zJaZStY5KXl0kcb1s/Du71t+w53ns2rWLXbt2YYxhOByytbXFxsYGr7/+OmEYMjc3x/z8PDMzM1fUB/VuGvD62Q3hrC6Henk7IitjDMePH+fEiRMcPHiQc+fOvS2F1uk0oNEKzr1O3lxEeAFCZRD3UUGDtmxSq1YJC947ncW4YQ0NaOnQcecRU/dge/+9hL01pLapMu1X6PuzNOgjckEUzFJJLKrPnapPVdMOQ3fG8tJVHUZODXKbvquYYRmFtdgmFhVCE1FzYjbyWWKnwu5mytlhg1qB/kvU5CvkignIYcym/kZ/hpl5DyntPdjdmlxLo1bFFOhFVI4RxXdB5USVGXIR4Ew5+mmffyE4spQOsamWgoeeK8lTjQGLxktNITFv+6pybfu6RpnPMHVt5KMFw3SSVouzsZsT5EXtS+qYblxFFyKRE5qiqRoVgoqnUdogoYy+lBbM1BTtoURpQaum2BrKgl9Qc3LTxRhRgDrsebsjp3SCjluh5mj6ib3vFTksamUOqx2X+Wqf1W4NbQTrXcme2Zxz2w5CQJpZpo1dLcV6V5Ir2zZ9tu0ihKVfmqlZ5+lpTatqdbWiTFILDNLJOOieYLdpUxcJMo0waYLIM4w7g7nnIzYqfofsrTqraRtrdtXrdQ4cOIBSiu3tbba2tnj99deJ45iZmZnSeV0MvfcuwOL62Q3hrC5n0wCI6wHnzLKMF154geFwyEMPPYTWmtOnT1+HK32z7UgDrp1Azy5h0hFkAhFUIY1oRynu3B5AYbIU6YWYZGidmxfSpoXr2GZXrTSOIzGEbM0dYmHzdQByfGTgI3LLDjFdBwqyHiN/lmpqndeAOq1qjicVNd0lR+KiqYqYDhb9J4CtrM6yG2EMdGhyU8seu+ElZXSyEA5LyfblZkQ38WgFGbvrMS+3Z9m718dxDOe2HJbnFI2KQXoBOkswWhWktDGuI4nSjIrrELk1kgxCB5TSTJyFwnUkSmnyPEdKgdaGLMtxXYdeVtvBep7mk0bfC0HUhYBMeQxTO8H6riJLHYwRhL5ilDpWEr6Arie5wCQ9tvQcIKkWY3pTzBG9SBI4mkRJhonEwbA58Apknx2zPeV8toYONV/hu4aTGy6zNc3W0EZpY5h6pmxUV/EUG12BwcGjT0aDSNWKGpWh7g4416lRoU3EPAbBuW23WCDYZwDQw2qDLTQUg1jgOJbpvRpoVjouvmtoVTWeaxOfrarBdWIO+afYZbaokSDiyELTs4xUOJh7Pwrezw4SfiG7ns7qfHMch4WFhZJEe7op+cSJEyV8fgyPH8tyXGvN6nypoXftL5GzAoo0z1u75H6/z5EjR6jVahw+fBjP8+j3+29bTWw6DagHW2g/tM2fyZAkzRgpl+r8bpwgwOgM0hjh+Rgse0O/vg+VGVwoJldF6LtkCroLh5jpnSV3KoSuIppKv4RZjyiYo1LUp8YFlVjWqFVMAVdXuOS0aTGPdWRmKlHYkhb9d0rvYf9sRD/1afgpTT9lI6qxKxziO5o3tqrcPDe0JKzDkFaQcarfJA3qOI4FP1iperXjWuzmWAAehHTIq/OgXRqeTfkBxHFEGNp6QZrmOEUo47ouaZrhOA6RrpXAjooniDIbOYUFdVKaGzzXZhSTzOC7gmEeMEqdMoqaNOPanqrx4mjs6CqeYWM4Ebz0ytstChqlSY1KJQatRVm60UZQD5SlQjKCiqeIMytnUvE1JzdcwCIPxzRMm32HZqjoxQ6ONOS5Idfj779H6Cri3GGz77B3NuPEes0egwVmKxG92AWtaHdyUl1hsR4xSgOGqaBZMZzYmPyW9s1bNgtjoFk1jDKHWV9RCcB1RtzinmKX6VIhRiSRpQFLY5SBlYXbmfOvL4ruWmzMU/iz6E+qVqulbpfWmk6nw9bWFidOnCjh8XNzc1dVQxvbcDh8Nw14AbshnNXlvlxjB/VWndVYAOzgwYPceuut5XnHDuV6RW7TNk4D6mEXNeyA61sG8WRInmV4SzfbPqI0xg1CNDFG5QgvoO/No4QLZORF9JBneelO3CBg68Bh5jZeQKKoZR1G3gzVrAPsZJerZh3acpFKzaUqU9rMlTpUNSfDqCL9Z7qMdEhVxtSchNfTA9w0byOqbhbS8G21P9cTx1jxJ2HLfC3jZK9Oa1cVP7PwZkfCvgWNMRakYHuobHSTF7LmUkjc+jzDYjLOpmqUlUqtTPsZJk4kjmOkdOhmtWJCsGN2BlGTRl9XSjI0UkKqAwZFKq0eGgaxBT3UAsMwgUxZRF+UC+JcErqKc90A8PFIyAl2RFG9eIL6izKB0dCJnB0R1dbQgk5GmcP2yKUR5jgCTm54BTTdJUon9EnaCJQRLNRyTm9ap76raZF+ipBQ5gSuphFqfrrmMt9Q9EeSVAm2owpzdUXgUCgtCzYGFUDR8jokScB83UcZSbNiGKaSmaom9A1RJvFdgyMElWDALe4ZFmWPMI8QaQxpgslzcAO299+Jjt66DtX1sHeKF1BKWUZUt956K0mS0G632dy09d9nnnmmTBfOzc1dFsD1rrO6sN1QdEsXs/Fq6VqjH601r7zyCi+99BL33nsvt91225sYMsZIouttpbPqrEJYQ2QxUa/LyLiI+X0WduwHtvnVGBASncUkwSxD7ds0gpRoY5DFMl0rhRCCzgjW6neQuBNqpenIKEw7RK5lXc+ET1yZI5R2YmmYPqp4/KEZ0RN2nBSGdmZ/KCfS3VRqk8XBbBCji1u0EA4ZFbITS/WYdkFJNMh9oqCJ6wrqFcN6d8xFpHEKlWOMQRbbAjBOgKrOMtQuvueOh+CV26bcdhwHt0ixGAMbfctSnuYgjHVwSWZKFd44NaVWU5LZvq5YhQxSq30FkOSTe6anvgJCjiM1GKU+YJnexzRHpoio7LUIqr4FT0SJLDWrxsCM8Xi/vJ0WHXi2bZ1+d0oJeL3vliKKFW9SAwOL3JsZS9knDjPVopEXaPcdfM8wW1c0q4pBJDiz5TJbMyzN2FTjQsPQzeZoj2q0By4q7XF226MzdHALZ9uoaGqBoRr2udU/w5LsEKqRdVRZCkXEa+56mNwLbxji2BuFxDYIApaXl7njDssu/773vY9qtcrZs2f5d//u3/GDH/yA48ePX5Sg+11ndWF755/sFZgQ4ppBFkmS8MMf/pDNzU0OHz7Mrl273jRmOs14vW183bqzZpFpGkLH4C7sQzsOKksQYzqhNLZNsdUFRtJ+WbNM4RYz79iZZirHCI+BY5uWTzfuLs9Xy7vE/kQWxLguOQ5RdZFd7haxKXj2RMZAzpTjxFQKcd7rczLbzfKCYTEc0kvtPjXXpv8APMewOpxAeTtJwNlBldpclSSdfK327p4U3KfXAnFkozUDiEqToZo4pguNnyZHFELa1J+YpdmafIYsnYhD5llc7jaWqBcCckKGqeXGqxe+M8knkh1RKgjdMXcfhD6c6/r0YgnKHjM2FdwCMNJPnHI7ziDNBYPEYbtA94FlPR+rA3cih1ZV0QgUpzY9djULZV8ldhDHDmOrRXVq02G957BUcPjlWjBIJK7p0Qwtc7oFR9jjDGJpWbs8Xd6/7aFku2CrQAj2zin2zioO7lLUqlVmwz4tr816X5BEPaI4oeZucYt7iiXRxVMxMksgjW17geNi3vsI1FoXlQh5J+xGcVZjG8PWZ2ZmuPnmm3nggQf48Ic/zE033VRyGH7ve9/jhRde4OzZs/T7ttH/fGe1vb3NY489RqvVotVq8dhjj9HpdC55bmMMv/u7v8vy8jKVSoWPfvSjvPTSSzvGJEnCr//6r7OwsECtVuPTn/40Z86c2THmSs596tQp/pP/5D+hVquxsLDAF77whR0ahSdOnCgDjum/b37zm1d1P2+IJ3slX/ZroVzqdDo89dRT+L7PQw89dFGEzfgL/nY4KyklMx421Tfqg18lm91PbgSOF9g4SOcIxwOdo5yQDVUnzxW+6+zoq1Qqx3MdXDegnc5gaysQNfYx8Caks1pMoqEw6bAR3ETNiZDC0GMib+/pCTN63XQZ5Hb23shbENp7JQT084lTSrPJhDqGsAM4DvgzdXxPsG9BkRaPSmVJ2XOjp7Y9CQgHHc7Qz51SOj7LVcF0fv52Xn5PtDb08jq5lsTZJJb0gnrp1IxwS8c3ihIEmmEWFHWpYswFoiiYyNFXfEmcTpjoq36RijQTRzeOnEJX0x05uEKXr1fcSQQ2PqYUBqE1q9v2hc2BVf8F27S72MgRGGq+pheJMpW71nHYXUD+tQadZyXqcZRIBrFkz6xioZGz3Rec3XLJNSzPKnY1FIFn2Og5bPYc1rqS3AhOtz02hwGtRoVma4Z9c5paxaUiOxzgDfzOCeLtDdJ+DxNHoBRCKcxtD0B9trgX1z91fq12LT1Nb6dd6Hp83y9p3B555BHuu+8+ms0ma2tr/MZv/AZ33XUXaZryyiuvlLpWn/3sZzl69Cjf/OY3+eY3v8nRo0d57LHHLnnuP/iDP+ArX/kKf/iHf8izzz7L0tISH//4x0uHCPD444/zta99ja9+9at8//vfZzAY8KlPfWrHPHi5cyul+I//4/+Y4XDI97//fb761a/yp3/6p/zWb/3Wm67pz//8z1lZWSn//spf+StXdT/fcaVgsCuicTH9Yva9732PO++884ol7U+fPs0rr7zCrbfeekVs6f/6X/9rHn300Wtq/LuURVHE+tHvMRNIqkKRz+4lcULIYpygUlALadygglaKbW8JZQxKa6qBS5KmeG6RStSawPc50W3hO4ZRqpHSoRmC1z3LHVvfK8+buFX8bMh2dT+pCFiUNn+eGB+PlDFN7EA0qZseAKeiORwJc7OCdlJnV0G/NMo9Apnawr+GKJuIAW70PYyQuDMN1jqSm3YVQoJ+iErjYjuwnxMYxin1ordKV2fpZ3bSDjyXtPgO+J5LWvRWBb4FUYxfV9rQzar4rkOU2nONYel22/L/gQVXxJkBo+gMcvDtBOuQoAiwir02GgJD4BgyJRDCELqClZ6PQGN0DtK37OXCws8dYSmJbJ1L048kg9iSyFr4uk0z1gPFMLWLh9lqRpRINvvODsXe2aqiM7IpRs8xtMKcU2373rjZF0AIw66GYhjbxl9HGhaamrWOvYe7Woo0A4lho1+oBldsA3CaC5pVY1nYC/Sf0uB50I8ku1oKbQTNYIvbvXMsOEP0sEc66qMGPfIktc3W+95L46bbSqj28ePHUUqVKa930jY3N/nJT36yQ6b9nbROp8NLL73EI49cGe3U1tYW3/jGN/jyl79MFEV0Oh3uv/9+nnzySZ5++unycz399NMcPnyYV1555YL33RjD8vIyjz/+OF/84hcBG0Xt3r2bL3/5y3z+85+n2+2yuLjIH//xH/OZz3wGgHPnzrF//36+/vWv88lPfpJjx45x1113XfLc3/jGN/jUpz7F6dOnS3Xmr371q/zyL/8y6+vrNJtNTpw4waFDhzhy5Ajvf//7r/l+3jjLkMvYlaYBtda8+OKLvPbaa9x3330cOnToiiO36x1ZRVHEkR89x0IoCKp1dGORkXGQhUKtzlKkF1hmBCHZdBdJlMZ1ZaGrZCfjLFe4joMjJZtx00YUuUBnI1vJENCpLDPyZspzJ6LKur+fljtkVnZICuXeQKR0xUQqW01Jf2gjaM04SClYCIf0Uhs+VN2Mc32r8OtIWBtOkF9baQXZahAEFgwwNrODQmkKRu/b61DhDEPlldm9bEraI5/Cmee5nkh+AN2sSq4l+VRxaRqWPk06C+BKiFSNWmNybyYaWQKVjMptKxdi8F1JlBdpSSTNQqBaGUGzSNVZ8lobAW32HMJxac4IaiWCW5T1MldqkhS2B/Ynt9m3sh5glYB3NRWONISu7bvynCKi6rqljEfoGgbRpE6mtGCt47A0o1ieyVjZErT7ko2+5fjbv5ATuoZhIoqGXxilkrNbDsPE1hSNESzNKFwH5iqb3BGssssd4GQRPoqG59JqNJmp10n2v5cN5fDDH/6QJ598kldeeYXhcHjD6EfdiGnAq4Gtz83N8dnPfpZer8ef/dmf8fzzz3PgwAGq1eoOB/zQQw/RarV48sknL3icN954g9XVVT7xiU+UrwVBwEc+8pFyn+eee44sy3aMWV5e5u677y7HPPXUU7RarUue+6mnnuLuu+8uHRXAJz/5SZIk4bnnnttxXZ/+9KfZtWsXjzzyCP/iX/yLK74vY7shnuz1ciZRFPHMM8/Q6/V4+OGHmZ+fv+T4qz3+1dj29jZPPfUUu5sVy0bheESVWZuyMgrhuBb1JyUIyTZNpHQnEiDYidpzncIhCbbTGaLMIXAt7Q+FFlWuLaDgbH2iwprIkGoR/bhC0zMz5XuumeST67pDPwtYS2dYWHBoJzZXLgT08mk48uQZzYb2vJujgOpsiFPoQB3YpUtYus7SUjZe5xlRUvD3SYEK5+grD6UNXpEf0zu2dbmtim3HcdhOKjhyLPMB/pT+VIG/sOAKZ0JcO1IBce4wjEWpI4VXQxYNzEpWbOQE9EYKnWes9nw6Q4MpABtR7jEG2PdjUepDKQ2doSRTkq2hLFGR7aEsHVEvdlmo56AN6z2PuUJ6fuxUx2m+7ZGkFWasdR0GsaRZHfeXwUrHZe9shlKGzkiy0nGpsFleE8aw3nXYO6dLJ+e5htVth42+gyNtM3YtsACSemjwPatQXPENUgiabpvb3FUWZQ+RJvYvscg/jEbcdj+77voA73vf+3j00Ue58847cRyHbrfLuXPn+NGPfsSpU6cYDAbvmPO60ZzVtfRYga1ZNZtN7rjjDu655x727t37pjG7du1idXX1gvuPX9+9e/eO13fv3l2+t7q6iu/7zM7OXnLMhWr80+deXV1903lmZ2fxfb8cU6/X+cpXvsK/+Bf/gq9//ev8/M//PJ/5zGf45//8n1/2XkzbDQFdvxK7HOVSu93m+eefZ9euXeUP6WrsejqrU6dO8eqrr3LHHXewLEcM17fJnSruuDaWJrh+gM4zjFaMKktEmUvFs8R1EzZxhVfId3TTKv3UQ2loViFVYNcahjiDWgDblf3E/ZfomBYLQcRG3qJaKPY25ACNQGJoiAEDbPrPEYZzcZPlXUVdZYqFYi4Y2ZSXNCw3IrqxSyvMmaumHN9q0JwPma3CT1cktxQM6o4foBJ7Tum4paR8UKmCztBhi0QGULw+vVCZJqjd8bp02I4DlJaFQKK9RlvnKiZnKcimtKgAeklAMAanIKj4hn5sHUUjtNsISbMi6McGozWjWIFja14BPVIaZEowW1Vsj2yT8GxVkWvBSsdhrmaIMnvMqqcLbSuBKKD5gQf9kWCYjCOqCSFtP3ZYbGR0IwdhFBs9p2w0bpepQodmaGtcszXNMLF9YBELNEJFq2JZL8DSKDnScHAxJ1VQC+x3oxpYeL3ApgVdR9AbCRYaBjDMhxvc5K4xyxCRFn1U40hXK/TB98HSTeXzmNaHGv9m6vU67Xabn/70p3ieV0pwzM7O/szk2W80Z3WpGtqXvvQlfv/3f/+i+9511108++yzwIUX81dSKzz//SvZ5/wxV3Luy41ZWFjgN37jN8r3HnjgAba3t/mDP/gDfumXfumS1zNtf2mc1UV1oa6TrMelpO2v1LTWHDt2jLW1Ne6//35mZ2fJXnuGLW+GppQYleN4PiaNyn1ip0EiQoTIy4p/mitC3yHLLPOAknU2+wEVH0bpJMZx3JDAtRRBUoDnOfzYu48PeC8CMOsOyLSDJxShSGjrWeaFbRLuxQ71ANpZg9asjyyad3dVRvTSgKafUHFzTm5XODgzwpGwOQpphQO2I5+hrLFUtZFEqzYF/Z7KyaVxhCwolCQGHTTpKw+hJ4sO+xntxJ5meTnJp0U/mXQctuIQU0Qi0RRbenze9thyBb04IFEOShvGjb5RKhhTb4zlPcYgFVc6tLMAIWIwGoQkJ8QYjRCSznAcNdqm3s2egzaSraEp2SjaQ1k28fbGcvNdyTCxab71ntUUy5QoCXKHiUPdV5zdLtjoQ4UrNbmWrPdclmdSVrcdktw6x4WGYpgIosTWxk5uWoaKXFvo+u6W5uSGU7Yw7GpZ4EeuBbN1RZZLPNcwU9e4rqHlrnOzXKUpImQaI7IEoXKrR2U0et9dsOfmi37njTEEQcC+ffvYt28fSik6nQ7tdpvjx4+XtERj51WtVt82QMa1RjJvl10qDfj5z3+ev/pX/+qbXu90OnzsYx/jmWee4e677+aFF15gbW3tTeM2NjbeFNGMbWlpCbBRzzQTxvr6ernP0tISaZqyvb29I7paX1/n4YcfLsdc7txLS0s888wzO97f3t4my7KLXh/YdOI/+Sf/5KLvX8hunGXIZexCzmos63HixAk++MEPviX9qUvJz1+JJUnCs88+S7fb5fDhw7Z7PR4SV2YRXgBCoLK0hIibPMNUZtnKq2VdI8s0flGvGj+YTPusjypoQ9k7lCu7LbywIF0FrQWvnA04nu4v61Muiu3p9J+Y3L9dQZezUROnUWWmkrFWwNCFsM2/5X2ZjrSqGZ3YI6402bdLExVI8fmmnsDvVQZF+s+RAllQ8KigTuLYcxgDvjdG2YHvT37Q434qu+3RSWvkSpby88ZAGBSpRgOhP9mu+JYEeDsO8L1xKtDWlsAKCtaLjzaGrkthyLVDUoA1jAyZqRZpPvxy2wgPR/XQyRanNiUuo+K8tpHYmmDsPmuBpt2XJZ/ges8SxIKV4Zita2qBJk4Naz1bYwLoxw6NipUPmatlnGm71EON79j3NwvgRMWss96122tdG4kdmLeLgKVZza6mYt9CjiOtpMdNi5aeKlUCz7WM6wvBKre4azTl2FFFkGW2xphnmD23wv7buZSdH82MaYduv/12Dh8+zIMPPsjCwgJbW1s8++yzPPXUU7z66qtsbm5e9xrxjRhZXcxZje/R+X/j2s8HPvABwjDk8OHDdLtdfvCDH5T7PvPMM3S73dKpnG+HDh1iaWmJb33rW+VraZryne98p9zn/vvvx/O8HWPGpAnjMVdy7sOHD/Piiy+ysrJSjnniiScIgoD777//ovfmyJEjV00pdUNEVtcCXR8Ohxw5cgTP83j44YffsqzHW0kDdrtdjhw5wuzsLHfffXf5BY2TlMRIQtcB6SC0Aq0Q0kFLn66cARRKaxxHoNXkx6aNwXF9jm9VmakKsnzcsGrTTvUARomdHgMXXj7jINQA7TQ55xzikH4NgIYT2UBBQEv0aSc15v0hAxXS8+eYLzSoJqRHNv2XKytfvtyK6UQeM5UMKQynkzkOzo+L/w6Voi4mXQ+VFr0+wxH10DopKSVZ2KSfe1MNsTth42qK0G+8YHAch15WQRUUSrm68PhpQUUDbMcBuZaT+hTsAH6MnYe9xwAOW0MX8gG49ppTNSGRtZRIdrteb1rGciEZ5hWkiTEypD2QVNyEKA8sp18j59yWjYYWG6pE5o1SWYo4JpkgcBSbyZixw1ANNKNEsj102DeXs7JlpefbA5daoKlXcgaxxJOKbbNEPVA0q4Z232G+oUsEIcDeecXKtqVwWmopMm1TgBhB4Bl2hyvs1xvUZIxTNvvmiDTFaIXZfQhz4E4uZ5dLLY1pifbv37+DDPa1114jTdMdUVelUnlLUddfJmd1MRsOh1QqlXK/O++8k1/4hV/gc5/7HP/L//K/APDX//pf51Of+tQOJOB73vMefu/3fo//7D/7zxBC8Pjjj/OlL32J2267jdtuu40vfelLVKtVPvvZzwLQarX4lV/5FX7rt36rZNb47d/+be655x4+9rGPXfG5P/GJT3DXXXfx2GOP8T/+j/8jW1tb/PZv/zaf+9znaDZtm8w/+2f/DM/z+MAHPoCUkn/5L/8l/+Af/AO+/OUvX9W9uSGcFViHdanC7LSzWl9f54UXXmDv3r3ccccd1+ULeq3O6ty5c7z00ksXhMin8Qjpekijy9lZZQmOX2FVzRJQCP5lmkrgkEyl0LQxbMRNhJBlET7JIfTsv+OPrDS8sS5Itc9sTdON4aQ+yAFzHEdoK5hoZpkXlvtvpEO8LCerzrAoc7QBKWB3LaKX+DSDlKqnONkOOThn03ibIx8pDQN/Bp1M6kR75iW68CIqzyfCjqGPEQJhDLkMGKhCBiRXhUaSISu2lTbkSk8IapXG9zzaUaVkSE9yC6gIPEGaG7s9/bpr79DGIMB3Bbm27OIVzxBlNv1XbmdWsTfJYZQ6RHEOuODWyxTeKJW0Kopu5BDnljHCaMOptlc4HzA4zDYMW0MAUbRe+HgMONcOMWjAAhzGYohxZtV2cyXYGkCmHOYbinbfnrMeaiq+phka3liTNsJyDIPYkuIaA/O1nLVO0XaQOAwTw+5Wzii1DOtSCDzXoLRkz4wm8A3ntl0WGjbdWg0MS5Vz7DNtak6MTGPIEkSWjhu40LsPwaH3XdH3/2q476bJYG+77TaiKKLdbpcpwyAIWFhYYG5ujtnZ2aue6G80Z3UtacnBYPAmBvf//X//3/nCF75QIvc+/elP84d/+Ic79nv11Vfpdrvl///O7/wOURTxa7/2a2xvb/Pggw/yxBNP0GhM2G7+/t//+7iuyy/+4i8SRRE///M/zz/9p/90xzVf7tyO4/Cv/tW/4td+7dd45JFHqFQqfPazn+Xv/b2/t+P6/of/4X/g5MmTOI7D7bffzh/90R9dVb0KbpA+K7Bh6qUu5Sc/+Un5IN944w3e+9737oBLvlU7cuQIMzMzHDp06IrGa6157bXXOHPmDPfeey+Li4s73jdas3n6J3hBiIpt9OK4LkJl9II9DFKBK6xybZbnhL5DkuY2vScl5wZNAs+hG0HdN2TKRhGtqgUGNALDIMo4t+Wi8pRMzrDY0LQHFvb8weB59nEKgI5uMis6APSykK43w64ClXauV2G5Ya/vZKfGTTNDAM50AvYXLOtrfZ8kqDM345Ar0MoQFIwP0nXRxSJiGMXUKzbCdbwAJXx6yiPwHZIi6gp8h7ToGA48l6Tsp7Kvu46V+Rgm9sda9QWjooeqGjDVTzXprar6grWBVetthIZBsa8FUdjtejB5vR5q+rFDN3KReRe8VjFe048nKrmj1GIzZ0LF6S0rbOhKS980jr6aoWFQgCcWqgmntzy0kfh6i1TaFgFPKhzHRmlzNev4zhW9Ub5rqHiGbmSPsTyTszUQ9Iv/913DfEMziARZbhgmEoli14xBKYEQNsUItg9raUax3vMAw4F5hRECR9rFTdVXLFVW2Ks3qMoUmUbILLVglzE7xewy5o4PXubbP7Hnn3+e+fl59u3bd8X7XMjGUdfYeZ0fdV1J/+Px48fRWnP77ZdOXf6s7LXXXkMIwW233XbF+3z3u9/lb/yNv8Ebb7xxwzRb3yh2w0RWV2LtdptOp8NDDz20Y4VwPexqalZpmvL888+TJAmHDx++IDNGWjTBGq1tmk9YvaUtMYvSEtdhZ9pPG6QUKK2JdZ0oc/EcjYAy7Tec4gtNM8PaZkRkdrPY9NkcQJJZBNgoFay4h9iXW2c1I3tsJVUqMmUUzpKoELCd7NML0flKZOthDiy3ErZGLqFrSII6vdhhDvtelKVQSCXaWpV1OLVaDQrwhHICetmYgHgqbVf0TQls/9jk8+gCnl6FEmaxE1ARpRMRxTHnn+sINoY+Y9jJILaigrkWDGJKjapBYqXptYFe5DCIchAu2m1R9zWjVNKPJfVAM0gko1QyU1EoLTi15ZXRUa5FuQ22gdgYw1xNs9L18VxDkkEq55ipxHSikEw7GNUlFHBuq4FhrCnlkObW4TQrlgXj5IaD7xoWW4qN4v0oFfiOIo1joI7GYXtgqFUM3YFgvmFl6j3XkCrBTFVRDTWbA5ddzdzW1fycvZVzLMsOIdZRiSyxpLTGIPIcM7eMuf3KHRVcPwaL6ajLGMNoNCrJYI8fP04YhqXjmpmZuWDEcqNFVkqpqy5PjLWs3nVUb7YbxlldKg3Y7/c5efIkxhgefvjhUivmetqVpgH7/T4/+tGPaDQaPPTQQxeF5WaRjVZUlpDlGseT9EWdSHkYY6j6kkRN8f3lisBz6OR1Em0/X5Jbbro0mwj85QocoVnZjIgzF1zQxrIZ9GPBrqZ1Vj1dZ00tsNuxzBX9vEJUbTJfVThpuiP91409WmFGPdC8sRlyaCG26b9hQNAMmZ+VDNYm6b8wDCxiDkiiEVJIS4uklS2O+XW6qYfrsCPNlysrRui7Dlmu0Mbgew5ppnCkJFKVsv+oWiAfjYFqIBglVuG3EtiIymCjq9WeVetthppeLEqJjn65bbA0gYLQM2wOJaPUxTNDtLBoC9eZfO8mc4RtmD27PSaalSWCr92fOLVuJFlqZvx0zUZeCw1N0VLGMA0KLSxJs1ZhGKUlSm+jC3VvxCCrkmQwU9FEybhvTLDRlSzPKnIFGz1BrlygzmxNUwut0z23ZSVGeiOYawjOblt+wcWmIc4dds/YtGDNH3KgssEeOvh5gsgihMoRWmOUQqQxurFgaZSu0t4OByGEoFarUavVOHDgAHmel1HXK6+8skNuflzrgusjIXQ97V3hxetrN86TvYiNESoLCwvEcfy2OCq4Mme1urrKj3/8Yw4dOsQtt9xyydVPGo9wXM8K1GEYijqjPMB3JWmmykkxz5VVtM0VUR6wPgioBQbPMSS5YMY3pJkFAwgMo9TQ3+6wlSxTcTrkwCC2fUO9SCIK9N5mD54d7uVTuzdJlYQwpF6xn6/hZ6z2q+ypjxACNvouraLR1ytIXONMYsIKraa90H27jNWeMjbaGzc1u45Dmmv8AtLYjTUIf3JPi0jLKZwV7ATUGAOu47CVVMomX9jJSpFPRWZZgfAPfcHWKEBZBiSiKUVfm76z28PU3jfXgfZQMIoNSMidJoE0JErQGUlqvibKbHTVDG1d6fSWy0JdsTmwxLfjiMogShTmXE1xuu0SeIY4sxpUu5sWVJEpyye4UFecbnuAx54ZxUrHweAwykJCtskVnGnPFGk8K00PgkxBf2QITAfFDAZZNiIPYnsNrZrGdy14I/SgXhEME6tJZZCEXsShSpvdsoenCmh6GmPS1NbV8gzdWIC7P7xDZ+xK7WfBDei6LouLiywuLu6Qm19fX+f111+nUqkwPz9PFEWE4TuvqzW2a61Zvcu4fmG7YZ3V+TUhgNdff/1tO5/jOCRJcsH3jDEcP36cEydO8L73ve+S/QMAWuWoLLX1qgwyfEY0EWYSISmli8ZfjS8lyg3ZHlUQwjBKoFGxNarxNJBkBknKYDAiK2ohCp/AyUmVy2zhZEaJIUtzslTx8qkZHpqpMXLq7JrJOduvsa9h+f7E1Gp4qZmRawuH3zeTsNrzGLl1lndLTqzAoT025eZ4HnnBbN6fQvxVKiEqS9FupXRUYAlkHUcihCi5/uxnUcgikjbYqM9KyFvgRJbbqNLy9o3BFTbNmSmoBpLVvo2o6oEu9KcE9VAziK38fKNit5UWNEPNWk+SKI+KMyAynqVFChXJ0KbzPMeiLIUwYGC14OMbJLIUZ2wPZBkpdSPJrkbGG+t28bTQ0EXvFrQHk8jLc3RJ6guw2pXsbinWug6OIwj9JpicqG+pmlY7LlWnT+AZ1rbrGCQwh+8a9swqlIZ+ZIUffRcLR8+gGWp8X9AZCnbPWFdd9YfcHG6wW/ZxVYzIU0SeWmFPNCIeoaozZLd9ELIMKWXJiH2l0dK1iAu+FTtfbn466ur1enS7Xfr9folwG0dd74RdC7HuaDS67vyk/77YDems0jTl6NGjpGnKQw89VHbHXy3r+tXYxZqC8zznhRdeYDAYXHGtLI0n9SrhVxmYFg4KkOgCRp7mhoovSTNQxuGnWzU8abWTokzgSA0IcmXlOFIlyKKIM71dLDQNjjRkpkLVTUhTt4y8+rGk04noDwVIh2e6t/LzN58DoO5npRz9YnXE9shhtqqoh4oTmyEH52NSJTgXN7n9JusmZ5sTVolpxF81DDHCIIxBZSkEdQaZDwI8V9rUp+Oi8gTX8zEG8izB9WwO33MdtDFsxRUqviApHq0rYczc5zoW/QcgxZgRQtBPLIgBdiiHoPVkhZ8VSr+hBxt9SHIHBOSyhtTW+XRGEr+4t51I0Ag1eS44s+0xV1NsDS2Cbww/N0bguRpSw2xVc3bLJZyKqMZ1qFwLEIb5Ws6pTbuyXp4tIiojWO9J9swoeiPY6EnAZ3lOsbItMUYQBo5tAhdbZKZCRpWqn3FuyyMroPezNU2mJdsD66RzI3G1Ya5hkA6EzpBbwjUW3CFuGoFKLZgiz0ArpFKY+izyno/gIkrx0XHdduywLue43mmJkOmoK45jGo0GruuytrbGa6+9RrVaLYUPZ2ZmfqaO9VrTgO9GVhe2G8ZZjb/w456lVqvFfffdV+agXdd926Tn4cIAi+FwyI9+9CPCMOShhx7C9/2L7L3TsrGzMoZtMY+QBgqeuTRT+J4kK1KBrutwulsncG0vle/aFb7SdpUfpcb2AAmHSM8BkmFsbNovlshCkqIzgMEgw/Mka+sZYS3EC+CF1Rn+g5tW8RxNK0jZGFXZVR2VNanZqq2tua6NTk6OZlne7RR1JmjVNEJKjNYYrYjimGoY2mv3AlQaI4I6sagyBlrIqcmrUqmWqr9ySjOr1x8SyVmr+DtFuD+9HRUgCm0soKLiC1b7fiFiaPuVhomk4mri3Kb/Kr4hSgVxZiOqla4kNz51P2aQhWRKMFdVbI2coqFXkY4sf54who2+W1yHlecwCMv759k0YXckWazlnNgcR1SqYFiHdl/SLCI6R+jCAVv3fm7bYc+sYrXjUPEMnQHUK4aOBV9ybss2BreqijfWbDQwMpbbcqHaI05zXA2e6xN6At8LqDmCXU0rbT+IBTN1jUBQ8frcEmwwLwe46QiRp8g0wWiFMBoxGmLqM/C+j9jWiuJ+jx2WUgqt9Y7fm5Sy/Ju2G0kixBhDpVJheXmZm266iTzP2draot1u8/LLL6OU2lHrertThu86q+trN4yzAjhz5gzHjh27YM/S28GKPm3nH39jY4Pnn3+effv2cfvtt1/ViixLIqTrsanmcT0bmRgEviPIleX9y7Bpn7O9JlFmJcVtNGCPEWfgS0WurdbTia0WgQdV3xDngpmagdjKrwvsyjqKcja3ACnJ0hw/9BgNBK9uzXH3ogVaTDfILjXTMv231Eh5pTPDTfsKLsKBw0zBuCAdD6Vt+q9WrWF0odNkDCKo0ckCpJyg/NIdKD8L29baIB0X15FoA5Gcw+QJwqtYx6xijBOiDNR8i/wbAypGicH3JKPMLdJiELq6kPYAz4W4iMwKWkBCz7DeycixP3yFz9hx9KdSe51I2r6rVHB222O2Znn7Rqksa1RKC8KKJskty/pKxyVwbU1xXKNa6zlFA7OhVVGsFMCMPbOT7ZVt2+y73pWMEkkvgqUZxfZQkmT2mt9Yc6jILrVqla2hy9KsYaVAEALM+hG9JMSkktDpUwkcqqFkseURuOB7XW5z15lxItwsgjyzwol5hlAZJAkmrMA9j4K7s/47/o5PL9zGjssYU2Y2pqOun3Ua8FJ2PtjDdV127drFrl27MMYwGAxot9usrq6WUdfYcbVarev+Od51VtfXbhhn9frrr/PGG29w3333XZAtfexM3q6V3PTx33jjDX7yk59cUy+XVjkqz4j9XSSZg9C64JpzcR2b1jPGRgxnejWktNDlMeWSbXIt0n5JDz9s0kmaJLlNlS02DfFg3CZsGKY+vW5Ks+mz2c5xfB8/kMRRih96OI7kubMTZ7VYGdGNJK2Kph5qTm6G7J+N+UmvRS4mk9fcjIsu1P3iOMKVtpZhxog/Y9DSY2hCwKC1ISiQfcZA4Dkkxf6u45AWQAvpOGwNbTt0rVolysafZGLDKEE6Nl2Y5YbAFawNvIKgdgycKPgGEfTjCdfeIIFaoDm3LTGiTt3PGWauXRBUNJ1I7IiuXAkO0BnZSSVXE/7AzlASuJokl/RGFsp+umCJWGgokv5YQNGiEaPMspDY9YC9zpUiolrZtpHTubZgrqGJEolBsNpxqPqGA/M5b6zb40W6RTSA/QuKNBcsz2uMERZ6T0CjZnCdnPVejUynRPGINMmomJwD/hZ1J8VNE0SWILPEgmK0gjTFVGpw96PgXRpSPZ64J60Vesff+LcydmY3Amz8UoAGIQSNRoNGo8HBgwfJsqyMul566SWUUmW6cH5+/i0z4lzuei5mw+GQubm5yw/8f6DdMM5qeXmZPXv2XLQgOi09/3bAU6WUZX1qe3ubD33oQ7Raras+ThpHxN48ifEBuxoXJkNIr0RL5LkmMnUGicdsza5gc2V56qIMfBGBqFCv1Vnr+nRGDq2qoTuSRS3LSq7naYoXBOS5ZmUlQuOg05ygEDc0xhCEHitdw7lBneX6AEdapodWxaYqHQeO92ZYXvYYRgatBVKCyjPLgm601dJyPXRuG71cz0fh0M0CAl+SM1bEnZia0pvKciux7nkOW6OgJKUdpRTwdsAJ8RwLoMAJUNkIx6syGg1JqQGSTI0bdwvgRMWq6ZqCmaEXFRFVe4hx5orPNymCTaflenGR2ksk231ZNgT3Y8lCXdEe2tpTq6pRkcFDs9FzyhqXFVC05LRKCxxHExirbQWUqD+wEdX++ZyTa4JcC85t2d6oXNseqtBTvLHu4suI+aZHZ+QwVzec3pxMdPsXNKe37Pd+77xGG8GulsFxfCq+Ryi32Odu4Mdb9LYHJFrjOxBIgatzSGKM68Ndj4B/9aCD6RTg2Dm99tprZertQlHXz9p5XY3D9DyP3bt3s3v37h1R18rKCq+++iq1Wq0EaVxr1HUtAIt3I6uL2w3jrOr1+iUBFGMH9XY5qzzPGQ6HuK7L4cOHr3ll1U08thMXT9qIIFWmpEsaIwBjHTBKxw4FwBClUPENo1SSZwn4Id0ooBdJklwwW5+QsbrSWCnzgWY2gE5XYYSD50uiYUoQgutJ8iQnqHggBE//tMH/630WCbirkRU1MRjokErdXkutInA9nzyzTqk/HNGo2rz+dDSrhEsn9QuqqPMaews6pVxpPEeSKW2jLt9jo3BUtcA6W7BUSWOGCt+llGqv1WooZdgyc0gVld/UUZSBsM8mnoKoDxJB6CqL4nNmyoioGxWIwVQSZZLZqmZ7ZOVQJJSMFeNFAEAvlmUzcT+W1FzFuY69gMWmlfIAi/prhppMwyCyn2Vc51rpOKXDWmgo3lgTLDYN7R5lr1Y1MOxuZpzdBHBIdYXVjmHPrKEXCZZmNFJamZFcCXa3NKFvWCkEF42xNFKzQZdDlS2qWuM6IcYBNeqRxDHdaITMM4Qfku5/P7OOz/Vo/nj11VfZ3t7mgx/8IGEYlpHWlda63g671ujuUlHXiy++iDFmR63rSuaGcdR5LZHV9SY8+PfFbhhndTkbr9bejrrV1tYWr7zyCkIIPvjBD76lH1YvLhBt2hB40upOFf1JaS5w3YDTW9UyoopziwBMMsFo0AV/hmq9SZJKjq87LLZstDWuZXVHgjTO8CsWYbd6bkBuHEyW41cC25xrNL7vEY0SAjxGg4R/d87l47dbddpGqDi5GZDIgL17fY6f1sy3xqzmk4ioVq0WiTbIMytrjhvSSXwLEikipmk6Jc+1tFFgYdUoCyLpZ2H5Gab8G0lWZt12MFRoI9iKfECinRoVx2oyKRFg0j7Cb5AqQSBiEhPiCsV6ewD+gr12X5UIQ2fqcVrOQE1/JElyWcp7dEZOiQBMc1FIcUiyzLCdWAqrTAk2ejsjKs/VJLGVhu9ja1CrHdusu9Jx2D+X8caalRVZ6wgaFYPv2gZgKTSnNj0ckbM8Y1N+rgPntu0FD2PYMwenNh1cx7B/QZMrwfKcxpWCMDDMhlvc5G5SzUc4eQxZhmMUju8R6AyDT2oCzs3dwsq5dYavv0Gr1WJhYYHFxcWrZkvQWvPjH/+Y4XDIAw88UIIUpqOu89OD4/rXtUDjr8auVyry/Kir3+/Tbrc5d+4cr7zyCvV6vXRczWbzguecJmS+GnsXun5xu2Gc1TshPW+M4dSpU7z22mvcdNNNnDlz5i192ZW26L2KL1CpwYwrS0JiVEYQ1tgc2R93ltveniQT1H1LX+T5PhmglGS9K0FMmk+jFHxHkRuHONH4FegPc5LE4LguUWzwAddzyNMcrxIgpSTqjxgOMpQRPHuyzl+5w5JdbqUBt91s0Y0Hl2URnxhUnpGkGYHvTSH+ijDIr9GJXSgmnLHpqZRfOkWnlGYKz3XZLPSofMdyHE73TeXacvtFmWVlqPoCZQSr/YBWRdMtpL88F5Li0dcbtTIyS3JNnm7TNXNIr1VGNp1oAkvvRhal148tsMIoiDL7nGvBpD8qykQJvBglAsco2pH9iYwdFNiIqlGxTrzdF9R8U+63WkQ9qx3J7qbiJ6sOC03NILIIw34kaFYMzUrGZjcHqijjstk3NCuG9Z5gtqYJPPCLRUyzomlUYasvWWjZ9GElMCxUt9jvblIxMU6eIrMEmcZ2waFy6/WDGv57HuRgY46DQBzHbG5usrGxwU9/+lN83y9pjubm5i45uSqleP7558myjAceeOCC6NhpkIb9bugdII2rhcZfjb1dbBrNZpNms8mhQ4dI07SMun784x9bqq2pWtf4nlyrsxoMBu9GVhexG8ZZXYldTi34akxrzcsvv8z6+joPPPAAnudx8uTJt3TMKClWkMU8nimD5wiUlqS54sxmjXpI2fjbrFpy1eFggAxmkW6FqjS8cMJlaWZMw2SlOvqRII1TKjUHrWFrMyJJHVSeIt2iNqMVnu8wGuS4oSFNctrrA1zPwXEdnny1ykdv7/LaVpObb/ZpdzTzM3Y173qe7ZcCKpUKulDyHUdD0rdKxeO6W5JpHCHQxpDlEzqlaaCF67pEaiKc6LmCrGCjmI52pnwdGsFa3/7gh8kkrTZIpgULJaFrUZGuI+mlLvgOGgeRtsGft4ztTkKqgvJz1APNZs9OkCV10sApGdajVLLYUIxSQX9kIfiOMCgjWD8voqp4mq0exKkkTkXR6GsjqtWOw765jJNrhTpwz8rHL80o0kzQj6E78gGPXS0rRa+NYGXbEucOE+ucV7YFraphvmnTvgstjZTQCg0LlXX2OtvWUWUxMkstfZIQiHgIeY6RHtz2ADQmBfswDHcIJW5vb7O5ucmrr75KkiTMzs6yuLjIwsLCjvpxlmUcOXIEKSX333//Fafiz691XQ4aP96+FvtZgDx832dpaYmlpSWMMfR6PdrtdolkbjQazM/Plw7nWmpW79ItXdj+Ujmr6xVZxXHM0aNH0Vrz8MMPE4Yho9HoLaMNh8kYLGFwpEWy1UKJTjTtYQ0tJaPUUPNhlAriaACyTqXaIDcWnLy2bSe8cTquFwlUliNcjwIxTpZpOl37Wq7ANQY/dNEqx3EcW6+KE7bbI4QQeA7kRtAeenz7+BwffF/B+B24jBt+h8MRYSE4NYamgwVayKBGJ7GOyvckaWY/p+dNUn6OFKXmlIEyovLk5F7u6KFKrMNSmpK5QkjJai8o2B8sGGHM+afNhPMPLC1UmkRsxnWqVU1UrGG86mzRSGybh9EROBWyLEMpl0zZyWNaZ8rC+e0+SQ4q10SpZbbY3VKs9+x92RpYIIbvGs61Bc2qQabW0ax1HTu2a/kBf7pqZegzZemwolRQDwXGZEgVAy1AEKeGkZnQJ81UNY4rGMb2uXmuIEqhFmqEsBIxS5UN9vldvDzByRNkmiBVgskVQuWgNcYJ4LYPwMxONYBpO588djgcsrm5ydraGq+++irVapWFhQVmZmb4yU9+QhiGvO9977tmNd7LRV1vFaTxs0YkCiFotVq0Wi1uvvlm0jSl3W6ztbXFmTNnAHj55ZfLyOtyfZrjZ/BuZHVhu2Gc1c8qDdjpdDhy5Ajz8/O8973vLX8443/firMaJTY1lOSGqi9K2fWVQZMMTejZ3hxXKsA23gppSJVDLYBjpx1mCyDQMBHUAs0odYhjRbXuYYSg30vo9EwZ8jiWvh3peUSRIXQ1ShnOne3iOA7CkcRxjh84CNflR8cFHyykihq1AlUPBL6HdCS6cNhjoIX0K8QmhKL5eBryl+U7e6vGU74xgl5mI6pUjVk5rGOqFZIfBjvxjtN5jnRZ6RegEyb33/ZSTUAU43Rbrz+kl1RASkaZ1Z/qxQ5pLsvaE0LSrDokacRmP0ToBCMdhJBsTlEnDWIbUaU5bHQFFZ8yolvrOiw2czb7Lrm2TcSr25JcC7YGtil3o2dh6OtdwZ4Zxdm2nTC3h9IKWM4plLLvaxMAAc2KYbZunVlnaD9XLbDFuyyH2YatS3VGgj2zBXGvl7Nc32LZ7eCrCJnHyDRBGNsuILLEKv0KCTffA3NXrsQ6TWM0DTJYXV3l5MmTCCGoVqusra2xsLBwxQ3yl7ILIQynofHnj7sck8Y73fPl+z579uxhz5499Ho9jhw5QqVSeVPUNa51XWieebdmdXG7YZzVldj5asFXa2fPnuXll1/mtttu46abbnpT0zFcG9wUrJMbJRrfE6SZlfsQwPogJFUuWgpCT5Pkgl6/D94MbtDAd0Ep2OpZvaM0tyv3QSyIRwnVegVZIOyEEKytjsAJyJSlZPJ8F50lOJ6H5zuQZ2ysDhBCEviGNJcIxyGOchwz4qdbijS3MhRWiiGiVrXpHum4aDVu+NVIL6ST+kixsyY1joiUNgSuQ5qrEmihNWyOKtQCQVp4tunf5LSy7zjSCjyHlb43leYTJY1RnE84/yzPn6LbG9JXc8zUcjpFTUtPNTv3Y1mm74R0SZSHwcHIKs1gRD+tYowgifsgrZpprgz9ke3ByiKrrrtayMZ3hhZRWPENJ9cls3UrAKmL9OBiU9EdWlHHE+tWlr4mNJ1RIWtvBL1RTigGRGbWNoh7hnNbsqRPmqtrUiXpjGyNKlPW0c03DI5jm6D31trsdjq4eYIsWCmkSjEqt+KJaYJxfLjpLljcf9Xf4WnzPI96vU6v12N5eZm9e/fSbrc5ffo0L7/8Ms1ms4zKGo3GW+59vFC6cOy4riTqmgZx3AimtcbzPG6++WZuvvlmkiQpa11nzpxBCFFGXHNzc6XzfzeyurjdGE/2Cu1aKZe01hw7doxXXnmFD3zgA29ix4CdzupaLMkMqmCDADuRZ6bC5sAjcHOLqMvt7Ox4VXzXTsy+Y9jsSKLUMqZ3R4LAtSmfQoUDx3UYDlLOrSaMi0aO64BSFiGpJXmWYwycOdO1jk0KBsMciUI6kmgY0+0m/MovzjE7M+njqE7lx/MsK49vhMtAVwCBLpp8x+a5U2mgqdtojGAzqmAQjNLJW6N0cl/Gasf2HkHoO6z1fYwRVKcW6/6UbMc059/WVpe+mgGgFzsEBYHvmC0dLLy/VdW0KpqzbbkTDZhXyn1y2aTqDHDVNmc2IUsjxqHjWlcyX58crx4q1jq2p2trIFlo6LIloTsSzNYU/cheZz8S9CLB3nnN3lnF2S3JKPUZ6Tk8Fw7uUgSeoVUzzNQ0S7MK34NmVXPToqISWLqo0DcEriHwcg7U1tnldvFUgpNFVjhRZzYTkBW0/I4LB+6EpUO8Vev3+/zwhz9k9+7d3HXXXczMzHDLLbfw4IMP8uijj7J3795yzHe/+11eeukl1tbWrktNWUqJ4zh4nkcQBPi+j+d5O36jeZ6Tpil5nr8JcXgj2PmL3iAI2LNnD3fffTcf/vCHueeeewjDkFOnTvH973+f3/zN3+SLX/wis7OzO2pW29vbPPbYY2W68bHHHqPT6Vzy3MYYfvd3f5fl5WUqlQof/ehHeemll3aMSZKEX//1X2dhYYFarcanP/3pMnV5Nef+m3/zb3L//fcTBAHvf//7L3g9P/7xj/nIRz5CpVJh7969/N2/+3cvKbR7MbsxnixvXxowTVN++MMf0m63OXz4MAsLCxc9/1uBxo+KetX4GaS5y3bkW0RfEZkMRgkSTW48Kp7tv+oMbKqpHwuaFbtaP7NqgQ5+4JDEOVJKNjYiEJbPLktzHNcKOBpj8AMPB8XauR5KCXynqCO5LkmSk4/6uELxhccWeeQDNU6fObODB9FxilqVMbieh/RCttMQZ+rHpqa+XNO9VUnRW+W5LhvDkKCQ+dDG6k2NLZhq7hk/a99z6EaTN8aACoB+LEqHNUoFoZOTjbYZiV3MVk1xDpsqHVuuJ7U+rWGjKzFMYOlgHc/0PtVKSGqaIBxiXaMqO/ZeIOgObeptsaE4se5SD00ZZa73HBYahlqgcdCc3nQQAuYbBU2RBqMNax1N3dki9OzrC00bnZ1tOwXwAtp9h42eg9aCJBdUA81iwyIC69WUg5UNFpwuXlak/rIEqTJLoRQNIB5ZBvvl22DPzbxV63Q6/PCHP+TAgQPcdtttb/ptBkHA3r17uffee/noRz/KPffcg+u6HD9+nG9/+9s899xznDx5kuFw+JavBawDcl0X3/fLP8dxkFKWUddYMWEacfhO2qV6rKSUpfP/0Ic+xCOPPMLdd9/NsWPH2Nra4r777uO//C//S/7kT/6E/+K/+C84evQo3/zmN/nmN7/J0aNHeeyxxy557j/4gz/gK1/5Cn/4h3/Is88+y9LSEh//+Mfp9/vlmMcff5yvfe1rfPWrX+X73/8+g8GAT33qUzvmv89+9rOXPbcxhv/6v/6v+cxnPnPBa+n1enz84x9neXmZZ599ln/4D/8hf+/v/T2+8pWvXOmtLO2GkbUHLirRMbYf//jHhGF4xTLRY6HEZrNZ/qAuZX/+53/Ohz70IZrN5hVf89jObKZsDRSetF/GVzdqzNcMnUji6j5K1JAS6qGdlGermt7I9t6kmV1JV9yM1HgMejG1uo90JL1OTKeboZVGCxcpJVmcEtYCG01pcH2HzdUucWxXcyqzhLZIl/5WnyxN+OQH13ngnirD4RAhBHfcfhtZap2iNyX94fgVthOrvCvEuHZjzXMhL1jQp+mUAt9jY2AJZseiiWBBE+kY/VcQ0o6PFbiC9YFF6tUCzbCQhm9VLKBivG3l3g1pf5PY3VPsa9NwBlvr8V1DXEDRZ6u2jnamPdGiAitTH2eiTBfO1RRSGE5t2ihpsz+Rhm9VFJ2RCxi8fJ2hmsMUVFSLDUV7IAv1XU3oabojC3W3Zlie1SAMZ9uT75vAcGDR9kkZrHpyGIDSNqIOXNjoO1R8Qz00hL6hEsYcqm4zK3q4KkZmUZH6sxEVWWaja5Vh9txqo6q3aO12m+eff55bb72VAwcOXPX+o9GIzc1NNjc32draIgzDEl04Ozt73SMfrTVZlnHs2DGiKNqxuv9ZNiSfbysrK6ysrHDfffdd8T6nTp3i7rvv5t/8m3/DE088wf/5f/6fvPbaazz99NM8+OCDADz99NMcPnyYV155hTvuuONNxzDGsLy8zOOPP84Xv/hFwM6ru3fv5stf/jKf//zn6Xa7LC4u8sd//Melkzl37hz79+/n61//Op/85Cc5duwYd9111xWf+3d/93f5v/6v/4ujR4/ueP0f/aN/xN/6W3+LtbW1spn693//9/mH//AflunQK7UbJrKCy0dXVxNZra6u8vTTT7Nv3z7e//73XxHU9mqk7c+3UWprOULA+micPitW2figRphCNwlsveb4iqQ3so2iQsDapo2IqjWP0ShHCMH2dooyDkY6ZKlVfg0Cy0Hnei6eY1g70yWJNdXA1qEcz0Vrg4qHuCLn//Mru/nUx29ma2uLOI4ZDoecPXuuvPY8z20E6IVsJz5+kbMzxkY/5f0RUzWCYo3juS7tKChBEaMUvAul/AxUi8Z/z3GI80lENf3U42wSHfVjgSAnGXaJ3T00iogoyUXZVK0LFoeJGc62J5x9jSI1OEol8/WpaFKaUml3s2+RfPYzC6LMoRpoZquafr6LZqgR2H03+g6hHFLzYuJUs7JtZV/G+8tC/uXcpqDpdWnVFGBYnjOc3HA4uyU5tyXxPcGZtsvKtkRKGxXuairqoaFeMTTCiFtrbWZlHze30HSZ5whjLHt61EdEA+uoFvdfF0e1vr7O0aNHec973nNNjgqgWq1y4MAB7rvvPj760Y9y++23o5TipZde4tvf/jZHjx7lzJkzxHH8lq93bGNH9YEPfKCMuoSwsifT6cIx8vBnYdfKC1ir1fjIRz7C7/3e7/HFL36RVqtVOguAhx56iFarxZNPPnnBY7zxxhusrq7yiU98onwtCAI+8pGPlPs899xzZFm2Y8zy8jJ33313Oeapp5666nNfyJ566ik+8pGP7GD9+OQnP8m5c+c4ceLEFR8H/pIBLFzXJcuyS44xxvD6669z6tQp7r33Xnbt2nXFx79WtKHWhjg1hJ7k+GaVZqE80BumSEdjpG9lPqhhDNR8w7FTklbV0BlKVjcS3DDED23aL6y4ZFnKmdN9cmXQRlkgRdH7hHBI05wg9Om2R9aJuQ7DSCOFRjge/V6EzjJ+53O7ObSc8vzzx1heXub2228niiI2NzdJswzf8zDGkGSGVNuISsoxAs9GAGNLMl0i/rJc43s29WcQ1ALbOwY25ZcV24LJsTIFjnTYGHg4wpQgiEEiqbiGKLdpsGbF0I9tKi8ebpM6NqKa8pU7RBG3RxYA4Qg43famYOmi1MECUTgvjedoTm44Oxp9N/uyjORyBa1Qc65A9XUin10tTXtgzyeEQ5KkhfRJ1QJBug7LswqlNSvb1hF30xYiM+ybtxHV3nkNxtInZUqyZ9am+qx8iCY3Vpa+4o04WNuiKYaFo4qRKkGktmYpcosmxXcwCwfg4N1X/Z09386dO8exY8e45557ruo3cym7EOv55ubmDiaIMUij1WpdNUhDKcULL7xAmqZlryS8Mw3JF7q2qz32YDDYwSiyurp6wWexa9cuVldXL3iM8evnC8Tu3r277CNdXV3F931mZ2ffNGa8/7Wc+2LXc/DgwTedZ/zeoUNXXl/9S+WsHMe55IosyzJeeOEFhsNhKdp4tce/FmcVFVHVdhSSKckoSUELjFOlUdH0IhhP2FrDG6uCVEkcUaz6E6h5mkrFo9+LCSsuo2FGlEikdMnjDM93qQSSLMvxAxdXKjbOdUmSnMA3JLlGOi7SKHQywiXjtz+/m4VWlx/96Bh33HEH+/btAyar39FwSL/XRRnBiCpuEWhHiSoQfLaJt6RWYkKt5LkuoywsI6pcTZxSlE1ok0bphPNPGYc0n/Q2tQoiWrB9U1Eh+ZHmFo2YRkMyZ6lECfaiCeFsmosJRB0LyDjVtpPV9lCWEh7dSE6cl4HAVSVB7BjJt1E0+mbKpiSlMZzacFhsajZ7FLB0my6UEtY6PrkK8F3DfBDTHoY4JGxsZyS6SsPrUanU2B5K5huGM0Wk50jD7hnByU2JwHBwl3VQ++YVUtootFUZcKCyRZ3INvuqBKGygr3fIKK+9dquh7njIahefcr6fDt9+jSvv/4673//+y+oeHA9bJp/b8wE0W632dzc5MiRIwghSsc1Pz9fOp6LmVKKo0ePopTivvvuu+D4q21Ivp6O61I1qy996Uv8/u///kX3lVLy7LPPAhfONl1Je83571/JPuePudZzX8m1XOz4l7IbylmN9XEuZpeCrg8Gg7Kv4fDhw5f9sl/ILqYWfDkbJoatUZXc2PpKlDvUAkWippgapE/oaV46JZmtQS+Ck+cy6k2HStUjGmU0WwFGQ3tzxDACpXLCioXIGWOQrkseZfiBSzSMSeIUIR0Lo85ypOsyGMTkScLf/n8vUXVXePXV0xedhCqVCqMoppsESEfgubaRWQhBEvUJKhZCm2UZiLGEhsF1nII2SuAIm+JLCo7DeNxPNUVW6zsCbSTtgWd7iQqzdZ4xC/pEUDFOIY17RCyCgZlQsT168xe7F1nOvrGjmq9N2NJnqoqkqFdtjyzDuudoTm26F4SlDxKJMtDwNCtb9qFt9CS7W5r1rnVYQhhGMVR8i/hLc0E7D9k7p0gSWO/bxVE/azLIMup+RJR4LM04uK7AldYhLjY1vmdTirtnrMOq+oaZyoibKltUTYQzjqjyGJEngETkKTguZt97YPdbR/wZYzhx4gQnTpzgvvvuY2Zm5i0f80ptuidJa02322Vzc5M33niDF1988ZL8hXmec/ToUYwxOwRaL2Vvd0Py+XYpZ/X5z3+ev/pX/+qbXv/2t7/NP/7H/5h/+S//JQcPHuSFF15gbW3tTeM2NjbeFDmNbWlpCbBRy549kz679fX1cp+lpSXSNGV7e3tHdLW+vs7DDz9cjrnac1/ses6PxNbX14E3R3+XsxuqZnU5uxh0fX19naeffppdu3Zx//33X5OjgmuPrM5t+2wOPfojjVQjhHCoBPYHlCuDEAaES6cLSSYZRQVvmCtJ4gzPc8iLBqQ0U2y2c6TjoIsUnOdJsiRHCFvf2Fztsr2d4kq7UkQ4+IFEZxFSp/ydv7Eb0p+wurrKBz/4wYuuloWUaLfOuGokpvJs1dpkxa6NY5nggTTN6KceYy75yhTcXE6xVYwBgwKIlVMAFiy4pOrrYoygGY6dl8CXCowiTRNkMLnmTmRZ0MFC1Gcq9uDKQN3XrHXtsUeZ1bgC2Bw4zFbHdSgLsFgtJDvWunIHOlAIKz1ics2ZtsN8c8KUv9aVLDQNSy3FypZge2jplZbnNGBoVTUbXVjvB8xUYpbnNDNVTbMqGWR1enHARkex3U043XbY6luFYGMs6a0jDc3QMFftcVNl0zqqNEJmCUJnjKWxRBJhZpYw937sujmq48ePc+rUKR544IGfqaM636SUzM7Octttt3H48GEeeeQRlpaW6HQ6PPPMM3z/+9/n2LFjbGxskCQJR44cAbhiR3Wxc14NNP5q7VI1q/n5eW6//fY3/c3MzDA/P8973vMewjDk8OHDdLtdfvCDH5T7PvPMM3S73dKpnG+HDh1iaWmJb33rW+VraZryne98p9xnPEdOj1lZWeHFF18sx1zLuS9khw8f5rvf/S5pAeYCeOKJJ1heXn5TevBy9pfKWZ3vTIwx/OQnP+H555/nve99L3fcccdbak68VoDFSk8iTYwRLq1GWFybfW+UCKqeYWVDlXREq227ogsrPnE0ZiiHaJjSHxjyQvJeSGFlRTwXVVyXyVPiYWJh9kaicoVWmkE/ZdSL+W8+N89o+yWSJOFDH/rQZVOhM7WJY48zXUrSK23Tf2BXm9UwxGhFN2vSH0TlPqPUTtpgI6lxP1WaQ9UHIR3aAwv7Hps79a3bCagw5FlKpBsME8lM4WyUFjv2T1WBApRwasulXgAvolSW0HGAOLc9a76jOLVpFX2Bog9MEnoT4UdP6LJPar0r2TUz6aOS0mplNSoTmZZzW5L98xphklLWvhOF9COb4tRGsqulObCg2D3n4vseVXeEK2LW2hEqG5BkGZ6jWaz1OBBuU2Vk6ZNUYpF/SQRZilAKc/uDcOjeN6n7XosZYzh27Birq6s88MADN1wTaqVSYf/+/XzgAx/gox/9KHfeeSdCCF555RW++93vMhwOWVxc3DEBvhW7EDTedd0d0Pg0TQvKrisDaVyrltV0j9Wdd97JL/zCL/C5z32Op59+mqeffprPfe5zfOpTn9qBxnvPe97D1772NcD+Vh9//HG+9KUv8bWvfY0XX3yRX/7lX6ZarfLZz34WgFarxa/8yq/wW7/1W/zFX/wFR44c4Zd+6Ze45557+NjHPnZV5z5+/DhHjx5ldXWVKIo4evQoR48eLZ/NZz/7WYIg4Jd/+Zd58cUX+drXvsaXvvQlfvM3f/MvfxrwUjadBszznBdffJFOp8ODDz54TXDzCx3/aiOr9c0ucbZIKHNSgvIzWCkKQ5JBnMIgaxBvpxjpEFY9sjjDrQUlMFxKwclTQ7zALydv37cIQLca4AhBd2tIr2ORB8oYPN+jErrEowSTRvw3f32GrfUXmJ2d5a677rqiH4vvSaqBLPvEPE+Wch/TT8MIh25WBSFwwwYOOQoXbSR53OX/3955h8dR3ev/MzPbtFqtenUvuGFsybIB00tCM8aGcCGNBEISSCGUmwQIKfeXEBJSgOQSQyCk3xAu2AZCIHSb5nCxJPde5Ka26tt3yvn9MZqRZMtFXYL5PA/Pg1ej3bOj3XnnnPP9vq/LZwZVelwCLWXOVlQh09Ixo4qlOk1p2xMSXrfpOJ/UJNJcSeKqjK6D1+fHas9R9c5lwpZYZ5xHSpNIdxscau3YUO+SRdUUke1SeF2XCPg0DjSaYwh1CVlMqBJZftMCqzUCiZRCYZZOfUe8R32rQl6GgSIbHGw0z6NbEZTkGNQ0S+QHDQ42Sugijcw0lYBfRtfMLKx4yjxzLkUQT5nLtFnpEjlBF9GkRMCXQleTCK0dEQvhd8cROoCOjIGsp5BER5NvVhGiaEp3G5B+YBgGmzdvpr29nfnz5x817HSkYPkXZmZm0tLSQlZWFnl5eYRCIXbu3Inf7yc/P5/c3FyysrL6ve90tOXC3mZ19TXL6nAT2//5n//hG9/4hl25d8UVV/Dwww93O2b79u20tbXZ//72t79NPB7nq1/9Ki0tLZx22mm88sor3W5KHnzwQVwuF9dccw3xeJwLL7yQP/7xj93GfCKv/cUvfpHVq1fb/y4rKwPMqsSJEyeSmZnJq6++yte+9jXmz59PdnY2d9xxB3fccUevzg2MsD4rq7z0aFhhaAsWLKCqqgqXy2WXqw4EGzZswO/3M3Xq1BM6vqamhvU7QiQyFpAX0GmOyqR7BZpuliJnpxu0R6AtYi4n6ZrZ2+P2umlriZGd4ycWSeLxKhw8GCMe10hL96KmNCQJvD438ViKNL8HLZ6gqSGMZpjVeqmUaVqrplTUWIxv3ZhOe9NWJk6cyKRJk3p11xKJa9Q0d/RZyWZ1o/XbLgVkWaEh6iXdK9n7UAFvZz+VW9ZRjY4vt64hSwaqrhDV/aR7dOIp88tsJvse+f8i1Y4upxPTvOb5UQRJzeqb0mnpiJzP9puO6IYO0ZSMMEDtcLfIy9Bp7tijykwzYz903UxXzsvQ7T6qgM/MFVN18+8jGUZHfpT5PEW2YNER7SF1OIt0XpDG5+m0tidoTfjt38vPNGiLmm4fAZ8g0y+IqzJJ1dyLM4REht80OPZ7BV63ID/QRoHUBIk29HgEEY/iRcMtyyhpAdyTS5F9A+cTZ1XPJRIJ5s2bNyDR7UNBKpWioqICv9/PKaecYouD5V8YCoVoamrCMAxyc3PtQo2Bui5YdC3SOLz5+PC9rvXr15Obm2sXNZ0I999/P3v27OFvf/vbgI77w8KImlkdD6t0fc2aNRQXFzNjxowBreA50ZmVEIIdO3Zw4MAB8iecxYF2SOlmtVc0CdnpgnBCIpWC7QdlhKaiGQZpfjdqPAG4zSUGw3RL37e3DdnlQZLNJTG3RyEZN0v0XW6ZaDhBSyiC0M10WMnrxut1E4/EEakE3/iMTHuT2cRnbbD2hnSfglsxK/90A9LcMknVyuNxUR8xS9q7nppYyuwpMgSohmKb1SouBTVpEDXMi2wsFkFymbPepNo5UwrHJWRSJj1AVwAAc5xJREFU6DroSgZet0xM63SlsIIT42rnjCyaklEwaOqYrXV1Tg/HOwMSUxqkuXUOhV32zyyvwUjCXCqUMKhrltANs2zcjOeAulaz50o36OjDMgV7TI5OTbNMUbZBdYMMpBPw6WSmS8iyQUtYsX3+0jyChnazwTcr3UCSJeJJs8/MJQs8bkFxoIUCTzteXUN2gez3IbklUqkUtSKN/aEYRv379oU3Ly+vz3ux0FmUYBhGtzLvkU4ymaSiooJAIMDs2bO7fd8PD0lsb2+nsbGR/fv3s3nzZrtIYyD9C+H4sy5JktA0rdevF4lEnEj7YzBqxEoIYXuPzZ49u1d3LCfKiexZaZrGhg0biEQinH766ayvDSAhiCYkMtLMRlZFFqR7DHYfEBi6YVbxReLgd6PpZkii3+9GU1VqauMkU4I0lxm5oXZE0Usy6JoOhkGsNYwwOqoBVQ1N1TB0g1QswVevVdESDf2q5pIkicx0N43t5lTJaq3yuBUaYx5kyZwxJDSzoCKeMkUq3QPRlPUcpnhpwkVK9naEVEkIVxBZJBGSl6Qm4RYRVClgFmgkI2iuHGIpFwaGHRncEjWr9+KqTEKVyU3XiSRlkirIkllxaTbzdi7rJTvSfZOqRDQuaA4r9uzNaiK2zGcRAmEIdMO8+NS2yBRn69S2yLgVs6JR1UyvvvYOM9pDzQpjczRa2mKAWZQSSShkphscalTshu/ibIPGiIKqQ67XbEbOCpjW9i6XIM1jUJzWSq67Da+WQNZTyIaOrKUgqxBf/gQmyTITOy6+oVCI6upq++Kbn59Pfn4+fr//hC+GqVTKXokoLy/vc8THUJNIJKioqCAzM/O4y9pd4zqmTJlCMpm0nTSqq6vt5cT8/HxycnL6XJjRlaOVxkejUaLRKC6Xi1QqdcIJybFYzMmyOgYjSqyO9uXTdZ0tW7YQCoUAGDNmzKC8vqIox7R8isViVFZW4vV6WbhwIS6Xm5ZoR0NsSsItG4CEMATrdpqb7PFYikDQa3vKSbJCKqni87mpOxhBM2QURUYSOorLhdqRD+XxuknGU7Q1RkildFOsFBm3x008mkBSE3x2URiPEqds/qn93nvITHfRFE4hhOnA4HO7aYi6AYmADyId7W1dCv7MDKmOjJCkBgYK7QnzI5XpM2hLSObv+z2EO2oydNwdSbYJ4iKApFlJyGaFXkvMvPB7XQYdk0uSmoSmCSIJ8yKbn6ERCrvMcnIzlASQSKoSkmEQTXbc+QqBIgt0wxTAwkwdXYeaJgmBTEm2Tk2LeWxti0Jxtk4sYYYlgrnnZM2oCjN19je6gCBpHoOcgOmO3xwx9+fM6j5zuTAzzcCfZS5VelzmKDP94PXolPibyXVH8WhJZMs9HQMmzEbydl6oul58p06dSiKRIBQK0djYyO7du/F6vbZwHWuvJpFIUFlZSXp6ercltJFOPB6noqLC3n/t7SzF8i8cM2YMhmHQ2tpq73PF43Gys7PtWddACIR1XpPJJBs3bqSkpMTOCTvRhuRoNNrrcu6PEiNKrHoikUjYpaoLFizg3XffRdf1AbkzOpxjLQM2NzdTVVXVbfkxkugov04TxFJgAH6PoHIHxKIq/oAXTbfK1M3ydJ/fg5pIcOhAnLb2jgZfj4tUSsWT5kJRZHRNAyRibRFU1RQxTVVtZ3WRjPPpSxopKfQyZ86pA3IuFFkimOaiLabhcStEVVOowIyft4gmwaOYy56abu5dJTVIGUrHrMfErNHo6KGKW5H2EobkRdFaiRlZqMKFW28D2SzOaI8ZmMXnEq1xhaBPRzMk2qLdqwHb4oodWd+ekCkImjOq5jBIdF/yy+/ImwIJQxcIA0RHEWxNh0DVtij4PaapsCSJbjOqmmaZoiyNtvYoEkEEZgGFJAmqQ50zlHF5hi18WQFTaDP9wszz8grSPCpj0tvIVsJ4tCSKnkIWBlJWAVLO8W++fD4f48aNY9y4cei6bjfUbty48ah7NdYFPysr64QLbkYC8XictWvXkpeXx4wZMwZk+S4nJ4ecnBymT5/ezb9w586dA+ZfGIvFqKiooLCwkGnTptnjPtGGZCcl+NiMaLFqaWlh3bp15OXldbu7GiyxOlpTsBWeNn369G6eaS2x7h9qXYdte1Q0w43a0Wjk85jlr26fl3g0RUamj1hMJZ4El8eFIhkgKagGIARur4t4NEm0LUoyaZhFGbKEy+0mFU+CFmfJuQ1MnWz2agzkBSgr4CauQijqQcIsMdcMOpwdujT5uiwxMhuAk7qLSFJB6iiOUHWJuGot0UkdS2Qqqu42lwRdWSTjHU3GcpA0t05cVdCEG0VtRHhMZ3xhqLRGvKR0mYQqCPqsgEWpm0mtpkMiaWZ3gZkNZWZlSYTaFYoyzbYBy0ni8BnV2BydULtEpGMmaM2oQm0yAZ/GoWY3kGU6VmQY+DyCcNycUasaFOcYxJIyxVk6Xjc0RRVyArqZv+WHdE+SMf52gkoUj5FC0VNIsoxUPBXJ1fsiAEVRutkYhcNhQqEQ+/fvZ8uWLWRmZhIMBqmtraWwsHBALvhDRTQapaKigoKCgn63ohwNy8Fl/PjxaJpGc3MzjY2NbN68GU3TyMnJsYXf5/Od0HNaNwYFBQXdhAq673VZM6yeGpJrampGzRLtcDCixKrrH9gSiMODEvsT43E8Dp9ZGYbB9u3bqampYd68eUc017ZEO2YemlnWvGmXTlOLRjDLbZcae7wKibhKIMOLhEFzY4zmJhVDSLi9LgzNtNpxeVxgaBjIJMJRknEV2eXGkDoiQRQZSY9x1Xk1zC+b3Gej0WPhdcskDbNCzEzyFUQ6BErvspUXTZku6pIE4ZQLpeO9ig4n8ta41avV+feMJBW8UoJ2zd8tzVcg4esozgCQvLko6MgiRW2LG7doASW3S3qw5fOnkJuuI4DaZol0X6fXYHNEtl0qhBComqBrzWvXGVWm36CmRepwOzcj6DVdorFdJuBTiUfDKFIWulDQDXMrrrqhq30SHGx2IUuCMXmmuBdmGghJwu8VBLxxSvxhgkRxa3EUoaNk5iMFjx433xskSSIYDBIMBpkyZQqJRIKDBw/aJqGNjY1s376d/Pz8QXE9H0gikQgVFRUUFxf3GE0yGPTkXxgKhXrlX2jNBPPz848QqsOxzv/hCclvv/02VVVVXHDBBYPzRj8EjCixAvOPt23bNtte/3CBcLlcAxLw1hNd73xUVWXdunUkk0kWLlzYY9R0S0cceTwJe/bG0ZU0hDCvuh63Qiqp4fW5UDuaaLVUiuZmgSQrKIb5HlTdLP12uV0kYiqJtjCxuIGmCbwusyFYTSRJJSJccnojZ59xMvn5A3Oh64miIB1ehhDr4vGXUDuLK4SANC+0xN3EUrLdlAsQTkq2aMRSEl45QdLw4VEMJCmNVEe5ete+q5aYQoZPJ5wwrZLyA4IDTWkISUIo2SjC7OlqTyikSa3ERRZgNus2tJpxHeG4RGGmTn2HjVJdm1nVp6pQ06wgISjMNKhvs4oqFMblahxolDssnswCiZJsnfaYjGFoNIY9QC6KLCjJMgMSEylsx/fMdHMpcmyODpJEfavM+HwdTTed9DN8cYp9bQSlBG49geJ2o+RM6NNs6kSJx+McOHCAqVOnMm7cuCNmDV2XC0dS6bolVGPGjGHKlCnDMhPs6l84efLkE/IvtIpA8vLy+jQTlGWZ999/n0996lP8+te/5uabbx6kdzf6GVFilUwm+eCDD9A07agC0VdLpBPBeu5oNEplZSV+v5/TTz+9xyVHQ5gJsR7FYMeeJM3tkJkNLpeMrht409xEw0m8PheSBNFwkqZWgarqeNMUhKF3lKm7UVARhkKsNdrh9+dGUWRSSc30lBMxPnFRiPPPmTvobgOZfuwydN2QyPAJu7jC7r2SIZx02c2vkaQpWNGUjG5IZKV1zq6SSQ2vT6U17kE3JLuxN6HK5AQ6e6MsWcxMM9jf5CLdI2hPSKiGTH6GoDFiHpUQQdzEMdQo+0M5ZHjiJDAjWeo7BKq+TUGWBMmU2TMGpmtFqB17RpWboVPdIJOVLkikTBsoVTdFTxIphBrBo2SR0hU8LtNpo7aj/yrNI/D74ECTmVRckGWK17g80/nd7xcEfDHGpEcIGDEz3TeQhSvYc/DnQBEKhdi4cSPTpk2zq2WtIowZM2bYs4ZDhw6xdetWO5o+Pz+fQCAwbEuF4XCYiooKxo8fz+TJ/Q+PHCiO51+YkZFBLBaz98L6cv4++OADPvGJT3Dvvfdy8803j5rl2uFgRDUFh8Nhtm/fzsyZM4+6J/X2228zc+bMoyb+9odQKMTmzZvRdZ2xY8cec0rfFpN4Y6ub6uoIuuwnGk6QnuFFCEEirpIR9NHeGiczO41UPMmBg2Ypq5pM4UnzYug6hiHw+jyoySThljDxuEBXzYgQXcioyRR6IsKV50dZetnsIbsTDoWhutH8f3MPqlOo/G5oTbiJq3KHKJkX8AyfaQYL5vKY6VnoIs2lkdJctHbsUWWn67R0OKW7FXN5TutYLswPaOxrdGF0OLK3xc09JMCuFAQoyNDYH5LtpUEfTSQwZ+ASgtyAQSIFTWEZRRZkp3eGKwKMz1PZXacgOoIY3YopOPGkRDhmkNLNz54sCYpzDFyyWUhjZXj5PGYpvyJDQpWJJCTG5pk9cGleg5xAlCJPO36RwC0LPNmFSO7B/dvV1dWxefNmTj755BPqteta2t3U1ITL5bKLDHJycoZs76StrY3Kykq7mX200Nrayrp161AUhVQqhcfjsWddJ3r+qqqquPzyy7nnnnv4z//8T0eojsOIEishxHH9vt577z0mT57cp+bX4732tm3b2LdvH6eccspxy+Nf/kBlV61EQ7MgIzMNTdXRdZ00v4f21jhZHe4Ubo9CqCFKJKziS/MgY1a4KW4FRWgI2U1rQwtqIolquEACXdXwuEGPt3H9UpVzzzxlSDdeDQPWHzD3XwACXrPZ2a2AEIrdlOvpSOy1BCXdaxDrcKuQtVZ8aRmE2l34PB2R9R3ZUukeQbTjOMspPcuv0xQ2l+SsvS6rRB0gzW2gGRJBn8GBRrOYob69M9036FVpS3iQRQo9FUNW3CR0c2auyGZhREPHzOtAo0R+UBBNdib85mXoRGIaioiiyVkkVJnsgEFSlewZZHa6gWqY1YDBNIPMdHPfzqUIfB5T9LLToxT6IvhFHI8vDXcwZ9AvQgcPHmTHjh2ccsopfVoiNgyDlpYWQqEQoVCIVCrVbbnwRIsMektraytVVVVMnjyZCRMmDMprDAbJZJK1a9faVZbW+WtsbLTPX9cijZ7aSjZu3Mhll13GHXfcwXe+8x1HqE6AUSdW77//PmPHjh3QXivDMGxjT8Mw+PjHP37MMf71+VYOhd1IaUHUlI7HLaG4XITb4gSz0oi2JwgEfei6wYF9bUiKi1QyhdfrRpIkUikVn9+LLDRaQmFiUR1d0/B4ZAxJQdd01Egbl5/TQnFOjOzsbPLz8ykoKBi0C8fh1LTCoRbz/31ugWFAJOkmqUl4XGbvE3SNnod0j0ZMNcUlw6vTGHbZtkldZ0Zdf0fuiJE/0OQCpG6BiG7F7JNKqGb+U0GGxt4Ga8YtyA8aNEU6jw36DNpjHdEhso5LSpLscNKQEBRnJqlu7JzhuBVBQaaBpkN9q2SXtNMRQS+EWTRizqIEsiKBAEWB5ohiilNA4HJBmluQFwhT4Anjk1TSMrNRvIPvu1ddXc3evXspLS09IkyvLwghiEajdk9XW1sbgUDAXk4cCCcIMCt9q6qqOOmkkxg3bly/n2+osBw1gsEgJ598co9ZTdFo1J61tra22v6F7e3tzJ49m927d3PZZZfxla98hf/6r/9yhOoEGVF7VifyRztaTEhfsbr7NU1j7ty5dk9Xj8eqgj+tbOK1NVFmlo3Bqxt4vS7URAqly7Kl1+8mHk8Rbk2QShl400xTWk3V8fjcuF0SQhi0NEZIxhMIQ0FxudANHbdLRaRauevLmZTNmUk8HicUCtHQ0MCOHTsIBAIUFBQM+j5DQQbUtpp7c4YhoeoyiQ7hSXMbtlglulgoRVMuXCRI83moaXWRky5Iduw1xbsUVLTFzZgPs5fKNJ21Zmeh9k5XClWXyPAZpDSzbH1vg6uLmElmFpXPIJIwm2/jqc4KRNVQkF1pZPk1WqIKfqWd6sYg6UobSZGOZrhQdQndEDS362S442hy0CxBzzaoaZbt5yrKNmjoCGgsyNRRFImcDAO3IkjzgMctyEuPkOdqJ80jk5ZZiCQP7kzYShw4ePAg8+bNIzMzc0CeV5IkAoEAgUDADkm0Lrz79+9HlmV7uTA3N7dPM/7m5mbWrVvXbW9tNGB5FB5NqKD7+Zs4cWI3/8IbbriBuro6MjIyOPXUU/na177mCFUvGFEzK+CYDhIA69atIzMzc0DWt8PhMJWVlQSDQU455RRUVWX16tVcfPHFR3yI2sI6v/h9CEWBbXtTlJ0xET2l4vF57RlVMp7C7ZZxe90c2NuCZshoqoasyLhcCqlkCp/faxqjNrUTaTd9iyQM02EbSLS38p2bcyg75chOdlVV7aWaxsbGE3Yx6Cv7m6AlBm1xNx4Foh3LYbIkUKROE9l0V5KoZs5YstN0DjS7EMhHmNLmBjqTff0eA7csONTisn9mzZIyfAaRjv4sRRLkpGvsC5nHuRWBxyU6BM40pvW4BC1hiYRqFoRoBvbSnVsRFGXp7K3vvKi6ZA0v7RiGICa6V5tOLNBRNQlJFiDMpU8Dc3blcUFtq0JuhoHHBWk+SPMYFATC5Hqi+H0e0gL9d/8/HkIItm/fTkODabM1VH5yhy93JZNJe7krPz//hGb9jY2NbNiwgRkzZlBSUjIEox4YUqkUa9euJSMjg9mzZ/dJZHbs2MHnPvc5MjIy7FyuBQsW8MQTT3DyyScPwqg/XIyomRX0Ly24NzQ0NLBhwwYmTJjA1KlTkSTJLls/PDjtQG2KX/0lxIFajZICF/6Ax/T66oj3UFwyhm7gTfMQDcdpbooTi+m4vRIutwtV1XC7FVNMDJ1oJE6kJYouzBwds2dJxUiG+eHtY5g5NavHMbvdbkpKSigpKUHXdZqbm2loaGDjxo0IIWzh6usd7+EUBmFfk5uUJpPSIODViabMUvGgr7PiL5o0QBFk+AT7m10E08z8p8NNadtinWazbkWgqp1f+GhStq2Rwh2uFM0RGa9iBiJaS4eqLuHzdDYfK7JA17CzwsIJ0+0iI80gljTHubfeNKttbDcr/lRdIS8rg1CbQYarhaSeRkr4yPK2sS/UOUMZm2dwoNk8j+PydAQSxVk6LpfZl+XzGBSlRwh6owQDAdzewV+iNQyDLVu20NrayoIFC4Y04kOWZXJzc+3wwFgsRigUor6+nu3bt5Oenm5/BoPB4BEX9FAoxIYNG5g1a1a3FNuRjjWjCgQCR51RHY/q6mqWLFnC4sWL+fWvf40sy9TV1fHSSy+NKtEeTkbczCqVSh1TrLZu3YokScyYMaNPz29Fee/atYvZs2d3+9JomsZrr73GBRdcYFvWbNge55d/aGBMoZvd+1UkCcaMz6RwfC5qIonba86UhKbh8nqoO9ROIikwDNPE1uNzk0qk8HjdyIpEe3OYcGscWZZQVQ2X4kJWZJKRVn75nfGMK+m93YoQgra2NhoaGgiFQiQSCXJzc+0LR3+iErbUuKhp7TLjSXbOrhAaBqZ7d1bHjAo6l/E6Rke6t3MmlBswZy61rQqKLDqW78yfdXVR97oEfpdmz7wCPoNESrIrB3MzDIQhCLWZj+VmmBEd1s8DPoPMNIN9XSyR0jyC7HTTMaC+ves5EYzNUYkldHQ9iappeN3gcvtRFA/+NJn6NoWSbAPNgKAf0jwahelhctJUgpkZyIO87AemUG3cuJFoNMq8efOGbP/yRFBV1V4ubGxsRJblbj1Jzc3NbNy4kdmzZ48q/7ujxZP0hkOHDnHRRRdx0UUX8cgjj4zoxuyRzIibWR0PRVFQVfX4B/aAruts3ryZpqYmTj311CPW+bta/wO8vTbCsr81ohvg6Yi3FQKyc827WdnlJpVQSUv3kNQMwuEo4bCKxy2bPVuqOaVwuRVckk4yodPaFEECZMVNmtdscJa1GN//xtg+CRWYs9GsrCyysrI46aST7A1yq5/GcusuKCjosXftWEzM06htNcvEwwmZdK9OrGN2JaUi4Mkmwyeoa3MjY/ojhhNylxwqc/YDZpGDWV1nPrduSKa7ese/Q2GFnHSdaFJGGAYtsU4PwEhCJi/DoDFsPpMkBAJhi1NTWCY73bRZEsL8Q+0LyYzJMeM/DCGRVCGZ0miL6uSmR0kZaUQSEsXZgkMtpnjJko+ibIP6NhdoBkFXiFTSQ5ZXxtA9ZAbcpHs1CvwxctINMjJ6djUYaHRdZ926dWiaxvz58wc8q6m/uN3ubj1Jra2ttumuNfMvKSkZkJDUoUJVVbvfsq9CVVtby2WXXcZ5553HsmXLHKHqByNOrE5kGTCRSPT6ea01YiEECxcu7PGu1HJC1jSNvz7fzAur2s0S8iS0RTqLOlxeD8mESlqaBzRTOCMRlbZ2FcWlkEppeHwKkmwGLrrdCi1N7UTaErhcClpKM4XM5UJLxnn4R5PJyhyYPpzDN8gtt+5QKMSuXbvw+/12gUZPSzWH4/eY9kF1Hc4Q3Y72ZpHhMTjYbM6u8rrsSSW1LsnAcVO8kirUtrgIpnXGgTRHlW7hiLoBkjBoiXYEIGboNIbNAozGsExRlumcfqjDOb0oS6euI923JWoGLUoIaju8/w51WDFlpulE4xqNEfM8N0XNCr+SHANdlxiToyNJEi7F7KnKzTDwuqE9UUCOP0lCNVBTEWKtcXJykriEgs+XNyRCpaoqVVVVyLJMeXn5oPhiDiRdjWMzMjLYsmULJSUlxONx3n33Xbs6Lj8//6gWRsONqqpUVFTg8/n6LFT19fVcfvnlnHrqqTz++OOO718/Gdmf+h7oy55Ve3s7lZWVZGdnM3v27GN+aAQu/vbPMK+tMQs9Sgo87DmQojak4vNKaIaM7HKhRhOkpYGsSMQiSdraVVyyQEgSWsczuT0u051CM4i0xdE1HcXtwu11oaV0PCLCN24cM2BC1RNd3bo1TbM3xysrK1EUxZ5xHcs3blKeTn2bObuKJBUUEcGQ00l3C7uvCqA1JuOWBaohEUvJdg+VIpvpyaGOkvT2uBm5YS35tXVE1isytEVN5/rOzCqlQ5A6xEw3G4mtMvO6VoWCTIPmsJnA2xY1o0KsaA/RERrZHtVJpAwKggkSmodESiLoF9Q0dy47BtOhtlXG7xYU5RjEUxLFmQaS4iYrAOk+QY7PgESUQwdq2NZhGmtdeAfDMTuVStmxNHPmzBlVF7xDhw6xfft2SktLbdu0rtVx69atAzjCwmi4sWZU1jnvi1A1NjayePFiZs+ezR//+McRf4MxGhhxe1aqqh4zAPHgwYPU1tayYMGCE3q+uro6Nm7cyJQpU44b994W1vnuAztR3H5qGswxzJriZctuU7gmj3PTHPMw9eQiYuE4gaCfRDTJoUNhs3RdV5HdHnTNwKUIJJebSEuY9pYIkqygqWZ6qOJW8IgYy/7fJPz+4flydm0EbWhoQNd1cnNzKSgoIC8v74gv17pqQWPMnI0GvGaxQW2ri8ObfLvOrlyywKOYQtUUUbr1ULlkgc/TKXYFQY26ZrmjFB6Ks3RqWzsvzIWZOoYBB5vM40tydFtowHRSD8ehJdJ5Ycn0G/i9gtawTkztXDZL8woCPtP/wq2AopjCqOoSmgGyLCEBwXSBLIHfJ8jwxikKqBTkeHB1CEYikbDFv7m52Y6asGYM/V3yicfjVFZW2hVoo2kJ6cCBA+zcuZPS0lJycnJ6PMbaa7Vm/rFYzM6ZsgImhxprFut2u5k7d26fznlLSwuXX345EyZM4H//939H3JLtaGXEiZWmacfso6qtrWXfvn2cfvrpx3weqw9l7969zJkz57ibugfrUvx+eTObdiZQZLPxM6XC2CI3B+vMpb4Zkz2ERYDicdkYqopuSNTVhMHQEbIbTdXweBSQZCQ9BS43dfub0JIqbq8bAeiaRrpX5YYr8zn7tKzenp5BwYoEty4a0WiUnJwc+8IbjUZZv2knRv45gDnzSagSrR1Nvll+3f5/CWE6XqTMyj+/2+Bgsyl8kmT+zCq+yPIbtMclMv2CpnbIDRi2QEkI8oIGoXbTOb0gQ0fVsY1qwYr6kMkLGDSGTYHJCwpqW8znD6YZJJIaCI3soJtYSulwX7fiQMxjkGSiScgJmLOreFIi6DdQFPC6MA1pswwKs9zHDAhtamqyzyFgX3Rzc3N7fWdt+VPm5uYyc+bMEblUdjT279/P7t27KSsr61V6tdVT2NjYSHNzM2lpaXZP12C0ZhyOpmlUVlbicrmYO3dun2axbW1tLF68mMLCQlasWDGizIJHO6NOrKzm2LPOOuuox+i6zsaNG2lra2PevHnHNX/duCPOA38IMaHEbc+iJo11s/eg2l24Cl2kFxbgz/CiyBL7djeDbBZPaEJBkiQ8io6Om0Q0Qbi5Hd2Q0HUNIQSKohD0aTx4zwT8/pG7nGOVJDc0NNDa2gpAQUEBUvYphJNe6ttdBLxWZaDlZKHTFu90qDDTfc0lvq5+gME0g3Bcsn39SrI09jeaQYeSJMhJ73SlcMmCQJqBjBk9L0umgDV0EazxuRq1LbLdVwVmpaDXZVDfCprRKRLZAdOVwq2A2wVet1m8oWpmEUhcVQj6DdyK6drh9wky0xIUBQ3ys0787rinGUNX8T9eFZ/V/1dSUmK3VYwWLEeN/jYqa5pmO543NjZiGIa9XJiXlzfgy4UDIVThcJglS5YQDAZ5/vnnR1S15oeBUbeQejzXdSvGW1EUFi5ceNwp+Btr2nlieTOqBrFE5/Kjz2vexekGjCtyU12jUtuoMXeql0Rcpa05jprScfsUNMMspPD43KgpAxSdtuYwqbiK1+cCl8u0U5JTfGpx3ogWKugMp9M0jXA4zNixY4nFYjTseo9w8AKQTKd1K5MKLDPajnh5TcItGbaHYEozo1QMYRZbWMuBuQGdfQ0ymX7DzLYSZtVfutd0tTAEuCQz6BDMvq3Gdtl2Ti/K1NnbIJPmgYJMg4aO+A+zwELCKycpyDarE30eaIuaS31gHm/tg+UHDRSXhCQb+L3gdZuCluVPMCYbsgO9W8bpqTqzsbHR7kc6ln2R5Zc3ceJEJk6cOKqEas+ePezfv5/y8vJ+V/25XC4KCwspLCzsNvOvrq5m8+bN3fYK/X5/v86TpmlUVVWhKEqfhSoajXL11VeTlpbGypUrHaEaBEbczErX9WMWULS1tVFRUdFjSJn1Rc/Pzz9ujLcQgr/9o4WVr7UyrtjDoXoNj1tC1QyEkJhQ4mZfjbn8N3Oyh617UvgDHmbNG8ehfc2oqoQsmb5yLrcLt6Sh4cLQDBLtrYQjZh+Qoeu4PTJZ6Qb/9Y0S8nNH/rKA1Xja0tJCWVmZ7ZCg6zqVe3QOtJt3zJKRQMhuwPxy5wZ04imJaFxCFxKGQac4dNmvMrOlNKobTIHyuQWSDIlUZ4+UECAjaAybMfUel6A93vn3nJCnsbfeLKG3KMg0cMkaNR0OGhZFWTrtcbmjmdgslIglZZIaBHzQFjerCA0kgmmCNLcpVOPzIJA2sDcWVkaStdxluZ3n5+cjhGDjxo2jzi9PCMGePXs4cOAA5eXlgx5jY1W4WsuFlpNLX2LpdV2nsrISWZYpLS3tk1DF43GuvvpqNE3jxRdfHPT3/1HlQzOzqqmpYfPmzUckC/dESjX42z/M0nSAjHS543FBXpZOY6uLmgYVl2I6I0Tipp6nB320hdqJRXUUlwyKjKFr4AbNEKSSKdpbIiRiSbweGUNWkGUFWSS54aqiUSFUmqaxfv16VFVlwYIF3e4QFUWhdLJCwyZBUpMQsg+/HCZmmF/O9kiCpOYmrprvMz+o2xWADe2dy4F5AZ26Fhmvywx1TKiS6Wiugi4kUppE0KcTajf/LglVQgjsEveiTJ3ddQpZ6QYgaI1aMyqdQ81uAp44wYCX9phM0G/uYQlhOqyPzTM40Kjg95rLhbowX9vjlvC6zaKPHH+KiXmSPbseSA7PSLKKXDZt2oSqqgSDwW6xEyMdIQS7du2ipqaG+fPnD4n1U9cKV2uvsLGxkU2bNmEYRjfH+GOdQ13X7VDFvgpVIpHgU5/6FPF4nJdfftkRqkFk1IqVEMLuydqxYwcHDhygtLT0uBEJbWGdn/2ujpqGzsbiZKpzcunvuDarGowtUjhYp3OwTiUYkNBUg6ZmFZfLdEZXXIpZzq7pyC43eqKFVKKz1yotTSI7ILj9hrFMHDd0tjh9JZFIUFVVhdfrZf78+T0WBbgVmFmism6/eRFIiABpHgNJGDSHfbhEFDDFKtSukJWm0hp3A2Yab0FAY1+jNRMzm3itHqn8oG56AhqCQ82Kncib0sylRTUiMz5PY0+d+futUdONvSTbQIgUNS3mHy+S8hNphrG5Zo9WTkDgkgXpaYJwXMbrhpwMQWtcJj8oSKqm71+aR5AbSDE53wzRHGws+6JkMsnBgweZNm0auq5z4MABtgxBWXx/sb579fX1zJ8/f1jGqChKt1j6cDhMKBRi//79bNmyhWAw2O0cWjexVpO1EIJ58+b1SaiSySTXXXcdzc3NvPrqqwNmJuzQMyNOrI639mxdQK3Z1YYNG4hEIpx++unHvas7VJ/iz882sX2vWURRkOuioUmjtkHF2m+RFR90dEppqXYgHSGgMEcmFFcwdAO3x4VAgBDokkIykSQRTZCIJRCGgax4cLndCD3JVz5dMiqEKhwOU1VVZVefHWspZVKezt6Q6dVnCImAR+dgkwvNkNDIJCddpSVmboC3x3QkCQRufC6VWFLBOtdNEblbD1UsIZHhNTjQJCNJ0BKVCaaZRrWxpEROwGBPndlXFUmYMyVDgKalqG/3kpVmOr63RiVyMgQHmzoalFMwNl9woFEhmGb2UCVSMjkZZplHVkCQ5hYUZqpMzJeGdJ/IKvEuKyuze5EmT57crSx+9+7d3cris7Kyhn0vyzLTDYVCzJ8/f1jKzA9HkiSCwSDBYJApU6bY57CxsZE9e/bg8XjIz88nJyeH/fv3I4SgrKysT0KlqirXX389Bw8e5I033hiQeBaHYzPi9qwMwzimnZJhGLzyyiucfvrpbNq0Ca/Xy9y5c4+7ZLJpZ5xfPFHP2CI32/aYYjVzspete6zmXzd1jTo+r0QyZTaeFuUp1DWaopidoSGn56OmVDw+D7JkmJHtaT7CTa20t8SRXTIyOh63QnYGfHpxNqeVjfy7raamJtvU93i9aPbvRGRWb/eQmSaobzH35CwXijS3QBed+1VBb5xUMkVLwtx0z/SEaUt1LpcUZ5kzqmjCdEsvzjaoa5HMZEPMUMcsv247r4NZKViYJUilEjSEOy+UEoLiHEFzWMLvFSgy+L3QnjBndoXZ5hJmpt9A1U13iwyvQUmOxticoRMAy6Oyurr6uCXemqbZjbQDURbfX4QQbN26lebmZsrLy4fUTLevWMbPoVCI2tpau7rQ6ivsTYm5pml84QtfYNu2bbzxxhsUFBQM4sgdLEadWAH861//wuVyUVJSwowZM05oQ/WL9+yjNayTn+Mi1GzOnKZP9LK9ulO4tlebrzumwMWhBvOYgmyFhhYdj89DRlY6qmqGJCIrxCNxdM0g2hZFUzUUtwtZlvG7NP7fbWMYP2bkVwTV1NSwdetWZs6c2Wv35w0HXGzY58YQEl6XwOgiUHkZZuSHhCDLbzb01tlNvgIfbSTIAiDgSeJ1ydS0dl50C7MMmsMSigxel0FzRKIkxyDUZjqvK5Igw5ugNe4hL0NDUVy0RWWC6YL6VvPz4HYJcoPQEpHIShcE/YL2mExOhgESeBSJjDSNcXk6RZlDK1Q7d+6ktrb2hForDv/d/pTF9xchBJs3b6atrY3y8vJRVfVmGAbr168nlUoxffp0mpub7YDJrsuFx8qJ03Wdm266iaqqKt58880BTyx3ODojbhnweBw8eBCA8ePHM23atBP+vSnjvVRsjhFq1ggGZNojBvXNnaKo6Z2aHcxQbLHKzXbR0KLj9phuDR6PQjyaxB/0oyZVIi1RFLcLr9eMCckOwJILM0e8UAkh2Lt3L/v27etmh9MbphVpbDvkJqGaXoBdCyoawwoFQY2UKlHTrCBLoku/lYQqZxL0qGhqiuaIF4FM0N1Gu2rOROtbzRJ3Iejw/oOaZoU0jyA/UyccSdISN2dUobBpeJuZLmiPmcuFXhdICkQSsulSIUskVNOVQgA+t0TQrzMpVycnY2iFauvWrTQ1NfVpn+dopsV1dXXHLYvvL4ZhsHnzZsLhMPPnzx9VDa9dhWrevHm43W6ysrKYPHkyyWTSXi6srq62KzTz8vLIycmxlwl1XeeWW27hgw8+YNWqVY5QDTEjTqyO9uUyDIPt27dTU1OD2+0+biHF4cyc4qNicwyAwlw37ZEkre0GOZkyzW0GdY0a1l6KNfMCaG4zlwHdHheKZGBI5imLhWPEwnEEZnk6KHiUGB+f18CYnBxqa1Xy8/NHpCeYYRjdLph9rWDyuWHBlBRvb+ssqLAq9lyyIJnqdFg3hERClUnzGMRTZgpvmkcirPvsMvN2NZMMVythLRO3lKQlLKMZLsbkGBxq7mxAbmlPkdTcFGWpCGTiCQlZwa4eVGQQEqhJiax0A69Hoj0mkRUQSJJZTJHl15hSZJDhGzqhMgyDTZs2EQ6Hj6i07Cvp6emkp6czceJEO9U3FAqxb98+3G63vVyYk5PTLwcIK54kFotRXl4+6oRqw4YN3YSqK16vlzFjxjBmzJhuFZrbtm0jlUrxxz/+kdLSUnbv3s1bb73Fm2++yZgxY4bp3Xx0GXHLgEIIUqlUt8dUVWX9+vUkEgnmzZtHRUUFM2bM6JVg7dib4DsP1gAwY7LX3reaNtHDjmrz9cYXezhYbwpVSYGLmo7ZVVG+G1UJEI/E8AfTiYVjtNS3oLjdSBLomk5mhsQV5wW48Ew/DQ0NNDQ02LZFlsv5SPiCa5rGhg0bSCaTlJWVDcgF880tHg40dSb5prkFmiFoDpux80m1sxk3mGbmUmX6TQHyuEz7JstlHWBMdorGNoipnRcVvztJwCdojUDS6BxzRpphm9X6PGZ2VkqXiSbN/i1ZMdODkcxqvzQPZKUbTCvS8Q6hLaOu6/Z5nzdv3qCXpXe96IZCIVRVtTPOjlfS3dNzbdiwwf7+jYaSegtLZOPxOOXl5b1yvrCqC3/605/y4osvsmvXLk4++WQ+8YlPcPnllzNv3rxR5dc42hnxZzoajfLvf/8bSZI4/fTT8fv9x3Wx6InJ4714PeYFs7W983flLjO5dH/n/2cGrH0PSOvQGNmlkIonaG9pB0nC0HWEIcjKkPnZt8ey9JJCMjIymDJlCgsXLuSMM84gJyeHmpoa3n77bT744AP27dtHPB7v6+noF4lEgrVr1yKEGLA7e4CFJ6VI85hl5rIEblmnJWyey0hCtvuhwHRczw/q1LdKCGE6pEcSEgWZ5t8kN6BT1+pCkl0UZHY6igjDoKldQjcgy9dOtj9FTrqOpktE4hLxlLm/1RJTSGmQl2EQ9JtzMZ9bEPCB1wN5QZ1ZY4ZWqCyHBFVVKS8vH5KLvVUWP2PGDM466ywWLFhAIBBg//79vPXWW3zwwQdUV1cTjUaP+TxWiXcymRyysQ8U/REq6IzbkSSJaDTKe++9x7e//W22bt3KhRdeyPr16wdknMuWLWPSpEn4fD7Ky8t5++23j3psbW0tn/70p5k+fTqyLHPbbbf1eNzy5cuZNWsWXq+XWbNmsXLlygEZ63Ay4taoui4DNjY2sn79esaOHcu0adPsn7lcrl6JlRACWRJMHe9h864k9U0a/jSJWFxQE+rct6oNdS4F1jeZlYFBT4KakEJ6poGsyIQOhnC5FBSXgqHppLkNrvpYJnk5R86a/H6/bZtjdd03NDSwc+dOe2+hoKDgmBu6A0UkEqGqqoqcnJzjlqb3Fp8bzpqW4t0dHlRNcKjF1a0kPdSuUJhpxozkZxjsqVfICRhEEmYPlapLhNpkJuRr7G+Q0QyJlAbRpERBpoGMRkObC70jlbg14SNNCZPSfSiKTtAHWRkykYRiilk6RFNmGKPfCy6XOesqztKZlG8wlFXfqVTKdvHua5l0f5EkiYyMDPtGqqeMM2u5sGtZvCVUuq73uHw2krGWXK1ly76MXQjBvffey5NPPsmbb77JzJkzOf3007nuuutIpVIDcj6eeuopbrvtNpYtW8aZZ57Jb3/7Wy699FK2bNnC+PHjjzg+mUySn5/PPffcw4MPPtjjc65Zs4Zrr72WH/3oR1x55ZWsXLmSa665hnfeeYfTTjut32MeLkbcMiCYM4D9+/ezY8cOZs2adcT6cEVFBfn5+T3+MQ9HCGE3ET/zchvLXzFdK6aM97B7v7n819VaaXyJh4N1GulpEvmBBJXr25h0yhRkoRKqC2MYOoYhcLtdBNLgx3cUU1zYu9JdKwK8oaGBxsZGvF6v3dg4GGF0zc3NrF+/nvHjxzN58uRBE8bKPS7W7un8AncVLIRgbI7G7vrO+6NMv4FumLOvwkyd2maziVrTsa2V8gIqTWGZNLdKVoabWErCo5g2THpHSnDQ00bMyMCnJMlMM8CVBpJMhs/A4zYjQSbn6xRlDe1H3fKpTE9P73OA32BjGcZa9kWAnS1lFTOVlZWNyL3Xo2EJVTQa7fNsUAjB/fffzyOPPMIbb7zBKaecMggjhdNOO4158+bxyCOP2I/NnDmTpUuX8pOf/OSYv3veeedRWlrKQw891O3xa6+9lvb2dl566SX7sUsuuYTs7GyefPLJAR3/UDLiPoGWL53VFd9Ts92JLgNaQmUYBrIsM2tqmi1WblfnBTvN13kRSU+TyMqQSYXbaUoaeNO8SJJEw6E2M55dUQADr0vns4tzei1U0D0C3LKLaWhosNNgLeHqrc9ZT9TW1rJly5Y+lab3lrJJGvVtMgc6mnGtir6GdpmcgMHueldHDpWZ7NsWk/G6BRPyNHbXmY81hU3T25JsA01PUd/mAWSiqkK0GUpyTMNavxfcikFmOjRHMpFSkJEGkZQbd6oJ2e1HcwmCaW5mlQiy0oe2iTYWi9mBnwM9kx1IDjeMbW1tpb6+ni1bttjWRbW1tUNSFj8QWKX1kUiE+fPn91moHnzwQR5++GFef/31QROqVCpFRUUFd911V7fHL7roIt57770+P++aNWu4/fbbuz128cUXHyFqo40RJ1ZgrrcvXLjwqM2GLpfruGnBhwuVJElMm+Cx/f4O1qnIEhiCbtZL0ZhOvLWF2vqOqJCp2TTVNCEEJFM6fr9Eml/hC5/I5MwFPYfK9YaudjHWpnhDQwObN29G13W7DDkvL69XS0gDUZreWyQJLpid4tkPvLTFzJTelqhMcaZGdUdDb02zQnG2Tn2r6X6RnW6wu06hJMegsQ1SuoQhJFJqksawh2x/ijSfh3BMIjPdsF0pwnEYny841CyTlS7IzxLEkmnk+g1cci4ySVyinXjtRjaGsItchiIXKRKJUFlZSUFBAdOnTx92t4kTxdqj2bFjB9nZ2UydOpXm5uYjyuKHaum6t1hCFQ6H+zWjevjhh/nlL3/Jyy+/TFlZ2SCM1KSxsRFd14/I2issLKSurq7Pz1tXVzfgzzkSGHFiJcsys2bN4lirk8ebWRmGYf9nCRWAxyMza6qPDdsTRGIGE8e4qT6k0h4xmFDiRlUNqneGGD/GZ4tVU7OKrgkklwvFpaBIgi9dk83p87IG9H1D56a4tTHe3t5OQ0MDu3btYtOmTXaSb35+/jHXyw3DYNu2bTQ2NvarNL0veN2waF6K5z7wkNIkvC6DA03WnpUpNLUtpu+f36OzL2Q+VtNsuquPydJJJpKEOlwpWmI+WmIwNsegvlUm0y9QZEHQD80dFYTpPogmTdFKaTJpXoNxuW6mFmZhGGfYzgUbN25ECGHvz/T2BuBEaGtro6qqinHjxg3qkutgkEql7Dh3KyU3GAwetSzeupEaiBWA/iKEYMuWLbS1tfW5B0wIwWOPPcZ9993HSy+9xKmnnjoIIz2Swz8jlu/pSHvO4WbEiRVgG9QeDUVRenS5EELYMyqgm1BZnD7Xz4btCYBurtqZAYkP3m8gFtfZo+l4PQYpVcEQCrLLjKn3+RQuPTttUITqcCRJIjMzk8zMTKZOnUo0GqWhocE26MzOzraFq+vyTNfS9FNPPXVYlm4CPsFl81K8tt5tN/SGuuRQSQjciqCmRe7WQxVPQTyeoDnqIT+YRFZctEVkcoKCg83m80STgoJMibo2iSy/QUGmRDghkR0wAJksv2DmGI38oPn5URSlWwSH5f5g3QB0bS3ob6WbtTc4efJkJkyY0K/nGmqsJSm/39/j/prH46GkpISSkhIMw7BvADZv3oymad1uAIa6EMMSqtbW1j73gAkh+OMf/8gPfvADXnjhBc4444xBGGl3rJulw2c8DQ0Nx002PxZFRUUD/pwjgRFZYKGqKoZhHPXnu3fvJhKJMHfuXPsxIYQ9mwLzYt/TnUR7ROfmHxxEN8xokEjMYPIYF1s21uFPU2jscLU4aZKLfXUuPGk+JEnC5xX8542FzJvdv1C5gSAej9u9XF2tYrKysti+fTtut5s5c+YMe/VWc0Ti2f/zEE10/h1KcnTUFNS2dl4Ms9INvC5BPB6nNdnp6KBIgoIsaItJpHkELsVs6G2LyRhAdrpAls2ZFUBuUFA6XsVzgrdg1g1AKBSivb29Xy7n1sxt+vTpo65hNJlMUlFRQSAQYPbs2b2aJXV1Og+FQkQiEbKysmzhGmwn9q4+hfPnz+/TzZkQgv/5n//hP//zP3nuued6zMobLE477TTKy8tZtmyZ/disWbNYsmRJvwoswuEwL774ov3YpZdeSlZWllNgMdQcvgzYdX9KkqRjftmCAYWZU3xs2pkgHDWYc5KLt9+uwTCgpFChsdk8ble1Rk5BgERSx+9XuPB0/4gQKoC0tDQmTJjAhAkTSKVSNDQ0UFtby65du3C5XOTm5hKPx3G5XMM69c8JCK4+PcULFW6awjJuRRBLgKpJ5GYYNIXNv1MkLqEqMZKaQkm2RlKTiSYkgn5snz8JSE8zhSsz3cDnkWiLSRRkmWa1M0o0xuYc/QanJ9LT05k0aRKTJk0imUzarQVWObe1PxMMBo95Huvq6ti8eTMnn3zyqLPgSSQSVFRUkJmZycknn9zrz0tPTueWcO3cudM+j/n5+QNe6SqEYNu2bf0Wqqeffpo77riDZ555ZkiFCuCOO+7guuuuY/78+SxcuJDHHnuM/fv3c/PNNwNw9913c+jQIf785z/bv7Nu3TrA3BsNhUKsW7cOj8fDrFmzALj11ls555xzuP/++1myZAnPPfccr732Gu+8886QvreBZkTOrDRNO+ae1MGDB6mtrWXBggU9FlIcj1ffDfO7Z5qZNk6hqaGNXXtNGyZJEhTlu6lt0JBkmfSsID4f3P3lQk6ZOTKEqies5aexY8cSCATsMmS3220XbwxnrISqw+rNLg42SrRErBsJQVG2IJUSxBIJYlqnc7rfI/B7TVNcr1vgcws0QyacMA1zXS7TLV1RJAozDUonnPhs6kSwyrmt1gKrQrMn26KDBw+yY8cO5syZQ15e3sANYgiIx+NUVFSQnZ3NrFmzBvzz0VNZvCVcOTk5/SqHt4SqqampX87vK1eu5Mtf/jJPPfUUl19+eZ/H0x+WLVvGz372M2pra5k9ezYPPvgg55xzDgDXX3891dXVrFq1yj6+p7/ThAkTqK6utv/9zDPP8N3vfpc9e/YwZcoUfvzjH3PVVVcN9lsZVEalWNXW1lJdXc3pp5/ea6ECM4DxF48f4v336wEoKYCaBvNnhfkeXIpEc0Qhze9i8flBPnNVcb/f02BhlabPmDGj2/KTFYlgLXMB9kwhNzd3WDbEd9TIvLnRRSxp/p3SPAa6lkCRDTIzvETiVrGD6WoBkB0w0AwZWTL7shRFIpqUmFpsMGuMRkba4H58u9oWNTQ0dNuficVi7N+/n9LS0lGXZxSLxaioqCAvL48ZM2YM+o2MVRZvzboSiYTtFp+Xl9erWdHhWVp9FaoXXniBG264gb/+9a9ceeWVfXoOh6FjVIpVQ0MDO3bs4PTTT7erXHr7ZfvZb/bw2ltNABTkQkNT588yAgreQAZ3f6WIU2aMzJhqKw9p7969x72rty4U1j6XqqrdsnyGsuEzqULlHoXtB6E9qqKKzotUTsBAN2QUWeB2QbpXEEt1LAmmC1RdYlKBwexxGlnpQ/+xtfZnGhoaOHToEKlUimAwSElJyajpQwJzr66iooLCwsJuzjBDPQZLuNra2sjIyDihiA4rnbihoaFfQvWvf/2L6667jt///vdce+21/XkrDkPEiBQrXdeP2kdlXXg/+OADioqK+jxT2LG7kW98dw9GhwvCnFkBNmyJ4PNKZOf4WfyxXK6+fGRWz1gO9A0NDZSVlREMnvgSZdcLbkNDA7FYrFtJ/FB4v4XDYT6oWI+RdhLtRgk1zTJBvxk5n9LMv0dxtkFzVCbgE4zPM5hUaDA210AZ5t5a666+oaGBWbNm2Rfd1tZW+4JbUFDQLUJ9JBGJRKioqKCkpISpU6eOiDF2LYtvamo6alm8lQNWV1fXr3TiN954g09+8pM8+uijfOYznxkR58Dh+IwqsepqnWQtKVhLM12XuI7XO1NfX8/mzZv594YCVq3pNJU9eXqA7CwX55+Vy9mn9r/hdzCw3Lvj8ThlZWX9TmntqSLO2ucajARYa3/N8kw02xSgLQbhuERKMx0s/D7ITBP4RpBvquWuYpVIdz0/1gW3oaGBpqYmvF6v/ZkcCTH0YN4kVFRUMHbsWKZMmTIixnQ4uq53c4u3ll3z8vLsvsP+CNVbb73Ff/zHf/CrX/2KG264YUSeA4eeGTVi1VWoui77CSHsD3F9fT2pVOqoS1yWq0N1dbXZS+LK5Pbvb6OuIUmaT6a40MtNnxtP2Qip+jucZDLJunXrUBSFuXPnDnhpelez3ZaWFgKBgC1cAzFTqK+vZ9OmTUfsr40GLAdvy2/uWL08loVW1xh6a6ZwIjdTg0F7ezuVlZW2P+Ro4GjLrkVFReTn5/dasN577z2uuuoqfvazn3HTTTc5QjXKGJFidXi0/dGE6nCEEEQiEVu44vE4OTk5FBYWkpuby86dO2lubqasrMx2dWgPazzw6F4C6QrzSzM574zBtyXqC9FolMrKSrKysjj55JMHvUBCVVVbuJqamvD5fLZwHa+UuycOHDjAzp07OeWUU3odnDncWO7jmqZRVlbWq6XSw1cBksmkveza21ypvtLW1kZlZSWTJk1i4sSJg/56A4kQgt27d3Po0CFOOeUUe9m1ubm5V2XxH3zwAUuWLOFHP/oRX//61x2hGoWMaLGyHCl6sk46Eawlrrq6OiKRCIqiMHnyZEpKSkZVLk9LSwvr1q1j3Lhxw7J8o+u6vacQCoVsV4gTMdsVQrBnzx72799PWVkZWVlZQzfwAUBVVdtguLS0tN/l1l2XXcPhsL3s2peZwonQ2tpKVVUVU6ZMOaGUgpHG7t27OXjwIOXl5QQCAfvxY5XFHz57raqq4vLLL+e73/0ud9xxhyNUo5QRK1apVOqEHCmORzQapaqqCr/fT3Z2tr03k5WVZc8URnIVl9VwOn36dMaOHTvcw+lmttvQ0IBhGEfdL7TcBRobG5k3b163i81oIJlMUllZic/nY86cOQO+fNe1gba5uZn09HT7XGZkZPT7otrS0kJVVRUnnXQS48aNG6BRDx3WTc78+fOP+dkxDMO20bLK4jdv3oyqqsyZM4cvfOELfPOb3+Suu+5yhGoUMyLFStM0EomE/e++Lnl1bZbtWvmUSCTsi21rayvBYJCCggIKCwsHpaigLwgh2LdvH3v27BmxS2eW1551LpPJpL1fmJ2dzfbt24lGowNSCDLUxONxKisrCQaDQ7bs2rUR2eVy2TOuvhjFNjU1sX79+lFp/wTYiQHHE6qeiEaj/OUvf+H3v/89W7Zsobi4mJtvvpklS5ZwyimnOII1ShmRYvX5z3+e3bt3s3TpUq644grGjBnT6w/YwYMH2b59+3E38y27ooaGBpqbm+2igsLCwkH3NTsaVnl0fX19r0vTh4uu+4UNDQ1EIhFcLheTJk2iuLi4T+aiw4W1PzhUDbOH09UoNhQKoet6N6PY4y1FNjY2smHDBmbMmDHoGWaDgSVU5eXlfU4M2L59O5deeinXXnstZWVl/OMf/+Dll1/m+uuv5+GHHx6wsS5btoyf//zn1NbWcvLJJ/PQQw9x9tlnH/X41atXc8cdd7B582ZKSkr49re/bVsrWTz00EM88sgj7N+/n7y8PK6++mp+8pOfjOgVoKFgRIrVwYMHeeaZZ1ixYgXvvfce8+fPZ8mSJSxZsoQJEyYc8+Jh9WLU1NQwZ84ccnJOvAT98KKCtLQ0W7iGKr9H13U2btxILBYblTMSa+nM7XaTm5tLY2OjbbZrLbsOxt7MQBEOh6msrBwxfUhWtav1uYzFYt2c4g+/CQiFQmzYsGFU+hQCVFdXU11d3S+h2r17N5dccgmf/OQn+fnPf27PShOJBK2trQN2Xp566imuu+66bpH0v/vd744aSb93715mz57Nl770JW666SbeffddvvrVr/Lkk0/yiU98AoD/+Z//4cYbb+T3v/89Z5xxBjt27OD666/n2muvPWqM/UeFESlWFkIIamtrWblyJStWrOCtt95izpw5tnAdfjGxLvTRaJTS0tJ+zYw0TesWPe/xeCgsLOxzNdyJkEqlqKqqGrTS9MHGmpFYXnPWRaKrSay1N2MJ10gK8bOKESZOnMikSZOGezg9crjzg+W4n5+fTyQSYfPmzcyePXtUxkFYy97l5eV9Xk2orq7m0ksvZfHixfz6178e1OXb3kbS33nnnTz//PNs3brVfuzmm29m/fr1rFmzBoCvf/3rbN26lddff90+5j//8z/5v//7P95+++1Bey+jgREtVl0RQtDY2GgL1xtvvMGMGTNs4fL5fNx8881861vf4rzzzhvQC33X6HmrGs6acQ1Uw6dVCBIMBnsd0zASsEIHS0pKOOmkk456TlRVtSsLrZuAkdA8a+3xjKZihFQq1W0lQAhBQUEBEyZMGHCH88Fm//797N69u19CdfDgQS6++GIuuugiHnnkkUH9DqVSKfx+P08//XQ3X8Fbb72VdevWsXr16iN+55xzzqGsrIxf/epX9mMrV67kmmuuIRaL4Xa7+fvf/87NN9/MK6+8wqmnnsqePXtYtGgRn//857nrrrsG7f2MBkZNRIgkSeTn5/PlL3+ZL33pS7S0tPD888+zfPly7r//frtiKzs7e8Crtg6PnrcMYtevX2+Pq7CwsM+Jqa2traxbt44xY8aMiKWn3mJd6KdMmXLc0EG3201xcTHFxcXdzHa7nsuCgoIj3M0Hk4aGBjZu3MisWbMoLh65psWH4/F47P3c5uZmJk6cSCKRYN26dfa5tBzOh6MR+USxhGrevHl9Fqra2loWLVrE+eefz7Jlywb9s9OXSPqjxc1bqzjFxcV88pOfJBQKcdZZZyGEQNM0vvKVr3zkhQpGkVh1RZIkcnJyuP7668nKymLVqlUsWrSIRCLBpZdeSnFxMUuWLGHp0qWUlZUN6AdXlmXb/mXGjBm2QeymTZsQQvTa2dyyfhpNd/RdsUrr+3Kh75riaxiGfS63bt1q2+xY53KwzHZramrYtm0bp5xyCgUFBYPyGoPJoUOH2L59O6WlpeTmmg3t1rkMhUJs27YNVVXJzc21z/VIWl4+cOCALVSZmZl9eo76+noWLVrEaaedxuOPPz6kwtzb+Pieju/6+KpVq/jxj3/MsmXLOO2009i1axe33norxcXFfO973xvg0Y8uRqVYWaxfv57rrruOP//5z/ZUPBKJ8OKLL7J8+XIWLVpETk4OV1xxBUuXLmXBggUD+kGWZZmcnBxycnKYPn06bW1t1NfX2xcIS7is+OrD2bdvH7t37x6xpenHY//+/ezatYu5c+f2O8vp8HNpFRXs3r17wOPnexq/daEfTViuIGVlZd0iSrqey2nTptlVmvv372fLli12j2F+fv6wFvAcPHiQnTt39kuoGhsbWbx4MXPmzOGPf/zjkAlVXyLpjxY3bwWmAnzve9/juuuu44tf/CKA7drx5S9/mXvuuWfUbQ8MJKNarObOncuWLVu6zUgCgQDXXHONvQ788ssvs3z5cj7xiU+Qnp7O4sWLWbp0KQsXLhzQu3VJksjKyiIrK4tp06YRDoepr69n165dbNq0yZ4l5OfnoygKO3bsoK6ujvLy8j5/UYcLIQS7du3i0KFDgzJ+SZLIzMwkMzOTqVOn2q4PBw8eZOvWrf1u6LY8Ivft28e8efNGnasGdF86O9b4JUkiIyODjIwMpkyZQjwetws0duzYQSAQsG+qhrLYxQqt7I+rSXNzM4sXL+akk07ir3/965BG3Xg8HsrLy3n11Ve77Vm9+uqrLFmypMffWbhwIf/4xz+6PfbKK68wf/58e7Ybi8WOECRFUWw3n48yo6bAor8kEglee+01VqxYwXPPPYfL5WLx4sVceeWVnHXWWYO2NGJZ7NTX19PQ0EA0GrVfa7T0UHXFMAy2bt1Kc3Mz8+bNG/JetMMbujMyMuybgBNpHrVaG2pra5k3b16fy6OHEyvHrD8zEugsdrEKNKxoDqvYZbDu4q2ly8NnhL2htbWVxYsXU1RUxIoVK4alj88qXX/00UftSPrHH3+czZs3M2HChCMi6a3S9ZtuuokvfelLrFmzhptvvrlb6fp//dd/8cADD/DYY4/Zy4Bf+cpXKC8v56mnnhry9ziS+MiIVVdUVeXNN99k+fLlPPvss+i6zqJFi1i6dCnnnXfeoH3wU6kUFRUVaJqG2+0mEomQnZ1tzxJGeuOsFU+SSCQoKysb9ibFw2M50tLS7IttT+0Flv1TU1PTsAjtQGBZEPWnGKEnrGIXa9Zl2WhZXnsDNWux9ghLS0t71QPZlfb2dpYuXUowGOT5558f1s9hbyPpV69eze233243Bd95553dmoI1TePHP/4xf/nLXzh06BD5+fksXryYH//4x6NyBWAg+UiKVVc0TeOdd97h6aef5tlnnyUajbJo0SKWLFnChRdeOGBr+rFYrJt9j6IoxONxe5bQ1tZGZmYmhYWFw76X0BOqqrJu3ToASktLR9QmPXQam1p9cV0rOK0v+aZNmwiHw5SXlw+70PaWru7jgz0jtGy0LOGy0gss8errTVVtbS1bt27tl1BFIhGuuuoqPB4PL7zwwohuMHcYWD7yYtUVXddZs2YNzzzzDCtXrqSlpYVLLrmEJUuWcNFFF/X5TtwqTT9WD1IymbSFq6WlxV7eKiwsHPYvZCKRoLKyEr/fzymnnDKiy6CBbu0FoVAIIQSyLCPL8hGhiaMBa4+wpqbmCPfxocBqRG5oaKC9vd12I8nPzz/h74QlVP0pZonFYlx99dUYhsGLL7446oyRHfqHI1ZHwTAMPvjgA1u4amtrueiii1iyZAmXXnrpCd/ZWmXtvSlNP7zZ03J8sPwKh7IPKxKJUFVVRW5uLjNmzBh11UiqqlJRUUEqlUKSJLuM26rSHGkzxMMRQrBjxw7q6+spLy8f9qVLy43Ecoq3ll6PlSlVV1fHli1b+iVUiUSCa6+9lkgkwssvvzzq9nod+o8jVieAYRisW7fOFq7q6mouvPBClixZwqJFi476JbVKo2fPnt3nHp6um+CNjY34fD7b9mkgYiSOhTUjHMkx6MfCsq9yu93MnTsXWZa7me1Go9Fj+uwNN0IItm3bRmNjI+Xl5cM+wz6crplSoVAIWZaPaOq20qH7096QTCb5zGc+QygU4pVXXulzUYbD6MYRq14ihGDz5s220e62bds4//zzWbp0KYsWLSI3NxfDMPjDH/7A1KlTKSsrG7DSbisEsb6+3rYqsvZlBtpeJxQKsXHjRqZOnToqQ/uspcv09HROOeWUHmeEsVjMFq729nYyMzPti+1wC4MQgi1bttDS0jIqli6tnDNLuFRVJRAI0N7ezsknn9xnZxBVVfnc5z7Hvn37eP3110dlP5zDwOCIVT+wlmiWL1/O8uXL2bBhA2eccQbxeJxDhw7xzjvvDJqhqFW9VV9f382v0MqS6o9w1dTUsHXr1lHr3G0Vs2RnZzNz5swTWrocSWa71g1RW1vbqC0G2bdvHzt37sTn85FMJsnOzrZvBE70/Wiaxhe+8AW2bdvGm2++OSob5x0GDkesBgghBGvXruU//uM/iEQiqKrK3LlzueKKK1iyZAklJSWDdsGz7mot4bIMTfvisVddXc2ePXtGratDJBKhoqKCoqIipk2b1qdzfvjSq9frHbQZ7OEYhsGmTZuIRCKUl5ePuKXJE8GKKbEsrKxGZKs3zsqMs3rjejqfuq5z0003UVVVxZtvvjkqb5ocBhZHrAaIvXv3cvHFFzNnzhz+/Oc/09jYyPLly1mxYgVr1qxhwYIFtu3T+PHjB+2CJ4ToFjuv6/pRY+cP/z2rWXY0NitDp/P7uHHjmDx58oCc466O+42NjYNqtmsYhp1lVl5ePmC2UkOJJVRHiymxeuMs132v12sXaFiNyLquc8stt/Duu++yatWqUZl07DDwOGI1QNTU1PDEE08c4d8lhKCmpsaONnn77beZM2cOS5cuZcmSJYNauGAF91nuGalUyrZ96po4axgGW7ZsobW1lXnz5g37fk1faG5uZv369UyePPm4zu99pavZrnUj0PV89qek3zAM1q9fTzKZZN68eaNSqBobG1m/fv0J52l1dd0PhUL86Ec/sm+o1q9fz+rVqwftb+kw+nDEaggRQtDQ0MCzzz7LihUrePPNN5kxY4YtXIMZoW7FzlvCFY/Hyc3NJS8vj7q6OjRNo6ysbNQuO23cuJHp06cP2V24dSNgCVcikbBL4nvrbK7rOuvXr0dVVebNmzfiy+l7orGxkQ0bNjBr1qw+LdkJIXj11Vf59a9/zfvvv4+u61x88cUsWbKEK664ot9GyV0ZjCj61tZW7rnnHlasWEFLSwuTJk3il7/8JZdddtmAjfujjiNWw4S1XPfcc8+xfPlyXnvtNSZPnmxHm5x88smD2tMUiUSora1l//79GIZBTk6OXRI/mu7qrYiS4U7H7VoS39VGKz8//5gFBbqus27dOgzDoKysbEjNWAcKK8+sr0IF5szyu9/9Lk8//TRvvvkmmqbx3HPP8eyzz/LlL3+ZG2+8cUDGOhhR9KlUijPPPJOCggK+853vMHbsWA4cOEBGRgZz584dkHE7OGI1Ymhra+Mf//gHy5cv5+WXX2bMmDG2cJWWlg64cMXjcSorKwkEAkyZMsUuKGhvbycrK8u2fRrJlWiWc/ecOXMG9M67vxxeUGC5kRQUFHRr6tU0jaqqKiRJorS0dFQKVXNzM+vWrWPmzJl9Lk8XQvDDH/6QP/3pT6xatYoZM2Yc8fOBWnEYjCj6Rx99lJ///Ods27ZtVM6KRwuOWI1AwuGwncn10ksvkZeXZzvEL1iwoN/CFQ6HqaqqIj8//4ilR8vVvL6+nra2NoLBoD3jGkm9PpbzeGlp6YhuEj3cjcTv99vFGTt37sTlclFaWjriLax6whKqGTNmUFJS0qfnEELw05/+lEcffZQ333yT2bNnD/AoOxmsKPrLLruMnJwc/H4/zz33HPn5+Xz605/mzjvvHJV/15HK6LuV+wiQkZHBtddey7XXXkssFuNf//oXy5cv58orryQQCNhVhQsXLuz1l6GlpYV169YxYcIEJk2adMQdq8/nY/z48YwfP97uPaqvr2fnzp0EAgFbuIbL9scydD148CDl5eUjvmrRip4fM2aMHV9eX1/P3r17kWWZkpIS2tra+t0bN9RYn6P+CtWDDz7IsmXLeP311wdVqGDwouj37NnDG2+8wWc+8xlefPFFdu7cyde+9jU0TeP73//+oL2fjxqOWI1w/H4/V111FVdddRWJRIJXX32VFStW8MlPfhKPx2PPuM4888zjLkFYPoXTpk1j7Nixx31tr9fL2LFjGTt2LKqq2sK1e/fuYWmaFUKwfft2GhoamD9//qgzMnW5XOTk5FBdXU1eXh5jxoyxCxOAbiXxI/mOvKWlhaqqKqZPn94voXr44Yd54IEHePnllyktLR3YQR6DgY6iNwyDgoICHnvsMRRFoby8nJqaGn7+8587YjWAOGI1ivD5fCxevJjFixeTSqXsTK7rr78ewzC4/PLL7Uyuw4skrP2dvvoUut1uSkpKKCkp6TZDqK6uxufz2cLVU47UQNC1vH7BggUjaknyRLHyzCz3elmWKSgoYObMmXZJ/LZt21BVtccWg5FAa2srVVVVTJs2rc+Vl0IIHnvsMX7yk5/w0ksvsWDBggEeZc8MVhR9cXExbre72w3GzJkzqaurI5VKjaqCpZHMyPkWOPQKj8fDxRdfzMUXX8yyZct4++23efrpp/nqV79KPB63M7nOP/98fvjDH6JpGvfcc8+A7O+4XC6KioooKiqym2br6+uprKzE5XJ1y5EaCOEyDIMNGzYQj8dZsGDBqCyvTyaTVFRUkJGRcUSlpyRJZGdnk52dzbRp0wiHwzQ0NLB37142bdpkm+0Od6WmJVQnnXTSCc3Me0IIwR//+Ed+8IMf8MILL7Bw4cIBHuXRGawo+jPPPJO//e1vGIZh/1137NhBcXGxI1QDiFNg8SFD13Xee+892yE+EokghOCuu+7ixhtvHNSGX8MwbLeHUCiEJEnd/Ar7UhhilXZrmjZqe5ASiQQVFRVkZmZy8skn90rAD8+SyszMtM/pUM4u29raqKysZOrUqSccdXM4Qgj++te/8s1vfpPnn3+e888/f4BHeXwGI4r+wIEDzJo1i+uvv55bbrmFnTt38oUvfIFvfOMb3HPPPUP+Hj+sjFix6m3jnkN3kskk1113He+//z6XXHIJr732GvX19Xz84x9n6dKlXHLJJYOaNmv5FVq9R0KIbrZPJyJcqqpSVVWFLMujtrQ7Ho9TUVFBTk4OM2fO7NdMM5FI2MLV0tJie+xZBS+DtW9oCdWUKVP67MAvhOB///d/ueWWW1i+fDkXX3zxAI/yxBnoKHqANWvWcPvtt7Nu3TrGjBnDjTfe6FQDDjAjUqx627jncCQ33HADW7Zs4YUXXiA/Px/DMKiqqrKjTfbv38/HPvYxlixZwmWXXTaoBq1CiG42RZqmkZeXR2Fh4VH9CpPJJJWVlfh8PubMmTMqv/SxWIyKigry8/OZPn36gJ5fq+Clq8ee1Rs3kH/L9vZ2Kioq+m1jtWLFCm666SaeeuopLr/88gEZm8NHixEpVr1t3HM4kr1799qu1ocjhGDTpk22cO3YsaNbJldOTs6g+xVavVzJZNIWLquYwGpYDgaDg+7kMVhEo1EqKiooLCzss/v7idLVbNeKi7FmsX1dfoVOoZo0aRITJ07s8/heeOEFbrjhBv7617922ytycOgNI06s+tK459B3rHJwK5Nr48aNnHPOOSxZsoTFixdTUFAw6H6FlnDF43EyMzOJRCLk5+cza9asUdV7ZGHFlJSUlDB16tQhfQ+HL78ahnFCrvuHEw6HqaioYOLEif0Sqn/9619cd911/OEPf+Caa67p8/M4OIw4saqpqWHMmDG8++67nHHGGfbj9913H3/605/Yvn37MI7uw43VcGtFm1RUVHDGGWfYZqKDmckFZknwxo0bcblcqKo6YqrgeoN1kR/ImJK+IoSgra3N3udKJBLk5eXZkRxHK1ax3oPVON5XXn/9dT71qU/x29/+lk9/+tOj8sbDYeQwYsXqvffe61bW+uMf/5i//OUvbNu2bRhH99FBCMH+/ftt4fr3v//NggULWLJkCUuWLBnwTC6rLHrixIlMmjSJeDxuO8RbfoWWcI1Uv8L29nYqKyv7fZEfDIQQRKPR45rtRiIR1q5dy/jx45k8eXKfX++tt97iP/7jP/j1r3/N9ddf7wiVQ78ZcWLlLAOOPKxMrhUrVrBixQreeecd5s6da0eb9HcGYbl2n3TSST2WRVt+hZYxbDAYtIVrpGRvWRVz/d3fGSri8bh9Ti0PyMzMTGpraxk3bhxTpkzp83O/9957XHXVVfziF7/gS1/6kiNUDgPCiBMrMAssysvLWbZsmf3YrFmzWLJkiVNgMcwIIaivr7czuVatWmUXvyxZsqTXVW/W0t+sWbNOyLU7lUrZF9nm5uZu5dvDZb9kzQr7U9o9nKRSKQ4ePMiePXsQQnSz0srIyOjV3/P//u//WLp0Kffeey9f+9rXHKFyGDBGpFgdr3HPYWQghKC5ublbJtfUqVPtaJNZs2YdsxKtpqaGbdu29dkCyirfthzN09LSKCgooLCwcMj8Ci3n8RP1WxyJRKNR1q5dy5gxY5gwYYJdWdjY2Ijb7T5hR5LKykoWL17M9773PW6//XZHqBwGlBEpVnDsxj2HkYe1mW9lcr3yyiuMHTvWFq65c+d2E67du3ezb98+SktLycnJ6ffrW36F1kXW4/HYwjVYfoXW8uVQJhQPNFaJfXFx8RGVi4fHzgP2Htfhjd0bNmxg0aJFfOtb3+LOO+90hMphwBmxYuUwugmHw/zzn/9k+fLl/Otf/yIvL48rrriCJUuWsHLlSj744ANWrlxJZmbmgL92T31HXW2fBuJCarml9yd0cLiJxWKsXbuWoqIiTjrppGOeF8MwaG1ttWeyqqqyceNGAoEAs2fP5pprruHrX/863//+9x2hchgURl+3pcOoICMjg09+8pM8/fTT1NXV8Ytf/IJQKMTll1/On/70J2bNmsWmTZvQdX3AX9sSp9mzZ3Puuecya9Ys2wz3rbfeYsuWLTQ1NWEYRp+ev6GhgQ0bNpzwPttIxHLXKCwsPK5QAciyTE5ODtOnT+ess85i/vz5RKNRfvGLX/Dxj3+crKwsO/JkMFi2bBmTJk3C5/NRXl7O22+/fczjV69eTXl5OT6fj8mTJ/Poo48e9di///3vSJLE0qVLB3jUDgOJI1bH4Sc/+QkLFiywo8mXLl3q9Hr1kvT0dJYuXUogECA/P5+f//znGIbBtddey7Rp07jttttYtWoVqqoO+GvLskxeXh6zZs3inHPOsaM5Nm3axOrVq9m0aROhUOiERbO+vp6NGzcye/ZsioqKBny8Q4HlV1hQUNAndw1JkggGg1xzzTVomsb111/PjTfeyOOPP05xcTHf+ta3BnS8Tz31FLfddhv33HMPVVVVnH322Vx66aXs37+/x+P37t3LZZddxtlnn01VVRXf+c53+MY3vsHy5cuPOHbfvn1885vfdHxHRwHOMuBxuOSSS/jkJz/JggUL7JiNjRs3smXLlmFLyx2N/O53v+OXv/wlr776ql2IkEqleOONN1i+fDnPPvssgJ3Jde655w5qI7C1x2b1clkZUpbtU09OD7W1tWzdupVTTjmF/Pz8QRvbYBKPx1m7dm2//Qqrq6u55JJLWLp0KQ899JC9f3Xo0CFaWloGNPW3t/Zrd955J88//zxbt261H7v55ptZv349a9assR/TdZ1zzz2XG264gbfffpvW1lb7c+gw8nDEqpeEQiEKCgpYvXq1U/DRC3Rdp729/ah5Wpqm8dZbb/H000/z7LPPkkgkuPzyy1myZAkXXHDBoDYCCyHsDKn6+nrb6cEKP3S73Xbl4ty5c+3QvdFGIpFg7dq15ObmMmPGjD4L1YEDB7j44ou55JJLWLZs2aB6N/al7/Kcc86hrKyMX/3qV/ZjK1eu5JprriEWi9nOHT/4wQ/YsGEDK1eu5Prrr3fEaoQz+jIXhpm2tjaAAalg+yihKMoxgx9dLhcXXHABF1xwAQ8//DDvvvsuzzzzDHfccQdtbW32XfzHP/7xAW8Etpa1gsEgU6ZMIRqN2inImzdvxu/3E4vFmDNnzqgXqpycnH4JVW1tLYsWLeKCCy7gN7/5zaCbDDc2NqLr+hFJvoWFhUck+FrU1dX1eLxVMVpcXMy7777LE088wbp16wZr6A4DjLNn1QuEENxxxx2cddZZA7rM4dAdRVE455xz+PWvf011dTX/+te/GDt2LPfccw8TJ07ks5/9LM888wzhcHjAX1uSJAKBAFOmTGHhwoW29VNaWhobNmygoqKCAwcOkEwmB/y1Bwsr/LG/mVr19fUsWrSIhQsX8vjjjw9pbMvhYxZCHPN99HS89Xg4HOazn/0sjz/+OHl5eQM/WIdBwRGrXvD1r3+dDRs28OSTTw73UD4yyLLMwoUL+eUvf8muXbtYtWoV06ZN495772XixIlce+21PPnkk7S1tTHQK9r79u1j//79lJeXc+aZZ3LmmWeSl5dHXV0db7/9Nh988AH79u0jHo8P6OsOJMlkkoqKCrKysvolVKFQiMWLFzN37lz+8Ic/DJlQWfuHh8+iGhoajpg9WRQVFfV4vMvlIjc3l927d1NdXc3ixYtxuVy4XC7+/Oc/8/zzz+Nyudi9e3evxhgKhSgqKuK+++6zH3v//ffxeDy88sorvXouh6Pj7FmdILfccgvPPvssb7311ogzKf0oYhhGt0yunTt3csEFF7BkyZIByeTau3cv1dXVzJs3r8desGQyads+tbS02NWiVmrvSMASKisXrK/no7m5mcsuu4wpU6bwv//7v0d1ax8semu/duedd/KPf/yDLVu22I995StfYd26daxZs4ZEIsGuXbu6/c53v/tdwuEwv/rVr5g2bVqvi3tefPFFli5dynvvvceMGTMoKytj0aJFPPTQQ717sw5HxRGr4yCE4JZbbmHlypWsWrWKk046abiH5HAYQgi2bdtmC9emTZs499xz7Uyu/Pz8Xl2od+/ezYEDBygvLycjI+O4x6dSqW62T5a3XmFh4aDGzR9vTGvXru23ULW2trJ48WKKi4tZsWLFsES1HM9+7e677+bQoUP8+c9/BswbjdmzZ3PTTTfxpS99iTVr1nDzzTfz5JNP8olPfKLH1xiIAouvfe1rvPbaayxYsID169fzwQcfjNiEgNGII1bH4atf/Sp/+9vfeO6555g+fbr9eGZmJmlpacM4MoeesDK5LOGqqqrqlslVXFx81Au39buHDh2ivLy8T8a4qqp2s33y+Xy2cPXWFLavWEKVkZHB7Nmz+/ya7e3tLF26lMzMTJ577rlhvfAey37t+uuvp7q6mlWrVtnHr169mttvv53NmzdTUlLCnXfeyc0333zU5x8IsYrH48yePZsDBw6wdu1a5syZ0+fncjgSR6yOw9G+6H/4wx+4/vrrh3YwDr1CCMG+fftYvnw5K1eu5N///jennnqqnck1btw4++9rGAbbt2+noaGB+fPnD8hSnq7rNDY2Ul9f380UtrCwkMzMzEERrlQqRUVFBenp6cyePbvP1XqRSISrrroKj8fDP//5T+fG7ATYvHkz8+fPR1VVVq5cyeLFi4d7SB8qHLFy+EgghODQoUN2Jte7775LaWkpS5cuZfHixfzkJz8hmUzy2GOPDUpGVldT2IaGhm5+hVlZWQNSAm4Jld/vt506+kIsFuPqq69GCME///nPYYteGU2kUilOPfVUSktLmTFjBg888AAbN248ahGIQ+9xxMrhI4eVybVy5UqWL1/Ou+++S3p6Op///Of57Gc/2ycLot5gGAYtLS3U19cTCoUQQtjClZOT0yeRUVWViooK0tLS+iVUiUSCa6+9lmg0yr/+9S+CwWCfnuejxre+9S2eeeYZ1q9fTyAQ4PzzzycjI4MXXnhhuIf2ocEpXR/F/OQnP0GSJG677bbhHsqoQpIkioqK+PKXv8y4ceMoKiri29/+Nlu3buX000/ntNNO495772Xz5s19Nrs9FrIsk5uba/sVWvEpW7Zssf0KGxoaTtiv0BIqn8/XL6FKJpN89rOfpa2tjRdffNERqhNk1apVPPTQQ/zlL38hGAwiyzJ/+ctfeOedd7pZRDn0D2dmNUr54IMPuOaaawgGg5x//vlOiWwf+NnPfsbvf/97Xn/9dcaMGWP7BT7//PN2Jtf48eO54ooruPLKK5kzZ86gOjYIIWhvb7f9ClOpVDfbJ5frSMMZVVWprKzE4/EckRnWG1KpFJ/73Oc4cOAAr7/+uuPQ4jDicMRqFBKJRJg3bx7Lli3j3nvvpbS01BGrPhCJRIhGo0fdV2hvb++WyVVQUGALV3l5+aALVyQSsYUrHo+Tm5trhx+63W40TaOyshK3290voVJVlRtvvJHt27fzxhtvjFqTXocPN45YjUI+//nPk5OTw4MPPsh5553niNUQEI1Geemll1ixYgX//Oc/yczM5IorrmDp0qWcdtppg+7oEIlE7OKMSCRCdnY28Xgcn89HWVlZn19f0zRuuukm1q9fz5tvvukUBDiMWBwj21HG3//+dyorK/nggw+GeygfKdLT07n66qu5+uqricfjvPLKK6xYsYJrrrkGn8/H4sWLWbp0KWeeeWaPy3X9JRAIEAgEmDx5MuFwmHXr1qFpGolEgqqqKrtAoze9ULquc8stt1BRUcGqVascoXIY0ThiNYo4cOAAt956K6+88orTGT+MpKWl2b1aqVSK1157jRUrVvC5z30OSZJYtGgRV155Jeecc86AOz5omsa2bdvw+/2UlpaiqqodbbJjxw6CwaDdy3Ws3ijDMLj99tt55513ePPNNykpKRnQcTo4DDTOMuAo4tlnn+XKK6/stuSj6zqSJCHLMslkckidsB26o6qqncn13HPPkUwmWbRoEUuXLuX888/v9w2GrutUVVUhSRKlpaVH/K2TySShUIj6+npaWloIBAIUFhYe4VdoGAbf/va3+ec//8mqVascr0uHUYEjVqOIcDjMvn37uj12ww03MGPGDO68804ntmQEoes677zzDs888wzPPvss7e3tXHrppSxdupSPfexjvW48toQKOKE9KlVVbeFqampCkiReeuklPvGJT/Dcc8+xYsUK3nzzTcfr0mHU4PRZjSIsr7eu/6Wnp5Obm+sI1QhDURTOPfdc/vu//5t9+/bx0ksvUVJSwne+8x0mTZrEddddx/Lly4lEIsd9Ll3XWbduHUKIEy6mcLvdlJSUUFZWxnnnnUdOTg7btm1j0aJFPPbYY1x66aW0trYOeKxKV5YtW8akSZPw+XyUl5fz9ttvH/P41atXU15ejs/nY/LkyTz66KPdfv74449z9tlnk52dTXZ2Nh/72Mf4v//7v0Ebv8PIwhErB4dBRpZlzjjjDB544AF27drFG2+8wUknncSPfvQjJk6cyCc/+cmjZnLpus769esxDKPPVX8ul4vS0lLOOOMMcnJy+OlPf0o8HudjH/sYEydOZPPmzQP1Vm2eeuopbrvtNu655x6qqqo4++yzufTSS9m/f3+Px+/du5fLLruMs88+m6qqKr7zne/wjW98g+XLl9vHrFq1ik996lO8+eabrFmzhvHjx3PRRRdx6NChAR+/w8jDWQZ0cBgmDMNgw4YNLF++nBUrVrBr1y4uvPBCrrjiCi6//HI8Hg9f/vKX+cxnPsPFF1/c5ypDIQQPPPAADz30EG+88QZz584FzD2u119/nfPPP3/AjWpPO+005s2b183BYebMmSxduvSoGVTPP/88W7dutR+7+eabWb9+PWvWrOnxNXRdJzs7m4cffpjPfe5zAzp+h5GHM7NycBgmZFmmtLSUH/3oR2zatImqqipOP/10Hn30USZNmsQZZ5zB5s2bmTZtWp8LZ4QQ/Pd//zcPPvggL7/8si1UAF6vl8suu2zAhcoy1L3ooou6PX7RRRfx3nvv9fg7a9asOeL4iy++mLVr16Kqao+/E4vFUFXVcdv4iOCIlUO/OHToEJ/97GfJzc21y6krKiqGe1ijDkmSmDVrFt///vd5//33Oeecc0gmk+Tm5jJ//nwuu+wyfvvb31JbW3vC+0xCCH7729/y05/+lBdffJH58+cP8rswaWxsRNf1I/q2CgsLj4ibt6irq+vxeE3TaGxs7PF37rrrLsaMGcPHPvaxgRm4w4jGESuHPtPS0sKZZ56J2+3mpZdeYsuWLfzyl78kKytruIc2atF1nWuvvZa2tjY2btzImjVr2LFjB4sXL+aZZ55h+vTpXHTRRTz88MMcOHDgqMIlhOAPf/gD//Vf/8U//vEPTj/99CF+J0dmwQkhjulm39PxPT0Opq/jk08+yYoVK5yew48Ijlg59Jn777+fcePG8Yc//IFTTz2ViRMncuGFFzJlypThHtqoRVEUFi9ezCuvvEJWVhaSJDFp0iS++c1v8s4777B3716uueYa/vnPf3LyySfbJsZ79+61L+5CCP7yl79w99138/zzz3P22WcP6XvIy8tDUZQjZlENDQ1HdckoKirq8XiXy0Vubm63x3/xi19w33338corrzhpvB8lhINDH5k5c6a47bbbxNVXXy3y8/NFaWmpeOyxx4Z7WB8JDMMQNTU14je/+Y248MILhcvlEqWlpeIHP/iBuO+++0R6erp4+eWXh218p556qvjKV77S7bGZM2eKu+66q8fjv/3tb4uZM2d2e+zmm28Wp59+erfHfvazn4lgMCjWrFkzsAN2GPE4YuXQZ7xer/B6veLuu+8WlZWV4tFHHxU+n0/86U9/Gu6hfaQwDEOEQiHxu9/9TlxwwQUCEH/961+HdUx///vfhdvtFk888YTYsmWLuO2220R6erqorq4WQghx1113ieuuu84+fs+ePcLv94vbb79dbNmyRTzxxBPC7XaLZ555xj7m/vvvFx6PRzzzzDOitrbW/i8cDg/5+3MYehyxcugzbrdbLFy4sNtjt9xyyxF3ww5Dh2EY4sCBA8M9DCGEEL/5zW/EhAkThMfjEfPmzROrV6+2f/b5z39enHvuud2OX7VqlSgrKxMej0dMnDhRPPLII91+PmHCBAEc8d8PfvCDIXg3DsON02fl0GcmTJjAxz/+cX73u9/Zjz3yyCPce++9TqOmg4PDgOIUWDj0mTPPPJPt27d3e2zHjh1MmDBhmEbk4ODwYcURK4c+c/vtt/Pvf/+b++67j127dvG3v/2Nxx57jK997WvDPTQHB4cPGc4yoEO/eOGFF7j77rvZuXMnkyZN4o477uBLX/rScA/LwcHhQ4YjVg4ODg4OIx5nGdDBwcHBYcTjiJXDhwpN0/jud7/LpEmTSEtLY/Lkyfzwhz/EMIzhHpqDg0M/6FvmgMOI5LzzzqO0tJSHHnpouIcybNx///08+uij/OlPf+Lkk09m7dq13HDDDWRmZnLrrbcO9/AcHBz6iCNWDh8q1qxZw5IlS1i0aBEAEydO5Mknn2Tt2rXDPDIHB4f+4CwDfki4/vrrWb16Nb/61a+QJAlJkqiurh7uYQ05Z511Fq+//jo7duwAYP369bzzzjtcdtllwzwyBweH/uDMrD4k/OpXv2LHjh3Mnj2bH/7whwDk5+cP86iGnjvvvJO2tjZmzJiBoijous6Pf/xjPvWpTw330BwcHPqBM7P6kJCZmYnH48Hv91NUVERRUVGf02VHM0899RR//etf+dvf/kZlZSV/+tOf+MUvfsGf/vSn4R7aiGbZsmVMmjQJn89HeXk5b7/99jGPX716NeXl5fh8PiZPnsyjjz56xDHLly9n1qxZeL1eZs2axcqVKwdr+A4fBYbTmNBhYDn33HPFrbfeOtzDGFbGjh0rHn744W6P/ehHPxLTp08fphGNfCyH9Mcff1xs2bJF3HrrrSI9PV3s27evx+Mth/Rbb71VbNmyRTz++ONHOKS/9957QlEUcd9994mtW7eK++67T7hcLvHvf/97qN6Ww4cMZ2bl8KEiFoshy90/1oqiOKXrx+CBBx7gxhtv5Itf/CIzZ87koYceYty4cTzyyCM9Hv/oo48yfvx4HnroIWbOnMkXv/hFvvCFL/CLX/zCPuahhx7i4x//OHfffTczZszg7rvv5sILL/xIV6o69A9HrD5EeDwedF0f7mEMK4sXL+bHP/4x//znP6murmblypU88MADXHnllcM9tBFJKpWioqKCiy66qNvjF110Ee+9916Pv7NmzZojjr/44otZu3Ytqqoe85ijPaeDw/FwCiw+REycOJH333+f6upqAoEAOTk5R8wyPuz893//N9/73vf46le/SkNDAyUlJdx00018//vfH+6hjUgaGxvRdf2IuPnCwsIjYuYt6urqejxe0zQaGxspLi4+6jFHe04Hh+Px0bqSfcj55je/iaIozJo1i/z8fPbv3z/cQxpyMjIyeOihh9i3bx/xeJzdu3dz77334vF4hntoIxpJkrr9WwhxxGPHO/7wx3v7nA4Ox8KZWX2ImDZtGmvWrBnuYTiMIvLy8lAU5YgZT0NDwxEzI4uioqIej3e5XOTm5h7zmKM9p4PD8XBmVg4OH2E8Hg/l5eW8+uqr3R5/9dVXOeOMM3r8nYULFx5x/CuvvML8+fNxu93HPOZoz+ngcFyGuRrRweFDyerVq8Xll18uiouLBSBWrlzZ7eeGYYgf/OAHori4WPh8PnHuueeKTZs2DctYrdL1J554QmzZskXcdtttIj09XVRXVwshhLjrrrvEddddZx9vla7ffvvtYsuWLeKJJ544onT93XffFYqiiJ/+9Kdi69at4qc//alTuu7QLxyxcnAYBF588UVxzz33iOXLl/coVj/96U9FRkaGWL58udi4caO49tprRXFxsWhvbx+W8f7mN78REyZMEB6PR8ybN0+sXr3a/tnnP/95ce6553Y7ftWqVaKsrEx4PB4xceJE8cgjjxzxnE8//bSYPn26cLvdYsaMGWL58uWD/TYcPsQ44YsODoOMJEmsXLmSpUuXAmahQUlJCbfddht33nknAMlkksLCQu6//35uuummYRytg8PIxNmzcnAYYvbu3UtdXV23PiSv18u5557r9CE5OBwFR6wcHIYYq0rO6UNycDhxHLFycBgmnD4kB4cTxxErB4chpqioCMDpQ3Jw6AWOWDk4DDGTJk2iqKioWx9SKpVi9erVTh+Sg8NRcBwsHBwGgUgkwq5du+x/7927l3Xr1pGTk8P48eO57bbbuO+++zjppJM46aSTuO+++/D7/Xz6058exlE7OIxcnNJ1B4dBYNWqVZx//vlHPP75z3+eP/7xjwgh+H//7//x29/+lpaWFk477TR+85vfMHv27GEYrYPDyMcRKwcHBweHEY+zZ+Xg4ODgMOJxxMrBwcHBYcTjiJWDg4ODw4jHESsHBwcHhxGPI1YODg4ODiMeR6wcHBwcHEY8jlg5ODg4OIx4HLFycHBwcBjxOGLl4ODg4DDiccTKwcHBwWHE44iVg4ODg8OI5/8DFi+KbbfYACwAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "import numpy as np\n",
    "\n",
    "# Plot inference\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "nof_pts = 100\n",
    "\n",
    "x_axis = np.linspace(0, L, nof_pts)\n",
    "t_axis = np.linspace(0, t_max, nof_pts)\n",
    "ms_t, ms_x = np.meshgrid(t_axis,x_axis, sparse=False)\n",
    "\n",
    "pts_x = ms_x.reshape(-1, 1)\n",
    "pts_t = ms_t.reshape(-1, 1)\n",
    "\n",
    "pts_x = Variable(torch.from_numpy(pts_x).float(), requires_grad=True).to(device)\n",
    "pts_t = Variable(torch.from_numpy(pts_t).float(), requires_grad=True).to(device)\n",
    "\n",
    "pts_u = model(pts_t, pts_x)\n",
    "ms_u = pts_u.to('cpu').detach().numpy()\n",
    "ms_u = ms_u.reshape(ms_x.shape, order='C')\n",
    "\n",
    "surface = ax.plot_surface(ms_t, ms_x, ms_u, \n",
    "                          cmap=cm.coolwarm, linewidth=0)#, antialiased=False)\n",
    "\n",
    "plt.axis('auto')\n",
    "ax.set_xlabel(\"t\")\n",
    "ax.set_ylabel(\"x\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Interpretation\n",
    "\n",
    "   After few trainings, we noted that the first and second boundary conditions aren't respected at all. We provide them higher weights than the one of the DPE cost. \n",
    "   With this changes, we notice better predictions of the overtones that start appearing on the curve: higher peak at $t=1s$ than the one at $t=2s$. The network tries to set all value to 0 on x=0.\n",
    "   By a better setting of the hyper-parameters, we obtain the characteristics overtones. \n",
    "\n",
    "   Some ideas to enhance the PINN :\n",
    "   - more neurons on the layers or more layers with the same amount of neurons\n",
    "   - change the setting of the learning rate\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c8cc5b7cf7ab45a4eb9c5d10fcde61976ab495d4e3d71a8f87de6440d779c3fd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
