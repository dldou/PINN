{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem to solve\n",
    "\n",
    "We consider the following DPE :\n",
    "$$\n",
    "    \\frac{\\partial ²u}{\\partial t²} = \\frac{T}{\\mu} \\frac{\\partial ²u}{\\partial x²} \n",
    "$$\n",
    "and its boundary conditions : \n",
    "$$\n",
    "\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; \\forall t > 0, \\; u(x=0, t) = 0 \\;\\; (1)\\\\\n",
    "\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; \\forall t > 0, u(x=L, t) = 0 \\;\\; (2)\\\\\n",
    "and \\;\\; for \\;\\; t=0, \\;\\;\\;\\;\\;\\; u(x, 0) = Acos(2\\pi\\frac{x}{\\lambda}) \\;\\;\\;\\;\\;\\; (3)\n",
    "$$\n",
    "where $\\lambda$ is the wave length and A is the maximum positive value reach by the string.  \n",
    "  \n",
    "To solve this equation, we'll use a PINN.\n",
    "We define $f(t,x)$ as $f := \\frac{\\partial ²u}{\\partial t²} - \\frac{T}{\\mu} \\frac{\\partial ²u}{\\partial x²}, \\;$ u the approximated function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "from math import pi\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class vanilla_PINN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, nof_hidden_layers):\n",
    "        super().__init__()\n",
    "\n",
    "        # To avoid bad entry: the inputs are x and t and the output is a scalar \n",
    "        try:\n",
    "\n",
    "            if input_size != 2:\n",
    "                raise ValueError(\"Input size must be equal to 2\")\n",
    "\n",
    "            elif output_size != 1:\n",
    "                raise ValueError(\"Output size must be equal to 1\")\n",
    "\n",
    "            self.input_layer = nn.Sequential(nn.Linear(input_size, hidden_size), nn.GELU())\n",
    "            self.output_layer = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "        except ValueError as error:\n",
    "            print(error)\n",
    "            return None\n",
    "\n",
    "        # To be flexible on the number of hidden layers when instanciate the NN\n",
    "        self.hidden_layers = nn.Sequential()\n",
    "\n",
    "        for i in range(nof_hidden_layers):\n",
    "            self.hidden_layers.add_module(\"hidden_layer_\" + str(i), nn.Linear(hidden_size, hidden_size))\n",
    "            self.hidden_layers.add_module(\"GELU_\" + str(i), nn.GELU())\n",
    "\n",
    "\n",
    "    def forward(self, t, x):\n",
    "        \n",
    "        V = torch.cat([t, x], dim=1)\n",
    "        V = self.input_layer(V)\n",
    "        V = self.hidden_layers(V)\n",
    "        V = self.output_layer(V)\n",
    "        \n",
    "        return V"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Partial Derivative Equation (PDE) as a one of the loss\n",
    "def f(t, x, neural_net, pb_params_list):\n",
    "    \"\"\"\n",
    "        (from the problem) PDE : u_tt - (T / mu) * u_xx\n",
    "        pb_params_list = [T, mu]\n",
    "    \"\"\"\n",
    "\n",
    "    u    = neural_net(t, x)\n",
    "    \n",
    "    #d²u/dx² = u_xx \n",
    "    u_x  = torch.autograd.grad(u.sum(), x, create_graph=True)[0]\n",
    "    u_xx = torch.autograd.grad(u_x.sum(), x, create_graph=True)[0]\n",
    "    \n",
    "    # d²u/dt² = u_tt\n",
    "    u_t  = torch.autograd.grad(u.sum(), t, create_graph=True)[0]\n",
    "    u_tt = torch.autograd.grad(u.sum(), t, create_graph=True)[0]\n",
    "\n",
    "    PDE = u_tt - (pb_params_list[0] / pb_params_list[1]) * u_xx\n",
    "\n",
    "    return PDE\n",
    "\n",
    "\n",
    "# Loss function on boundary conditions points\n",
    "def MSE_loss(groung_truth_pts, net_output_pts):\n",
    "    mse_bc = nn.MSELoss()\n",
    "    return mse_bc(groung_truth_pts, net_output_pts)\n",
    "\n",
    "\n",
    "def train(model, nof_collocations_pts,\n",
    "          x_bc1_pts, t_bc1_pts, u_bc1_pts,\n",
    "          x_bc2_pts, t_bc2_pts, u_bc2_pts,\n",
    "          x_bc3_pts, t_bc3_pts, u_bc3_pts,\n",
    "          pb_params_list,\n",
    "          optimizer, nof_iterations, \n",
    "          device):\n",
    "\n",
    "    training_loss = []\n",
    "\n",
    "    for epoch in range(nof_iterations):\n",
    "\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #Loss on first BC (u(x=0, t) = 0 for all t)\n",
    "        x_bc1_pts = Variable(x_bc1_pts, requires_grad=False).to(device)\n",
    "        t_bc1_pts = Variable(t_bc1_pts, requires_grad=False).to(device)\n",
    "        u_bc1_pts = Variable(u_bc1_pts, requires_grad=False).to(device)\n",
    "\n",
    "        net_output_bc1 = model(t_bc1_pts, x_bc1_pts)\n",
    "        mse_u_bc1      = MSE_loss(u_bc1_pts, net_output_bc1)\n",
    "\n",
    "        #Loss on second BC (u(x=L, t) = 0 for all t)\n",
    "        x_bc2_pts = Variable(x_bc2_pts, requires_grad=False).to(device)\n",
    "        t_bc2_pts = Variable(t_bc2_pts, requires_grad=False).to(device)\n",
    "        u_bc2_pts = Variable(u_bc2_pts, requires_grad=False).to(device)\n",
    "\n",
    "        net_output_bc2 = model(t_bc2_pts, x_bc2_pts)\n",
    "        mse_u_bc2      = MSE_loss(u_bc2_pts, net_output_bc2)\n",
    "\n",
    "        #Loss in third BC (u(x, t=0) = u_max * cos(2 * pi * x / lambda) for all x and where lambda is the wave length)\n",
    "        x_bc3_pts = Variable(x_bc3_pts, requires_grad=False).to(device)\n",
    "        t_bc3_pts = Variable(t_bc3_pts, requires_grad=False).to(device)\n",
    "        u_bc3_pts = Variable(u_bc3_pts, requires_grad=False).to(device)\n",
    "\n",
    "        net_output_bc3 = model(t_bc3_pts, x_bc3_pts)\n",
    "        mse_u_bc3      = MSE_loss(u_bc3_pts, net_output_bc3)\n",
    "\n",
    "        # PDE loss : collocations points form an equally spaced (t,x)-grid where x_max = L\n",
    "        L     = x_bc2_pts.data[0].item()\n",
    "        t_max = torch.max(t_bc1_pts).item()\n",
    "\n",
    "        x_collocation_pts = torch.linspace(0, L, nof_collocations_pts)\n",
    "        x_collocation_pts = x_collocation_pts.reshape(x_collocation_pts.shape[0], 1)\n",
    "        x_collocation_pts = Variable(x_collocation_pts, requires_grad=True).to(device)\n",
    "\n",
    "        t_collocation_pts = torch.linspace(0, t_max, nof_collocations_pts)\n",
    "        t_collocation_pts = t_collocation_pts.reshape(t_collocation_pts.shape[0], 1)\n",
    "        t_collocation_pts = Variable(t_collocation_pts, requires_grad=True).to(device)\n",
    "\n",
    "        f_output = f(t_collocation_pts, x_collocation_pts, model, pb_params_list)\n",
    "        mse_pde  = MSE_loss(f_output, torch.zeros_like(f_output))\n",
    "\n",
    "        #Compute loss as the sum of all cost\n",
    "        total_running_loss = mse_u_bc1 + mse_u_bc2 + mse_u_bc3 + mse_pde\n",
    "        training_loss.append(total_running_loss.detach().to('cpu'))\n",
    "\n",
    "        total_running_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        with torch.autograd.no_grad():\n",
    "            print('Epoch: {} | training loss: {:.12f}'.format(epoch, total_running_loss))\n",
    "\n",
    "    return training_loss\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | training loss: 1.299397110939\n",
      "Epoch: 1 | training loss: 1.283812522888\n",
      "Epoch: 2 | training loss: 1.268445968628\n",
      "Epoch: 3 | training loss: 1.253298759460\n",
      "Epoch: 4 | training loss: 1.238372802734\n",
      "Epoch: 5 | training loss: 1.223669648170\n",
      "Epoch: 6 | training loss: 1.209188103676\n",
      "Epoch: 7 | training loss: 1.194929838181\n",
      "Epoch: 8 | training loss: 1.180895209312\n",
      "Epoch: 9 | training loss: 1.167083382607\n",
      "Epoch: 10 | training loss: 1.153494954109\n",
      "Epoch: 11 | training loss: 1.140128374100\n",
      "Epoch: 12 | training loss: 1.126984119415\n",
      "Epoch: 13 | training loss: 1.114060401917\n",
      "Epoch: 14 | training loss: 1.101356387138\n",
      "Epoch: 15 | training loss: 1.088869810104\n",
      "Epoch: 16 | training loss: 1.076599597931\n",
      "Epoch: 17 | training loss: 1.064544677734\n",
      "Epoch: 18 | training loss: 1.052701950073\n",
      "Epoch: 19 | training loss: 1.041069626808\n",
      "Epoch: 20 | training loss: 1.029644966125\n",
      "Epoch: 21 | training loss: 1.018425941467\n",
      "Epoch: 22 | training loss: 1.007410287857\n",
      "Epoch: 23 | training loss: 0.996594190598\n",
      "Epoch: 24 | training loss: 0.985975861549\n",
      "Epoch: 25 | training loss: 0.975552201271\n",
      "Epoch: 26 | training loss: 0.965320408344\n",
      "Epoch: 27 | training loss: 0.955276966095\n",
      "Epoch: 28 | training loss: 0.945419192314\n",
      "Epoch: 29 | training loss: 0.935744225979\n",
      "Epoch: 30 | training loss: 0.926248848438\n",
      "Epoch: 31 | training loss: 0.916929721832\n",
      "Epoch: 32 | training loss: 0.907784342766\n",
      "Epoch: 33 | training loss: 0.898808956146\n",
      "Epoch: 34 | training loss: 0.890000343323\n",
      "Epoch: 35 | training loss: 0.881355702877\n",
      "Epoch: 36 | training loss: 0.872871696949\n",
      "Epoch: 37 | training loss: 0.864544987679\n",
      "Epoch: 38 | training loss: 0.856372654438\n",
      "Epoch: 39 | training loss: 0.848351240158\n",
      "Epoch: 40 | training loss: 0.840477645397\n",
      "Epoch: 41 | training loss: 0.832749009132\n",
      "Epoch: 42 | training loss: 0.825162291527\n",
      "Epoch: 43 | training loss: 0.817714095116\n",
      "Epoch: 44 | training loss: 0.810401916504\n",
      "Epoch: 45 | training loss: 0.803222298622\n",
      "Epoch: 46 | training loss: 0.796172678471\n",
      "Epoch: 47 | training loss: 0.789250075817\n",
      "Epoch: 48 | training loss: 0.782451510429\n",
      "Epoch: 49 | training loss: 0.775774419308\n",
      "Epoch: 50 | training loss: 0.769216179848\n",
      "Epoch: 51 | training loss: 0.762773752213\n",
      "Epoch: 52 | training loss: 0.756444811821\n",
      "Epoch: 53 | training loss: 0.750226676464\n",
      "Epoch: 54 | training loss: 0.744116604328\n",
      "Epoch: 55 | training loss: 0.738112568855\n",
      "Epoch: 56 | training loss: 0.732211828232\n",
      "Epoch: 57 | training loss: 0.726411938667\n",
      "Epoch: 58 | training loss: 0.720710873604\n",
      "Epoch: 59 | training loss: 0.715106010437\n",
      "Epoch: 60 | training loss: 0.709595441818\n",
      "Epoch: 61 | training loss: 0.704176962376\n",
      "Epoch: 62 | training loss: 0.698848128319\n",
      "Epoch: 63 | training loss: 0.693607330322\n",
      "Epoch: 64 | training loss: 0.688452363014\n",
      "Epoch: 65 | training loss: 0.683380901814\n",
      "Epoch: 66 | training loss: 0.678391575813\n",
      "Epoch: 67 | training loss: 0.673482239246\n",
      "Epoch: 68 | training loss: 0.668650865555\n",
      "Epoch: 69 | training loss: 0.663896083832\n",
      "Epoch: 70 | training loss: 0.659215807915\n",
      "Epoch: 71 | training loss: 0.654608249664\n",
      "Epoch: 72 | training loss: 0.650072097778\n",
      "Epoch: 73 | training loss: 0.645605623722\n",
      "Epoch: 74 | training loss: 0.641206979752\n",
      "Epoch: 75 | training loss: 0.636874794960\n",
      "Epoch: 76 | training loss: 0.632607817650\n",
      "Epoch: 77 | training loss: 0.628404438496\n",
      "Epoch: 78 | training loss: 0.624263048172\n",
      "Epoch: 79 | training loss: 0.620182275772\n",
      "Epoch: 80 | training loss: 0.616160750389\n",
      "Epoch: 81 | training loss: 0.612197637558\n",
      "Epoch: 82 | training loss: 0.608291149139\n",
      "Epoch: 83 | training loss: 0.604440093040\n",
      "Epoch: 84 | training loss: 0.600643515587\n",
      "Epoch: 85 | training loss: 0.596900105476\n",
      "Epoch: 86 | training loss: 0.593208551407\n",
      "Epoch: 87 | training loss: 0.589567899704\n",
      "Epoch: 88 | training loss: 0.585976958275\n",
      "Epoch: 89 | training loss: 0.582434892654\n",
      "Epoch: 90 | training loss: 0.578940212727\n",
      "Epoch: 91 | training loss: 0.575492203236\n",
      "Epoch: 92 | training loss: 0.572089910507\n",
      "Epoch: 93 | training loss: 0.568732202053\n",
      "Epoch: 94 | training loss: 0.565418481827\n",
      "Epoch: 95 | training loss: 0.562147438526\n",
      "Epoch: 96 | training loss: 0.558918416500\n",
      "Epoch: 97 | training loss: 0.555730462074\n",
      "Epoch: 98 | training loss: 0.552582859993\n",
      "Epoch: 99 | training loss: 0.549474596977\n",
      "Epoch: 100 | training loss: 0.546405076981\n",
      "Epoch: 101 | training loss: 0.543373346329\n",
      "Epoch: 102 | training loss: 0.540378570557\n",
      "Epoch: 103 | training loss: 0.537420272827\n",
      "Epoch: 104 | training loss: 0.534497559071\n",
      "Epoch: 105 | training loss: 0.531609892845\n",
      "Epoch: 106 | training loss: 0.528756439686\n",
      "Epoch: 107 | training loss: 0.525936722755\n",
      "Epoch: 108 | training loss: 0.523149669170\n",
      "Epoch: 109 | training loss: 0.520395100117\n",
      "Epoch: 110 | training loss: 0.517672181129\n",
      "Epoch: 111 | training loss: 0.514980316162\n",
      "Epoch: 112 | training loss: 0.512318968773\n",
      "Epoch: 113 | training loss: 0.509687423706\n",
      "Epoch: 114 | training loss: 0.507085382938\n",
      "Epoch: 115 | training loss: 0.504512071609\n",
      "Epoch: 116 | training loss: 0.501967072487\n",
      "Epoch: 117 | training loss: 0.499449759722\n",
      "Epoch: 118 | training loss: 0.496959805489\n",
      "Epoch: 119 | training loss: 0.494496554136\n",
      "Epoch: 120 | training loss: 0.492059588432\n",
      "Epoch: 121 | training loss: 0.489648401737\n",
      "Epoch: 122 | training loss: 0.487262666225\n",
      "Epoch: 123 | training loss: 0.484901666641\n",
      "Epoch: 124 | training loss: 0.482565045357\n",
      "Epoch: 125 | training loss: 0.480252563953\n",
      "Epoch: 126 | training loss: 0.477963685989\n",
      "Epoch: 127 | training loss: 0.475698113441\n",
      "Epoch: 128 | training loss: 0.473455160856\n",
      "Epoch: 129 | training loss: 0.471234798431\n",
      "Epoch: 130 | training loss: 0.469036340714\n",
      "Epoch: 131 | training loss: 0.466859579086\n",
      "Epoch: 132 | training loss: 0.464704215527\n",
      "Epoch: 133 | training loss: 0.462569773197\n",
      "Epoch: 134 | training loss: 0.460455894470\n",
      "Epoch: 135 | training loss: 0.458362281322\n",
      "Epoch: 136 | training loss: 0.456288665533\n",
      "Epoch: 137 | training loss: 0.454234600067\n",
      "Epoch: 138 | training loss: 0.452199816704\n",
      "Epoch: 139 | training loss: 0.450183987617\n",
      "Epoch: 140 | training loss: 0.448186993599\n",
      "Epoch: 141 | training loss: 0.446208178997\n",
      "Epoch: 142 | training loss: 0.444247603416\n",
      "Epoch: 143 | training loss: 0.442304730415\n",
      "Epoch: 144 | training loss: 0.440379351377\n",
      "Epoch: 145 | training loss: 0.438471257687\n",
      "Epoch: 146 | training loss: 0.436580181122\n",
      "Epoch: 147 | training loss: 0.434705853462\n",
      "Epoch: 148 | training loss: 0.432847857475\n",
      "Epoch: 149 | training loss: 0.431006252766\n",
      "Epoch: 150 | training loss: 0.429180562496\n",
      "Epoch: 151 | training loss: 0.427370637655\n",
      "Epoch: 152 | training loss: 0.425576239824\n",
      "Epoch: 153 | training loss: 0.423797130585\n",
      "Epoch: 154 | training loss: 0.422032982111\n",
      "Epoch: 155 | training loss: 0.420283794403\n",
      "Epoch: 156 | training loss: 0.418549001217\n",
      "Epoch: 157 | training loss: 0.416828811169\n",
      "Epoch: 158 | training loss: 0.415122777224\n",
      "Epoch: 159 | training loss: 0.413430809975\n",
      "Epoch: 160 | training loss: 0.411752641201\n",
      "Epoch: 161 | training loss: 0.410087972879\n",
      "Epoch: 162 | training loss: 0.408436775208\n",
      "Epoch: 163 | training loss: 0.406798809767\n",
      "Epoch: 164 | training loss: 0.405173838139\n",
      "Epoch: 165 | training loss: 0.403561830521\n",
      "Epoch: 166 | training loss: 0.401962518692\n",
      "Epoch: 167 | training loss: 0.400375664234\n",
      "Epoch: 168 | training loss: 0.398801147938\n",
      "Epoch: 169 | training loss: 0.397238850594\n",
      "Epoch: 170 | training loss: 0.395688593388\n",
      "Epoch: 171 | training loss: 0.394150108099\n",
      "Epoch: 172 | training loss: 0.392623245716\n",
      "Epoch: 173 | training loss: 0.391108095646\n",
      "Epoch: 174 | training loss: 0.389604330063\n",
      "Epoch: 175 | training loss: 0.388111770153\n",
      "Epoch: 176 | training loss: 0.386630266905\n",
      "Epoch: 177 | training loss: 0.385159701109\n",
      "Epoch: 178 | training loss: 0.383700013161\n",
      "Epoch: 179 | training loss: 0.382250934839\n",
      "Epoch: 180 | training loss: 0.380812346935\n",
      "Epoch: 181 | training loss: 0.379384279251\n",
      "Epoch: 182 | training loss: 0.377966433764\n",
      "Epoch: 183 | training loss: 0.376558661461\n",
      "Epoch: 184 | training loss: 0.375160992146\n",
      "Epoch: 185 | training loss: 0.373773157597\n",
      "Epoch: 186 | training loss: 0.372395098209\n",
      "Epoch: 187 | training loss: 0.371026754379\n",
      "Epoch: 188 | training loss: 0.369667887688\n",
      "Epoch: 189 | training loss: 0.368318349123\n",
      "Epoch: 190 | training loss: 0.366978198290\n",
      "Epoch: 191 | training loss: 0.365647196770\n",
      "Epoch: 192 | training loss: 0.364325344563\n",
      "Epoch: 193 | training loss: 0.363012433052\n",
      "Epoch: 194 | training loss: 0.361708343029\n",
      "Epoch: 195 | training loss: 0.360413074493\n",
      "Epoch: 196 | training loss: 0.359126389027\n",
      "Epoch: 197 | training loss: 0.357848346233\n",
      "Epoch: 198 | training loss: 0.356578677893\n",
      "Epoch: 199 | training loss: 0.355317413807\n",
      "Epoch: 200 | training loss: 0.354064494371\n",
      "Epoch: 201 | training loss: 0.352819591761\n",
      "Epoch: 202 | training loss: 0.351582884789\n",
      "Epoch: 203 | training loss: 0.350354015827\n",
      "Epoch: 204 | training loss: 0.349133133888\n",
      "Epoch: 205 | training loss: 0.347920000553\n",
      "Epoch: 206 | training loss: 0.346714615822\n",
      "Epoch: 207 | training loss: 0.345516860485\n",
      "Epoch: 208 | training loss: 0.344326585531\n",
      "Epoch: 209 | training loss: 0.343143910170\n",
      "Epoch: 210 | training loss: 0.341968476772\n",
      "Epoch: 211 | training loss: 0.340800344944\n",
      "Epoch: 212 | training loss: 0.339639425278\n",
      "Epoch: 213 | training loss: 0.338485687971\n",
      "Epoch: 214 | training loss: 0.337339013815\n",
      "Epoch: 215 | training loss: 0.336199223995\n",
      "Epoch: 216 | training loss: 0.335066378117\n",
      "Epoch: 217 | training loss: 0.333940327168\n",
      "Epoch: 218 | training loss: 0.332821130753\n",
      "Epoch: 219 | training loss: 0.331708580256\n",
      "Epoch: 220 | training loss: 0.330602675676\n",
      "Epoch: 221 | training loss: 0.329503327608\n",
      "Epoch: 222 | training loss: 0.328410387039\n",
      "Epoch: 223 | training loss: 0.327323824167\n",
      "Epoch: 224 | training loss: 0.326243758202\n",
      "Epoch: 225 | training loss: 0.325169831514\n",
      "Epoch: 226 | training loss: 0.324102163315\n",
      "Epoch: 227 | training loss: 0.323040544987\n",
      "Epoch: 228 | training loss: 0.321985125542\n",
      "Epoch: 229 | training loss: 0.320935666561\n",
      "Epoch: 230 | training loss: 0.319892287254\n",
      "Epoch: 231 | training loss: 0.318854659796\n",
      "Epoch: 232 | training loss: 0.317822933197\n",
      "Epoch: 233 | training loss: 0.316797047853\n",
      "Epoch: 234 | training loss: 0.315776824951\n",
      "Epoch: 235 | training loss: 0.314762353897\n",
      "Epoch: 236 | training loss: 0.313753426075\n",
      "Epoch: 237 | training loss: 0.312750101089\n",
      "Epoch: 238 | training loss: 0.311752200127\n",
      "Epoch: 239 | training loss: 0.310759723186\n",
      "Epoch: 240 | training loss: 0.309772670269\n",
      "Epoch: 241 | training loss: 0.308791041374\n",
      "Epoch: 242 | training loss: 0.307814687490\n",
      "Epoch: 243 | training loss: 0.306843578815\n",
      "Epoch: 244 | training loss: 0.305877625942\n",
      "Epoch: 245 | training loss: 0.304916858673\n",
      "Epoch: 246 | training loss: 0.303961098194\n",
      "Epoch: 247 | training loss: 0.303010463715\n",
      "Epoch: 248 | training loss: 0.302064806223\n",
      "Epoch: 249 | training loss: 0.301124036312\n",
      "Epoch: 250 | training loss: 0.300188243389\n",
      "Epoch: 251 | training loss: 0.299257278442\n",
      "Epoch: 252 | training loss: 0.298331081867\n",
      "Epoch: 253 | training loss: 0.297409713268\n",
      "Epoch: 254 | training loss: 0.296493053436\n",
      "Epoch: 255 | training loss: 0.295581102371\n",
      "Epoch: 256 | training loss: 0.294673740864\n",
      "Epoch: 257 | training loss: 0.293770879507\n",
      "Epoch: 258 | training loss: 0.292872667313\n",
      "Epoch: 259 | training loss: 0.291978955269\n",
      "Epoch: 260 | training loss: 0.291089773178\n",
      "Epoch: 261 | training loss: 0.290204942226\n",
      "Epoch: 262 | training loss: 0.289324462414\n",
      "Epoch: 263 | training loss: 0.288448393345\n",
      "Epoch: 264 | training loss: 0.287576586008\n",
      "Epoch: 265 | training loss: 0.286709040403\n",
      "Epoch: 266 | training loss: 0.285845786333\n",
      "Epoch: 267 | training loss: 0.284986734390\n",
      "Epoch: 268 | training loss: 0.284131824970\n",
      "Epoch: 269 | training loss: 0.283281058073\n",
      "Epoch: 270 | training loss: 0.282434374094\n",
      "Epoch: 271 | training loss: 0.281591802835\n",
      "Epoch: 272 | training loss: 0.280753195286\n",
      "Epoch: 273 | training loss: 0.279918551445\n",
      "Epoch: 274 | training loss: 0.279087841511\n",
      "Epoch: 275 | training loss: 0.278261125088\n",
      "Epoch: 276 | training loss: 0.277438253164\n",
      "Epoch: 277 | training loss: 0.276619255543\n",
      "Epoch: 278 | training loss: 0.275803923607\n",
      "Epoch: 279 | training loss: 0.274992495775\n",
      "Epoch: 280 | training loss: 0.274184763432\n",
      "Epoch: 281 | training loss: 0.273380726576\n",
      "Epoch: 282 | training loss: 0.272580474615\n",
      "Epoch: 283 | training loss: 0.271783828735\n",
      "Epoch: 284 | training loss: 0.270990848541\n",
      "Epoch: 285 | training loss: 0.270201474428\n",
      "Epoch: 286 | training loss: 0.269415676594\n",
      "Epoch: 287 | training loss: 0.268633395433\n",
      "Epoch: 288 | training loss: 0.267854601145\n",
      "Epoch: 289 | training loss: 0.267079353333\n",
      "Epoch: 290 | training loss: 0.266307473183\n",
      "Epoch: 291 | training loss: 0.265539050102\n",
      "Epoch: 292 | training loss: 0.264774024487\n",
      "Epoch: 293 | training loss: 0.264012396336\n",
      "Epoch: 294 | training loss: 0.263254106045\n",
      "Epoch: 295 | training loss: 0.262499094009\n",
      "Epoch: 296 | training loss: 0.261747419834\n",
      "Epoch: 297 | training loss: 0.260999053717\n",
      "Epoch: 298 | training loss: 0.260253846645\n",
      "Epoch: 299 | training loss: 0.259511858225\n",
      "Epoch: 300 | training loss: 0.258773028851\n",
      "Epoch: 301 | training loss: 0.258037418127\n",
      "Epoch: 302 | training loss: 0.257304966450\n",
      "Epoch: 303 | training loss: 0.256575584412\n",
      "Epoch: 304 | training loss: 0.255849272013\n",
      "Epoch: 305 | training loss: 0.255126088858\n",
      "Epoch: 306 | training loss: 0.254405915737\n",
      "Epoch: 307 | training loss: 0.253688782454\n",
      "Epoch: 308 | training loss: 0.252974539995\n",
      "Epoch: 309 | training loss: 0.252263367176\n",
      "Epoch: 310 | training loss: 0.251555174589\n",
      "Epoch: 311 | training loss: 0.250849902630\n",
      "Epoch: 312 | training loss: 0.250147521496\n",
      "Epoch: 313 | training loss: 0.249447971582\n",
      "Epoch: 314 | training loss: 0.248751312494\n",
      "Epoch: 315 | training loss: 0.248057574034\n",
      "Epoch: 316 | training loss: 0.247366607189\n",
      "Epoch: 317 | training loss: 0.246678426862\n",
      "Epoch: 318 | training loss: 0.245993092656\n",
      "Epoch: 319 | training loss: 0.245310425758\n",
      "Epoch: 320 | training loss: 0.244630545378\n",
      "Epoch: 321 | training loss: 0.243953391910\n",
      "Epoch: 322 | training loss: 0.243278935552\n",
      "Epoch: 323 | training loss: 0.242607176304\n",
      "Epoch: 324 | training loss: 0.241938039660\n",
      "Epoch: 325 | training loss: 0.241271585226\n",
      "Epoch: 326 | training loss: 0.240607783198\n",
      "Epoch: 327 | training loss: 0.239946499467\n",
      "Epoch: 328 | training loss: 0.239287868142\n",
      "Epoch: 329 | training loss: 0.238631814718\n",
      "Epoch: 330 | training loss: 0.237978279591\n",
      "Epoch: 331 | training loss: 0.237327277660\n",
      "Epoch: 332 | training loss: 0.236678823829\n",
      "Epoch: 333 | training loss: 0.236032843590\n",
      "Epoch: 334 | training loss: 0.235389366746\n",
      "Epoch: 335 | training loss: 0.234748333693\n",
      "Epoch: 336 | training loss: 0.234109729528\n",
      "Epoch: 337 | training loss: 0.233473598957\n",
      "Epoch: 338 | training loss: 0.232839852571\n",
      "Epoch: 339 | training loss: 0.232208475471\n",
      "Epoch: 340 | training loss: 0.231579616666\n",
      "Epoch: 341 | training loss: 0.230953052640\n",
      "Epoch: 342 | training loss: 0.230328783393\n",
      "Epoch: 343 | training loss: 0.229706823826\n",
      "Epoch: 344 | training loss: 0.229087293148\n",
      "Epoch: 345 | training loss: 0.228469997644\n",
      "Epoch: 346 | training loss: 0.227855026722\n",
      "Epoch: 347 | training loss: 0.227242335677\n",
      "Epoch: 348 | training loss: 0.226631850004\n",
      "Epoch: 349 | training loss: 0.226023674011\n",
      "Epoch: 350 | training loss: 0.225417658687\n",
      "Epoch: 351 | training loss: 0.224813908339\n",
      "Epoch: 352 | training loss: 0.224212318659\n",
      "Epoch: 353 | training loss: 0.223612919450\n",
      "Epoch: 354 | training loss: 0.223015740514\n",
      "Epoch: 355 | training loss: 0.222420692444\n",
      "Epoch: 356 | training loss: 0.221827775240\n",
      "Epoch: 357 | training loss: 0.221237003803\n",
      "Epoch: 358 | training loss: 0.220648378134\n",
      "Epoch: 359 | training loss: 0.220061838627\n",
      "Epoch: 360 | training loss: 0.219477370381\n",
      "Epoch: 361 | training loss: 0.218894988298\n",
      "Epoch: 362 | training loss: 0.218314677477\n",
      "Epoch: 363 | training loss: 0.217736423016\n",
      "Epoch: 364 | training loss: 0.217160210013\n",
      "Epoch: 365 | training loss: 0.216586008668\n",
      "Epoch: 366 | training loss: 0.216013804078\n",
      "Epoch: 367 | training loss: 0.215443670750\n",
      "Epoch: 368 | training loss: 0.214875474572\n",
      "Epoch: 369 | training loss: 0.214309260249\n",
      "Epoch: 370 | training loss: 0.213745057583\n",
      "Epoch: 371 | training loss: 0.213182747364\n",
      "Epoch: 372 | training loss: 0.212622404099\n",
      "Epoch: 373 | training loss: 0.212063997984\n",
      "Epoch: 374 | training loss: 0.211507469416\n",
      "Epoch: 375 | training loss: 0.210952907801\n",
      "Epoch: 376 | training loss: 0.210400223732\n",
      "Epoch: 377 | training loss: 0.209849476814\n",
      "Epoch: 378 | training loss: 0.209300518036\n",
      "Epoch: 379 | training loss: 0.208753481507\n",
      "Epoch: 380 | training loss: 0.208208292723\n",
      "Epoch: 381 | training loss: 0.207664936781\n",
      "Epoch: 382 | training loss: 0.207123428583\n",
      "Epoch: 383 | training loss: 0.206583723426\n",
      "Epoch: 384 | training loss: 0.206045866013\n",
      "Epoch: 385 | training loss: 0.205509766936\n",
      "Epoch: 386 | training loss: 0.204975455999\n",
      "Epoch: 387 | training loss: 0.204442918301\n",
      "Epoch: 388 | training loss: 0.203912168741\n",
      "Epoch: 389 | training loss: 0.203383266926\n",
      "Epoch: 390 | training loss: 0.202856004238\n",
      "Epoch: 391 | training loss: 0.202330529690\n",
      "Epoch: 392 | training loss: 0.201806783676\n",
      "Epoch: 393 | training loss: 0.201284766197\n",
      "Epoch: 394 | training loss: 0.200764462352\n",
      "Epoch: 395 | training loss: 0.200245857239\n",
      "Epoch: 396 | training loss: 0.199728965759\n",
      "Epoch: 397 | training loss: 0.199213728309\n",
      "Epoch: 398 | training loss: 0.198700249195\n",
      "Epoch: 399 | training loss: 0.198188379407\n",
      "Epoch: 400 | training loss: 0.197678148746\n",
      "Epoch: 401 | training loss: 0.197169572115\n",
      "Epoch: 402 | training loss: 0.196662664413\n",
      "Epoch: 403 | training loss: 0.196157395840\n",
      "Epoch: 404 | training loss: 0.195653766394\n",
      "Epoch: 405 | training loss: 0.195151686668\n",
      "Epoch: 406 | training loss: 0.194651275873\n",
      "Epoch: 407 | training loss: 0.194152459502\n",
      "Epoch: 408 | training loss: 0.193655252457\n",
      "Epoch: 409 | training loss: 0.193159595132\n",
      "Epoch: 410 | training loss: 0.192665517330\n",
      "Epoch: 411 | training loss: 0.192173033953\n",
      "Epoch: 412 | training loss: 0.191682085395\n",
      "Epoch: 413 | training loss: 0.191192716360\n",
      "Epoch: 414 | training loss: 0.190704882145\n",
      "Epoch: 415 | training loss: 0.190218597651\n",
      "Epoch: 416 | training loss: 0.189733818173\n",
      "Epoch: 417 | training loss: 0.189250543714\n",
      "Epoch: 418 | training loss: 0.188768833876\n",
      "Epoch: 419 | training loss: 0.188288629055\n",
      "Epoch: 420 | training loss: 0.187809899449\n",
      "Epoch: 421 | training loss: 0.187332704663\n",
      "Epoch: 422 | training loss: 0.186856970191\n",
      "Epoch: 423 | training loss: 0.186382740736\n",
      "Epoch: 424 | training loss: 0.185909956694\n",
      "Epoch: 425 | training loss: 0.185438647866\n",
      "Epoch: 426 | training loss: 0.184968829155\n",
      "Epoch: 427 | training loss: 0.184500455856\n",
      "Epoch: 428 | training loss: 0.184033513069\n",
      "Epoch: 429 | training loss: 0.183568015695\n",
      "Epoch: 430 | training loss: 0.183103933930\n",
      "Epoch: 431 | training loss: 0.182641297579\n",
      "Epoch: 432 | training loss: 0.182180091739\n",
      "Epoch: 433 | training loss: 0.181720316410\n",
      "Epoch: 434 | training loss: 0.181261941791\n",
      "Epoch: 435 | training loss: 0.180804967880\n",
      "Epoch: 436 | training loss: 0.180349394679\n",
      "Epoch: 437 | training loss: 0.179895207286\n",
      "Epoch: 438 | training loss: 0.179442420602\n",
      "Epoch: 439 | training loss: 0.178990960121\n",
      "Epoch: 440 | training loss: 0.178540870547\n",
      "Epoch: 441 | training loss: 0.178092166781\n",
      "Epoch: 442 | training loss: 0.177644789219\n",
      "Epoch: 443 | training loss: 0.177198812366\n",
      "Epoch: 444 | training loss: 0.176754206419\n",
      "Epoch: 445 | training loss: 0.176310896873\n",
      "Epoch: 446 | training loss: 0.175868988037\n",
      "Epoch: 447 | training loss: 0.175428330898\n",
      "Epoch: 448 | training loss: 0.174988999963\n",
      "Epoch: 449 | training loss: 0.174551039934\n",
      "Epoch: 450 | training loss: 0.174114376307\n",
      "Epoch: 451 | training loss: 0.173678994179\n",
      "Epoch: 452 | training loss: 0.173244953156\n",
      "Epoch: 453 | training loss: 0.172812193632\n",
      "Epoch: 454 | training loss: 0.172380745411\n",
      "Epoch: 455 | training loss: 0.171950608492\n",
      "Epoch: 456 | training loss: 0.171521723270\n",
      "Epoch: 457 | training loss: 0.171094149351\n",
      "Epoch: 458 | training loss: 0.170667856932\n",
      "Epoch: 459 | training loss: 0.170242801309\n",
      "Epoch: 460 | training loss: 0.169819027185\n",
      "Epoch: 461 | training loss: 0.169396474957\n",
      "Epoch: 462 | training loss: 0.168975159526\n",
      "Epoch: 463 | training loss: 0.168555125594\n",
      "Epoch: 464 | training loss: 0.168136328459\n",
      "Epoch: 465 | training loss: 0.167718783021\n",
      "Epoch: 466 | training loss: 0.167302459478\n",
      "Epoch: 467 | training loss: 0.166887387633\n",
      "Epoch: 468 | training loss: 0.166473552585\n",
      "Epoch: 469 | training loss: 0.166060939431\n",
      "Epoch: 470 | training loss: 0.165649503469\n",
      "Epoch: 471 | training loss: 0.165239319205\n",
      "Epoch: 472 | training loss: 0.164830341935\n",
      "Epoch: 473 | training loss: 0.164422556758\n",
      "Epoch: 474 | training loss: 0.164015993476\n",
      "Epoch: 475 | training loss: 0.163610607386\n",
      "Epoch: 476 | training loss: 0.163206443191\n",
      "Epoch: 477 | training loss: 0.162803441286\n",
      "Epoch: 478 | training loss: 0.162401631474\n",
      "Epoch: 479 | training loss: 0.162001013756\n",
      "Epoch: 480 | training loss: 0.161601558328\n",
      "Epoch: 481 | training loss: 0.161203265190\n",
      "Epoch: 482 | training loss: 0.160806179047\n",
      "Epoch: 483 | training loss: 0.160410210490\n",
      "Epoch: 484 | training loss: 0.160015448928\n",
      "Epoch: 485 | training loss: 0.159621834755\n",
      "Epoch: 486 | training loss: 0.159229353070\n",
      "Epoch: 487 | training loss: 0.158838033676\n",
      "Epoch: 488 | training loss: 0.158447861671\n",
      "Epoch: 489 | training loss: 0.158058822155\n",
      "Epoch: 490 | training loss: 0.157670944929\n",
      "Epoch: 491 | training loss: 0.157284170389\n",
      "Epoch: 492 | training loss: 0.156898558140\n",
      "Epoch: 493 | training loss: 0.156514033675\n",
      "Epoch: 494 | training loss: 0.156130641699\n",
      "Epoch: 495 | training loss: 0.155748367310\n",
      "Epoch: 496 | training loss: 0.155367210507\n",
      "Epoch: 497 | training loss: 0.154987186193\n",
      "Epoch: 498 | training loss: 0.154608249664\n",
      "Epoch: 499 | training loss: 0.154230430722\n",
      "Epoch: 500 | training loss: 0.153853699565\n",
      "Epoch: 501 | training loss: 0.153478100896\n",
      "Epoch: 502 | training loss: 0.153103590012\n",
      "Epoch: 503 | training loss: 0.152730166912\n",
      "Epoch: 504 | training loss: 0.152357816696\n",
      "Epoch: 505 | training loss: 0.151986584067\n",
      "Epoch: 506 | training loss: 0.151616469026\n",
      "Epoch: 507 | training loss: 0.151247426867\n",
      "Epoch: 508 | training loss: 0.150879442692\n",
      "Epoch: 509 | training loss: 0.150512561202\n",
      "Epoch: 510 | training loss: 0.150146752596\n",
      "Epoch: 511 | training loss: 0.149781957269\n",
      "Epoch: 512 | training loss: 0.149418294430\n",
      "Epoch: 513 | training loss: 0.149055659771\n",
      "Epoch: 514 | training loss: 0.148694068193\n",
      "Epoch: 515 | training loss: 0.148333564401\n",
      "Epoch: 516 | training loss: 0.147974133492\n",
      "Epoch: 517 | training loss: 0.147615715861\n",
      "Epoch: 518 | training loss: 0.147258356214\n",
      "Epoch: 519 | training loss: 0.146902054548\n",
      "Epoch: 520 | training loss: 0.146546825767\n",
      "Epoch: 521 | training loss: 0.146192640066\n",
      "Epoch: 522 | training loss: 0.145839482546\n",
      "Epoch: 523 | training loss: 0.145487338305\n",
      "Epoch: 524 | training loss: 0.145136266947\n",
      "Epoch: 525 | training loss: 0.144786193967\n",
      "Epoch: 526 | training loss: 0.144437134266\n",
      "Epoch: 527 | training loss: 0.144089117646\n",
      "Epoch: 528 | training loss: 0.143742144108\n",
      "Epoch: 529 | training loss: 0.143396213651\n",
      "Epoch: 530 | training loss: 0.143051281571\n",
      "Epoch: 531 | training loss: 0.142707377672\n",
      "Epoch: 532 | training loss: 0.142364472151\n",
      "Epoch: 533 | training loss: 0.142022579908\n",
      "Epoch: 534 | training loss: 0.141681671143\n",
      "Epoch: 535 | training loss: 0.141341820359\n",
      "Epoch: 536 | training loss: 0.141002938151\n",
      "Epoch: 537 | training loss: 0.140665084124\n",
      "Epoch: 538 | training loss: 0.140328258276\n",
      "Epoch: 539 | training loss: 0.139992386103\n",
      "Epoch: 540 | training loss: 0.139657527208\n",
      "Epoch: 541 | training loss: 0.139323651791\n",
      "Epoch: 542 | training loss: 0.138990759850\n",
      "Epoch: 543 | training loss: 0.138658866286\n",
      "Epoch: 544 | training loss: 0.138328000903\n",
      "Epoch: 545 | training loss: 0.137998118997\n",
      "Epoch: 546 | training loss: 0.137669190764\n",
      "Epoch: 547 | training loss: 0.137341260910\n",
      "Epoch: 548 | training loss: 0.137014299631\n",
      "Epoch: 549 | training loss: 0.136688292027\n",
      "Epoch: 550 | training loss: 0.136363297701\n",
      "Epoch: 551 | training loss: 0.136039271951\n",
      "Epoch: 552 | training loss: 0.135716244578\n",
      "Epoch: 553 | training loss: 0.135394155979\n",
      "Epoch: 554 | training loss: 0.135073021054\n",
      "Epoch: 555 | training loss: 0.134752854705\n",
      "Epoch: 556 | training loss: 0.134433671832\n",
      "Epoch: 557 | training loss: 0.134115472436\n",
      "Epoch: 558 | training loss: 0.133798196912\n",
      "Epoch: 559 | training loss: 0.133481889963\n",
      "Epoch: 560 | training loss: 0.133166536689\n",
      "Epoch: 561 | training loss: 0.132852151990\n",
      "Epoch: 562 | training loss: 0.132538720965\n",
      "Epoch: 563 | training loss: 0.132226258516\n",
      "Epoch: 564 | training loss: 0.131914719939\n",
      "Epoch: 565 | training loss: 0.131604120135\n",
      "Epoch: 566 | training loss: 0.131294488907\n",
      "Epoch: 567 | training loss: 0.130985796452\n",
      "Epoch: 568 | training loss: 0.130678057671\n",
      "Epoch: 569 | training loss: 0.130371242762\n",
      "Epoch: 570 | training loss: 0.130065366626\n",
      "Epoch: 571 | training loss: 0.129760459065\n",
      "Epoch: 572 | training loss: 0.129456490278\n",
      "Epoch: 573 | training loss: 0.129153445363\n",
      "Epoch: 574 | training loss: 0.128851309419\n",
      "Epoch: 575 | training loss: 0.128550127149\n",
      "Epoch: 576 | training loss: 0.128249883652\n",
      "Epoch: 577 | training loss: 0.127950549126\n",
      "Epoch: 578 | training loss: 0.127652153373\n",
      "Epoch: 579 | training loss: 0.127354681492\n",
      "Epoch: 580 | training loss: 0.127058148384\n",
      "Epoch: 581 | training loss: 0.126762524247\n",
      "Epoch: 582 | training loss: 0.126467823982\n",
      "Epoch: 583 | training loss: 0.126174032688\n",
      "Epoch: 584 | training loss: 0.125881195068\n",
      "Epoch: 585 | training loss: 0.125589251518\n",
      "Epoch: 586 | training loss: 0.125298216939\n",
      "Epoch: 587 | training loss: 0.125008106232\n",
      "Epoch: 588 | training loss: 0.124718904495\n",
      "Epoch: 589 | training loss: 0.124430619180\n",
      "Epoch: 590 | training loss: 0.124143227935\n",
      "Epoch: 591 | training loss: 0.123856745660\n",
      "Epoch: 592 | training loss: 0.123571194708\n",
      "Epoch: 593 | training loss: 0.123286522925\n",
      "Epoch: 594 | training loss: 0.123002767563\n",
      "Epoch: 595 | training loss: 0.122719898820\n",
      "Epoch: 596 | training loss: 0.122437939048\n",
      "Epoch: 597 | training loss: 0.122156880796\n",
      "Epoch: 598 | training loss: 0.121876716614\n",
      "Epoch: 599 | training loss: 0.121597453952\n",
      "Epoch: 600 | training loss: 0.121319100261\n",
      "Epoch: 601 | training loss: 0.121041640639\n",
      "Epoch: 602 | training loss: 0.120765037835\n",
      "Epoch: 603 | training loss: 0.120489329100\n",
      "Epoch: 604 | training loss: 0.120214544237\n",
      "Epoch: 605 | training loss: 0.119940608740\n",
      "Epoch: 606 | training loss: 0.119667589664\n",
      "Epoch: 607 | training loss: 0.119395464659\n",
      "Epoch: 608 | training loss: 0.119124203920\n",
      "Epoch: 609 | training loss: 0.118853829801\n",
      "Epoch: 610 | training loss: 0.118584327400\n",
      "Epoch: 611 | training loss: 0.118315711617\n",
      "Epoch: 612 | training loss: 0.118047982454\n",
      "Epoch: 613 | training loss: 0.117781095207\n",
      "Epoch: 614 | training loss: 0.117515146732\n",
      "Epoch: 615 | training loss: 0.117250047624\n",
      "Epoch: 616 | training loss: 0.116985827684\n",
      "Epoch: 617 | training loss: 0.116722442210\n",
      "Epoch: 618 | training loss: 0.116459958255\n",
      "Epoch: 619 | training loss: 0.116198353469\n",
      "Epoch: 620 | training loss: 0.115937583148\n",
      "Epoch: 621 | training loss: 0.115677706897\n",
      "Epoch: 622 | training loss: 0.115418687463\n",
      "Epoch: 623 | training loss: 0.115160539746\n",
      "Epoch: 624 | training loss: 0.114903271198\n",
      "Epoch: 625 | training loss: 0.114646852016\n",
      "Epoch: 626 | training loss: 0.114391274750\n",
      "Epoch: 627 | training loss: 0.114136584103\n",
      "Epoch: 628 | training loss: 0.113882735372\n",
      "Epoch: 629 | training loss: 0.113629750907\n",
      "Epoch: 630 | training loss: 0.113377623260\n",
      "Epoch: 631 | training loss: 0.113126344979\n",
      "Epoch: 632 | training loss: 0.112875908613\n",
      "Epoch: 633 | training loss: 0.112626314163\n",
      "Epoch: 634 | training loss: 0.112377591431\n",
      "Epoch: 635 | training loss: 0.112129703164\n",
      "Epoch: 636 | training loss: 0.111882686615\n",
      "Epoch: 637 | training loss: 0.111636519432\n",
      "Epoch: 638 | training loss: 0.111391164362\n",
      "Epoch: 639 | training loss: 0.111146651208\n",
      "Epoch: 640 | training loss: 0.110903017223\n",
      "Epoch: 641 | training loss: 0.110660210252\n",
      "Epoch: 642 | training loss: 0.110418230295\n",
      "Epoch: 643 | training loss: 0.110177092254\n",
      "Epoch: 644 | training loss: 0.109936796129\n",
      "Epoch: 645 | training loss: 0.109697327018\n",
      "Epoch: 646 | training loss: 0.109458707273\n",
      "Epoch: 647 | training loss: 0.109220921993\n",
      "Epoch: 648 | training loss: 0.108983948827\n",
      "Epoch: 649 | training loss: 0.108747817576\n",
      "Epoch: 650 | training loss: 0.108512513340\n",
      "Epoch: 651 | training loss: 0.108278021216\n",
      "Epoch: 652 | training loss: 0.108044371009\n",
      "Epoch: 653 | training loss: 0.107811532915\n",
      "Epoch: 654 | training loss: 0.107579514384\n",
      "Epoch: 655 | training loss: 0.107348337770\n",
      "Epoch: 656 | training loss: 0.107117980719\n",
      "Epoch: 657 | training loss: 0.106888435781\n",
      "Epoch: 658 | training loss: 0.106659665704\n",
      "Epoch: 659 | training loss: 0.106431744993\n",
      "Epoch: 660 | training loss: 0.106204643846\n",
      "Epoch: 661 | training loss: 0.105978317559\n",
      "Epoch: 662 | training loss: 0.105752848089\n",
      "Epoch: 663 | training loss: 0.105528160930\n",
      "Epoch: 664 | training loss: 0.105304285884\n",
      "Epoch: 665 | training loss: 0.105081215501\n",
      "Epoch: 666 | training loss: 0.104858964682\n",
      "Epoch: 667 | training loss: 0.104637496173\n",
      "Epoch: 668 | training loss: 0.104416832328\n",
      "Epoch: 669 | training loss: 0.104196965694\n",
      "Epoch: 670 | training loss: 0.103977903724\n",
      "Epoch: 671 | training loss: 0.103759646416\n",
      "Epoch: 672 | training loss: 0.103542186320\n",
      "Epoch: 673 | training loss: 0.103325508535\n",
      "Epoch: 674 | training loss: 0.103109650314\n",
      "Epoch: 675 | training loss: 0.102894559503\n",
      "Epoch: 676 | training loss: 0.102680258453\n",
      "Epoch: 677 | training loss: 0.102466747165\n",
      "Epoch: 678 | training loss: 0.102254025638\n",
      "Epoch: 679 | training loss: 0.102042086422\n",
      "Epoch: 680 | training loss: 0.101830944419\n",
      "Epoch: 681 | training loss: 0.101620554924\n",
      "Epoch: 682 | training loss: 0.101410940289\n",
      "Epoch: 683 | training loss: 0.101202145219\n",
      "Epoch: 684 | training loss: 0.100994095206\n",
      "Epoch: 685 | training loss: 0.100786820054\n",
      "Epoch: 686 | training loss: 0.100580319762\n",
      "Epoch: 687 | training loss: 0.100374609232\n",
      "Epoch: 688 | training loss: 0.100169628859\n",
      "Epoch: 689 | training loss: 0.099965438247\n",
      "Epoch: 690 | training loss: 0.099762015045\n",
      "Epoch: 691 | training loss: 0.099559344351\n",
      "Epoch: 692 | training loss: 0.099357455969\n",
      "Epoch: 693 | training loss: 0.099156320095\n",
      "Epoch: 694 | training loss: 0.098955914378\n",
      "Epoch: 695 | training loss: 0.098756268620\n",
      "Epoch: 696 | training loss: 0.098557405174\n",
      "Epoch: 697 | training loss: 0.098359271884\n",
      "Epoch: 698 | training loss: 0.098161883652\n",
      "Epoch: 699 | training loss: 0.097965255380\n",
      "Epoch: 700 | training loss: 0.097769394517\n",
      "Epoch: 701 | training loss: 0.097574248910\n",
      "Epoch: 702 | training loss: 0.097379878163\n",
      "Epoch: 703 | training loss: 0.097186207771\n",
      "Epoch: 704 | training loss: 0.096993289888\n",
      "Epoch: 705 | training loss: 0.096801131964\n",
      "Epoch: 706 | training loss: 0.096609681845\n",
      "Epoch: 707 | training loss: 0.096418976784\n",
      "Epoch: 708 | training loss: 0.096228979528\n",
      "Epoch: 709 | training loss: 0.096039749682\n",
      "Epoch: 710 | training loss: 0.095851205289\n",
      "Epoch: 711 | training loss: 0.095663398504\n",
      "Epoch: 712 | training loss: 0.095476351678\n",
      "Epoch: 713 | training loss: 0.095289997756\n",
      "Epoch: 714 | training loss: 0.095104336739\n",
      "Epoch: 715 | training loss: 0.094919413328\n",
      "Epoch: 716 | training loss: 0.094735212624\n",
      "Epoch: 717 | training loss: 0.094551712275\n",
      "Epoch: 718 | training loss: 0.094368912280\n",
      "Epoch: 719 | training loss: 0.094186812639\n",
      "Epoch: 720 | training loss: 0.094005435705\n",
      "Epoch: 721 | training loss: 0.093824774027\n",
      "Epoch: 722 | training loss: 0.093644797802\n",
      "Epoch: 723 | training loss: 0.093465521932\n",
      "Epoch: 724 | training loss: 0.093286946416\n",
      "Epoch: 725 | training loss: 0.093109056354\n",
      "Epoch: 726 | training loss: 0.092931866646\n",
      "Epoch: 727 | training loss: 0.092755384743\n",
      "Epoch: 728 | training loss: 0.092579551041\n",
      "Epoch: 729 | training loss: 0.092404417694\n",
      "Epoch: 730 | training loss: 0.092229962349\n",
      "Epoch: 731 | training loss: 0.092056214809\n",
      "Epoch: 732 | training loss: 0.091883145273\n",
      "Epoch: 733 | training loss: 0.091710731387\n",
      "Epoch: 734 | training loss: 0.091538965702\n",
      "Epoch: 735 | training loss: 0.091367907822\n",
      "Epoch: 736 | training loss: 0.091197520494\n",
      "Epoch: 737 | training loss: 0.091027781367\n",
      "Epoch: 738 | training loss: 0.090858697891\n",
      "Epoch: 739 | training loss: 0.090690284967\n",
      "Epoch: 740 | training loss: 0.090522542596\n",
      "Epoch: 741 | training loss: 0.090355463326\n",
      "Epoch: 742 | training loss: 0.090189024806\n",
      "Epoch: 743 | training loss: 0.090023219585\n",
      "Epoch: 744 | training loss: 0.089858084917\n",
      "Epoch: 745 | training loss: 0.089693598449\n",
      "Epoch: 746 | training loss: 0.089529737830\n",
      "Epoch: 747 | training loss: 0.089366540313\n",
      "Epoch: 748 | training loss: 0.089203983545\n",
      "Epoch: 749 | training loss: 0.089042045176\n",
      "Epoch: 750 | training loss: 0.088880769908\n",
      "Epoch: 751 | training loss: 0.088720105588\n",
      "Epoch: 752 | training loss: 0.088560074568\n",
      "Epoch: 753 | training loss: 0.088400661945\n",
      "Epoch: 754 | training loss: 0.088241882622\n",
      "Epoch: 755 | training loss: 0.088083699346\n",
      "Epoch: 756 | training loss: 0.087926179171\n",
      "Epoch: 757 | training loss: 0.087769255042\n",
      "Epoch: 758 | training loss: 0.087612934411\n",
      "Epoch: 759 | training loss: 0.087457239628\n",
      "Epoch: 760 | training loss: 0.087302148342\n",
      "Epoch: 761 | training loss: 0.087147653103\n",
      "Epoch: 762 | training loss: 0.086993768811\n",
      "Epoch: 763 | training loss: 0.086840488017\n",
      "Epoch: 764 | training loss: 0.086687810719\n",
      "Epoch: 765 | training loss: 0.086535707116\n",
      "Epoch: 766 | training loss: 0.086384207010\n",
      "Epoch: 767 | training loss: 0.086233302951\n",
      "Epoch: 768 | training loss: 0.086082965136\n",
      "Epoch: 769 | training loss: 0.085933223367\n",
      "Epoch: 770 | training loss: 0.085784055293\n",
      "Epoch: 771 | training loss: 0.085635490716\n",
      "Epoch: 772 | training loss: 0.085487499833\n",
      "Epoch: 773 | training loss: 0.085340082645\n",
      "Epoch: 774 | training loss: 0.085193239152\n",
      "Epoch: 775 | training loss: 0.085046954453\n",
      "Epoch: 776 | training loss: 0.084901235998\n",
      "Epoch: 777 | training loss: 0.084756083786\n",
      "Epoch: 778 | training loss: 0.084611482918\n",
      "Epoch: 779 | training loss: 0.084467425942\n",
      "Epoch: 780 | training loss: 0.084323935211\n",
      "Epoch: 781 | training loss: 0.084180995822\n",
      "Epoch: 782 | training loss: 0.084038607776\n",
      "Epoch: 783 | training loss: 0.083896756172\n",
      "Epoch: 784 | training loss: 0.083755470812\n",
      "Epoch: 785 | training loss: 0.083614729345\n",
      "Epoch: 786 | training loss: 0.083474509418\n",
      "Epoch: 787 | training loss: 0.083334848285\n",
      "Epoch: 788 | training loss: 0.083195701241\n",
      "Epoch: 789 | training loss: 0.083057083189\n",
      "Epoch: 790 | training loss: 0.082919001579\n",
      "Epoch: 791 | training loss: 0.082781441510\n",
      "Epoch: 792 | training loss: 0.082644410431\n",
      "Epoch: 793 | training loss: 0.082507878542\n",
      "Epoch: 794 | training loss: 0.082371875644\n",
      "Epoch: 795 | training loss: 0.082236386836\n",
      "Epoch: 796 | training loss: 0.082101419568\n",
      "Epoch: 797 | training loss: 0.081966944039\n",
      "Epoch: 798 | training loss: 0.081832967699\n",
      "Epoch: 799 | training loss: 0.081699497998\n",
      "Epoch: 800 | training loss: 0.081566534936\n",
      "Epoch: 801 | training loss: 0.081434063613\n",
      "Epoch: 802 | training loss: 0.081302061677\n",
      "Epoch: 803 | training loss: 0.081170566380\n",
      "Epoch: 804 | training loss: 0.081039547920\n",
      "Epoch: 805 | training loss: 0.080909021199\n",
      "Epoch: 806 | training loss: 0.080778971314\n",
      "Epoch: 807 | training loss: 0.080649413168\n",
      "Epoch: 808 | training loss: 0.080520316958\n",
      "Epoch: 809 | training loss: 0.080391697586\n",
      "Epoch: 810 | training loss: 0.080263569951\n",
      "Epoch: 811 | training loss: 0.080135904253\n",
      "Epoch: 812 | training loss: 0.080008707941\n",
      "Epoch: 813 | training loss: 0.079881966114\n",
      "Epoch: 814 | training loss: 0.079755678773\n",
      "Epoch: 815 | training loss: 0.079629838467\n",
      "Epoch: 816 | training loss: 0.079504467547\n",
      "Epoch: 817 | training loss: 0.079379558563\n",
      "Epoch: 818 | training loss: 0.079255089164\n",
      "Epoch: 819 | training loss: 0.079131066799\n",
      "Epoch: 820 | training loss: 0.079007491469\n",
      "Epoch: 821 | training loss: 0.078884340823\n",
      "Epoch: 822 | training loss: 0.078761629760\n",
      "Epoch: 823 | training loss: 0.078639365733\n",
      "Epoch: 824 | training loss: 0.078517526388\n",
      "Epoch: 825 | training loss: 0.078396141529\n",
      "Epoch: 826 | training loss: 0.078275166452\n",
      "Epoch: 827 | training loss: 0.078154601157\n",
      "Epoch: 828 | training loss: 0.078034445643\n",
      "Epoch: 829 | training loss: 0.077914744616\n",
      "Epoch: 830 | training loss: 0.077795445919\n",
      "Epoch: 831 | training loss: 0.077676571906\n",
      "Epoch: 832 | training loss: 0.077558092773\n",
      "Epoch: 833 | training loss: 0.077440023422\n",
      "Epoch: 834 | training loss: 0.077322363853\n",
      "Epoch: 835 | training loss: 0.077205106616\n",
      "Epoch: 836 | training loss: 0.077088251710\n",
      "Epoch: 837 | training loss: 0.076971799135\n",
      "Epoch: 838 | training loss: 0.076855719090\n",
      "Epoch: 839 | training loss: 0.076740056276\n",
      "Epoch: 840 | training loss: 0.076624780893\n",
      "Epoch: 841 | training loss: 0.076509878039\n",
      "Epoch: 842 | training loss: 0.076395377517\n",
      "Epoch: 843 | training loss: 0.076281242073\n",
      "Epoch: 844 | training loss: 0.076167486608\n",
      "Epoch: 845 | training loss: 0.076054126024\n",
      "Epoch: 846 | training loss: 0.075941123068\n",
      "Epoch: 847 | training loss: 0.075828492641\n",
      "Epoch: 848 | training loss: 0.075716227293\n",
      "Epoch: 849 | training loss: 0.075604341924\n",
      "Epoch: 850 | training loss: 0.075492821634\n",
      "Epoch: 851 | training loss: 0.075381644070\n",
      "Epoch: 852 | training loss: 0.075270846486\n",
      "Epoch: 853 | training loss: 0.075160384178\n",
      "Epoch: 854 | training loss: 0.075050279498\n",
      "Epoch: 855 | training loss: 0.074940554798\n",
      "Epoch: 856 | training loss: 0.074831150472\n",
      "Epoch: 857 | training loss: 0.074722096324\n",
      "Epoch: 858 | training loss: 0.074613392353\n",
      "Epoch: 859 | training loss: 0.074505038559\n",
      "Epoch: 860 | training loss: 0.074397005141\n",
      "Epoch: 861 | training loss: 0.074289306998\n",
      "Epoch: 862 | training loss: 0.074181951582\n",
      "Epoch: 863 | training loss: 0.074074923992\n",
      "Epoch: 864 | training loss: 0.073968231678\n",
      "Epoch: 865 | training loss: 0.073861859739\n",
      "Epoch: 866 | training loss: 0.073755830526\n",
      "Epoch: 867 | training loss: 0.073650091887\n",
      "Epoch: 868 | training loss: 0.073544688523\n",
      "Epoch: 869 | training loss: 0.073439598083\n",
      "Epoch: 870 | training loss: 0.073334850371\n",
      "Epoch: 871 | training loss: 0.073230393231\n",
      "Epoch: 872 | training loss: 0.073126249015\n",
      "Epoch: 873 | training loss: 0.073022417724\n",
      "Epoch: 874 | training loss: 0.072918884456\n",
      "Epoch: 875 | training loss: 0.072815649211\n",
      "Epoch: 876 | training loss: 0.072712726891\n",
      "Epoch: 877 | training loss: 0.072610102594\n",
      "Epoch: 878 | training loss: 0.072507783771\n",
      "Epoch: 879 | training loss: 0.072405748069\n",
      "Epoch: 880 | training loss: 0.072304017842\n",
      "Epoch: 881 | training loss: 0.072202555835\n",
      "Epoch: 882 | training loss: 0.072101384401\n",
      "Epoch: 883 | training loss: 0.072000503540\n",
      "Epoch: 884 | training loss: 0.071899905801\n",
      "Epoch: 885 | training loss: 0.071799576283\n",
      "Epoch: 886 | training loss: 0.071699544787\n",
      "Epoch: 887 | training loss: 0.071599796414\n",
      "Epoch: 888 | training loss: 0.071500301361\n",
      "Epoch: 889 | training loss: 0.071401096880\n",
      "Epoch: 890 | training loss: 0.071302160621\n",
      "Epoch: 891 | training loss: 0.071203485131\n",
      "Epoch: 892 | training loss: 0.071105077863\n",
      "Epoch: 893 | training loss: 0.071006931365\n",
      "Epoch: 894 | training loss: 0.070909067988\n",
      "Epoch: 895 | training loss: 0.070811450481\n",
      "Epoch: 896 | training loss: 0.070714078844\n",
      "Epoch: 897 | training loss: 0.070616990328\n",
      "Epoch: 898 | training loss: 0.070520147681\n",
      "Epoch: 899 | training loss: 0.070423543453\n",
      "Epoch: 900 | training loss: 0.070327199996\n",
      "Epoch: 901 | training loss: 0.070231094956\n",
      "Epoch: 902 | training loss: 0.070135250688\n",
      "Epoch: 903 | training loss: 0.070039652288\n",
      "Epoch: 904 | training loss: 0.069944292307\n",
      "Epoch: 905 | training loss: 0.069849163294\n",
      "Epoch: 906 | training loss: 0.069754302502\n",
      "Epoch: 907 | training loss: 0.069659650326\n",
      "Epoch: 908 | training loss: 0.069565244019\n",
      "Epoch: 909 | training loss: 0.069471068680\n",
      "Epoch: 910 | training loss: 0.069377131760\n",
      "Epoch: 911 | training loss: 0.069283410907\n",
      "Epoch: 912 | training loss: 0.069189921021\n",
      "Epoch: 913 | training loss: 0.069096662104\n",
      "Epoch: 914 | training loss: 0.069003619254\n",
      "Epoch: 915 | training loss: 0.068910814822\n",
      "Epoch: 916 | training loss: 0.068818219006\n",
      "Epoch: 917 | training loss: 0.068725846708\n",
      "Epoch: 918 | training loss: 0.068633690476\n",
      "Epoch: 919 | training loss: 0.068541750312\n",
      "Epoch: 920 | training loss: 0.068450018764\n",
      "Epoch: 921 | training loss: 0.068358510733\n",
      "Epoch: 922 | training loss: 0.068267218769\n",
      "Epoch: 923 | training loss: 0.068176120520\n",
      "Epoch: 924 | training loss: 0.068085223436\n",
      "Epoch: 925 | training loss: 0.067994542420\n",
      "Epoch: 926 | training loss: 0.067904047668\n",
      "Epoch: 927 | training loss: 0.067813776433\n",
      "Epoch: 928 | training loss: 0.067723706365\n",
      "Epoch: 929 | training loss: 0.067633822560\n",
      "Epoch: 930 | training loss: 0.067544139922\n",
      "Epoch: 931 | training loss: 0.067454643548\n",
      "Epoch: 932 | training loss: 0.067365340889\n",
      "Epoch: 933 | training loss: 0.067276261747\n",
      "Epoch: 934 | training loss: 0.067187353969\n",
      "Epoch: 935 | training loss: 0.067098632455\n",
      "Epoch: 936 | training loss: 0.067010104656\n",
      "Epoch: 937 | training loss: 0.066921763122\n",
      "Epoch: 938 | training loss: 0.066833607852\n",
      "Epoch: 939 | training loss: 0.066745623946\n",
      "Epoch: 940 | training loss: 0.066657818854\n",
      "Epoch: 941 | training loss: 0.066570200026\n",
      "Epoch: 942 | training loss: 0.066482774913\n",
      "Epoch: 943 | training loss: 0.066395521164\n",
      "Epoch: 944 | training loss: 0.066308438778\n",
      "Epoch: 945 | training loss: 0.066221550107\n",
      "Epoch: 946 | training loss: 0.066134825349\n",
      "Epoch: 947 | training loss: 0.066048271954\n",
      "Epoch: 948 | training loss: 0.065961882472\n",
      "Epoch: 949 | training loss: 0.065875671804\n",
      "Epoch: 950 | training loss: 0.065789632499\n",
      "Epoch: 951 | training loss: 0.065703749657\n",
      "Epoch: 952 | training loss: 0.065618030727\n",
      "Epoch: 953 | training loss: 0.065532475710\n",
      "Epoch: 954 | training loss: 0.065447092056\n",
      "Epoch: 955 | training loss: 0.065361864865\n",
      "Epoch: 956 | training loss: 0.065276809037\n",
      "Epoch: 957 | training loss: 0.065191902220\n",
      "Epoch: 958 | training loss: 0.065107159317\n",
      "Epoch: 959 | training loss: 0.065022580326\n",
      "Epoch: 960 | training loss: 0.064938142896\n",
      "Epoch: 961 | training loss: 0.064853861928\n",
      "Epoch: 962 | training loss: 0.064769744873\n",
      "Epoch: 963 | training loss: 0.064685776830\n",
      "Epoch: 964 | training loss: 0.064601965249\n",
      "Epoch: 965 | training loss: 0.064518295228\n",
      "Epoch: 966 | training loss: 0.064434789121\n",
      "Epoch: 967 | training loss: 0.064351424575\n",
      "Epoch: 968 | training loss: 0.064268201590\n",
      "Epoch: 969 | training loss: 0.064185105264\n",
      "Epoch: 970 | training loss: 0.064102172852\n",
      "Epoch: 971 | training loss: 0.064019367099\n",
      "Epoch: 972 | training loss: 0.063936710358\n",
      "Epoch: 973 | training loss: 0.063854217529\n",
      "Epoch: 974 | training loss: 0.063771843910\n",
      "Epoch: 975 | training loss: 0.063689619303\n",
      "Epoch: 976 | training loss: 0.063607536256\n",
      "Epoch: 977 | training loss: 0.063525587320\n",
      "Epoch: 978 | training loss: 0.063443772495\n",
      "Epoch: 979 | training loss: 0.063362091780\n",
      "Epoch: 980 | training loss: 0.063280545175\n",
      "Epoch: 981 | training loss: 0.063199125230\n",
      "Epoch: 982 | training loss: 0.063117846847\n",
      "Epoch: 983 | training loss: 0.063036695123\n",
      "Epoch: 984 | training loss: 0.062955692410\n",
      "Epoch: 985 | training loss: 0.062874808908\n",
      "Epoch: 986 | training loss: 0.062794044614\n",
      "Epoch: 987 | training loss: 0.062713414431\n",
      "Epoch: 988 | training loss: 0.062632910907\n",
      "Epoch: 989 | training loss: 0.062552534044\n",
      "Epoch: 990 | training loss: 0.062472295016\n",
      "Epoch: 991 | training loss: 0.062392182648\n",
      "Epoch: 992 | training loss: 0.062312170863\n",
      "Epoch: 993 | training loss: 0.062232296914\n",
      "Epoch: 994 | training loss: 0.062152534723\n",
      "Epoch: 995 | training loss: 0.062072906643\n",
      "Epoch: 996 | training loss: 0.061993401498\n",
      "Epoch: 997 | training loss: 0.061913996935\n",
      "Epoch: 998 | training loss: 0.061834726483\n",
      "Epoch: 999 | training loss: 0.061755575240\n",
      "Epoch: 1000 | training loss: 0.061676543206\n",
      "Epoch: 1001 | training loss: 0.061597622931\n",
      "Epoch: 1002 | training loss: 0.061518810689\n",
      "Epoch: 1003 | training loss: 0.061440125108\n",
      "Epoch: 1004 | training loss: 0.061361558735\n",
      "Epoch: 1005 | training loss: 0.061283111572\n",
      "Epoch: 1006 | training loss: 0.061204757541\n",
      "Epoch: 1007 | training loss: 0.061126530170\n",
      "Epoch: 1008 | training loss: 0.061048418283\n",
      "Epoch: 1009 | training loss: 0.060970392078\n",
      "Epoch: 1010 | training loss: 0.060892485082\n",
      "Epoch: 1011 | training loss: 0.060814693570\n",
      "Epoch: 1012 | training loss: 0.060737021267\n",
      "Epoch: 1013 | training loss: 0.060659438372\n",
      "Epoch: 1014 | training loss: 0.060581974685\n",
      "Epoch: 1015 | training loss: 0.060504619032\n",
      "Epoch: 1016 | training loss: 0.060427352786\n",
      "Epoch: 1017 | training loss: 0.060350202024\n",
      "Epoch: 1018 | training loss: 0.060273170471\n",
      "Epoch: 1019 | training loss: 0.060196243227\n",
      "Epoch: 1020 | training loss: 0.060119405389\n",
      "Epoch: 1021 | training loss: 0.060042671859\n",
      "Epoch: 1022 | training loss: 0.059966057539\n",
      "Epoch: 1023 | training loss: 0.059889521450\n",
      "Epoch: 1024 | training loss: 0.059813104570\n",
      "Epoch: 1025 | training loss: 0.059736784548\n",
      "Epoch: 1026 | training loss: 0.059660568833\n",
      "Epoch: 1027 | training loss: 0.059584453702\n",
      "Epoch: 1028 | training loss: 0.059508439153\n",
      "Epoch: 1029 | training loss: 0.059432521462\n",
      "Epoch: 1030 | training loss: 0.059356696904\n",
      "Epoch: 1031 | training loss: 0.059280980378\n",
      "Epoch: 1032 | training loss: 0.059205360711\n",
      "Epoch: 1033 | training loss: 0.059129822999\n",
      "Epoch: 1034 | training loss: 0.059054404497\n",
      "Epoch: 1035 | training loss: 0.058979064226\n",
      "Epoch: 1036 | training loss: 0.058903839439\n",
      "Epoch: 1037 | training loss: 0.058828685433\n",
      "Epoch: 1038 | training loss: 0.058753632009\n",
      "Epoch: 1039 | training loss: 0.058678686619\n",
      "Epoch: 1040 | training loss: 0.058603826910\n",
      "Epoch: 1041 | training loss: 0.058529064059\n",
      "Epoch: 1042 | training loss: 0.058454405516\n",
      "Epoch: 1043 | training loss: 0.058379821479\n",
      "Epoch: 1044 | training loss: 0.058305330575\n",
      "Epoch: 1045 | training loss: 0.058230936527\n",
      "Epoch: 1046 | training loss: 0.058156628162\n",
      "Epoch: 1047 | training loss: 0.058082427830\n",
      "Epoch: 1048 | training loss: 0.058008294553\n",
      "Epoch: 1049 | training loss: 0.057934273034\n",
      "Epoch: 1050 | training loss: 0.057860329747\n",
      "Epoch: 1051 | training loss: 0.057786475867\n",
      "Epoch: 1052 | training loss: 0.057712715119\n",
      "Epoch: 1053 | training loss: 0.057639040053\n",
      "Epoch: 1054 | training loss: 0.057565454394\n",
      "Epoch: 1055 | training loss: 0.057491950691\n",
      "Epoch: 1056 | training loss: 0.057418547571\n",
      "Epoch: 1057 | training loss: 0.057345211506\n",
      "Epoch: 1058 | training loss: 0.057271987200\n",
      "Epoch: 1059 | training loss: 0.057198826224\n",
      "Epoch: 1060 | training loss: 0.057125758380\n",
      "Epoch: 1061 | training loss: 0.057052794844\n",
      "Epoch: 1062 | training loss: 0.056979898363\n",
      "Epoch: 1063 | training loss: 0.056907091290\n",
      "Epoch: 1064 | training loss: 0.056834369898\n",
      "Epoch: 1065 | training loss: 0.056761741638\n",
      "Epoch: 1066 | training loss: 0.056689169258\n",
      "Epoch: 1067 | training loss: 0.056616712362\n",
      "Epoch: 1068 | training loss: 0.056544311345\n",
      "Epoch: 1069 | training loss: 0.056472018361\n",
      "Epoch: 1070 | training loss: 0.056399788707\n",
      "Epoch: 1071 | training loss: 0.056327652186\n",
      "Epoch: 1072 | training loss: 0.056255586445\n",
      "Epoch: 1073 | training loss: 0.056183613837\n",
      "Epoch: 1074 | training loss: 0.056111730635\n",
      "Epoch: 1075 | training loss: 0.056039925665\n",
      "Epoch: 1076 | training loss: 0.055968195200\n",
      "Epoch: 1077 | training loss: 0.055896546692\n",
      "Epoch: 1078 | training loss: 0.055824987590\n",
      "Epoch: 1079 | training loss: 0.055753495544\n",
      "Epoch: 1080 | training loss: 0.055682100356\n",
      "Epoch: 1081 | training loss: 0.055610768497\n",
      "Epoch: 1082 | training loss: 0.055539529771\n",
      "Epoch: 1083 | training loss: 0.055468358099\n",
      "Epoch: 1084 | training loss: 0.055397279561\n",
      "Epoch: 1085 | training loss: 0.055326264352\n",
      "Epoch: 1086 | training loss: 0.055255338550\n",
      "Epoch: 1087 | training loss: 0.055184490979\n",
      "Epoch: 1088 | training loss: 0.055113725364\n",
      "Epoch: 1089 | training loss: 0.055043026805\n",
      "Epoch: 1090 | training loss: 0.054972410202\n",
      "Epoch: 1091 | training loss: 0.054901871830\n",
      "Epoch: 1092 | training loss: 0.054831407964\n",
      "Epoch: 1093 | training loss: 0.054761022329\n",
      "Epoch: 1094 | training loss: 0.054690711200\n",
      "Epoch: 1095 | training loss: 0.054620478302\n",
      "Epoch: 1096 | training loss: 0.054550319910\n",
      "Epoch: 1097 | training loss: 0.054480236024\n",
      "Epoch: 1098 | training loss: 0.054410215467\n",
      "Epoch: 1099 | training loss: 0.054340295494\n",
      "Epoch: 1100 | training loss: 0.054270438850\n",
      "Epoch: 1101 | training loss: 0.054200652987\n",
      "Epoch: 1102 | training loss: 0.054130949080\n",
      "Epoch: 1103 | training loss: 0.054061330855\n",
      "Epoch: 1104 | training loss: 0.053991779685\n",
      "Epoch: 1105 | training loss: 0.053922303021\n",
      "Epoch: 1106 | training loss: 0.053852900863\n",
      "Epoch: 1107 | training loss: 0.053783580661\n",
      "Epoch: 1108 | training loss: 0.053714316338\n",
      "Epoch: 1109 | training loss: 0.053645145148\n",
      "Epoch: 1110 | training loss: 0.053576041013\n",
      "Epoch: 1111 | training loss: 0.053507007658\n",
      "Epoch: 1112 | training loss: 0.053438052535\n",
      "Epoch: 1113 | training loss: 0.053369157016\n",
      "Epoch: 1114 | training loss: 0.053300358355\n",
      "Epoch: 1115 | training loss: 0.053231611848\n",
      "Epoch: 1116 | training loss: 0.053162951022\n",
      "Epoch: 1117 | training loss: 0.053094349802\n",
      "Epoch: 1118 | training loss: 0.053025837988\n",
      "Epoch: 1119 | training loss: 0.052957385778\n",
      "Epoch: 1120 | training loss: 0.052889019251\n",
      "Epoch: 1121 | training loss: 0.052820712328\n",
      "Epoch: 1122 | training loss: 0.052752468735\n",
      "Epoch: 1123 | training loss: 0.052684307098\n",
      "Epoch: 1124 | training loss: 0.052616208792\n",
      "Epoch: 1125 | training loss: 0.052548203617\n",
      "Epoch: 1126 | training loss: 0.052480246872\n",
      "Epoch: 1127 | training loss: 0.052412379533\n",
      "Epoch: 1128 | training loss: 0.052344575524\n",
      "Epoch: 1129 | training loss: 0.052276842296\n",
      "Epoch: 1130 | training loss: 0.052209176123\n",
      "Epoch: 1131 | training loss: 0.052141577005\n",
      "Epoch: 1132 | training loss: 0.052074063569\n",
      "Epoch: 1133 | training loss: 0.052006606013\n",
      "Epoch: 1134 | training loss: 0.051939222962\n",
      "Epoch: 1135 | training loss: 0.051871910691\n",
      "Epoch: 1136 | training loss: 0.051804672927\n",
      "Epoch: 1137 | training loss: 0.051737502217\n",
      "Epoch: 1138 | training loss: 0.051670398563\n",
      "Epoch: 1139 | training loss: 0.051603369415\n",
      "Epoch: 1140 | training loss: 0.051536403596\n",
      "Epoch: 1141 | training loss: 0.051469519734\n",
      "Epoch: 1142 | training loss: 0.051402695477\n",
      "Epoch: 1143 | training loss: 0.051335934550\n",
      "Epoch: 1144 | training loss: 0.051269248128\n",
      "Epoch: 1145 | training loss: 0.051202632487\n",
      "Epoch: 1146 | training loss: 0.051136095077\n",
      "Epoch: 1147 | training loss: 0.051069617271\n",
      "Epoch: 1148 | training loss: 0.051003202796\n",
      "Epoch: 1149 | training loss: 0.050936855376\n",
      "Epoch: 1150 | training loss: 0.050870578736\n",
      "Epoch: 1151 | training loss: 0.050804380327\n",
      "Epoch: 1152 | training loss: 0.050738237798\n",
      "Epoch: 1153 | training loss: 0.050672177225\n",
      "Epoch: 1154 | training loss: 0.050606176257\n",
      "Epoch: 1155 | training loss: 0.050540249795\n",
      "Epoch: 1156 | training loss: 0.050474390388\n",
      "Epoch: 1157 | training loss: 0.050408590585\n",
      "Epoch: 1158 | training loss: 0.050342865288\n",
      "Epoch: 1159 | training loss: 0.050277195871\n",
      "Epoch: 1160 | training loss: 0.050211600959\n",
      "Epoch: 1161 | training loss: 0.050146076828\n",
      "Epoch: 1162 | training loss: 0.050080619752\n",
      "Epoch: 1163 | training loss: 0.050015229732\n",
      "Epoch: 1164 | training loss: 0.049949899316\n",
      "Epoch: 1165 | training loss: 0.049884639680\n",
      "Epoch: 1166 | training loss: 0.049819450825\n",
      "Epoch: 1167 | training loss: 0.049754325300\n",
      "Epoch: 1168 | training loss: 0.049689266831\n",
      "Epoch: 1169 | training loss: 0.049624275416\n",
      "Epoch: 1170 | training loss: 0.049559339881\n",
      "Epoch: 1171 | training loss: 0.049494486302\n",
      "Epoch: 1172 | training loss: 0.049429692328\n",
      "Epoch: 1173 | training loss: 0.049364969134\n",
      "Epoch: 1174 | training loss: 0.049300312996\n",
      "Epoch: 1175 | training loss: 0.049235716462\n",
      "Epoch: 1176 | training loss: 0.049171198159\n",
      "Epoch: 1177 | training loss: 0.049106724560\n",
      "Epoch: 1178 | training loss: 0.049042329192\n",
      "Epoch: 1179 | training loss: 0.048978000879\n",
      "Epoch: 1180 | training loss: 0.048913728446\n",
      "Epoch: 1181 | training loss: 0.048849541694\n",
      "Epoch: 1182 | training loss: 0.048785410821\n",
      "Epoch: 1183 | training loss: 0.048721347004\n",
      "Epoch: 1184 | training loss: 0.048657342792\n",
      "Epoch: 1185 | training loss: 0.048593405634\n",
      "Epoch: 1186 | training loss: 0.048529539257\n",
      "Epoch: 1187 | training loss: 0.048465736210\n",
      "Epoch: 1188 | training loss: 0.048401996493\n",
      "Epoch: 1189 | training loss: 0.048338312656\n",
      "Epoch: 1190 | training loss: 0.048274703324\n",
      "Epoch: 1191 | training loss: 0.048211161047\n",
      "Epoch: 1192 | training loss: 0.048147682101\n",
      "Epoch: 1193 | training loss: 0.048084273934\n",
      "Epoch: 1194 | training loss: 0.048020921648\n",
      "Epoch: 1195 | training loss: 0.047957625240\n",
      "Epoch: 1196 | training loss: 0.047894410789\n",
      "Epoch: 1197 | training loss: 0.047831259668\n",
      "Epoch: 1198 | training loss: 0.047768171877\n",
      "Epoch: 1199 | training loss: 0.047705143690\n",
      "Epoch: 1200 | training loss: 0.047642167658\n",
      "Epoch: 1201 | training loss: 0.047579277307\n",
      "Epoch: 1202 | training loss: 0.047516446561\n",
      "Epoch: 1203 | training loss: 0.047453679144\n",
      "Epoch: 1204 | training loss: 0.047390967607\n",
      "Epoch: 1205 | training loss: 0.047328315675\n",
      "Epoch: 1206 | training loss: 0.047265745699\n",
      "Epoch: 1207 | training loss: 0.047203227878\n",
      "Epoch: 1208 | training loss: 0.047140792012\n",
      "Epoch: 1209 | training loss: 0.047078404576\n",
      "Epoch: 1210 | training loss: 0.047016080469\n",
      "Epoch: 1211 | training loss: 0.046953830868\n",
      "Epoch: 1212 | training loss: 0.046891640872\n",
      "Epoch: 1213 | training loss: 0.046829517931\n",
      "Epoch: 1214 | training loss: 0.046767439693\n",
      "Epoch: 1215 | training loss: 0.046705439687\n",
      "Epoch: 1216 | training loss: 0.046643503010\n",
      "Epoch: 1217 | training loss: 0.046581629664\n",
      "Epoch: 1218 | training loss: 0.046519823372\n",
      "Epoch: 1219 | training loss: 0.046458072960\n",
      "Epoch: 1220 | training loss: 0.046396382153\n",
      "Epoch: 1221 | training loss: 0.046334762126\n",
      "Epoch: 1222 | training loss: 0.046273205429\n",
      "Epoch: 1223 | training loss: 0.046211712062\n",
      "Epoch: 1224 | training loss: 0.046150282025\n",
      "Epoch: 1225 | training loss: 0.046088904142\n",
      "Epoch: 1226 | training loss: 0.046027597040\n",
      "Epoch: 1227 | training loss: 0.045966360718\n",
      "Epoch: 1228 | training loss: 0.045905187726\n",
      "Epoch: 1229 | training loss: 0.045844074339\n",
      "Epoch: 1230 | training loss: 0.045783020556\n",
      "Epoch: 1231 | training loss: 0.045722030103\n",
      "Epoch: 1232 | training loss: 0.045661095530\n",
      "Epoch: 1233 | training loss: 0.045600242913\n",
      "Epoch: 1234 | training loss: 0.045539438725\n",
      "Epoch: 1235 | training loss: 0.045478694141\n",
      "Epoch: 1236 | training loss: 0.045418012887\n",
      "Epoch: 1237 | training loss: 0.045357394964\n",
      "Epoch: 1238 | training loss: 0.045296851546\n",
      "Epoch: 1239 | training loss: 0.045236364007\n",
      "Epoch: 1240 | training loss: 0.045175932348\n",
      "Epoch: 1241 | training loss: 0.045115571469\n",
      "Epoch: 1242 | training loss: 0.045055259019\n",
      "Epoch: 1243 | training loss: 0.044995024800\n",
      "Epoch: 1244 | training loss: 0.044934850186\n",
      "Epoch: 1245 | training loss: 0.044874724001\n",
      "Epoch: 1246 | training loss: 0.044814672321\n",
      "Epoch: 1247 | training loss: 0.044754672796\n",
      "Epoch: 1248 | training loss: 0.044694747776\n",
      "Epoch: 1249 | training loss: 0.044634878635\n",
      "Epoch: 1250 | training loss: 0.044575076550\n",
      "Epoch: 1251 | training loss: 0.044515330344\n",
      "Epoch: 1252 | training loss: 0.044455647469\n",
      "Epoch: 1253 | training loss: 0.044396027923\n",
      "Epoch: 1254 | training loss: 0.044336479157\n",
      "Epoch: 1255 | training loss: 0.044276975095\n",
      "Epoch: 1256 | training loss: 0.044217534363\n",
      "Epoch: 1257 | training loss: 0.044158156961\n",
      "Epoch: 1258 | training loss: 0.044098846614\n",
      "Epoch: 1259 | training loss: 0.044039595872\n",
      "Epoch: 1260 | training loss: 0.043980404735\n",
      "Epoch: 1261 | training loss: 0.043921276927\n",
      "Epoch: 1262 | training loss: 0.043862208724\n",
      "Epoch: 1263 | training loss: 0.043803200126\n",
      "Epoch: 1264 | training loss: 0.043744258583\n",
      "Epoch: 1265 | training loss: 0.043685369194\n",
      "Epoch: 1266 | training loss: 0.043626546860\n",
      "Epoch: 1267 | training loss: 0.043567784131\n",
      "Epoch: 1268 | training loss: 0.043509088457\n",
      "Epoch: 1269 | training loss: 0.043450452387\n",
      "Epoch: 1270 | training loss: 0.043391875923\n",
      "Epoch: 1271 | training loss: 0.043333366513\n",
      "Epoch: 1272 | training loss: 0.043274901807\n",
      "Epoch: 1273 | training loss: 0.043216511607\n",
      "Epoch: 1274 | training loss: 0.043158173561\n",
      "Epoch: 1275 | training loss: 0.043099902570\n",
      "Epoch: 1276 | training loss: 0.043041691184\n",
      "Epoch: 1277 | training loss: 0.042983531952\n",
      "Epoch: 1278 | training loss: 0.042925439775\n",
      "Epoch: 1279 | training loss: 0.042867410928\n",
      "Epoch: 1280 | training loss: 0.042809452862\n",
      "Epoch: 1281 | training loss: 0.042751543224\n",
      "Epoch: 1282 | training loss: 0.042693689466\n",
      "Epoch: 1283 | training loss: 0.042635906488\n",
      "Epoch: 1284 | training loss: 0.042578175664\n",
      "Epoch: 1285 | training loss: 0.042520515621\n",
      "Epoch: 1286 | training loss: 0.042462911457\n",
      "Epoch: 1287 | training loss: 0.042405363172\n",
      "Epoch: 1288 | training loss: 0.042347885668\n",
      "Epoch: 1289 | training loss: 0.042290456593\n",
      "Epoch: 1290 | training loss: 0.042233105749\n",
      "Epoch: 1291 | training loss: 0.042175807059\n",
      "Epoch: 1292 | training loss: 0.042118556798\n",
      "Epoch: 1293 | training loss: 0.042061369866\n",
      "Epoch: 1294 | training loss: 0.042004242539\n",
      "Epoch: 1295 | training loss: 0.041947185993\n",
      "Epoch: 1296 | training loss: 0.041890189052\n",
      "Epoch: 1297 | training loss: 0.041833255440\n",
      "Epoch: 1298 | training loss: 0.041776373982\n",
      "Epoch: 1299 | training loss: 0.041719540954\n",
      "Epoch: 1300 | training loss: 0.041662786156\n",
      "Epoch: 1301 | training loss: 0.041606087238\n",
      "Epoch: 1302 | training loss: 0.041549451649\n",
      "Epoch: 1303 | training loss: 0.041492871940\n",
      "Epoch: 1304 | training loss: 0.041436359286\n",
      "Epoch: 1305 | training loss: 0.041379906237\n",
      "Epoch: 1306 | training loss: 0.041323501617\n",
      "Epoch: 1307 | training loss: 0.041267164052\n",
      "Epoch: 1308 | training loss: 0.041210882366\n",
      "Epoch: 1309 | training loss: 0.041154656559\n",
      "Epoch: 1310 | training loss: 0.041098497808\n",
      "Epoch: 1311 | training loss: 0.041042402387\n",
      "Epoch: 1312 | training loss: 0.040986366570\n",
      "Epoch: 1313 | training loss: 0.040930379182\n",
      "Epoch: 1314 | training loss: 0.040874451399\n",
      "Epoch: 1315 | training loss: 0.040818590671\n",
      "Epoch: 1316 | training loss: 0.040762796998\n",
      "Epoch: 1317 | training loss: 0.040707059205\n",
      "Epoch: 1318 | training loss: 0.040651373565\n",
      "Epoch: 1319 | training loss: 0.040595747530\n",
      "Epoch: 1320 | training loss: 0.040540184826\n",
      "Epoch: 1321 | training loss: 0.040484678000\n",
      "Epoch: 1322 | training loss: 0.040429230779\n",
      "Epoch: 1323 | training loss: 0.040373843163\n",
      "Epoch: 1324 | training loss: 0.040318522602\n",
      "Epoch: 1325 | training loss: 0.040263265371\n",
      "Epoch: 1326 | training loss: 0.040208052844\n",
      "Epoch: 1327 | training loss: 0.040152899921\n",
      "Epoch: 1328 | training loss: 0.040097810328\n",
      "Epoch: 1329 | training loss: 0.040042784065\n",
      "Epoch: 1330 | training loss: 0.039987821132\n",
      "Epoch: 1331 | training loss: 0.039932910353\n",
      "Epoch: 1332 | training loss: 0.039878066629\n",
      "Epoch: 1333 | training loss: 0.039823271334\n",
      "Epoch: 1334 | training loss: 0.039768531919\n",
      "Epoch: 1335 | training loss: 0.039713859558\n",
      "Epoch: 1336 | training loss: 0.039659235626\n",
      "Epoch: 1337 | training loss: 0.039604675025\n",
      "Epoch: 1338 | training loss: 0.039550181478\n",
      "Epoch: 1339 | training loss: 0.039495751262\n",
      "Epoch: 1340 | training loss: 0.039441373199\n",
      "Epoch: 1341 | training loss: 0.039387054741\n",
      "Epoch: 1342 | training loss: 0.039332792163\n",
      "Epoch: 1343 | training loss: 0.039278585464\n",
      "Epoch: 1344 | training loss: 0.039224442095\n",
      "Epoch: 1345 | training loss: 0.039170347154\n",
      "Epoch: 1346 | training loss: 0.039116322994\n",
      "Epoch: 1347 | training loss: 0.039062347263\n",
      "Epoch: 1348 | training loss: 0.039008442312\n",
      "Epoch: 1349 | training loss: 0.038954593241\n",
      "Epoch: 1350 | training loss: 0.038900807500\n",
      "Epoch: 1351 | training loss: 0.038847073913\n",
      "Epoch: 1352 | training loss: 0.038793392479\n",
      "Epoch: 1353 | training loss: 0.038739781827\n",
      "Epoch: 1354 | training loss: 0.038686219603\n",
      "Epoch: 1355 | training loss: 0.038632716984\n",
      "Epoch: 1356 | training loss: 0.038579270244\n",
      "Epoch: 1357 | training loss: 0.038525894284\n",
      "Epoch: 1358 | training loss: 0.038472570479\n",
      "Epoch: 1359 | training loss: 0.038419313729\n",
      "Epoch: 1360 | training loss: 0.038366094232\n",
      "Epoch: 1361 | training loss: 0.038312960416\n",
      "Epoch: 1362 | training loss: 0.038259871304\n",
      "Epoch: 1363 | training loss: 0.038206830621\n",
      "Epoch: 1364 | training loss: 0.038153860718\n",
      "Epoch: 1365 | training loss: 0.038100931793\n",
      "Epoch: 1366 | training loss: 0.038048069924\n",
      "Epoch: 1367 | training loss: 0.037995275110\n",
      "Epoch: 1368 | training loss: 0.037942536175\n",
      "Epoch: 1369 | training loss: 0.037889845669\n",
      "Epoch: 1370 | training loss: 0.037837229669\n",
      "Epoch: 1371 | training loss: 0.037784658372\n",
      "Epoch: 1372 | training loss: 0.037732154131\n",
      "Epoch: 1373 | training loss: 0.037679702044\n",
      "Epoch: 1374 | training loss: 0.037627290934\n",
      "Epoch: 1375 | training loss: 0.037574965507\n",
      "Epoch: 1376 | training loss: 0.037522681057\n",
      "Epoch: 1377 | training loss: 0.037470463663\n",
      "Epoch: 1378 | training loss: 0.037418290973\n",
      "Epoch: 1379 | training loss: 0.037366196513\n",
      "Epoch: 1380 | training loss: 0.037314146757\n",
      "Epoch: 1381 | training loss: 0.037262160331\n",
      "Epoch: 1382 | training loss: 0.037210229784\n",
      "Epoch: 1383 | training loss: 0.037158362567\n",
      "Epoch: 1384 | training loss: 0.037106547505\n",
      "Epoch: 1385 | training loss: 0.037054795772\n",
      "Epoch: 1386 | training loss: 0.037003092468\n",
      "Epoch: 1387 | training loss: 0.036951448768\n",
      "Epoch: 1388 | training loss: 0.036899857223\n",
      "Epoch: 1389 | training loss: 0.036848332733\n",
      "Epoch: 1390 | training loss: 0.036796864122\n",
      "Epoch: 1391 | training loss: 0.036745440215\n",
      "Epoch: 1392 | training loss: 0.036694094539\n",
      "Epoch: 1393 | training loss: 0.036642789841\n",
      "Epoch: 1394 | training loss: 0.036591548473\n",
      "Epoch: 1395 | training loss: 0.036540355533\n",
      "Epoch: 1396 | training loss: 0.036489240825\n",
      "Epoch: 1397 | training loss: 0.036438170820\n",
      "Epoch: 1398 | training loss: 0.036387164146\n",
      "Epoch: 1399 | training loss: 0.036336209625\n",
      "Epoch: 1400 | training loss: 0.036285314709\n",
      "Epoch: 1401 | training loss: 0.036234475672\n",
      "Epoch: 1402 | training loss: 0.036183699965\n",
      "Epoch: 1403 | training loss: 0.036132968962\n",
      "Epoch: 1404 | training loss: 0.036082305014\n",
      "Epoch: 1405 | training loss: 0.036031700671\n",
      "Epoch: 1406 | training loss: 0.035981148481\n",
      "Epoch: 1407 | training loss: 0.035930652171\n",
      "Epoch: 1408 | training loss: 0.035880222917\n",
      "Epoch: 1409 | training loss: 0.035829842091\n",
      "Epoch: 1410 | training loss: 0.035779509693\n",
      "Epoch: 1411 | training loss: 0.035729244351\n",
      "Epoch: 1412 | training loss: 0.035679019988\n",
      "Epoch: 1413 | training loss: 0.035628866404\n",
      "Epoch: 1414 | training loss: 0.035578757524\n",
      "Epoch: 1415 | training loss: 0.035528719425\n",
      "Epoch: 1416 | training loss: 0.035478729755\n",
      "Epoch: 1417 | training loss: 0.035428803414\n",
      "Epoch: 1418 | training loss: 0.035378929228\n",
      "Epoch: 1419 | training loss: 0.035329110920\n",
      "Epoch: 1420 | training loss: 0.035279355943\n",
      "Epoch: 1421 | training loss: 0.035229656845\n",
      "Epoch: 1422 | training loss: 0.035180009902\n",
      "Epoch: 1423 | training loss: 0.035130418837\n",
      "Epoch: 1424 | training loss: 0.035080887377\n",
      "Epoch: 1425 | training loss: 0.035031415522\n",
      "Epoch: 1426 | training loss: 0.034981992096\n",
      "Epoch: 1427 | training loss: 0.034932639450\n",
      "Epoch: 1428 | training loss: 0.034883327782\n",
      "Epoch: 1429 | training loss: 0.034834083170\n",
      "Epoch: 1430 | training loss: 0.034784883261\n",
      "Epoch: 1431 | training loss: 0.034735746682\n",
      "Epoch: 1432 | training loss: 0.034686669707\n",
      "Epoch: 1433 | training loss: 0.034637648612\n",
      "Epoch: 1434 | training loss: 0.034588683397\n",
      "Epoch: 1435 | training loss: 0.034539774060\n",
      "Epoch: 1436 | training loss: 0.034490924329\n",
      "Epoch: 1437 | training loss: 0.034442130476\n",
      "Epoch: 1438 | training loss: 0.034393385053\n",
      "Epoch: 1439 | training loss: 0.034344702959\n",
      "Epoch: 1440 | training loss: 0.034296076745\n",
      "Epoch: 1441 | training loss: 0.034247510135\n",
      "Epoch: 1442 | training loss: 0.034198999405\n",
      "Epoch: 1443 | training loss: 0.034150540829\n",
      "Epoch: 1444 | training loss: 0.034102134407\n",
      "Epoch: 1445 | training loss: 0.034053783864\n",
      "Epoch: 1446 | training loss: 0.034005496651\n",
      "Epoch: 1447 | training loss: 0.033957257867\n",
      "Epoch: 1448 | training loss: 0.033909082413\n",
      "Epoch: 1449 | training loss: 0.033860962838\n",
      "Epoch: 1450 | training loss: 0.033812887967\n",
      "Epoch: 1451 | training loss: 0.033764876425\n",
      "Epoch: 1452 | training loss: 0.033716920763\n",
      "Epoch: 1453 | training loss: 0.033669028431\n",
      "Epoch: 1454 | training loss: 0.033621173352\n",
      "Epoch: 1455 | training loss: 0.033573381603\n",
      "Epoch: 1456 | training loss: 0.033525653183\n",
      "Epoch: 1457 | training loss: 0.033477973193\n",
      "Epoch: 1458 | training loss: 0.033430345356\n",
      "Epoch: 1459 | training loss: 0.033382784575\n",
      "Epoch: 1460 | training loss: 0.033335275948\n",
      "Epoch: 1461 | training loss: 0.033287823200\n",
      "Epoch: 1462 | training loss: 0.033240422606\n",
      "Epoch: 1463 | training loss: 0.033193077892\n",
      "Epoch: 1464 | training loss: 0.033145789057\n",
      "Epoch: 1465 | training loss: 0.033098556101\n",
      "Epoch: 1466 | training loss: 0.033051371574\n",
      "Epoch: 1467 | training loss: 0.033004250377\n",
      "Epoch: 1468 | training loss: 0.032957192510\n",
      "Epoch: 1469 | training loss: 0.032910179347\n",
      "Epoch: 1470 | training loss: 0.032863225788\n",
      "Epoch: 1471 | training loss: 0.032816328108\n",
      "Epoch: 1472 | training loss: 0.032769486308\n",
      "Epoch: 1473 | training loss: 0.032722700387\n",
      "Epoch: 1474 | training loss: 0.032675970346\n",
      "Epoch: 1475 | training loss: 0.032629288733\n",
      "Epoch: 1476 | training loss: 0.032582670450\n",
      "Epoch: 1477 | training loss: 0.032536100596\n",
      "Epoch: 1478 | training loss: 0.032489590347\n",
      "Epoch: 1479 | training loss: 0.032443132252\n",
      "Epoch: 1480 | training loss: 0.032396730036\n",
      "Epoch: 1481 | training loss: 0.032350383699\n",
      "Epoch: 1482 | training loss: 0.032304096967\n",
      "Epoch: 1483 | training loss: 0.032257851213\n",
      "Epoch: 1484 | training loss: 0.032211668789\n",
      "Epoch: 1485 | training loss: 0.032165538520\n",
      "Epoch: 1486 | training loss: 0.032119475305\n",
      "Epoch: 1487 | training loss: 0.032073445618\n",
      "Epoch: 1488 | training loss: 0.032027482986\n",
      "Epoch: 1489 | training loss: 0.031981576234\n",
      "Epoch: 1490 | training loss: 0.031935721636\n",
      "Epoch: 1491 | training loss: 0.031889930367\n",
      "Epoch: 1492 | training loss: 0.031844187528\n",
      "Epoch: 1493 | training loss: 0.031798504293\n",
      "Epoch: 1494 | training loss: 0.031752869487\n",
      "Epoch: 1495 | training loss: 0.031707290560\n",
      "Epoch: 1496 | training loss: 0.031661767513\n",
      "Epoch: 1497 | training loss: 0.031616296619\n",
      "Epoch: 1498 | training loss: 0.031570877880\n",
      "Epoch: 1499 | training loss: 0.031525522470\n",
      "Epoch: 1500 | training loss: 0.031480211765\n",
      "Epoch: 1501 | training loss: 0.031434960663\n",
      "Epoch: 1502 | training loss: 0.031389754266\n",
      "Epoch: 1503 | training loss: 0.031344614923\n",
      "Epoch: 1504 | training loss: 0.031299531460\n",
      "Epoch: 1505 | training loss: 0.031254496425\n",
      "Epoch: 1506 | training loss: 0.031209524721\n",
      "Epoch: 1507 | training loss: 0.031164599583\n",
      "Epoch: 1508 | training loss: 0.031119730324\n",
      "Epoch: 1509 | training loss: 0.031074913219\n",
      "Epoch: 1510 | training loss: 0.031030153856\n",
      "Epoch: 1511 | training loss: 0.030985441059\n",
      "Epoch: 1512 | training loss: 0.030940787867\n",
      "Epoch: 1513 | training loss: 0.030896186829\n",
      "Epoch: 1514 | training loss: 0.030851637945\n",
      "Epoch: 1515 | training loss: 0.030807154253\n",
      "Epoch: 1516 | training loss: 0.030762715265\n",
      "Epoch: 1517 | training loss: 0.030718335882\n",
      "Epoch: 1518 | training loss: 0.030674004927\n",
      "Epoch: 1519 | training loss: 0.030629729852\n",
      "Epoch: 1520 | training loss: 0.030585508794\n",
      "Epoch: 1521 | training loss: 0.030541347340\n",
      "Epoch: 1522 | training loss: 0.030497232452\n",
      "Epoch: 1523 | training loss: 0.030453175306\n",
      "Epoch: 1524 | training loss: 0.030409168452\n",
      "Epoch: 1525 | training loss: 0.030365215614\n",
      "Epoch: 1526 | training loss: 0.030321320519\n",
      "Epoch: 1527 | training loss: 0.030277479440\n",
      "Epoch: 1528 | training loss: 0.030233690515\n",
      "Epoch: 1529 | training loss: 0.030189950019\n",
      "Epoch: 1530 | training loss: 0.030146272853\n",
      "Epoch: 1531 | training loss: 0.030102636665\n",
      "Epoch: 1532 | training loss: 0.030059069395\n",
      "Epoch: 1533 | training loss: 0.030015546829\n",
      "Epoch: 1534 | training loss: 0.029972078279\n",
      "Epoch: 1535 | training loss: 0.029928665608\n",
      "Epoch: 1536 | training loss: 0.029885301366\n",
      "Epoch: 1537 | training loss: 0.029842004180\n",
      "Epoch: 1538 | training loss: 0.029798746109\n",
      "Epoch: 1539 | training loss: 0.029755547643\n",
      "Epoch: 1540 | training loss: 0.029712405056\n",
      "Epoch: 1541 | training loss: 0.029669307172\n",
      "Epoch: 1542 | training loss: 0.029626261443\n",
      "Epoch: 1543 | training loss: 0.029583279043\n",
      "Epoch: 1544 | training loss: 0.029540348798\n",
      "Epoch: 1545 | training loss: 0.029497470707\n",
      "Epoch: 1546 | training loss: 0.029454641044\n",
      "Epoch: 1547 | training loss: 0.029411867261\n",
      "Epoch: 1548 | training loss: 0.029369149357\n",
      "Epoch: 1549 | training loss: 0.029326474294\n",
      "Epoch: 1550 | training loss: 0.029283860698\n",
      "Epoch: 1551 | training loss: 0.029241308570\n",
      "Epoch: 1552 | training loss: 0.029198795557\n",
      "Epoch: 1553 | training loss: 0.029156345874\n",
      "Epoch: 1554 | training loss: 0.029113942757\n",
      "Epoch: 1555 | training loss: 0.029071597382\n",
      "Epoch: 1556 | training loss: 0.029029296711\n",
      "Epoch: 1557 | training loss: 0.028987057507\n",
      "Epoch: 1558 | training loss: 0.028944872320\n",
      "Epoch: 1559 | training loss: 0.028902728111\n",
      "Epoch: 1560 | training loss: 0.028860649094\n",
      "Epoch: 1561 | training loss: 0.028818616644\n",
      "Epoch: 1562 | training loss: 0.028776636347\n",
      "Epoch: 1563 | training loss: 0.028734711930\n",
      "Epoch: 1564 | training loss: 0.028692839667\n",
      "Epoch: 1565 | training loss: 0.028651017696\n",
      "Epoch: 1566 | training loss: 0.028609253466\n",
      "Epoch: 1567 | training loss: 0.028567541391\n",
      "Epoch: 1568 | training loss: 0.028525872156\n",
      "Epoch: 1569 | training loss: 0.028484266251\n",
      "Epoch: 1570 | training loss: 0.028442706913\n",
      "Epoch: 1571 | training loss: 0.028401201591\n",
      "Epoch: 1572 | training loss: 0.028359752148\n",
      "Epoch: 1573 | training loss: 0.028318356723\n",
      "Epoch: 1574 | training loss: 0.028277006000\n",
      "Epoch: 1575 | training loss: 0.028235714883\n",
      "Epoch: 1576 | training loss: 0.028194475919\n",
      "Epoch: 1577 | training loss: 0.028153277934\n",
      "Epoch: 1578 | training loss: 0.028112133965\n",
      "Epoch: 1579 | training loss: 0.028071051463\n",
      "Epoch: 1580 | training loss: 0.028030017391\n",
      "Epoch: 1581 | training loss: 0.027989035472\n",
      "Epoch: 1582 | training loss: 0.027948103845\n",
      "Epoch: 1583 | training loss: 0.027907226235\n",
      "Epoch: 1584 | training loss: 0.027866402641\n",
      "Epoch: 1585 | training loss: 0.027825631201\n",
      "Epoch: 1586 | training loss: 0.027784913778\n",
      "Epoch: 1587 | training loss: 0.027744241059\n",
      "Epoch: 1588 | training loss: 0.027703620493\n",
      "Epoch: 1589 | training loss: 0.027663055807\n",
      "Epoch: 1590 | training loss: 0.027622537687\n",
      "Epoch: 1591 | training loss: 0.027582084760\n",
      "Epoch: 1592 | training loss: 0.027541674674\n",
      "Epoch: 1593 | training loss: 0.027501316741\n",
      "Epoch: 1594 | training loss: 0.027461009100\n",
      "Epoch: 1595 | training loss: 0.027420757338\n",
      "Epoch: 1596 | training loss: 0.027380554006\n",
      "Epoch: 1597 | training loss: 0.027340408415\n",
      "Epoch: 1598 | training loss: 0.027300309390\n",
      "Epoch: 1599 | training loss: 0.027260258794\n",
      "Epoch: 1600 | training loss: 0.027220265940\n",
      "Epoch: 1601 | training loss: 0.027180328965\n",
      "Epoch: 1602 | training loss: 0.027140431106\n",
      "Epoch: 1603 | training loss: 0.027100596577\n",
      "Epoch: 1604 | training loss: 0.027060803026\n",
      "Epoch: 1605 | training loss: 0.027021063492\n",
      "Epoch: 1606 | training loss: 0.026981387287\n",
      "Epoch: 1607 | training loss: 0.026941755787\n",
      "Epoch: 1608 | training loss: 0.026902170852\n",
      "Epoch: 1609 | training loss: 0.026862638071\n",
      "Epoch: 1610 | training loss: 0.026823159307\n",
      "Epoch: 1611 | training loss: 0.026783734560\n",
      "Epoch: 1612 | training loss: 0.026744360104\n",
      "Epoch: 1613 | training loss: 0.026705035940\n",
      "Epoch: 1614 | training loss: 0.026665752754\n",
      "Epoch: 1615 | training loss: 0.026626532897\n",
      "Epoch: 1616 | training loss: 0.026587363333\n",
      "Epoch: 1617 | training loss: 0.026548240334\n",
      "Epoch: 1618 | training loss: 0.026509173214\n",
      "Epoch: 1619 | training loss: 0.026470148936\n",
      "Epoch: 1620 | training loss: 0.026431178674\n",
      "Epoch: 1621 | training loss: 0.026392269880\n",
      "Epoch: 1622 | training loss: 0.026353403926\n",
      "Epoch: 1623 | training loss: 0.026314590126\n",
      "Epoch: 1624 | training loss: 0.026275821030\n",
      "Epoch: 1625 | training loss: 0.026237109676\n",
      "Epoch: 1626 | training loss: 0.026198446751\n",
      "Epoch: 1627 | training loss: 0.026159835979\n",
      "Epoch: 1628 | training loss: 0.026121275499\n",
      "Epoch: 1629 | training loss: 0.026082757860\n",
      "Epoch: 1630 | training loss: 0.026044301689\n",
      "Epoch: 1631 | training loss: 0.026005892083\n",
      "Epoch: 1632 | training loss: 0.025967532769\n",
      "Epoch: 1633 | training loss: 0.025929227471\n",
      "Epoch: 1634 | training loss: 0.025890966877\n",
      "Epoch: 1635 | training loss: 0.025852758437\n",
      "Epoch: 1636 | training loss: 0.025814602152\n",
      "Epoch: 1637 | training loss: 0.025776496157\n",
      "Epoch: 1638 | training loss: 0.025738436729\n",
      "Epoch: 1639 | training loss: 0.025700429454\n",
      "Epoch: 1640 | training loss: 0.025662470609\n",
      "Epoch: 1641 | training loss: 0.025624567643\n",
      "Epoch: 1642 | training loss: 0.025586713105\n",
      "Epoch: 1643 | training loss: 0.025548910722\n",
      "Epoch: 1644 | training loss: 0.025511162356\n",
      "Epoch: 1645 | training loss: 0.025473458692\n",
      "Epoch: 1646 | training loss: 0.025435805321\n",
      "Epoch: 1647 | training loss: 0.025398202240\n",
      "Epoch: 1648 | training loss: 0.025360653177\n",
      "Epoch: 1649 | training loss: 0.025323150679\n",
      "Epoch: 1650 | training loss: 0.025285696611\n",
      "Epoch: 1651 | training loss: 0.025248294696\n",
      "Epoch: 1652 | training loss: 0.025210935622\n",
      "Epoch: 1653 | training loss: 0.025173636153\n",
      "Epoch: 1654 | training loss: 0.025136385113\n",
      "Epoch: 1655 | training loss: 0.025099178776\n",
      "Epoch: 1656 | training loss: 0.025062028319\n",
      "Epoch: 1657 | training loss: 0.025024924427\n",
      "Epoch: 1658 | training loss: 0.024987868965\n",
      "Epoch: 1659 | training loss: 0.024950869381\n",
      "Epoch: 1660 | training loss: 0.024913910776\n",
      "Epoch: 1661 | training loss: 0.024877008051\n",
      "Epoch: 1662 | training loss: 0.024840161204\n",
      "Epoch: 1663 | training loss: 0.024803355336\n",
      "Epoch: 1664 | training loss: 0.024766603485\n",
      "Epoch: 1665 | training loss: 0.024729903787\n",
      "Epoch: 1666 | training loss: 0.024693245068\n",
      "Epoch: 1667 | training loss: 0.024656638503\n",
      "Epoch: 1668 | training loss: 0.024620078504\n",
      "Epoch: 1669 | training loss: 0.024583570659\n",
      "Epoch: 1670 | training loss: 0.024547113106\n",
      "Epoch: 1671 | training loss: 0.024510703981\n",
      "Epoch: 1672 | training loss: 0.024474341422\n",
      "Epoch: 1673 | training loss: 0.024438038468\n",
      "Epoch: 1674 | training loss: 0.024401774630\n",
      "Epoch: 1675 | training loss: 0.024365568534\n",
      "Epoch: 1676 | training loss: 0.024329412729\n",
      "Epoch: 1677 | training loss: 0.024293297902\n",
      "Epoch: 1678 | training loss: 0.024257238954\n",
      "Epoch: 1679 | training loss: 0.024221220985\n",
      "Epoch: 1680 | training loss: 0.024185255170\n",
      "Epoch: 1681 | training loss: 0.024149341509\n",
      "Epoch: 1682 | training loss: 0.024113472551\n",
      "Epoch: 1683 | training loss: 0.024077650160\n",
      "Epoch: 1684 | training loss: 0.024041887373\n",
      "Epoch: 1685 | training loss: 0.024006163701\n",
      "Epoch: 1686 | training loss: 0.023970495909\n",
      "Epoch: 1687 | training loss: 0.023934870958\n",
      "Epoch: 1688 | training loss: 0.023899296299\n",
      "Epoch: 1689 | training loss: 0.023863773793\n",
      "Epoch: 1690 | training loss: 0.023828297853\n",
      "Epoch: 1691 | training loss: 0.023792866617\n",
      "Epoch: 1692 | training loss: 0.023757494986\n",
      "Epoch: 1693 | training loss: 0.023722166196\n",
      "Epoch: 1694 | training loss: 0.023686882108\n",
      "Epoch: 1695 | training loss: 0.023651652038\n",
      "Epoch: 1696 | training loss: 0.023616464809\n",
      "Epoch: 1697 | training loss: 0.023581333458\n",
      "Epoch: 1698 | training loss: 0.023546254262\n",
      "Epoch: 1699 | training loss: 0.023511214182\n",
      "Epoch: 1700 | training loss: 0.023476229981\n",
      "Epoch: 1701 | training loss: 0.023441281170\n",
      "Epoch: 1702 | training loss: 0.023406384513\n",
      "Epoch: 1703 | training loss: 0.023371545598\n",
      "Epoch: 1704 | training loss: 0.023336745799\n",
      "Epoch: 1705 | training loss: 0.023302003741\n",
      "Epoch: 1706 | training loss: 0.023267304525\n",
      "Epoch: 1707 | training loss: 0.023232646286\n",
      "Epoch: 1708 | training loss: 0.023198043928\n",
      "Epoch: 1709 | training loss: 0.023163489997\n",
      "Epoch: 1710 | training loss: 0.023128978908\n",
      "Epoch: 1711 | training loss: 0.023094523698\n",
      "Epoch: 1712 | training loss: 0.023060111329\n",
      "Epoch: 1713 | training loss: 0.023025751114\n",
      "Epoch: 1714 | training loss: 0.022991435602\n",
      "Epoch: 1715 | training loss: 0.022957162932\n",
      "Epoch: 1716 | training loss: 0.022922942415\n",
      "Epoch: 1717 | training loss: 0.022888772190\n",
      "Epoch: 1718 | training loss: 0.022854641080\n",
      "Epoch: 1719 | training loss: 0.022820565850\n",
      "Epoch: 1720 | training loss: 0.022786540911\n",
      "Epoch: 1721 | training loss: 0.022752560675\n",
      "Epoch: 1722 | training loss: 0.022718628868\n",
      "Epoch: 1723 | training loss: 0.022684745491\n",
      "Epoch: 1724 | training loss: 0.022650906816\n",
      "Epoch: 1725 | training loss: 0.022617118433\n",
      "Epoch: 1726 | training loss: 0.022583378479\n",
      "Epoch: 1727 | training loss: 0.022549683228\n",
      "Epoch: 1728 | training loss: 0.022516032681\n",
      "Epoch: 1729 | training loss: 0.022482439876\n",
      "Epoch: 1730 | training loss: 0.022448886186\n",
      "Epoch: 1731 | training loss: 0.022415380925\n",
      "Epoch: 1732 | training loss: 0.022381929681\n",
      "Epoch: 1733 | training loss: 0.022348513827\n",
      "Epoch: 1734 | training loss: 0.022315151989\n",
      "Epoch: 1735 | training loss: 0.022281836718\n",
      "Epoch: 1736 | training loss: 0.022248564288\n",
      "Epoch: 1737 | training loss: 0.022215342149\n",
      "Epoch: 1738 | training loss: 0.022182172164\n",
      "Epoch: 1739 | training loss: 0.022149041295\n",
      "Epoch: 1740 | training loss: 0.022115962580\n",
      "Epoch: 1741 | training loss: 0.022082919255\n",
      "Epoch: 1742 | training loss: 0.022049937397\n",
      "Epoch: 1743 | training loss: 0.022016996518\n",
      "Epoch: 1744 | training loss: 0.021984096617\n",
      "Epoch: 1745 | training loss: 0.021951250732\n",
      "Epoch: 1746 | training loss: 0.021918455139\n",
      "Epoch: 1747 | training loss: 0.021885702386\n",
      "Epoch: 1748 | training loss: 0.021852996200\n",
      "Epoch: 1749 | training loss: 0.021820334718\n",
      "Epoch: 1750 | training loss: 0.021787719801\n",
      "Epoch: 1751 | training loss: 0.021755155176\n",
      "Epoch: 1752 | training loss: 0.021722631529\n",
      "Epoch: 1753 | training loss: 0.021690154448\n",
      "Epoch: 1754 | training loss: 0.021657729521\n",
      "Epoch: 1755 | training loss: 0.021625349298\n",
      "Epoch: 1756 | training loss: 0.021593013778\n",
      "Epoch: 1757 | training loss: 0.021560722962\n",
      "Epoch: 1758 | training loss: 0.021528488025\n",
      "Epoch: 1759 | training loss: 0.021496290341\n",
      "Epoch: 1760 | training loss: 0.021464142948\n",
      "Epoch: 1761 | training loss: 0.021432045847\n",
      "Epoch: 1762 | training loss: 0.021399984136\n",
      "Epoch: 1763 | training loss: 0.021367972717\n",
      "Epoch: 1764 | training loss: 0.021336011589\n",
      "Epoch: 1765 | training loss: 0.021304091439\n",
      "Epoch: 1766 | training loss: 0.021272223443\n",
      "Epoch: 1767 | training loss: 0.021240396425\n",
      "Epoch: 1768 | training loss: 0.021208615974\n",
      "Epoch: 1769 | training loss: 0.021176883951\n",
      "Epoch: 1770 | training loss: 0.021145202219\n",
      "Epoch: 1771 | training loss: 0.021113557741\n",
      "Epoch: 1772 | training loss: 0.021081961691\n",
      "Epoch: 1773 | training loss: 0.021050414070\n",
      "Epoch: 1774 | training loss: 0.021018913016\n",
      "Epoch: 1775 | training loss: 0.020987456664\n",
      "Epoch: 1776 | training loss: 0.020956048742\n",
      "Epoch: 1777 | training loss: 0.020924685523\n",
      "Epoch: 1778 | training loss: 0.020893367007\n",
      "Epoch: 1779 | training loss: 0.020862091333\n",
      "Epoch: 1780 | training loss: 0.020830867812\n",
      "Epoch: 1781 | training loss: 0.020799681544\n",
      "Epoch: 1782 | training loss: 0.020768543705\n",
      "Epoch: 1783 | training loss: 0.020737456158\n",
      "Epoch: 1784 | training loss: 0.020706409588\n",
      "Epoch: 1785 | training loss: 0.020675411448\n",
      "Epoch: 1786 | training loss: 0.020644454286\n",
      "Epoch: 1787 | training loss: 0.020613539964\n",
      "Epoch: 1788 | training loss: 0.020582675934\n",
      "Epoch: 1789 | training loss: 0.020551860332\n",
      "Epoch: 1790 | training loss: 0.020521085709\n",
      "Epoch: 1791 | training loss: 0.020490353927\n",
      "Epoch: 1792 | training loss: 0.020459670573\n",
      "Epoch: 1793 | training loss: 0.020429028198\n",
      "Epoch: 1794 | training loss: 0.020398432389\n",
      "Epoch: 1795 | training loss: 0.020367886871\n",
      "Epoch: 1796 | training loss: 0.020337378606\n",
      "Epoch: 1797 | training loss: 0.020306918770\n",
      "Epoch: 1798 | training loss: 0.020276505500\n",
      "Epoch: 1799 | training loss: 0.020246131346\n",
      "Epoch: 1800 | training loss: 0.020215803757\n",
      "Epoch: 1801 | training loss: 0.020185526460\n",
      "Epoch: 1802 | training loss: 0.020155293867\n",
      "Epoch: 1803 | training loss: 0.020125102252\n",
      "Epoch: 1804 | training loss: 0.020094955340\n",
      "Epoch: 1805 | training loss: 0.020064858720\n",
      "Epoch: 1806 | training loss: 0.020034801215\n",
      "Epoch: 1807 | training loss: 0.020004794002\n",
      "Epoch: 1808 | training loss: 0.019974827766\n",
      "Epoch: 1809 | training loss: 0.019944900647\n",
      "Epoch: 1810 | training loss: 0.019915023819\n",
      "Epoch: 1811 | training loss: 0.019885191694\n",
      "Epoch: 1812 | training loss: 0.019855402410\n",
      "Epoch: 1813 | training loss: 0.019825654104\n",
      "Epoch: 1814 | training loss: 0.019795952365\n",
      "Epoch: 1815 | training loss: 0.019766293466\n",
      "Epoch: 1816 | training loss: 0.019736682996\n",
      "Epoch: 1817 | training loss: 0.019707113504\n",
      "Epoch: 1818 | training loss: 0.019677592441\n",
      "Epoch: 1819 | training loss: 0.019648108631\n",
      "Epoch: 1820 | training loss: 0.019618676975\n",
      "Epoch: 1821 | training loss: 0.019589282572\n",
      "Epoch: 1822 | training loss: 0.019559938461\n",
      "Epoch: 1823 | training loss: 0.019530631602\n",
      "Epoch: 1824 | training loss: 0.019501376897\n",
      "Epoch: 1825 | training loss: 0.019472155720\n",
      "Epoch: 1826 | training loss: 0.019442981109\n",
      "Epoch: 1827 | training loss: 0.019413858652\n",
      "Epoch: 1828 | training loss: 0.019384769723\n",
      "Epoch: 1829 | training loss: 0.019355732948\n",
      "Epoch: 1830 | training loss: 0.019326735288\n",
      "Epoch: 1831 | training loss: 0.019297782332\n",
      "Epoch: 1832 | training loss: 0.019268872216\n",
      "Epoch: 1833 | training loss: 0.019240003079\n",
      "Epoch: 1834 | training loss: 0.019211186096\n",
      "Epoch: 1835 | training loss: 0.019182397053\n",
      "Epoch: 1836 | training loss: 0.019153667614\n",
      "Epoch: 1837 | training loss: 0.019124977291\n",
      "Epoch: 1838 | training loss: 0.019096322358\n",
      "Epoch: 1839 | training loss: 0.019067713991\n",
      "Epoch: 1840 | training loss: 0.019039154053\n",
      "Epoch: 1841 | training loss: 0.019010640681\n",
      "Epoch: 1842 | training loss: 0.018982158974\n",
      "Epoch: 1843 | training loss: 0.018953731284\n",
      "Epoch: 1844 | training loss: 0.018925346434\n",
      "Epoch: 1845 | training loss: 0.018896995112\n",
      "Epoch: 1846 | training loss: 0.018868692219\n",
      "Epoch: 1847 | training loss: 0.018840434030\n",
      "Epoch: 1848 | training loss: 0.018812216818\n",
      "Epoch: 1849 | training loss: 0.018784042448\n",
      "Epoch: 1850 | training loss: 0.018755909055\n",
      "Epoch: 1851 | training loss: 0.018727822229\n",
      "Epoch: 1852 | training loss: 0.018699768931\n",
      "Epoch: 1853 | training loss: 0.018671767786\n",
      "Epoch: 1854 | training loss: 0.018643807620\n",
      "Epoch: 1855 | training loss: 0.018615890294\n",
      "Epoch: 1856 | training loss: 0.018588013947\n",
      "Epoch: 1857 | training loss: 0.018560178578\n",
      "Epoch: 1858 | training loss: 0.018532387912\n",
      "Epoch: 1859 | training loss: 0.018504638225\n",
      "Epoch: 1860 | training loss: 0.018476935104\n",
      "Epoch: 1861 | training loss: 0.018449272960\n",
      "Epoch: 1862 | training loss: 0.018421651796\n",
      "Epoch: 1863 | training loss: 0.018394073471\n",
      "Epoch: 1864 | training loss: 0.018366536126\n",
      "Epoch: 1865 | training loss: 0.018339039758\n",
      "Epoch: 1866 | training loss: 0.018311586231\n",
      "Epoch: 1867 | training loss: 0.018284182996\n",
      "Epoch: 1868 | training loss: 0.018256815150\n",
      "Epoch: 1869 | training loss: 0.018229492009\n",
      "Epoch: 1870 | training loss: 0.018202211708\n",
      "Epoch: 1871 | training loss: 0.018174972385\n",
      "Epoch: 1872 | training loss: 0.018147770315\n",
      "Epoch: 1873 | training loss: 0.018120612949\n",
      "Epoch: 1874 | training loss: 0.018093502149\n",
      "Epoch: 1875 | training loss: 0.018066434190\n",
      "Epoch: 1876 | training loss: 0.018039405346\n",
      "Epoch: 1877 | training loss: 0.018012413755\n",
      "Epoch: 1878 | training loss: 0.017985474318\n",
      "Epoch: 1879 | training loss: 0.017958564684\n",
      "Epoch: 1880 | training loss: 0.017931699753\n",
      "Epoch: 1881 | training loss: 0.017904877663\n",
      "Epoch: 1882 | training loss: 0.017878092825\n",
      "Epoch: 1883 | training loss: 0.017851354554\n",
      "Epoch: 1884 | training loss: 0.017824659124\n",
      "Epoch: 1885 | training loss: 0.017798004672\n",
      "Epoch: 1886 | training loss: 0.017771385610\n",
      "Epoch: 1887 | training loss: 0.017744811252\n",
      "Epoch: 1888 | training loss: 0.017718281597\n",
      "Epoch: 1889 | training loss: 0.017691787332\n",
      "Epoch: 1890 | training loss: 0.017665339634\n",
      "Epoch: 1891 | training loss: 0.017638929188\n",
      "Epoch: 1892 | training loss: 0.017612567171\n",
      "Epoch: 1893 | training loss: 0.017586240545\n",
      "Epoch: 1894 | training loss: 0.017559953034\n",
      "Epoch: 1895 | training loss: 0.017533710226\n",
      "Epoch: 1896 | training loss: 0.017507506534\n",
      "Epoch: 1897 | training loss: 0.017481341958\n",
      "Epoch: 1898 | training loss: 0.017455218360\n",
      "Epoch: 1899 | training loss: 0.017429145053\n",
      "Epoch: 1900 | training loss: 0.017403101549\n",
      "Epoch: 1901 | training loss: 0.017377106473\n",
      "Epoch: 1902 | training loss: 0.017351150513\n",
      "Epoch: 1903 | training loss: 0.017325233668\n",
      "Epoch: 1904 | training loss: 0.017299355939\n",
      "Epoch: 1905 | training loss: 0.017273519188\n",
      "Epoch: 1906 | training loss: 0.017247729003\n",
      "Epoch: 1907 | training loss: 0.017221970484\n",
      "Epoch: 1908 | training loss: 0.017196260393\n",
      "Epoch: 1909 | training loss: 0.017170589417\n",
      "Epoch: 1910 | training loss: 0.017144959420\n",
      "Epoch: 1911 | training loss: 0.017119366676\n",
      "Epoch: 1912 | training loss: 0.017093814909\n",
      "Epoch: 1913 | training loss: 0.017068304121\n",
      "Epoch: 1914 | training loss: 0.017042828724\n",
      "Epoch: 1915 | training loss: 0.017017399892\n",
      "Epoch: 1916 | training loss: 0.016992008314\n",
      "Epoch: 1917 | training loss: 0.016966661438\n",
      "Epoch: 1918 | training loss: 0.016941348091\n",
      "Epoch: 1919 | training loss: 0.016916079447\n",
      "Epoch: 1920 | training loss: 0.016890849918\n",
      "Epoch: 1921 | training loss: 0.016865653917\n",
      "Epoch: 1922 | training loss: 0.016840500757\n",
      "Epoch: 1923 | training loss: 0.016815386713\n",
      "Epoch: 1924 | training loss: 0.016790321097\n",
      "Epoch: 1925 | training loss: 0.016765287146\n",
      "Epoch: 1926 | training loss: 0.016740296036\n",
      "Epoch: 1927 | training loss: 0.016715347767\n",
      "Epoch: 1928 | training loss: 0.016690440476\n",
      "Epoch: 1929 | training loss: 0.016665564850\n",
      "Epoch: 1930 | training loss: 0.016640732065\n",
      "Epoch: 1931 | training loss: 0.016615940258\n",
      "Epoch: 1932 | training loss: 0.016591180116\n",
      "Epoch: 1933 | training loss: 0.016566466540\n",
      "Epoch: 1934 | training loss: 0.016541792080\n",
      "Epoch: 1935 | training loss: 0.016517158598\n",
      "Epoch: 1936 | training loss: 0.016492560506\n",
      "Epoch: 1937 | training loss: 0.016468003392\n",
      "Epoch: 1938 | training loss: 0.016443487257\n",
      "Epoch: 1939 | training loss: 0.016419013962\n",
      "Epoch: 1940 | training loss: 0.016394572333\n",
      "Epoch: 1941 | training loss: 0.016370169818\n",
      "Epoch: 1942 | training loss: 0.016345810145\n",
      "Epoch: 1943 | training loss: 0.016321482137\n",
      "Epoch: 1944 | training loss: 0.016297198832\n",
      "Epoch: 1945 | training loss: 0.016272958368\n",
      "Epoch: 1946 | training loss: 0.016248757020\n",
      "Epoch: 1947 | training loss: 0.016224585474\n",
      "Epoch: 1948 | training loss: 0.016200460494\n",
      "Epoch: 1949 | training loss: 0.016176372766\n",
      "Epoch: 1950 | training loss: 0.016152326018\n",
      "Epoch: 1951 | training loss: 0.016128312796\n",
      "Epoch: 1952 | training loss: 0.016104338691\n",
      "Epoch: 1953 | training loss: 0.016080407426\n",
      "Epoch: 1954 | training loss: 0.016056504101\n",
      "Epoch: 1955 | training loss: 0.016032649204\n",
      "Epoch: 1956 | training loss: 0.016008827835\n",
      "Epoch: 1957 | training loss: 0.015985053033\n",
      "Epoch: 1958 | training loss: 0.015961306170\n",
      "Epoch: 1959 | training loss: 0.015937605873\n",
      "Epoch: 1960 | training loss: 0.015913940966\n",
      "Epoch: 1961 | training loss: 0.015890320763\n",
      "Epoch: 1962 | training loss: 0.015866730362\n",
      "Epoch: 1963 | training loss: 0.015843175352\n",
      "Epoch: 1964 | training loss: 0.015819665045\n",
      "Epoch: 1965 | training loss: 0.015796193853\n",
      "Epoch: 1966 | training loss: 0.015772758052\n",
      "Epoch: 1967 | training loss: 0.015749357641\n",
      "Epoch: 1968 | training loss: 0.015726000071\n",
      "Epoch: 1969 | training loss: 0.015702674165\n",
      "Epoch: 1970 | training loss: 0.015679391101\n",
      "Epoch: 1971 | training loss: 0.015656143427\n",
      "Epoch: 1972 | training loss: 0.015632936731\n",
      "Epoch: 1973 | training loss: 0.015609764494\n",
      "Epoch: 1974 | training loss: 0.015586632304\n",
      "Epoch: 1975 | training loss: 0.015563536435\n",
      "Epoch: 1976 | training loss: 0.015540478751\n",
      "Epoch: 1977 | training loss: 0.015517457388\n",
      "Epoch: 1978 | training loss: 0.015494470485\n",
      "Epoch: 1979 | training loss: 0.015471526422\n",
      "Epoch: 1980 | training loss: 0.015448620543\n",
      "Epoch: 1981 | training loss: 0.015425747260\n",
      "Epoch: 1982 | training loss: 0.015402913094\n",
      "Epoch: 1983 | training loss: 0.015380119905\n",
      "Epoch: 1984 | training loss: 0.015357363969\n",
      "Epoch: 1985 | training loss: 0.015334641561\n",
      "Epoch: 1986 | training loss: 0.015311956406\n",
      "Epoch: 1987 | training loss: 0.015289311297\n",
      "Epoch: 1988 | training loss: 0.015266699716\n",
      "Epoch: 1989 | training loss: 0.015244122595\n",
      "Epoch: 1990 | training loss: 0.015221585520\n",
      "Epoch: 1991 | training loss: 0.015199088491\n",
      "Epoch: 1992 | training loss: 0.015176624991\n",
      "Epoch: 1993 | training loss: 0.015154195949\n",
      "Epoch: 1994 | training loss: 0.015131808817\n",
      "Epoch: 1995 | training loss: 0.015109458007\n",
      "Epoch: 1996 | training loss: 0.015087143518\n",
      "Epoch: 1997 | training loss: 0.015064863488\n",
      "Epoch: 1998 | training loss: 0.015042622574\n",
      "Epoch: 1999 | training loss: 0.015020421706\n",
      "Epoch: 2000 | training loss: 0.014998251572\n",
      "Epoch: 2001 | training loss: 0.014976117760\n",
      "Epoch: 2002 | training loss: 0.014954020269\n",
      "Epoch: 2003 | training loss: 0.014931966551\n",
      "Epoch: 2004 | training loss: 0.014909938909\n",
      "Epoch: 2005 | training loss: 0.014887949452\n",
      "Epoch: 2006 | training loss: 0.014865997247\n",
      "Epoch: 2007 | training loss: 0.014844087884\n",
      "Epoch: 2008 | training loss: 0.014822204597\n",
      "Epoch: 2009 | training loss: 0.014800364152\n",
      "Epoch: 2010 | training loss: 0.014778559096\n",
      "Epoch: 2011 | training loss: 0.014756793156\n",
      "Epoch: 2012 | training loss: 0.014735057950\n",
      "Epoch: 2013 | training loss: 0.014713361859\n",
      "Epoch: 2014 | training loss: 0.014691700228\n",
      "Epoch: 2015 | training loss: 0.014670075849\n",
      "Epoch: 2016 | training loss: 0.014648483135\n",
      "Epoch: 2017 | training loss: 0.014626929536\n",
      "Epoch: 2018 | training loss: 0.014605412260\n",
      "Epoch: 2019 | training loss: 0.014583923854\n",
      "Epoch: 2020 | training loss: 0.014562478289\n",
      "Epoch: 2021 | training loss: 0.014541061595\n",
      "Epoch: 2022 | training loss: 0.014519690536\n",
      "Epoch: 2023 | training loss: 0.014498344623\n",
      "Epoch: 2024 | training loss: 0.014477040619\n",
      "Epoch: 2025 | training loss: 0.014455770142\n",
      "Epoch: 2026 | training loss: 0.014434538782\n",
      "Epoch: 2027 | training loss: 0.014413337223\n",
      "Epoch: 2028 | training loss: 0.014392174780\n",
      "Epoch: 2029 | training loss: 0.014371045865\n",
      "Epoch: 2030 | training loss: 0.014349955134\n",
      "Epoch: 2031 | training loss: 0.014328896999\n",
      "Epoch: 2032 | training loss: 0.014307877049\n",
      "Epoch: 2033 | training loss: 0.014286886901\n",
      "Epoch: 2034 | training loss: 0.014265939593\n",
      "Epoch: 2035 | training loss: 0.014245019294\n",
      "Epoch: 2036 | training loss: 0.014224136248\n",
      "Epoch: 2037 | training loss: 0.014203287661\n",
      "Epoch: 2038 | training loss: 0.014182475396\n",
      "Epoch: 2039 | training loss: 0.014161694795\n",
      "Epoch: 2040 | training loss: 0.014140950516\n",
      "Epoch: 2041 | training loss: 0.014120241627\n",
      "Epoch: 2042 | training loss: 0.014099567197\n",
      "Epoch: 2043 | training loss: 0.014078926295\n",
      "Epoch: 2044 | training loss: 0.014058322646\n",
      "Epoch: 2045 | training loss: 0.014037754387\n",
      "Epoch: 2046 | training loss: 0.014017220587\n",
      "Epoch: 2047 | training loss: 0.013996720314\n",
      "Epoch: 2048 | training loss: 0.013976251706\n",
      "Epoch: 2049 | training loss: 0.013955823146\n",
      "Epoch: 2050 | training loss: 0.013935426250\n",
      "Epoch: 2051 | training loss: 0.013915061951\n",
      "Epoch: 2052 | training loss: 0.013894730248\n",
      "Epoch: 2053 | training loss: 0.013874439523\n",
      "Epoch: 2054 | training loss: 0.013854179531\n",
      "Epoch: 2055 | training loss: 0.013833952136\n",
      "Epoch: 2056 | training loss: 0.013813757338\n",
      "Epoch: 2057 | training loss: 0.013793598861\n",
      "Epoch: 2058 | training loss: 0.013773479499\n",
      "Epoch: 2059 | training loss: 0.013753382489\n",
      "Epoch: 2060 | training loss: 0.013733327389\n",
      "Epoch: 2061 | training loss: 0.013713303022\n",
      "Epoch: 2062 | training loss: 0.013693315908\n",
      "Epoch: 2063 | training loss: 0.013673356734\n",
      "Epoch: 2064 | training loss: 0.013653433882\n",
      "Epoch: 2065 | training loss: 0.013633543625\n",
      "Epoch: 2066 | training loss: 0.013613691553\n",
      "Epoch: 2067 | training loss: 0.013593870215\n",
      "Epoch: 2068 | training loss: 0.013574080542\n",
      "Epoch: 2069 | training loss: 0.013554326259\n",
      "Epoch: 2070 | training loss: 0.013534610160\n",
      "Epoch: 2071 | training loss: 0.013514918275\n",
      "Epoch: 2072 | training loss: 0.013495266438\n",
      "Epoch: 2073 | training loss: 0.013475645334\n",
      "Epoch: 2074 | training loss: 0.013456060551\n",
      "Epoch: 2075 | training loss: 0.013436503708\n",
      "Epoch: 2076 | training loss: 0.013416984119\n",
      "Epoch: 2077 | training loss: 0.013397498056\n",
      "Epoch: 2078 | training loss: 0.013378046453\n",
      "Epoch: 2079 | training loss: 0.013358623721\n",
      "Epoch: 2080 | training loss: 0.013339232653\n",
      "Epoch: 2081 | training loss: 0.013319878839\n",
      "Epoch: 2082 | training loss: 0.013300555758\n",
      "Epoch: 2083 | training loss: 0.013281262480\n",
      "Epoch: 2084 | training loss: 0.013262003660\n",
      "Epoch: 2085 | training loss: 0.013242780231\n",
      "Epoch: 2086 | training loss: 0.013223588467\n",
      "Epoch: 2087 | training loss: 0.013204426505\n",
      "Epoch: 2088 | training loss: 0.013185298070\n",
      "Epoch: 2089 | training loss: 0.013166205958\n",
      "Epoch: 2090 | training loss: 0.013147145510\n",
      "Epoch: 2091 | training loss: 0.013128114864\n",
      "Epoch: 2092 | training loss: 0.013109114952\n",
      "Epoch: 2093 | training loss: 0.013090153225\n",
      "Epoch: 2094 | training loss: 0.013071223162\n",
      "Epoch: 2095 | training loss: 0.013052321039\n",
      "Epoch: 2096 | training loss: 0.013033454306\n",
      "Epoch: 2097 | training loss: 0.013014619239\n",
      "Epoch: 2098 | training loss: 0.012995819561\n",
      "Epoch: 2099 | training loss: 0.012977047823\n",
      "Epoch: 2100 | training loss: 0.012958307751\n",
      "Epoch: 2101 | training loss: 0.012939603068\n",
      "Epoch: 2102 | training loss: 0.012920928188\n",
      "Epoch: 2103 | training loss: 0.012902285904\n",
      "Epoch: 2104 | training loss: 0.012883672491\n",
      "Epoch: 2105 | training loss: 0.012865098193\n",
      "Epoch: 2106 | training loss: 0.012846552767\n",
      "Epoch: 2107 | training loss: 0.012828037143\n",
      "Epoch: 2108 | training loss: 0.012809551321\n",
      "Epoch: 2109 | training loss: 0.012791099027\n",
      "Epoch: 2110 | training loss: 0.012772683054\n",
      "Epoch: 2111 | training loss: 0.012754289433\n",
      "Epoch: 2112 | training loss: 0.012735932134\n",
      "Epoch: 2113 | training loss: 0.012717605568\n",
      "Epoch: 2114 | training loss: 0.012699313462\n",
      "Epoch: 2115 | training loss: 0.012681046501\n",
      "Epoch: 2116 | training loss: 0.012662815861\n",
      "Epoch: 2117 | training loss: 0.012644614093\n",
      "Epoch: 2118 | training loss: 0.012626447715\n",
      "Epoch: 2119 | training loss: 0.012608306482\n",
      "Epoch: 2120 | training loss: 0.012590200640\n",
      "Epoch: 2121 | training loss: 0.012572122738\n",
      "Epoch: 2122 | training loss: 0.012554082088\n",
      "Epoch: 2123 | training loss: 0.012536065653\n",
      "Epoch: 2124 | training loss: 0.012518084608\n",
      "Epoch: 2125 | training loss: 0.012500131503\n",
      "Epoch: 2126 | training loss: 0.012482213788\n",
      "Epoch: 2127 | training loss: 0.012464322150\n",
      "Epoch: 2128 | training loss: 0.012446463108\n",
      "Epoch: 2129 | training loss: 0.012428632937\n",
      "Epoch: 2130 | training loss: 0.012410838157\n",
      "Epoch: 2131 | training loss: 0.012393069454\n",
      "Epoch: 2132 | training loss: 0.012375332415\n",
      "Epoch: 2133 | training loss: 0.012357626110\n",
      "Epoch: 2134 | training loss: 0.012339956127\n",
      "Epoch: 2135 | training loss: 0.012322309427\n",
      "Epoch: 2136 | training loss: 0.012304695323\n",
      "Epoch: 2137 | training loss: 0.012287110090\n",
      "Epoch: 2138 | training loss: 0.012269560248\n",
      "Epoch: 2139 | training loss: 0.012252035551\n",
      "Epoch: 2140 | training loss: 0.012234544381\n",
      "Epoch: 2141 | training loss: 0.012217080221\n",
      "Epoch: 2142 | training loss: 0.012199651450\n",
      "Epoch: 2143 | training loss: 0.012182245031\n",
      "Epoch: 2144 | training loss: 0.012164870277\n",
      "Epoch: 2145 | training loss: 0.012147529982\n",
      "Epoch: 2146 | training loss: 0.012130216695\n",
      "Epoch: 2147 | training loss: 0.012112933211\n",
      "Epoch: 2148 | training loss: 0.012095678598\n",
      "Epoch: 2149 | training loss: 0.012078454718\n",
      "Epoch: 2150 | training loss: 0.012061263435\n",
      "Epoch: 2151 | training loss: 0.012044098228\n",
      "Epoch: 2152 | training loss: 0.012026960962\n",
      "Epoch: 2153 | training loss: 0.012009859085\n",
      "Epoch: 2154 | training loss: 0.011992784217\n",
      "Epoch: 2155 | training loss: 0.011975740083\n",
      "Epoch: 2156 | training loss: 0.011958723888\n",
      "Epoch: 2157 | training loss: 0.011941738427\n",
      "Epoch: 2158 | training loss: 0.011924782768\n",
      "Epoch: 2159 | training loss: 0.011907855049\n",
      "Epoch: 2160 | training loss: 0.011890957132\n",
      "Epoch: 2161 | training loss: 0.011874092743\n",
      "Epoch: 2162 | training loss: 0.011857255362\n",
      "Epoch: 2163 | training loss: 0.011840443127\n",
      "Epoch: 2164 | training loss: 0.011823660694\n",
      "Epoch: 2165 | training loss: 0.011806910858\n",
      "Epoch: 2166 | training loss: 0.011790189892\n",
      "Epoch: 2167 | training loss: 0.011773498729\n",
      "Epoch: 2168 | training loss: 0.011756832711\n",
      "Epoch: 2169 | training loss: 0.011740197428\n",
      "Epoch: 2170 | training loss: 0.011723594740\n",
      "Epoch: 2171 | training loss: 0.011707016267\n",
      "Epoch: 2172 | training loss: 0.011690468527\n",
      "Epoch: 2173 | training loss: 0.011673952453\n",
      "Epoch: 2174 | training loss: 0.011657458730\n",
      "Epoch: 2175 | training loss: 0.011640994810\n",
      "Epoch: 2176 | training loss: 0.011624560691\n",
      "Epoch: 2177 | training loss: 0.011608158238\n",
      "Epoch: 2178 | training loss: 0.011591784656\n",
      "Epoch: 2179 | training loss: 0.011575434357\n",
      "Epoch: 2180 | training loss: 0.011559113860\n",
      "Epoch: 2181 | training loss: 0.011542825028\n",
      "Epoch: 2182 | training loss: 0.011526564136\n",
      "Epoch: 2183 | training loss: 0.011510328390\n",
      "Epoch: 2184 | training loss: 0.011494123377\n",
      "Epoch: 2185 | training loss: 0.011477947235\n",
      "Epoch: 2186 | training loss: 0.011461799033\n",
      "Epoch: 2187 | training loss: 0.011445677839\n",
      "Epoch: 2188 | training loss: 0.011429585516\n",
      "Epoch: 2189 | training loss: 0.011413522996\n",
      "Epoch: 2190 | training loss: 0.011397486553\n",
      "Epoch: 2191 | training loss: 0.011381476186\n",
      "Epoch: 2192 | training loss: 0.011365495622\n",
      "Epoch: 2193 | training loss: 0.011349542066\n",
      "Epoch: 2194 | training loss: 0.011333618313\n",
      "Epoch: 2195 | training loss: 0.011317718774\n",
      "Epoch: 2196 | training loss: 0.011301847175\n",
      "Epoch: 2197 | training loss: 0.011286003515\n",
      "Epoch: 2198 | training loss: 0.011270192452\n",
      "Epoch: 2199 | training loss: 0.011254402809\n",
      "Epoch: 2200 | training loss: 0.011238642968\n",
      "Epoch: 2201 | training loss: 0.011222911067\n",
      "Epoch: 2202 | training loss: 0.011207206175\n",
      "Epoch: 2203 | training loss: 0.011191528291\n",
      "Epoch: 2204 | training loss: 0.011175878346\n",
      "Epoch: 2205 | training loss: 0.011160256341\n",
      "Epoch: 2206 | training loss: 0.011144666001\n",
      "Epoch: 2207 | training loss: 0.011129095219\n",
      "Epoch: 2208 | training loss: 0.011113554239\n",
      "Epoch: 2209 | training loss: 0.011098039337\n",
      "Epoch: 2210 | training loss: 0.011082557961\n",
      "Epoch: 2211 | training loss: 0.011067096144\n",
      "Epoch: 2212 | training loss: 0.011051665992\n",
      "Epoch: 2213 | training loss: 0.011036261916\n",
      "Epoch: 2214 | training loss: 0.011020884849\n",
      "Epoch: 2215 | training loss: 0.011005532928\n",
      "Epoch: 2216 | training loss: 0.010990209877\n",
      "Epoch: 2217 | training loss: 0.010974913836\n",
      "Epoch: 2218 | training loss: 0.010959648527\n",
      "Epoch: 2219 | training loss: 0.010944402777\n",
      "Epoch: 2220 | training loss: 0.010929186828\n",
      "Epoch: 2221 | training loss: 0.010913996957\n",
      "Epoch: 2222 | training loss: 0.010898833163\n",
      "Epoch: 2223 | training loss: 0.010883693583\n",
      "Epoch: 2224 | training loss: 0.010868582875\n",
      "Epoch: 2225 | training loss: 0.010853498243\n",
      "Epoch: 2226 | training loss: 0.010838440619\n",
      "Epoch: 2227 | training loss: 0.010823408142\n",
      "Epoch: 2228 | training loss: 0.010808405466\n",
      "Epoch: 2229 | training loss: 0.010793427937\n",
      "Epoch: 2230 | training loss: 0.010778473690\n",
      "Epoch: 2231 | training loss: 0.010763545521\n",
      "Epoch: 2232 | training loss: 0.010748646222\n",
      "Epoch: 2233 | training loss: 0.010733774863\n",
      "Epoch: 2234 | training loss: 0.010718926787\n",
      "Epoch: 2235 | training loss: 0.010704104789\n",
      "Epoch: 2236 | training loss: 0.010689309798\n",
      "Epoch: 2237 | training loss: 0.010674540885\n",
      "Epoch: 2238 | training loss: 0.010659795254\n",
      "Epoch: 2239 | training loss: 0.010645076632\n",
      "Epoch: 2240 | training loss: 0.010630386882\n",
      "Epoch: 2241 | training loss: 0.010615723208\n",
      "Epoch: 2242 | training loss: 0.010601080954\n",
      "Epoch: 2243 | training loss: 0.010586466640\n",
      "Epoch: 2244 | training loss: 0.010571877472\n",
      "Epoch: 2245 | training loss: 0.010557318106\n",
      "Epoch: 2246 | training loss: 0.010542778298\n",
      "Epoch: 2247 | training loss: 0.010528265499\n",
      "Epoch: 2248 | training loss: 0.010513779707\n",
      "Epoch: 2249 | training loss: 0.010499321856\n",
      "Epoch: 2250 | training loss: 0.010484885424\n",
      "Epoch: 2251 | training loss: 0.010470473208\n",
      "Epoch: 2252 | training loss: 0.010456088930\n",
      "Epoch: 2253 | training loss: 0.010441730730\n",
      "Epoch: 2254 | training loss: 0.010427392088\n",
      "Epoch: 2255 | training loss: 0.010413081385\n",
      "Epoch: 2256 | training loss: 0.010398796760\n",
      "Epoch: 2257 | training loss: 0.010384539142\n",
      "Epoch: 2258 | training loss: 0.010370302014\n",
      "Epoch: 2259 | training loss: 0.010356093757\n",
      "Epoch: 2260 | training loss: 0.010341908783\n",
      "Epoch: 2261 | training loss: 0.010327746160\n",
      "Epoch: 2262 | training loss: 0.010313608684\n",
      "Epoch: 2263 | training loss: 0.010299498215\n",
      "Epoch: 2264 | training loss: 0.010285412893\n",
      "Epoch: 2265 | training loss: 0.010271351784\n",
      "Epoch: 2266 | training loss: 0.010257313028\n",
      "Epoch: 2267 | training loss: 0.010243302211\n",
      "Epoch: 2268 | training loss: 0.010229313746\n",
      "Epoch: 2269 | training loss: 0.010215353221\n",
      "Epoch: 2270 | training loss: 0.010201413184\n",
      "Epoch: 2271 | training loss: 0.010187502950\n",
      "Epoch: 2272 | training loss: 0.010173613206\n",
      "Epoch: 2273 | training loss: 0.010159748606\n",
      "Epoch: 2274 | training loss: 0.010145907290\n",
      "Epoch: 2275 | training loss: 0.010132093914\n",
      "Epoch: 2276 | training loss: 0.010118301958\n",
      "Epoch: 2277 | training loss: 0.010104536079\n",
      "Epoch: 2278 | training loss: 0.010090789758\n",
      "Epoch: 2279 | training loss: 0.010077075101\n",
      "Epoch: 2280 | training loss: 0.010063381866\n",
      "Epoch: 2281 | training loss: 0.010049710982\n",
      "Epoch: 2282 | training loss: 0.010036065243\n",
      "Epoch: 2283 | training loss: 0.010022445582\n",
      "Epoch: 2284 | training loss: 0.010008843616\n",
      "Epoch: 2285 | training loss: 0.009995267726\n",
      "Epoch: 2286 | training loss: 0.009981716983\n",
      "Epoch: 2287 | training loss: 0.009968194179\n",
      "Epoch: 2288 | training loss: 0.009954688139\n",
      "Epoch: 2289 | training loss: 0.009941208176\n",
      "Epoch: 2290 | training loss: 0.009927752428\n",
      "Epoch: 2291 | training loss: 0.009914322756\n",
      "Epoch: 2292 | training loss: 0.009900912642\n",
      "Epoch: 2293 | training loss: 0.009887528606\n",
      "Epoch: 2294 | training loss: 0.009874166921\n",
      "Epoch: 2295 | training loss: 0.009860832244\n",
      "Epoch: 2296 | training loss: 0.009847515263\n",
      "Epoch: 2297 | training loss: 0.009834223427\n",
      "Epoch: 2298 | training loss: 0.009820956737\n",
      "Epoch: 2299 | training loss: 0.009807711467\n",
      "Epoch: 2300 | training loss: 0.009794489481\n",
      "Epoch: 2301 | training loss: 0.009781293571\n",
      "Epoch: 2302 | training loss: 0.009768119082\n",
      "Epoch: 2303 | training loss: 0.009754966013\n",
      "Epoch: 2304 | training loss: 0.009741837159\n",
      "Epoch: 2305 | training loss: 0.009728730656\n",
      "Epoch: 2306 | training loss: 0.009715650231\n",
      "Epoch: 2307 | training loss: 0.009702588432\n",
      "Epoch: 2308 | training loss: 0.009689548984\n",
      "Epoch: 2309 | training loss: 0.009676535614\n",
      "Epoch: 2310 | training loss: 0.009663543664\n",
      "Epoch: 2311 | training loss: 0.009650573134\n",
      "Epoch: 2312 | training loss: 0.009637628682\n",
      "Epoch: 2313 | training loss: 0.009624707513\n",
      "Epoch: 2314 | training loss: 0.009611804970\n",
      "Epoch: 2315 | training loss: 0.009598923847\n",
      "Epoch: 2316 | training loss: 0.009586070664\n",
      "Epoch: 2317 | training loss: 0.009573237970\n",
      "Epoch: 2318 | training loss: 0.009560428560\n",
      "Epoch: 2319 | training loss: 0.009547637776\n",
      "Epoch: 2320 | training loss: 0.009534874000\n",
      "Epoch: 2321 | training loss: 0.009522131644\n",
      "Epoch: 2322 | training loss: 0.009509410709\n",
      "Epoch: 2323 | training loss: 0.009496712126\n",
      "Epoch: 2324 | training loss: 0.009484038688\n",
      "Epoch: 2325 | training loss: 0.009471380152\n",
      "Epoch: 2326 | training loss: 0.009458749555\n",
      "Epoch: 2327 | training loss: 0.009446139447\n",
      "Epoch: 2328 | training loss: 0.009433551691\n",
      "Epoch: 2329 | training loss: 0.009420985356\n",
      "Epoch: 2330 | training loss: 0.009408443235\n",
      "Epoch: 2331 | training loss: 0.009395922534\n",
      "Epoch: 2332 | training loss: 0.009383425117\n",
      "Epoch: 2333 | training loss: 0.009370943531\n",
      "Epoch: 2334 | training loss: 0.009358490817\n",
      "Epoch: 2335 | training loss: 0.009346056730\n",
      "Epoch: 2336 | training loss: 0.009333644994\n",
      "Epoch: 2337 | training loss: 0.009321253747\n",
      "Epoch: 2338 | training loss: 0.009308886714\n",
      "Epoch: 2339 | training loss: 0.009296540171\n",
      "Epoch: 2340 | training loss: 0.009284214117\n",
      "Epoch: 2341 | training loss: 0.009271910414\n",
      "Epoch: 2342 | training loss: 0.009259629063\n",
      "Epoch: 2343 | training loss: 0.009247372858\n",
      "Epoch: 2344 | training loss: 0.009235132486\n",
      "Epoch: 2345 | training loss: 0.009222914465\n",
      "Epoch: 2346 | training loss: 0.009210716002\n",
      "Epoch: 2347 | training loss: 0.009198538959\n",
      "Epoch: 2348 | training loss: 0.009186383337\n",
      "Epoch: 2349 | training loss: 0.009174250998\n",
      "Epoch: 2350 | training loss: 0.009162137285\n",
      "Epoch: 2351 | training loss: 0.009150046855\n",
      "Epoch: 2352 | training loss: 0.009137975983\n",
      "Epoch: 2353 | training loss: 0.009125929326\n",
      "Epoch: 2354 | training loss: 0.009113898501\n",
      "Epoch: 2355 | training loss: 0.009101890959\n",
      "Epoch: 2356 | training loss: 0.009089904837\n",
      "Epoch: 2357 | training loss: 0.009077941999\n",
      "Epoch: 2358 | training loss: 0.009065994993\n",
      "Epoch: 2359 | training loss: 0.009054072201\n",
      "Epoch: 2360 | training loss: 0.009042171761\n",
      "Epoch: 2361 | training loss: 0.009030290879\n",
      "Epoch: 2362 | training loss: 0.009018429555\n",
      "Epoch: 2363 | training loss: 0.009006589651\n",
      "Epoch: 2364 | training loss: 0.008994773030\n",
      "Epoch: 2365 | training loss: 0.008982973173\n",
      "Epoch: 2366 | training loss: 0.008971195668\n",
      "Epoch: 2367 | training loss: 0.008959438652\n",
      "Epoch: 2368 | training loss: 0.008947702125\n",
      "Epoch: 2369 | training loss: 0.008935985155\n",
      "Epoch: 2370 | training loss: 0.008924288675\n",
      "Epoch: 2371 | training loss: 0.008912615478\n",
      "Epoch: 2372 | training loss: 0.008900959045\n",
      "Epoch: 2373 | training loss: 0.008889323100\n",
      "Epoch: 2374 | training loss: 0.008877708577\n",
      "Epoch: 2375 | training loss: 0.008866116405\n",
      "Epoch: 2376 | training loss: 0.008854541928\n",
      "Epoch: 2377 | training loss: 0.008842986077\n",
      "Epoch: 2378 | training loss: 0.008831453510\n",
      "Epoch: 2379 | training loss: 0.008819934912\n",
      "Epoch: 2380 | training loss: 0.008808439597\n",
      "Epoch: 2381 | training loss: 0.008796963841\n",
      "Epoch: 2382 | training loss: 0.008785512298\n",
      "Epoch: 2383 | training loss: 0.008774075657\n",
      "Epoch: 2384 | training loss: 0.008762659505\n",
      "Epoch: 2385 | training loss: 0.008751264773\n",
      "Epoch: 2386 | training loss: 0.008739886805\n",
      "Epoch: 2387 | training loss: 0.008728531189\n",
      "Epoch: 2388 | training loss: 0.008717196062\n",
      "Epoch: 2389 | training loss: 0.008705881424\n",
      "Epoch: 2390 | training loss: 0.008694583550\n",
      "Epoch: 2391 | training loss: 0.008683305234\n",
      "Epoch: 2392 | training loss: 0.008672049269\n",
      "Epoch: 2393 | training loss: 0.008660810068\n",
      "Epoch: 2394 | training loss: 0.008649592288\n",
      "Epoch: 2395 | training loss: 0.008638394997\n",
      "Epoch: 2396 | training loss: 0.008627213538\n",
      "Epoch: 2397 | training loss: 0.008616052568\n",
      "Epoch: 2398 | training loss: 0.008604913019\n",
      "Epoch: 2399 | training loss: 0.008593792096\n",
      "Epoch: 2400 | training loss: 0.008582689799\n",
      "Epoch: 2401 | training loss: 0.008571607992\n",
      "Epoch: 2402 | training loss: 0.008560543880\n",
      "Epoch: 2403 | training loss: 0.008549496531\n",
      "Epoch: 2404 | training loss: 0.008538469672\n",
      "Epoch: 2405 | training loss: 0.008527463302\n",
      "Epoch: 2406 | training loss: 0.008516477421\n",
      "Epoch: 2407 | training loss: 0.008505506441\n",
      "Epoch: 2408 | training loss: 0.008494555950\n",
      "Epoch: 2409 | training loss: 0.008483625948\n",
      "Epoch: 2410 | training loss: 0.008472711779\n",
      "Epoch: 2411 | training loss: 0.008461819030\n",
      "Epoch: 2412 | training loss: 0.008450945839\n",
      "Epoch: 2413 | training loss: 0.008440089412\n",
      "Epoch: 2414 | training loss: 0.008429249749\n",
      "Epoch: 2415 | training loss: 0.008418431506\n",
      "Epoch: 2416 | training loss: 0.008407634683\n",
      "Epoch: 2417 | training loss: 0.008396852762\n",
      "Epoch: 2418 | training loss: 0.008386089467\n",
      "Epoch: 2419 | training loss: 0.008375345729\n",
      "Epoch: 2420 | training loss: 0.008364619687\n",
      "Epoch: 2421 | training loss: 0.008353914134\n",
      "Epoch: 2422 | training loss: 0.008343225345\n",
      "Epoch: 2423 | training loss: 0.008332557045\n",
      "Epoch: 2424 | training loss: 0.008321903646\n",
      "Epoch: 2425 | training loss: 0.008311267942\n",
      "Epoch: 2426 | training loss: 0.008300654590\n",
      "Epoch: 2427 | training loss: 0.008290056139\n",
      "Epoch: 2428 | training loss: 0.008279479109\n",
      "Epoch: 2429 | training loss: 0.008268917911\n",
      "Epoch: 2430 | training loss: 0.008258374408\n",
      "Epoch: 2431 | training loss: 0.008247847669\n",
      "Epoch: 2432 | training loss: 0.008237342350\n",
      "Epoch: 2433 | training loss: 0.008226852864\n",
      "Epoch: 2434 | training loss: 0.008216382004\n",
      "Epoch: 2435 | training loss: 0.008205929771\n",
      "Epoch: 2436 | training loss: 0.008195497096\n",
      "Epoch: 2437 | training loss: 0.008185079321\n",
      "Epoch: 2438 | training loss: 0.008174681105\n",
      "Epoch: 2439 | training loss: 0.008164302446\n",
      "Epoch: 2440 | training loss: 0.008153938688\n",
      "Epoch: 2441 | training loss: 0.008143592626\n",
      "Epoch: 2442 | training loss: 0.008133265190\n",
      "Epoch: 2443 | training loss: 0.008122953586\n",
      "Epoch: 2444 | training loss: 0.008112659678\n",
      "Epoch: 2445 | training loss: 0.008102385327\n",
      "Epoch: 2446 | training loss: 0.008092124015\n",
      "Epoch: 2447 | training loss: 0.008081884123\n",
      "Epoch: 2448 | training loss: 0.008071660995\n",
      "Epoch: 2449 | training loss: 0.008061459288\n",
      "Epoch: 2450 | training loss: 0.008051268756\n",
      "Epoch: 2451 | training loss: 0.008041099645\n",
      "Epoch: 2452 | training loss: 0.008030947298\n",
      "Epoch: 2453 | training loss: 0.008020811714\n",
      "Epoch: 2454 | training loss: 0.008010692894\n",
      "Epoch: 2455 | training loss: 0.008000594564\n",
      "Epoch: 2456 | training loss: 0.007990509272\n",
      "Epoch: 2457 | training loss: 0.007980442606\n",
      "Epoch: 2458 | training loss: 0.007970393635\n",
      "Epoch: 2459 | training loss: 0.007960359566\n",
      "Epoch: 2460 | training loss: 0.007950343192\n",
      "Epoch: 2461 | training loss: 0.007940343581\n",
      "Epoch: 2462 | training loss: 0.007930360734\n",
      "Epoch: 2463 | training loss: 0.007920393720\n",
      "Epoch: 2464 | training loss: 0.007910447195\n",
      "Epoch: 2465 | training loss: 0.007900513709\n",
      "Epoch: 2466 | training loss: 0.007890598848\n",
      "Epoch: 2467 | training loss: 0.007880701683\n",
      "Epoch: 2468 | training loss: 0.007870823145\n",
      "Epoch: 2469 | training loss: 0.007860958576\n",
      "Epoch: 2470 | training loss: 0.007851112634\n",
      "Epoch: 2471 | training loss: 0.007841283455\n",
      "Epoch: 2472 | training loss: 0.007831469178\n",
      "Epoch: 2473 | training loss: 0.007821672596\n",
      "Epoch: 2474 | training loss: 0.007811892312\n",
      "Epoch: 2475 | training loss: 0.007802128326\n",
      "Epoch: 2476 | training loss: 0.007792378776\n",
      "Epoch: 2477 | training loss: 0.007782650180\n",
      "Epoch: 2478 | training loss: 0.007772932295\n",
      "Epoch: 2479 | training loss: 0.007763233967\n",
      "Epoch: 2480 | training loss: 0.007753552403\n",
      "Epoch: 2481 | training loss: 0.007743887138\n",
      "Epoch: 2482 | training loss: 0.007734235842\n",
      "Epoch: 2483 | training loss: 0.007724605501\n",
      "Epoch: 2484 | training loss: 0.007714986335\n",
      "Epoch: 2485 | training loss: 0.007705387194\n",
      "Epoch: 2486 | training loss: 0.007695803419\n",
      "Epoch: 2487 | training loss: 0.007686235011\n",
      "Epoch: 2488 | training loss: 0.007676681504\n",
      "Epoch: 2489 | training loss: 0.007667145226\n",
      "Epoch: 2490 | training loss: 0.007657624781\n",
      "Epoch: 2491 | training loss: 0.007648120634\n",
      "Epoch: 2492 | training loss: 0.007638632320\n",
      "Epoch: 2493 | training loss: 0.007629159372\n",
      "Epoch: 2494 | training loss: 0.007619703189\n",
      "Epoch: 2495 | training loss: 0.007610262837\n",
      "Epoch: 2496 | training loss: 0.007600838784\n",
      "Epoch: 2497 | training loss: 0.007591429166\n",
      "Epoch: 2498 | training loss: 0.007582038175\n",
      "Epoch: 2499 | training loss: 0.007572658826\n",
      "Epoch: 2500 | training loss: 0.007563297637\n",
      "Epoch: 2501 | training loss: 0.007553953212\n",
      "Epoch: 2502 | training loss: 0.007544623688\n",
      "Epoch: 2503 | training loss: 0.007535308599\n",
      "Epoch: 2504 | training loss: 0.007526008878\n",
      "Epoch: 2505 | training loss: 0.007516728714\n",
      "Epoch: 2506 | training loss: 0.007507458329\n",
      "Epoch: 2507 | training loss: 0.007498207036\n",
      "Epoch: 2508 | training loss: 0.007488972042\n",
      "Epoch: 2509 | training loss: 0.007479750551\n",
      "Epoch: 2510 | training loss: 0.007470543962\n",
      "Epoch: 2511 | training loss: 0.007461355999\n",
      "Epoch: 2512 | training loss: 0.007452180143\n",
      "Epoch: 2513 | training loss: 0.007443018258\n",
      "Epoch: 2514 | training loss: 0.007433874998\n",
      "Epoch: 2515 | training loss: 0.007424746174\n",
      "Epoch: 2516 | training loss: 0.007415631320\n",
      "Epoch: 2517 | training loss: 0.007406529505\n",
      "Epoch: 2518 | training loss: 0.007397445850\n",
      "Epoch: 2519 | training loss: 0.007388377562\n",
      "Epoch: 2520 | training loss: 0.007379323244\n",
      "Epoch: 2521 | training loss: 0.007370282430\n",
      "Epoch: 2522 | training loss: 0.007361260243\n",
      "Epoch: 2523 | training loss: 0.007352250628\n",
      "Epoch: 2524 | training loss: 0.007343254518\n",
      "Epoch: 2525 | training loss: 0.007334276102\n",
      "Epoch: 2526 | training loss: 0.007325309794\n",
      "Epoch: 2527 | training loss: 0.007316359784\n",
      "Epoch: 2528 | training loss: 0.007307425141\n",
      "Epoch: 2529 | training loss: 0.007298505865\n",
      "Epoch: 2530 | training loss: 0.007289599627\n",
      "Epoch: 2531 | training loss: 0.007280711085\n",
      "Epoch: 2532 | training loss: 0.007271834183\n",
      "Epoch: 2533 | training loss: 0.007262974977\n",
      "Epoch: 2534 | training loss: 0.007254130207\n",
      "Epoch: 2535 | training loss: 0.007245296147\n",
      "Epoch: 2536 | training loss: 0.007236479782\n",
      "Epoch: 2537 | training loss: 0.007227677852\n",
      "Epoch: 2538 | training loss: 0.007218889426\n",
      "Epoch: 2539 | training loss: 0.007210114039\n",
      "Epoch: 2540 | training loss: 0.007201356348\n",
      "Epoch: 2541 | training loss: 0.007192611694\n",
      "Epoch: 2542 | training loss: 0.007183883339\n",
      "Epoch: 2543 | training loss: 0.007175168954\n",
      "Epoch: 2544 | training loss: 0.007166467607\n",
      "Epoch: 2545 | training loss: 0.007157781161\n",
      "Epoch: 2546 | training loss: 0.007149108220\n",
      "Epoch: 2547 | training loss: 0.007140450180\n",
      "Epoch: 2548 | training loss: 0.007131804712\n",
      "Epoch: 2549 | training loss: 0.007123173680\n",
      "Epoch: 2550 | training loss: 0.007114555687\n",
      "Epoch: 2551 | training loss: 0.007105955388\n",
      "Epoch: 2552 | training loss: 0.007097365800\n",
      "Epoch: 2553 | training loss: 0.007088793907\n",
      "Epoch: 2554 | training loss: 0.007080234587\n",
      "Epoch: 2555 | training loss: 0.007071686909\n",
      "Epoch: 2556 | training loss: 0.007063156459\n",
      "Epoch: 2557 | training loss: 0.007054639049\n",
      "Epoch: 2558 | training loss: 0.007046134211\n",
      "Epoch: 2559 | training loss: 0.007037643809\n",
      "Epoch: 2560 | training loss: 0.007029169239\n",
      "Epoch: 2561 | training loss: 0.007020705845\n",
      "Epoch: 2562 | training loss: 0.007012258284\n",
      "Epoch: 2563 | training loss: 0.007003822364\n",
      "Epoch: 2564 | training loss: 0.006995399483\n",
      "Epoch: 2565 | training loss: 0.006986993831\n",
      "Epoch: 2566 | training loss: 0.006978597492\n",
      "Epoch: 2567 | training loss: 0.006970218848\n",
      "Epoch: 2568 | training loss: 0.006961854175\n",
      "Epoch: 2569 | training loss: 0.006953500677\n",
      "Epoch: 2570 | training loss: 0.006945161149\n",
      "Epoch: 2571 | training loss: 0.006936837919\n",
      "Epoch: 2572 | training loss: 0.006928524002\n",
      "Epoch: 2573 | training loss: 0.006920225453\n",
      "Epoch: 2574 | training loss: 0.006911942270\n",
      "Epoch: 2575 | training loss: 0.006903668866\n",
      "Epoch: 2576 | training loss: 0.006895412225\n",
      "Epoch: 2577 | training loss: 0.006887166295\n",
      "Epoch: 2578 | training loss: 0.006878936198\n",
      "Epoch: 2579 | training loss: 0.006870717742\n",
      "Epoch: 2580 | training loss: 0.006862512324\n",
      "Epoch: 2581 | training loss: 0.006854321808\n",
      "Epoch: 2582 | training loss: 0.006846144330\n",
      "Epoch: 2583 | training loss: 0.006837977562\n",
      "Epoch: 2584 | training loss: 0.006829825230\n",
      "Epoch: 2585 | training loss: 0.006821690127\n",
      "Epoch: 2586 | training loss: 0.006813564338\n",
      "Epoch: 2587 | training loss: 0.006805452053\n",
      "Epoch: 2588 | training loss: 0.006797351409\n",
      "Epoch: 2589 | training loss: 0.006789264735\n",
      "Epoch: 2590 | training loss: 0.006781193428\n",
      "Epoch: 2591 | training loss: 0.006773130968\n",
      "Epoch: 2592 | training loss: 0.006765083876\n",
      "Epoch: 2593 | training loss: 0.006757051218\n",
      "Epoch: 2594 | training loss: 0.006749029737\n",
      "Epoch: 2595 | training loss: 0.006741021760\n",
      "Epoch: 2596 | training loss: 0.006733024959\n",
      "Epoch: 2597 | training loss: 0.006725043524\n",
      "Epoch: 2598 | training loss: 0.006717072800\n",
      "Epoch: 2599 | training loss: 0.006709116045\n",
      "Epoch: 2600 | training loss: 0.006701170467\n",
      "Epoch: 2601 | training loss: 0.006693241652\n",
      "Epoch: 2602 | training loss: 0.006685322151\n",
      "Epoch: 2603 | training loss: 0.006677417085\n",
      "Epoch: 2604 | training loss: 0.006669522263\n",
      "Epoch: 2605 | training loss: 0.006661641411\n",
      "Epoch: 2606 | training loss: 0.006653773598\n",
      "Epoch: 2607 | training loss: 0.006645917427\n",
      "Epoch: 2608 | training loss: 0.006638076156\n",
      "Epoch: 2609 | training loss: 0.006630246527\n",
      "Epoch: 2610 | training loss: 0.006622429006\n",
      "Epoch: 2611 | training loss: 0.006614624057\n",
      "Epoch: 2612 | training loss: 0.006606828887\n",
      "Epoch: 2613 | training loss: 0.006599048153\n",
      "Epoch: 2614 | training loss: 0.006591280922\n",
      "Epoch: 2615 | training loss: 0.006583525799\n",
      "Epoch: 2616 | training loss: 0.006575781852\n",
      "Epoch: 2617 | training loss: 0.006568049546\n",
      "Epoch: 2618 | training loss: 0.006560331676\n",
      "Epoch: 2619 | training loss: 0.006552624982\n",
      "Epoch: 2620 | training loss: 0.006544928998\n",
      "Epoch: 2621 | training loss: 0.006537246983\n",
      "Epoch: 2622 | training loss: 0.006529579405\n",
      "Epoch: 2623 | training loss: 0.006521920208\n",
      "Epoch: 2624 | training loss: 0.006514276378\n",
      "Epoch: 2625 | training loss: 0.006506641861\n",
      "Epoch: 2626 | training loss: 0.006499020383\n",
      "Epoch: 2627 | training loss: 0.006491412874\n",
      "Epoch: 2628 | training loss: 0.006483813748\n",
      "Epoch: 2629 | training loss: 0.006476229057\n",
      "Epoch: 2630 | training loss: 0.006468655542\n",
      "Epoch: 2631 | training loss: 0.006461095531\n",
      "Epoch: 2632 | training loss: 0.006453547161\n",
      "Epoch: 2633 | training loss: 0.006446008570\n",
      "Epoch: 2634 | training loss: 0.006438484415\n",
      "Epoch: 2635 | training loss: 0.006430971902\n",
      "Epoch: 2636 | training loss: 0.006423470564\n",
      "Epoch: 2637 | training loss: 0.006415981334\n",
      "Epoch: 2638 | training loss: 0.006408504676\n",
      "Epoch: 2639 | training loss: 0.006401038263\n",
      "Epoch: 2640 | training loss: 0.006393585820\n",
      "Epoch: 2641 | training loss: 0.006386143155\n",
      "Epoch: 2642 | training loss: 0.006378712133\n",
      "Epoch: 2643 | training loss: 0.006371292751\n",
      "Epoch: 2644 | training loss: 0.006363885943\n",
      "Epoch: 2645 | training loss: 0.006356490776\n",
      "Epoch: 2646 | training loss: 0.006349106319\n",
      "Epoch: 2647 | training loss: 0.006341732573\n",
      "Epoch: 2648 | training loss: 0.006334372330\n",
      "Epoch: 2649 | training loss: 0.006327022798\n",
      "Epoch: 2650 | training loss: 0.006319687702\n",
      "Epoch: 2651 | training loss: 0.006312360521\n",
      "Epoch: 2652 | training loss: 0.006305045448\n",
      "Epoch: 2653 | training loss: 0.006297741551\n",
      "Epoch: 2654 | training loss: 0.006290448830\n",
      "Epoch: 2655 | training loss: 0.006283169612\n",
      "Epoch: 2656 | training loss: 0.006275900640\n",
      "Epoch: 2657 | training loss: 0.006268644705\n",
      "Epoch: 2658 | training loss: 0.006261397619\n",
      "Epoch: 2659 | training loss: 0.006254162174\n",
      "Epoch: 2660 | training loss: 0.006246940233\n",
      "Epoch: 2661 | training loss: 0.006239727139\n",
      "Epoch: 2662 | training loss: 0.006232526619\n",
      "Epoch: 2663 | training loss: 0.006225335877\n",
      "Epoch: 2664 | training loss: 0.006218157709\n",
      "Epoch: 2665 | training loss: 0.006210991647\n",
      "Epoch: 2666 | training loss: 0.006203834433\n",
      "Epoch: 2667 | training loss: 0.006196691655\n",
      "Epoch: 2668 | training loss: 0.006189557258\n",
      "Epoch: 2669 | training loss: 0.006182435900\n",
      "Epoch: 2670 | training loss: 0.006175325718\n",
      "Epoch: 2671 | training loss: 0.006168224383\n",
      "Epoch: 2672 | training loss: 0.006161135156\n",
      "Epoch: 2673 | training loss: 0.006154056173\n",
      "Epoch: 2674 | training loss: 0.006146989763\n",
      "Epoch: 2675 | training loss: 0.006139932200\n",
      "Epoch: 2676 | training loss: 0.006132885348\n",
      "Epoch: 2677 | training loss: 0.006125852000\n",
      "Epoch: 2678 | training loss: 0.006118826568\n",
      "Epoch: 2679 | training loss: 0.006111815572\n",
      "Epoch: 2680 | training loss: 0.006104813423\n",
      "Epoch: 2681 | training loss: 0.006097821053\n",
      "Epoch: 2682 | training loss: 0.006090842187\n",
      "Epoch: 2683 | training loss: 0.006083871704\n",
      "Epoch: 2684 | training loss: 0.006076914724\n",
      "Epoch: 2685 | training loss: 0.006069965195\n",
      "Epoch: 2686 | training loss: 0.006063029636\n",
      "Epoch: 2687 | training loss: 0.006056103855\n",
      "Epoch: 2688 | training loss: 0.006049187854\n",
      "Epoch: 2689 | training loss: 0.006042284425\n",
      "Epoch: 2690 | training loss: 0.006035388913\n",
      "Epoch: 2691 | training loss: 0.006028506905\n",
      "Epoch: 2692 | training loss: 0.006021634210\n",
      "Epoch: 2693 | training loss: 0.006014770828\n",
      "Epoch: 2694 | training loss: 0.006007919088\n",
      "Epoch: 2695 | training loss: 0.006001078058\n",
      "Epoch: 2696 | training loss: 0.005994247738\n",
      "Epoch: 2697 | training loss: 0.005987426266\n",
      "Epoch: 2698 | training loss: 0.005980616435\n",
      "Epoch: 2699 | training loss: 0.005973816384\n",
      "Epoch: 2700 | training loss: 0.005967027973\n",
      "Epoch: 2701 | training loss: 0.005960251670\n",
      "Epoch: 2702 | training loss: 0.005953482818\n",
      "Epoch: 2703 | training loss: 0.005946725607\n",
      "Epoch: 2704 | training loss: 0.005939978175\n",
      "Epoch: 2705 | training loss: 0.005933241453\n",
      "Epoch: 2706 | training loss: 0.005926515907\n",
      "Epoch: 2707 | training loss: 0.005919800606\n",
      "Epoch: 2708 | training loss: 0.005913095549\n",
      "Epoch: 2709 | training loss: 0.005906399339\n",
      "Epoch: 2710 | training loss: 0.005899713840\n",
      "Epoch: 2711 | training loss: 0.005893038586\n",
      "Epoch: 2712 | training loss: 0.005886374973\n",
      "Epoch: 2713 | training loss: 0.005879720673\n",
      "Epoch: 2714 | training loss: 0.005873075686\n",
      "Epoch: 2715 | training loss: 0.005866442807\n",
      "Epoch: 2716 | training loss: 0.005859818310\n",
      "Epoch: 2717 | training loss: 0.005853204988\n",
      "Epoch: 2718 | training loss: 0.005846599583\n",
      "Epoch: 2719 | training loss: 0.005840006750\n",
      "Epoch: 2720 | training loss: 0.005833422765\n",
      "Epoch: 2721 | training loss: 0.005826849956\n",
      "Epoch: 2722 | training loss: 0.005820287392\n",
      "Epoch: 2723 | training loss: 0.005813731812\n",
      "Epoch: 2724 | training loss: 0.005807189737\n",
      "Epoch: 2725 | training loss: 0.005800655577\n",
      "Epoch: 2726 | training loss: 0.005794132594\n",
      "Epoch: 2727 | training loss: 0.005787618458\n",
      "Epoch: 2728 | training loss: 0.005781113636\n",
      "Epoch: 2729 | training loss: 0.005774619989\n",
      "Epoch: 2730 | training loss: 0.005768135656\n",
      "Epoch: 2731 | training loss: 0.005761663429\n",
      "Epoch: 2732 | training loss: 0.005755199119\n",
      "Epoch: 2733 | training loss: 0.005748744123\n",
      "Epoch: 2734 | training loss: 0.005742298905\n",
      "Epoch: 2735 | training loss: 0.005735863466\n",
      "Epoch: 2736 | training loss: 0.005729439668\n",
      "Epoch: 2737 | training loss: 0.005723025184\n",
      "Epoch: 2738 | training loss: 0.005716620013\n",
      "Epoch: 2739 | training loss: 0.005710223690\n",
      "Epoch: 2740 | training loss: 0.005703838076\n",
      "Epoch: 2741 | training loss: 0.005697462708\n",
      "Epoch: 2742 | training loss: 0.005691097118\n",
      "Epoch: 2743 | training loss: 0.005684737582\n",
      "Epoch: 2744 | training loss: 0.005678391550\n",
      "Epoch: 2745 | training loss: 0.005672054831\n",
      "Epoch: 2746 | training loss: 0.005665727891\n",
      "Epoch: 2747 | training loss: 0.005659410264\n",
      "Epoch: 2748 | training loss: 0.005653099623\n",
      "Epoch: 2749 | training loss: 0.005646800622\n",
      "Epoch: 2750 | training loss: 0.005640510470\n",
      "Epoch: 2751 | training loss: 0.005634229165\n",
      "Epoch: 2752 | training loss: 0.005627957173\n",
      "Epoch: 2753 | training loss: 0.005621695891\n",
      "Epoch: 2754 | training loss: 0.005615445320\n",
      "Epoch: 2755 | training loss: 0.005609203130\n",
      "Epoch: 2756 | training loss: 0.005602969322\n",
      "Epoch: 2757 | training loss: 0.005596745294\n",
      "Epoch: 2758 | training loss: 0.005590531975\n",
      "Epoch: 2759 | training loss: 0.005584327038\n",
      "Epoch: 2760 | training loss: 0.005578130949\n",
      "Epoch: 2761 | training loss: 0.005571943708\n",
      "Epoch: 2762 | training loss: 0.005565766711\n",
      "Epoch: 2763 | training loss: 0.005559597164\n",
      "Epoch: 2764 | training loss: 0.005553439725\n",
      "Epoch: 2765 | training loss: 0.005547292065\n",
      "Epoch: 2766 | training loss: 0.005541151389\n",
      "Epoch: 2767 | training loss: 0.005535021890\n",
      "Epoch: 2768 | training loss: 0.005528899841\n",
      "Epoch: 2769 | training loss: 0.005522787105\n",
      "Epoch: 2770 | training loss: 0.005516683217\n",
      "Epoch: 2771 | training loss: 0.005510588642\n",
      "Epoch: 2772 | training loss: 0.005504503381\n",
      "Epoch: 2773 | training loss: 0.005498427898\n",
      "Epoch: 2774 | training loss: 0.005492360331\n",
      "Epoch: 2775 | training loss: 0.005486302078\n",
      "Epoch: 2776 | training loss: 0.005480254535\n",
      "Epoch: 2777 | training loss: 0.005474216770\n",
      "Epoch: 2778 | training loss: 0.005468186922\n",
      "Epoch: 2779 | training loss: 0.005462166388\n",
      "Epoch: 2780 | training loss: 0.005456153769\n",
      "Epoch: 2781 | training loss: 0.005450149998\n",
      "Epoch: 2782 | training loss: 0.005444154143\n",
      "Epoch: 2783 | training loss: 0.005438169464\n",
      "Epoch: 2784 | training loss: 0.005432192702\n",
      "Epoch: 2785 | training loss: 0.005426225718\n",
      "Epoch: 2786 | training loss: 0.005420266185\n",
      "Epoch: 2787 | training loss: 0.005414317362\n",
      "Epoch: 2788 | training loss: 0.005408377387\n",
      "Epoch: 2789 | training loss: 0.005402446259\n",
      "Epoch: 2790 | training loss: 0.005396522582\n",
      "Epoch: 2791 | training loss: 0.005390609149\n",
      "Epoch: 2792 | training loss: 0.005384704564\n",
      "Epoch: 2793 | training loss: 0.005378808361\n",
      "Epoch: 2794 | training loss: 0.005372920539\n",
      "Epoch: 2795 | training loss: 0.005367042031\n",
      "Epoch: 2796 | training loss: 0.005361171439\n",
      "Epoch: 2797 | training loss: 0.005355310626\n",
      "Epoch: 2798 | training loss: 0.005349456333\n",
      "Epoch: 2799 | training loss: 0.005343612283\n",
      "Epoch: 2800 | training loss: 0.005337777082\n",
      "Epoch: 2801 | training loss: 0.005331950728\n",
      "Epoch: 2802 | training loss: 0.005326134153\n",
      "Epoch: 2803 | training loss: 0.005320324562\n",
      "Epoch: 2804 | training loss: 0.005314525217\n",
      "Epoch: 2805 | training loss: 0.005308733322\n",
      "Epoch: 2806 | training loss: 0.005302950740\n",
      "Epoch: 2807 | training loss: 0.005297175143\n",
      "Epoch: 2808 | training loss: 0.005291410256\n",
      "Epoch: 2809 | training loss: 0.005285653286\n",
      "Epoch: 2810 | training loss: 0.005279905163\n",
      "Epoch: 2811 | training loss: 0.005274164490\n",
      "Epoch: 2812 | training loss: 0.005268432200\n",
      "Epoch: 2813 | training loss: 0.005262707360\n",
      "Epoch: 2814 | training loss: 0.005256992299\n",
      "Epoch: 2815 | training loss: 0.005251285620\n",
      "Epoch: 2816 | training loss: 0.005245587789\n",
      "Epoch: 2817 | training loss: 0.005239897408\n",
      "Epoch: 2818 | training loss: 0.005234217271\n",
      "Epoch: 2819 | training loss: 0.005228542723\n",
      "Epoch: 2820 | training loss: 0.005222879350\n",
      "Epoch: 2821 | training loss: 0.005217223428\n",
      "Epoch: 2822 | training loss: 0.005211574491\n",
      "Epoch: 2823 | training loss: 0.005205936730\n",
      "Epoch: 2824 | training loss: 0.005200305488\n",
      "Epoch: 2825 | training loss: 0.005194684491\n",
      "Epoch: 2826 | training loss: 0.005189070478\n",
      "Epoch: 2827 | training loss: 0.005183463451\n",
      "Epoch: 2828 | training loss: 0.005177867133\n",
      "Epoch: 2829 | training loss: 0.005172277801\n",
      "Epoch: 2830 | training loss: 0.005166695453\n",
      "Epoch: 2831 | training loss: 0.005161124747\n",
      "Epoch: 2832 | training loss: 0.005155559164\n",
      "Epoch: 2833 | training loss: 0.005150003359\n",
      "Epoch: 2834 | training loss: 0.005144455004\n",
      "Epoch: 2835 | training loss: 0.005138915963\n",
      "Epoch: 2836 | training loss: 0.005133383442\n",
      "Epoch: 2837 | training loss: 0.005127861165\n",
      "Epoch: 2838 | training loss: 0.005122344010\n",
      "Epoch: 2839 | training loss: 0.005116837099\n",
      "Epoch: 2840 | training loss: 0.005111336708\n",
      "Epoch: 2841 | training loss: 0.005105844699\n",
      "Epoch: 2842 | training loss: 0.005100360606\n",
      "Epoch: 2843 | training loss: 0.005094886757\n",
      "Epoch: 2844 | training loss: 0.005089418963\n",
      "Epoch: 2845 | training loss: 0.005083959550\n",
      "Epoch: 2846 | training loss: 0.005078508053\n",
      "Epoch: 2847 | training loss: 0.005073065870\n",
      "Epoch: 2848 | training loss: 0.005067631137\n",
      "Epoch: 2849 | training loss: 0.005062202923\n",
      "Epoch: 2850 | training loss: 0.005056784488\n",
      "Epoch: 2851 | training loss: 0.005051373504\n",
      "Epoch: 2852 | training loss: 0.005045969039\n",
      "Epoch: 2853 | training loss: 0.005040574353\n",
      "Epoch: 2854 | training loss: 0.005035187118\n",
      "Epoch: 2855 | training loss: 0.005029807333\n",
      "Epoch: 2856 | training loss: 0.005024435930\n",
      "Epoch: 2857 | training loss: 0.005019070581\n",
      "Epoch: 2858 | training loss: 0.005013716873\n",
      "Epoch: 2859 | training loss: 0.005008369219\n",
      "Epoch: 2860 | training loss: 0.005003029481\n",
      "Epoch: 2861 | training loss: 0.004997697193\n",
      "Epoch: 2862 | training loss: 0.004992374219\n",
      "Epoch: 2863 | training loss: 0.004987056833\n",
      "Epoch: 2864 | training loss: 0.004981750622\n",
      "Epoch: 2865 | training loss: 0.004976449534\n",
      "Epoch: 2866 | training loss: 0.004971156828\n",
      "Epoch: 2867 | training loss: 0.004965870641\n",
      "Epoch: 2868 | training loss: 0.004960593767\n",
      "Epoch: 2869 | training loss: 0.004955324810\n",
      "Epoch: 2870 | training loss: 0.004950062837\n",
      "Epoch: 2871 | training loss: 0.004944806919\n",
      "Epoch: 2872 | training loss: 0.004939561710\n",
      "Epoch: 2873 | training loss: 0.004934323020\n",
      "Epoch: 2874 | training loss: 0.004929091316\n",
      "Epoch: 2875 | training loss: 0.004923867993\n",
      "Epoch: 2876 | training loss: 0.004918652121\n",
      "Epoch: 2877 | training loss: 0.004913442768\n",
      "Epoch: 2878 | training loss: 0.004908242729\n",
      "Epoch: 2879 | training loss: 0.004903049208\n",
      "Epoch: 2880 | training loss: 0.004897864070\n",
      "Epoch: 2881 | training loss: 0.004892685451\n",
      "Epoch: 2882 | training loss: 0.004887515679\n",
      "Epoch: 2883 | training loss: 0.004882352427\n",
      "Epoch: 2884 | training loss: 0.004877196625\n",
      "Epoch: 2885 | training loss: 0.004872047808\n",
      "Epoch: 2886 | training loss: 0.004866908304\n",
      "Epoch: 2887 | training loss: 0.004861773923\n",
      "Epoch: 2888 | training loss: 0.004856649321\n",
      "Epoch: 2889 | training loss: 0.004851532169\n",
      "Epoch: 2890 | training loss: 0.004846418276\n",
      "Epoch: 2891 | training loss: 0.004841314629\n",
      "Epoch: 2892 | training loss: 0.004836218432\n",
      "Epoch: 2893 | training loss: 0.004831129685\n",
      "Epoch: 2894 | training loss: 0.004826046061\n",
      "Epoch: 2895 | training loss: 0.004820973147\n",
      "Epoch: 2896 | training loss: 0.004815905355\n",
      "Epoch: 2897 | training loss: 0.004810845945\n",
      "Epoch: 2898 | training loss: 0.004805793054\n",
      "Epoch: 2899 | training loss: 0.004800747614\n",
      "Epoch: 2900 | training loss: 0.004795709625\n",
      "Epoch: 2901 | training loss: 0.004790680483\n",
      "Epoch: 2902 | training loss: 0.004785655998\n",
      "Epoch: 2903 | training loss: 0.004780639894\n",
      "Epoch: 2904 | training loss: 0.004775630776\n",
      "Epoch: 2905 | training loss: 0.004770629574\n",
      "Epoch: 2906 | training loss: 0.004765636288\n",
      "Epoch: 2907 | training loss: 0.004760649987\n",
      "Epoch: 2908 | training loss: 0.004755668808\n",
      "Epoch: 2909 | training loss: 0.004750697874\n",
      "Epoch: 2910 | training loss: 0.004745731596\n",
      "Epoch: 2911 | training loss: 0.004740774166\n",
      "Epoch: 2912 | training loss: 0.004735823721\n",
      "Epoch: 2913 | training loss: 0.004730880726\n",
      "Epoch: 2914 | training loss: 0.004725941923\n",
      "Epoch: 2915 | training loss: 0.004721013829\n",
      "Epoch: 2916 | training loss: 0.004716091789\n",
      "Epoch: 2917 | training loss: 0.004711176269\n",
      "Epoch: 2918 | training loss: 0.004706268664\n",
      "Epoch: 2919 | training loss: 0.004701366648\n",
      "Epoch: 2920 | training loss: 0.004696472082\n",
      "Epoch: 2921 | training loss: 0.004691586364\n",
      "Epoch: 2922 | training loss: 0.004686706234\n",
      "Epoch: 2923 | training loss: 0.004681833554\n",
      "Epoch: 2924 | training loss: 0.004676967394\n",
      "Epoch: 2925 | training loss: 0.004672109149\n",
      "Epoch: 2926 | training loss: 0.004667256959\n",
      "Epoch: 2927 | training loss: 0.004662410822\n",
      "Epoch: 2928 | training loss: 0.004657572135\n",
      "Epoch: 2929 | training loss: 0.004652739502\n",
      "Epoch: 2930 | training loss: 0.004647915717\n",
      "Epoch: 2931 | training loss: 0.004643097520\n",
      "Epoch: 2932 | training loss: 0.004638288170\n",
      "Epoch: 2933 | training loss: 0.004633483943\n",
      "Epoch: 2934 | training loss: 0.004628687631\n",
      "Epoch: 2935 | training loss: 0.004623896908\n",
      "Epoch: 2936 | training loss: 0.004619114567\n",
      "Epoch: 2937 | training loss: 0.004614337813\n",
      "Epoch: 2938 | training loss: 0.004609568976\n",
      "Epoch: 2939 | training loss: 0.004604806192\n",
      "Epoch: 2940 | training loss: 0.004600050859\n",
      "Epoch: 2941 | training loss: 0.004595302511\n",
      "Epoch: 2942 | training loss: 0.004590560682\n",
      "Epoch: 2943 | training loss: 0.004585824907\n",
      "Epoch: 2944 | training loss: 0.004581097513\n",
      "Epoch: 2945 | training loss: 0.004576374777\n",
      "Epoch: 2946 | training loss: 0.004571660887\n",
      "Epoch: 2947 | training loss: 0.004566952121\n",
      "Epoch: 2948 | training loss: 0.004562249873\n",
      "Epoch: 2949 | training loss: 0.004557554144\n",
      "Epoch: 2950 | training loss: 0.004552864935\n",
      "Epoch: 2951 | training loss: 0.004548182245\n",
      "Epoch: 2952 | training loss: 0.004543506075\n",
      "Epoch: 2953 | training loss: 0.004538838752\n",
      "Epoch: 2954 | training loss: 0.004534176085\n",
      "Epoch: 2955 | training loss: 0.004529520869\n",
      "Epoch: 2956 | training loss: 0.004524871241\n",
      "Epoch: 2957 | training loss: 0.004520229530\n",
      "Epoch: 2958 | training loss: 0.004515592940\n",
      "Epoch: 2959 | training loss: 0.004510965198\n",
      "Epoch: 2960 | training loss: 0.004506343044\n",
      "Epoch: 2961 | training loss: 0.004501726478\n",
      "Epoch: 2962 | training loss: 0.004497118294\n",
      "Epoch: 2963 | training loss: 0.004492512904\n",
      "Epoch: 2964 | training loss: 0.004487916362\n",
      "Epoch: 2965 | training loss: 0.004483326338\n",
      "Epoch: 2966 | training loss: 0.004478742369\n",
      "Epoch: 2967 | training loss: 0.004474164452\n",
      "Epoch: 2968 | training loss: 0.004469594453\n",
      "Epoch: 2969 | training loss: 0.004465029109\n",
      "Epoch: 2970 | training loss: 0.004460471682\n",
      "Epoch: 2971 | training loss: 0.004455920774\n",
      "Epoch: 2972 | training loss: 0.004451376386\n",
      "Epoch: 2973 | training loss: 0.004446836654\n",
      "Epoch: 2974 | training loss: 0.004442305770\n",
      "Epoch: 2975 | training loss: 0.004437779542\n",
      "Epoch: 2976 | training loss: 0.004433258902\n",
      "Epoch: 2977 | training loss: 0.004428745713\n",
      "Epoch: 2978 | training loss: 0.004424237646\n",
      "Epoch: 2979 | training loss: 0.004419736564\n",
      "Epoch: 2980 | training loss: 0.004415241536\n",
      "Epoch: 2981 | training loss: 0.004410754889\n",
      "Epoch: 2982 | training loss: 0.004406272434\n",
      "Epoch: 2983 | training loss: 0.004401797894\n",
      "Epoch: 2984 | training loss: 0.004397328477\n",
      "Epoch: 2985 | training loss: 0.004392866045\n",
      "Epoch: 2986 | training loss: 0.004388409201\n",
      "Epoch: 2987 | training loss: 0.004383959342\n",
      "Epoch: 2988 | training loss: 0.004379514605\n",
      "Epoch: 2989 | training loss: 0.004375075921\n",
      "Epoch: 2990 | training loss: 0.004370644689\n",
      "Epoch: 2991 | training loss: 0.004366217647\n",
      "Epoch: 2992 | training loss: 0.004361798055\n",
      "Epoch: 2993 | training loss: 0.004357384518\n",
      "Epoch: 2994 | training loss: 0.004352976102\n",
      "Epoch: 2995 | training loss: 0.004348575138\n",
      "Epoch: 2996 | training loss: 0.004344181158\n",
      "Epoch: 2997 | training loss: 0.004339791834\n",
      "Epoch: 2998 | training loss: 0.004335408565\n",
      "Epoch: 2999 | training loss: 0.004331031814\n",
      "Epoch: 3000 | training loss: 0.004326660652\n",
      "Epoch: 3001 | training loss: 0.004322295077\n",
      "Epoch: 3002 | training loss: 0.004317936022\n",
      "Epoch: 3003 | training loss: 0.004313583020\n",
      "Epoch: 3004 | training loss: 0.004309234209\n",
      "Epoch: 3005 | training loss: 0.004304893315\n",
      "Epoch: 3006 | training loss: 0.004300558474\n",
      "Epoch: 3007 | training loss: 0.004296229221\n",
      "Epoch: 3008 | training loss: 0.004291906022\n",
      "Epoch: 3009 | training loss: 0.004287588876\n",
      "Epoch: 3010 | training loss: 0.004283277318\n",
      "Epoch: 3011 | training loss: 0.004278971814\n",
      "Epoch: 3012 | training loss: 0.004274671897\n",
      "Epoch: 3013 | training loss: 0.004270377103\n",
      "Epoch: 3014 | training loss: 0.004266090225\n",
      "Epoch: 3015 | training loss: 0.004261808004\n",
      "Epoch: 3016 | training loss: 0.004257531371\n",
      "Epoch: 3017 | training loss: 0.004253260791\n",
      "Epoch: 3018 | training loss: 0.004248997197\n",
      "Epoch: 3019 | training loss: 0.004244738258\n",
      "Epoch: 3020 | training loss: 0.004240485374\n",
      "Epoch: 3021 | training loss: 0.004236238543\n",
      "Epoch: 3022 | training loss: 0.004231996369\n",
      "Epoch: 3023 | training loss: 0.004227761645\n",
      "Epoch: 3024 | training loss: 0.004223532043\n",
      "Epoch: 3025 | training loss: 0.004219307099\n",
      "Epoch: 3026 | training loss: 0.004215088673\n",
      "Epoch: 3027 | training loss: 0.004210877232\n",
      "Epoch: 3028 | training loss: 0.004206669051\n",
      "Epoch: 3029 | training loss: 0.004202467389\n",
      "Epoch: 3030 | training loss: 0.004198271316\n",
      "Epoch: 3031 | training loss: 0.004194081295\n",
      "Epoch: 3032 | training loss: 0.004189897329\n",
      "Epoch: 3033 | training loss: 0.004185718950\n",
      "Epoch: 3034 | training loss: 0.004181546159\n",
      "Epoch: 3035 | training loss: 0.004177378025\n",
      "Epoch: 3036 | training loss: 0.004173216410\n",
      "Epoch: 3037 | training loss: 0.004169059452\n",
      "Epoch: 3038 | training loss: 0.004164907150\n",
      "Epoch: 3039 | training loss: 0.004160762765\n",
      "Epoch: 3040 | training loss: 0.004156621639\n",
      "Epoch: 3041 | training loss: 0.004152486566\n",
      "Epoch: 3042 | training loss: 0.004148358013\n",
      "Epoch: 3043 | training loss: 0.004144235514\n",
      "Epoch: 3044 | training loss: 0.004140116740\n",
      "Epoch: 3045 | training loss: 0.004136004485\n",
      "Epoch: 3046 | training loss: 0.004131896421\n",
      "Epoch: 3047 | training loss: 0.004127794877\n",
      "Epoch: 3048 | training loss: 0.004123699851\n",
      "Epoch: 3049 | training loss: 0.004119609017\n",
      "Epoch: 3050 | training loss: 0.004115523770\n",
      "Epoch: 3051 | training loss: 0.004111445043\n",
      "Epoch: 3052 | training loss: 0.004107369110\n",
      "Epoch: 3053 | training loss: 0.004103300627\n",
      "Epoch: 3054 | training loss: 0.004099235404\n",
      "Epoch: 3055 | training loss: 0.004095178097\n",
      "Epoch: 3056 | training loss: 0.004091124982\n",
      "Epoch: 3057 | training loss: 0.004087076522\n",
      "Epoch: 3058 | training loss: 0.004083034582\n",
      "Epoch: 3059 | training loss: 0.004078996833\n",
      "Epoch: 3060 | training loss: 0.004074965138\n",
      "Epoch: 3061 | training loss: 0.004070938099\n",
      "Epoch: 3062 | training loss: 0.004066917114\n",
      "Epoch: 3063 | training loss: 0.004062898923\n",
      "Epoch: 3064 | training loss: 0.004058888648\n",
      "Epoch: 3065 | training loss: 0.004054883029\n",
      "Epoch: 3066 | training loss: 0.004050882533\n",
      "Epoch: 3067 | training loss: 0.004046887625\n",
      "Epoch: 3068 | training loss: 0.004042896442\n",
      "Epoch: 3069 | training loss: 0.004038911778\n",
      "Epoch: 3070 | training loss: 0.004034930840\n",
      "Epoch: 3071 | training loss: 0.004030956887\n",
      "Epoch: 3072 | training loss: 0.004026986659\n",
      "Epoch: 3073 | training loss: 0.004023022484\n",
      "Epoch: 3074 | training loss: 0.004019062035\n",
      "Epoch: 3075 | training loss: 0.004015107639\n",
      "Epoch: 3076 | training loss: 0.004011157900\n",
      "Epoch: 3077 | training loss: 0.004007213283\n",
      "Epoch: 3078 | training loss: 0.004003274720\n",
      "Epoch: 3079 | training loss: 0.003999338951\n",
      "Epoch: 3080 | training loss: 0.003995410167\n",
      "Epoch: 3081 | training loss: 0.003991486505\n",
      "Epoch: 3082 | training loss: 0.003987565171\n",
      "Epoch: 3083 | training loss: 0.003983651288\n",
      "Epoch: 3084 | training loss: 0.003979741596\n",
      "Epoch: 3085 | training loss: 0.003975837026\n",
      "Epoch: 3086 | training loss: 0.003971937578\n",
      "Epoch: 3087 | training loss: 0.003968041856\n",
      "Epoch: 3088 | training loss: 0.003964152187\n",
      "Epoch: 3089 | training loss: 0.003960266709\n",
      "Epoch: 3090 | training loss: 0.003956386819\n",
      "Epoch: 3091 | training loss: 0.003952511586\n",
      "Epoch: 3092 | training loss: 0.003948641475\n",
      "Epoch: 3093 | training loss: 0.003944775090\n",
      "Epoch: 3094 | training loss: 0.003940913361\n",
      "Epoch: 3095 | training loss: 0.003937058151\n",
      "Epoch: 3096 | training loss: 0.003933206201\n",
      "Epoch: 3097 | training loss: 0.003929360304\n",
      "Epoch: 3098 | training loss: 0.003925518598\n",
      "Epoch: 3099 | training loss: 0.003921680152\n",
      "Epoch: 3100 | training loss: 0.003917848691\n",
      "Epoch: 3101 | training loss: 0.003914021421\n",
      "Epoch: 3102 | training loss: 0.003910198808\n",
      "Epoch: 3103 | training loss: 0.003906380385\n",
      "Epoch: 3104 | training loss: 0.003902568016\n",
      "Epoch: 3105 | training loss: 0.003898759605\n",
      "Epoch: 3106 | training loss: 0.003894954454\n",
      "Epoch: 3107 | training loss: 0.003891155589\n",
      "Epoch: 3108 | training loss: 0.003887359519\n",
      "Epoch: 3109 | training loss: 0.003883570433\n",
      "Epoch: 3110 | training loss: 0.003879785072\n",
      "Epoch: 3111 | training loss: 0.003876003902\n",
      "Epoch: 3112 | training loss: 0.003872226458\n",
      "Epoch: 3113 | training loss: 0.003868455067\n",
      "Epoch: 3114 | training loss: 0.003864688333\n",
      "Epoch: 3115 | training loss: 0.003860925091\n",
      "Epoch: 3116 | training loss: 0.003857164877\n",
      "Epoch: 3117 | training loss: 0.003853411879\n",
      "Epoch: 3118 | training loss: 0.003849662142\n",
      "Epoch: 3119 | training loss: 0.003845917527\n",
      "Epoch: 3120 | training loss: 0.003842177335\n",
      "Epoch: 3121 | training loss: 0.003838442266\n",
      "Epoch: 3122 | training loss: 0.003834710922\n",
      "Epoch: 3123 | training loss: 0.003830982372\n",
      "Epoch: 3124 | training loss: 0.003827261040\n",
      "Epoch: 3125 | training loss: 0.003823542502\n",
      "Epoch: 3126 | training loss: 0.003819829086\n",
      "Epoch: 3127 | training loss: 0.003816120094\n",
      "Epoch: 3128 | training loss: 0.003812414128\n",
      "Epoch: 3129 | training loss: 0.003808713052\n",
      "Epoch: 3130 | training loss: 0.003805017099\n",
      "Epoch: 3131 | training loss: 0.003801324870\n",
      "Epoch: 3132 | training loss: 0.003797636367\n",
      "Epoch: 3133 | training loss: 0.003793952055\n",
      "Epoch: 3134 | training loss: 0.003790272633\n",
      "Epoch: 3135 | training loss: 0.003786598099\n",
      "Epoch: 3136 | training loss: 0.003782927291\n",
      "Epoch: 3137 | training loss: 0.003779260674\n",
      "Epoch: 3138 | training loss: 0.003775598248\n",
      "Epoch: 3139 | training loss: 0.003771939781\n",
      "Epoch: 3140 | training loss: 0.003768284572\n",
      "Epoch: 3141 | training loss: 0.003764635185\n",
      "Epoch: 3142 | training loss: 0.003760989057\n",
      "Epoch: 3143 | training loss: 0.003757346189\n",
      "Epoch: 3144 | training loss: 0.003753709607\n",
      "Epoch: 3145 | training loss: 0.003750074655\n",
      "Epoch: 3146 | training loss: 0.003746445756\n",
      "Epoch: 3147 | training loss: 0.003742819652\n",
      "Epoch: 3148 | training loss: 0.003739197738\n",
      "Epoch: 3149 | training loss: 0.003735580714\n",
      "Epoch: 3150 | training loss: 0.003731966717\n",
      "Epoch: 3151 | training loss: 0.003728358075\n",
      "Epoch: 3152 | training loss: 0.003724753391\n",
      "Epoch: 3153 | training loss: 0.003721150802\n",
      "Epoch: 3154 | training loss: 0.003717554500\n",
      "Epoch: 3155 | training loss: 0.003713960527\n",
      "Epoch: 3156 | training loss: 0.003710371442\n",
      "Epoch: 3157 | training loss: 0.003706785385\n",
      "Epoch: 3158 | training loss: 0.003703203518\n",
      "Epoch: 3159 | training loss: 0.003699626308\n",
      "Epoch: 3160 | training loss: 0.003696051892\n",
      "Epoch: 3161 | training loss: 0.003692480270\n",
      "Epoch: 3162 | training loss: 0.003688914701\n",
      "Epoch: 3163 | training loss: 0.003685351694\n",
      "Epoch: 3164 | training loss: 0.003681794042\n",
      "Epoch: 3165 | training loss: 0.003678238951\n",
      "Epoch: 3166 | training loss: 0.003674687352\n",
      "Epoch: 3167 | training loss: 0.003671139712\n",
      "Epoch: 3168 | training loss: 0.003667596029\n",
      "Epoch: 3169 | training loss: 0.003664056771\n",
      "Epoch: 3170 | training loss: 0.003660521237\n",
      "Epoch: 3171 | training loss: 0.003656987567\n",
      "Epoch: 3172 | training loss: 0.003653460182\n",
      "Epoch: 3173 | training loss: 0.003649935359\n",
      "Epoch: 3174 | training loss: 0.003646414727\n",
      "Epoch: 3175 | training loss: 0.003642896889\n",
      "Epoch: 3176 | training loss: 0.003639382310\n",
      "Epoch: 3177 | training loss: 0.003635873087\n",
      "Epoch: 3178 | training loss: 0.003632365726\n",
      "Epoch: 3179 | training loss: 0.003628862090\n",
      "Epoch: 3180 | training loss: 0.003625362646\n",
      "Epoch: 3181 | training loss: 0.003621866228\n",
      "Epoch: 3182 | training loss: 0.003618373768\n",
      "Epoch: 3183 | training loss: 0.003614884568\n",
      "Epoch: 3184 | training loss: 0.003611398628\n",
      "Epoch: 3185 | training loss: 0.003607917344\n",
      "Epoch: 3186 | training loss: 0.003604438622\n",
      "Epoch: 3187 | training loss: 0.003600963391\n",
      "Epoch: 3188 | training loss: 0.003597491654\n",
      "Epoch: 3189 | training loss: 0.003594023641\n",
      "Epoch: 3190 | training loss: 0.003590560053\n",
      "Epoch: 3191 | training loss: 0.003587099491\n",
      "Epoch: 3192 | training loss: 0.003583641024\n",
      "Epoch: 3193 | training loss: 0.003580185818\n",
      "Epoch: 3194 | training loss: 0.003576734802\n",
      "Epoch: 3195 | training loss: 0.003573287511\n",
      "Epoch: 3196 | training loss: 0.003569843480\n",
      "Epoch: 3197 | training loss: 0.003566402476\n",
      "Epoch: 3198 | training loss: 0.003562964732\n",
      "Epoch: 3199 | training loss: 0.003559529781\n",
      "Epoch: 3200 | training loss: 0.003556097159\n",
      "Epoch: 3201 | training loss: 0.003552670125\n",
      "Epoch: 3202 | training loss: 0.003549245419\n",
      "Epoch: 3203 | training loss: 0.003545823041\n",
      "Epoch: 3204 | training loss: 0.003542403923\n",
      "Epoch: 3205 | training loss: 0.003538989462\n",
      "Epoch: 3206 | training loss: 0.003535577562\n",
      "Epoch: 3207 | training loss: 0.003532168455\n",
      "Epoch: 3208 | training loss: 0.003528763773\n",
      "Epoch: 3209 | training loss: 0.003525361419\n",
      "Epoch: 3210 | training loss: 0.003521961858\n",
      "Epoch: 3211 | training loss: 0.003518564859\n",
      "Epoch: 3212 | training loss: 0.003515172517\n",
      "Epoch: 3213 | training loss: 0.003511782503\n",
      "Epoch: 3214 | training loss: 0.003508395283\n",
      "Epoch: 3215 | training loss: 0.003505010623\n",
      "Epoch: 3216 | training loss: 0.003501628991\n",
      "Epoch: 3217 | training loss: 0.003498251084\n",
      "Epoch: 3218 | training loss: 0.003494875040\n",
      "Epoch: 3219 | training loss: 0.003491502255\n",
      "Epoch: 3220 | training loss: 0.003488133429\n",
      "Epoch: 3221 | training loss: 0.003484767396\n",
      "Epoch: 3222 | training loss: 0.003481402528\n",
      "Epoch: 3223 | training loss: 0.003478041850\n",
      "Epoch: 3224 | training loss: 0.003474684432\n",
      "Epoch: 3225 | training loss: 0.003471328178\n",
      "Epoch: 3226 | training loss: 0.003467976116\n",
      "Epoch: 3227 | training loss: 0.003464627545\n",
      "Epoch: 3228 | training loss: 0.003461281536\n",
      "Epoch: 3229 | training loss: 0.003457937855\n",
      "Epoch: 3230 | training loss: 0.003454596503\n",
      "Epoch: 3231 | training loss: 0.003451258177\n",
      "Epoch: 3232 | training loss: 0.003447923344\n",
      "Epoch: 3233 | training loss: 0.003444589907\n",
      "Epoch: 3234 | training loss: 0.003441260895\n",
      "Epoch: 3235 | training loss: 0.003437933745\n",
      "Epoch: 3236 | training loss: 0.003434607759\n",
      "Epoch: 3237 | training loss: 0.003431287128\n",
      "Epoch: 3238 | training loss: 0.003427966963\n",
      "Epoch: 3239 | training loss: 0.003424650058\n",
      "Epoch: 3240 | training loss: 0.003421335947\n",
      "Epoch: 3241 | training loss: 0.003418023698\n",
      "Epoch: 3242 | training loss: 0.003414715640\n",
      "Epoch: 3243 | training loss: 0.003411409911\n",
      "Epoch: 3244 | training loss: 0.003408106044\n",
      "Epoch: 3245 | training loss: 0.003404804971\n",
      "Epoch: 3246 | training loss: 0.003401506227\n",
      "Epoch: 3247 | training loss: 0.003398210276\n",
      "Epoch: 3248 | training loss: 0.003394916188\n",
      "Epoch: 3249 | training loss: 0.003391625825\n",
      "Epoch: 3250 | training loss: 0.003388336161\n",
      "Epoch: 3251 | training loss: 0.003385049989\n",
      "Epoch: 3252 | training loss: 0.003381765680\n",
      "Epoch: 3253 | training loss: 0.003378483700\n",
      "Epoch: 3254 | training loss: 0.003375204280\n",
      "Epoch: 3255 | training loss: 0.003371927654\n",
      "Epoch: 3256 | training loss: 0.003368651960\n",
      "Epoch: 3257 | training loss: 0.003365380224\n",
      "Epoch: 3258 | training loss: 0.003362109419\n",
      "Epoch: 3259 | training loss: 0.003358841175\n",
      "Epoch: 3260 | training loss: 0.003355576657\n",
      "Epoch: 3261 | training loss: 0.003352313070\n",
      "Epoch: 3262 | training loss: 0.003349051578\n",
      "Epoch: 3263 | training loss: 0.003345792647\n",
      "Epoch: 3264 | training loss: 0.003342536511\n",
      "Epoch: 3265 | training loss: 0.003339281539\n",
      "Epoch: 3266 | training loss: 0.003336028894\n",
      "Epoch: 3267 | training loss: 0.003332779277\n",
      "Epoch: 3268 | training loss: 0.003329530591\n",
      "Epoch: 3269 | training loss: 0.003326285165\n",
      "Epoch: 3270 | training loss: 0.003323039738\n",
      "Epoch: 3271 | training loss: 0.003319798270\n",
      "Epoch: 3272 | training loss: 0.003316558897\n",
      "Epoch: 3273 | training loss: 0.003313320922\n",
      "Epoch: 3274 | training loss: 0.003310085274\n",
      "Epoch: 3275 | training loss: 0.003306851257\n",
      "Epoch: 3276 | training loss: 0.003303620266\n",
      "Epoch: 3277 | training loss: 0.003300389508\n",
      "Epoch: 3278 | training loss: 0.003297161078\n",
      "Epoch: 3279 | training loss: 0.003293935675\n",
      "Epoch: 3280 | training loss: 0.003290711436\n",
      "Epoch: 3281 | training loss: 0.003287489526\n",
      "Epoch: 3282 | training loss: 0.003284269944\n",
      "Epoch: 3283 | training loss: 0.003281050362\n",
      "Epoch: 3284 | training loss: 0.003277834039\n",
      "Epoch: 3285 | training loss: 0.003274619114\n",
      "Epoch: 3286 | training loss: 0.003271406051\n",
      "Epoch: 3287 | training loss: 0.003268194385\n",
      "Epoch: 3288 | training loss: 0.003264985280\n",
      "Epoch: 3289 | training loss: 0.003261777107\n",
      "Epoch: 3290 | training loss: 0.003258571494\n",
      "Epoch: 3291 | training loss: 0.003255367745\n",
      "Epoch: 3292 | training loss: 0.003252164461\n",
      "Epoch: 3293 | training loss: 0.003248963971\n",
      "Epoch: 3294 | training loss: 0.003245764179\n",
      "Epoch: 3295 | training loss: 0.003242565552\n",
      "Epoch: 3296 | training loss: 0.003239369253\n",
      "Epoch: 3297 | training loss: 0.003236175049\n",
      "Epoch: 3298 | training loss: 0.003232982010\n",
      "Epoch: 3299 | training loss: 0.003229788970\n",
      "Epoch: 3300 | training loss: 0.003226599190\n",
      "Epoch: 3301 | training loss: 0.003223411506\n",
      "Epoch: 3302 | training loss: 0.003220223356\n",
      "Epoch: 3303 | training loss: 0.003217038931\n",
      "Epoch: 3304 | training loss: 0.003213854274\n",
      "Epoch: 3305 | training loss: 0.003210671479\n",
      "Epoch: 3306 | training loss: 0.003207491012\n",
      "Epoch: 3307 | training loss: 0.003204311011\n",
      "Epoch: 3308 | training loss: 0.003201132175\n",
      "Epoch: 3309 | training loss: 0.003197954269\n",
      "Epoch: 3310 | training loss: 0.003194778692\n",
      "Epoch: 3311 | training loss: 0.003191603348\n",
      "Epoch: 3312 | training loss: 0.003188430564\n",
      "Epoch: 3313 | training loss: 0.003185257548\n",
      "Epoch: 3314 | training loss: 0.003182087094\n",
      "Epoch: 3315 | training loss: 0.003178917337\n",
      "Epoch: 3316 | training loss: 0.003175748978\n",
      "Epoch: 3317 | training loss: 0.003172581783\n",
      "Epoch: 3318 | training loss: 0.003169415984\n",
      "Epoch: 3319 | training loss: 0.003166250419\n",
      "Epoch: 3320 | training loss: 0.003163087415\n",
      "Epoch: 3321 | training loss: 0.003159924177\n",
      "Epoch: 3322 | training loss: 0.003156761872\n",
      "Epoch: 3323 | training loss: 0.003153602127\n",
      "Epoch: 3324 | training loss: 0.003150442848\n",
      "Epoch: 3325 | training loss: 0.003147283802\n",
      "Epoch: 3326 | training loss: 0.003144126385\n",
      "Epoch: 3327 | training loss: 0.003140969435\n",
      "Epoch: 3328 | training loss: 0.003137814347\n",
      "Epoch: 3329 | training loss: 0.003134659026\n",
      "Epoch: 3330 | training loss: 0.003131506033\n",
      "Epoch: 3331 | training loss: 0.003128353506\n",
      "Epoch: 3332 | training loss: 0.003125200514\n",
      "Epoch: 3333 | training loss: 0.003122050781\n",
      "Epoch: 3334 | training loss: 0.003118900349\n",
      "Epoch: 3335 | training loss: 0.003115750849\n",
      "Epoch: 3336 | training loss: 0.003112602048\n",
      "Epoch: 3337 | training loss: 0.003109453712\n",
      "Epoch: 3338 | training loss: 0.003106307704\n",
      "Epoch: 3339 | training loss: 0.003103161464\n",
      "Epoch: 3340 | training loss: 0.003100015456\n",
      "Epoch: 3341 | training loss: 0.003096869448\n",
      "Epoch: 3342 | training loss: 0.003093725303\n",
      "Epoch: 3343 | training loss: 0.003090580925\n",
      "Epoch: 3344 | training loss: 0.003087438643\n",
      "Epoch: 3345 | training loss: 0.003084296593\n",
      "Epoch: 3346 | training loss: 0.003081153845\n",
      "Epoch: 3347 | training loss: 0.003078012960\n",
      "Epoch: 3348 | training loss: 0.003074872773\n",
      "Epoch: 3349 | training loss: 0.003071733052\n",
      "Epoch: 3350 | training loss: 0.003068593331\n",
      "Epoch: 3351 | training loss: 0.003065453842\n",
      "Epoch: 3352 | training loss: 0.003062315751\n",
      "Epoch: 3353 | training loss: 0.003059178125\n",
      "Epoch: 3354 | training loss: 0.003056039801\n",
      "Epoch: 3355 | training loss: 0.003052902874\n",
      "Epoch: 3356 | training loss: 0.003049766878\n",
      "Epoch: 3357 | training loss: 0.003046629252\n",
      "Epoch: 3358 | training loss: 0.003043492092\n",
      "Epoch: 3359 | training loss: 0.003040356562\n",
      "Epoch: 3360 | training loss: 0.003037220566\n",
      "Epoch: 3361 | training loss: 0.003034084337\n",
      "Epoch: 3362 | training loss: 0.003030948574\n",
      "Epoch: 3363 | training loss: 0.003027813975\n",
      "Epoch: 3364 | training loss: 0.003024679143\n",
      "Epoch: 3365 | training loss: 0.003021544311\n",
      "Epoch: 3366 | training loss: 0.003018409712\n",
      "Epoch: 3367 | training loss: 0.003015274880\n",
      "Epoch: 3368 | training loss: 0.003012141446\n",
      "Epoch: 3369 | training loss: 0.003009007080\n",
      "Epoch: 3370 | training loss: 0.003005872946\n",
      "Epoch: 3371 | training loss: 0.003002739279\n",
      "Epoch: 3372 | training loss: 0.002999604680\n",
      "Epoch: 3373 | training loss: 0.002996470081\n",
      "Epoch: 3374 | training loss: 0.002993336413\n",
      "Epoch: 3375 | training loss: 0.002990202513\n",
      "Epoch: 3376 | training loss: 0.002987068379\n",
      "Epoch: 3377 | training loss: 0.002983933548\n",
      "Epoch: 3378 | training loss: 0.002980798250\n",
      "Epoch: 3379 | training loss: 0.002977664582\n",
      "Epoch: 3380 | training loss: 0.002974529751\n",
      "Epoch: 3381 | training loss: 0.002971395152\n",
      "Epoch: 3382 | training loss: 0.002968259621\n",
      "Epoch: 3383 | training loss: 0.002965123393\n",
      "Epoch: 3384 | training loss: 0.002961988328\n",
      "Epoch: 3385 | training loss: 0.002958853263\n",
      "Epoch: 3386 | training loss: 0.002955716802\n",
      "Epoch: 3387 | training loss: 0.002952580806\n",
      "Epoch: 3388 | training loss: 0.002949444111\n",
      "Epoch: 3389 | training loss: 0.002946306253\n",
      "Epoch: 3390 | training loss: 0.002943168161\n",
      "Epoch: 3391 | training loss: 0.002940030769\n",
      "Epoch: 3392 | training loss: 0.002936892677\n",
      "Epoch: 3393 | training loss: 0.002933753422\n",
      "Epoch: 3394 | training loss: 0.002930613700\n",
      "Epoch: 3395 | training loss: 0.002927473979\n",
      "Epoch: 3396 | training loss: 0.002924333559\n",
      "Epoch: 3397 | training loss: 0.002921191975\n",
      "Epoch: 3398 | training loss: 0.002918050624\n",
      "Epoch: 3399 | training loss: 0.002914908342\n",
      "Epoch: 3400 | training loss: 0.002911765361\n",
      "Epoch: 3401 | training loss: 0.002908622613\n",
      "Epoch: 3402 | training loss: 0.002905478003\n",
      "Epoch: 3403 | training loss: 0.002902333392\n",
      "Epoch: 3404 | training loss: 0.002899188315\n",
      "Epoch: 3405 | training loss: 0.002896042774\n",
      "Epoch: 3406 | training loss: 0.002892895369\n",
      "Epoch: 3407 | training loss: 0.002889747964\n",
      "Epoch: 3408 | training loss: 0.002886598231\n",
      "Epoch: 3409 | training loss: 0.002883450361\n",
      "Epoch: 3410 | training loss: 0.002880299697\n",
      "Epoch: 3411 | training loss: 0.002877149032\n",
      "Epoch: 3412 | training loss: 0.002873996738\n",
      "Epoch: 3413 | training loss: 0.002870843280\n",
      "Epoch: 3414 | training loss: 0.002867689822\n",
      "Epoch: 3415 | training loss: 0.002864534501\n",
      "Epoch: 3416 | training loss: 0.002861378947\n",
      "Epoch: 3417 | training loss: 0.002858222695\n",
      "Epoch: 3418 | training loss: 0.002855064115\n",
      "Epoch: 3419 | training loss: 0.002851904370\n",
      "Epoch: 3420 | training loss: 0.002848745324\n",
      "Epoch: 3421 | training loss: 0.002845583949\n",
      "Epoch: 3422 | training loss: 0.002842422109\n",
      "Epoch: 3423 | training loss: 0.002839257941\n",
      "Epoch: 3424 | training loss: 0.002836094238\n",
      "Epoch: 3425 | training loss: 0.002832928440\n",
      "Epoch: 3426 | training loss: 0.002829761244\n",
      "Epoch: 3427 | training loss: 0.002826592885\n",
      "Epoch: 3428 | training loss: 0.002823423129\n",
      "Epoch: 3429 | training loss: 0.002820252674\n",
      "Epoch: 3430 | training loss: 0.002817080123\n",
      "Epoch: 3431 | training loss: 0.002813907340\n",
      "Epoch: 3432 | training loss: 0.002810732694\n",
      "Epoch: 3433 | training loss: 0.002807556186\n",
      "Epoch: 3434 | training loss: 0.002804378746\n",
      "Epoch: 3435 | training loss: 0.002801199676\n",
      "Epoch: 3436 | training loss: 0.002798018046\n",
      "Epoch: 3437 | training loss: 0.002794836648\n",
      "Epoch: 3438 | training loss: 0.002791653154\n",
      "Epoch: 3439 | training loss: 0.002788468264\n",
      "Epoch: 3440 | training loss: 0.002785281511\n",
      "Epoch: 3441 | training loss: 0.002782093128\n",
      "Epoch: 3442 | training loss: 0.002778903348\n",
      "Epoch: 3443 | training loss: 0.002775711473\n",
      "Epoch: 3444 | training loss: 0.002772517735\n",
      "Epoch: 3445 | training loss: 0.002769323764\n",
      "Epoch: 3446 | training loss: 0.002766127465\n",
      "Epoch: 3447 | training loss: 0.002762929071\n",
      "Epoch: 3448 | training loss: 0.002759729512\n",
      "Epoch: 3449 | training loss: 0.002756526694\n",
      "Epoch: 3450 | training loss: 0.002753323177\n",
      "Epoch: 3451 | training loss: 0.002750118030\n",
      "Epoch: 3452 | training loss: 0.002746911254\n",
      "Epoch: 3453 | training loss: 0.002743702382\n",
      "Epoch: 3454 | training loss: 0.002740490716\n",
      "Epoch: 3455 | training loss: 0.002737278817\n",
      "Epoch: 3456 | training loss: 0.002734063426\n",
      "Epoch: 3457 | training loss: 0.002730846405\n",
      "Epoch: 3458 | training loss: 0.002727627987\n",
      "Epoch: 3459 | training loss: 0.002724407008\n",
      "Epoch: 3460 | training loss: 0.002721185097\n",
      "Epoch: 3461 | training loss: 0.002717959927\n",
      "Epoch: 3462 | training loss: 0.002714732662\n",
      "Epoch: 3463 | training loss: 0.002711503999\n",
      "Epoch: 3464 | training loss: 0.002708273008\n",
      "Epoch: 3465 | training loss: 0.002705040853\n",
      "Epoch: 3466 | training loss: 0.002701805672\n",
      "Epoch: 3467 | training loss: 0.002698568162\n",
      "Epoch: 3468 | training loss: 0.002695328789\n",
      "Epoch: 3469 | training loss: 0.002692086855\n",
      "Epoch: 3470 | training loss: 0.002688841894\n",
      "Epoch: 3471 | training loss: 0.002685596002\n",
      "Epoch: 3472 | training loss: 0.002682347782\n",
      "Epoch: 3473 | training loss: 0.002679097466\n",
      "Epoch: 3474 | training loss: 0.002675844356\n",
      "Epoch: 3475 | training loss: 0.002672588686\n",
      "Epoch: 3476 | training loss: 0.002669329988\n",
      "Epoch: 3477 | training loss: 0.002666070359\n",
      "Epoch: 3478 | training loss: 0.002662806772\n",
      "Epoch: 3479 | training loss: 0.002659542020\n",
      "Epoch: 3480 | training loss: 0.002656273777\n",
      "Epoch: 3481 | training loss: 0.002653003205\n",
      "Epoch: 3482 | training loss: 0.002649731236\n",
      "Epoch: 3483 | training loss: 0.002646456240\n",
      "Epoch: 3484 | training loss: 0.002643179148\n",
      "Epoch: 3485 | training loss: 0.002639898099\n",
      "Epoch: 3486 | training loss: 0.002636615420\n",
      "Epoch: 3487 | training loss: 0.002633330179\n",
      "Epoch: 3488 | training loss: 0.002630042611\n",
      "Epoch: 3489 | training loss: 0.002626751782\n",
      "Epoch: 3490 | training loss: 0.002623458393\n",
      "Epoch: 3491 | training loss: 0.002620162442\n",
      "Epoch: 3492 | training loss: 0.002616862999\n",
      "Epoch: 3493 | training loss: 0.002613561461\n",
      "Epoch: 3494 | training loss: 0.002610257361\n",
      "Epoch: 3495 | training loss: 0.002606950700\n",
      "Epoch: 3496 | training loss: 0.002603641478\n",
      "Epoch: 3497 | training loss: 0.002600329462\n",
      "Epoch: 3498 | training loss: 0.002597014885\n",
      "Epoch: 3499 | training loss: 0.002593697049\n",
      "Epoch: 3500 | training loss: 0.002590375952\n",
      "Epoch: 3501 | training loss: 0.002587052295\n",
      "Epoch: 3502 | training loss: 0.002583725611\n",
      "Epoch: 3503 | training loss: 0.002580395900\n",
      "Epoch: 3504 | training loss: 0.002577063162\n",
      "Epoch: 3505 | training loss: 0.002573726699\n",
      "Epoch: 3506 | training loss: 0.002570389071\n",
      "Epoch: 3507 | training loss: 0.002567047486\n",
      "Epoch: 3508 | training loss: 0.002563703572\n",
      "Epoch: 3509 | training loss: 0.002560356399\n",
      "Epoch: 3510 | training loss: 0.002557006432\n",
      "Epoch: 3511 | training loss: 0.002553653670\n",
      "Epoch: 3512 | training loss: 0.002550297184\n",
      "Epoch: 3513 | training loss: 0.002546938602\n",
      "Epoch: 3514 | training loss: 0.002543576062\n",
      "Epoch: 3515 | training loss: 0.002540210262\n",
      "Epoch: 3516 | training loss: 0.002536841901\n",
      "Epoch: 3517 | training loss: 0.002533470280\n",
      "Epoch: 3518 | training loss: 0.002530094702\n",
      "Epoch: 3519 | training loss: 0.002526716329\n",
      "Epoch: 3520 | training loss: 0.002523335395\n",
      "Epoch: 3521 | training loss: 0.002519951668\n",
      "Epoch: 3522 | training loss: 0.002516564215\n",
      "Epoch: 3523 | training loss: 0.002513173269\n",
      "Epoch: 3524 | training loss: 0.002509779995\n",
      "Epoch: 3525 | training loss: 0.002506382298\n",
      "Epoch: 3526 | training loss: 0.002502982505\n",
      "Epoch: 3527 | training loss: 0.002499579219\n",
      "Epoch: 3528 | training loss: 0.002496171976\n",
      "Epoch: 3529 | training loss: 0.002492761705\n",
      "Epoch: 3530 | training loss: 0.002489347942\n",
      "Epoch: 3531 | training loss: 0.002485931618\n",
      "Epoch: 3532 | training loss: 0.002482511336\n",
      "Epoch: 3533 | training loss: 0.002479087794\n",
      "Epoch: 3534 | training loss: 0.002475660760\n",
      "Epoch: 3535 | training loss: 0.002472230466\n",
      "Epoch: 3536 | training loss: 0.002468798077\n",
      "Epoch: 3537 | training loss: 0.002465362195\n",
      "Epoch: 3538 | training loss: 0.002461922821\n",
      "Epoch: 3539 | training loss: 0.002458480187\n",
      "Epoch: 3540 | training loss: 0.002455033828\n",
      "Epoch: 3541 | training loss: 0.002451583976\n",
      "Epoch: 3542 | training loss: 0.002448130632\n",
      "Epoch: 3543 | training loss: 0.002444674494\n",
      "Epoch: 3544 | training loss: 0.002441214398\n",
      "Epoch: 3545 | training loss: 0.002437751507\n",
      "Epoch: 3546 | training loss: 0.002434284193\n",
      "Epoch: 3547 | training loss: 0.002430813620\n",
      "Epoch: 3548 | training loss: 0.002427340252\n",
      "Epoch: 3549 | training loss: 0.002423862927\n",
      "Epoch: 3550 | training loss: 0.002420381410\n",
      "Epoch: 3551 | training loss: 0.002416898496\n",
      "Epoch: 3552 | training loss: 0.002413410926\n",
      "Epoch: 3553 | training loss: 0.002409920096\n",
      "Epoch: 3554 | training loss: 0.002406426240\n",
      "Epoch: 3555 | training loss: 0.002402929123\n",
      "Epoch: 3556 | training loss: 0.002399427351\n",
      "Epoch: 3557 | training loss: 0.002395923482\n",
      "Epoch: 3558 | training loss: 0.002392415656\n",
      "Epoch: 3559 | training loss: 0.002388904337\n",
      "Epoch: 3560 | training loss: 0.002385389525\n",
      "Epoch: 3561 | training loss: 0.002381870523\n",
      "Epoch: 3562 | training loss: 0.002378348261\n",
      "Epoch: 3563 | training loss: 0.002374822740\n",
      "Epoch: 3564 | training loss: 0.002371294191\n",
      "Epoch: 3565 | training loss: 0.002367761219\n",
      "Epoch: 3566 | training loss: 0.002364226151\n",
      "Epoch: 3567 | training loss: 0.002360686194\n",
      "Epoch: 3568 | training loss: 0.002357144142\n",
      "Epoch: 3569 | training loss: 0.002353598131\n",
      "Epoch: 3570 | training loss: 0.002350048861\n",
      "Epoch: 3571 | training loss: 0.002346496331\n",
      "Epoch: 3572 | training loss: 0.002342940075\n",
      "Epoch: 3573 | training loss: 0.002339381259\n",
      "Epoch: 3574 | training loss: 0.002335818252\n",
      "Epoch: 3575 | training loss: 0.002332252217\n",
      "Epoch: 3576 | training loss: 0.002328682691\n",
      "Epoch: 3577 | training loss: 0.002325109206\n",
      "Epoch: 3578 | training loss: 0.002321533393\n",
      "Epoch: 3579 | training loss: 0.002317953622\n",
      "Epoch: 3580 | training loss: 0.002314370824\n",
      "Epoch: 3581 | training loss: 0.002310784999\n",
      "Epoch: 3582 | training loss: 0.002307195682\n",
      "Epoch: 3583 | training loss: 0.002303602872\n",
      "Epoch: 3584 | training loss: 0.002300007269\n",
      "Epoch: 3585 | training loss: 0.002296407940\n",
      "Epoch: 3586 | training loss: 0.002292804653\n",
      "Epoch: 3587 | training loss: 0.002289199037\n",
      "Epoch: 3588 | training loss: 0.002285589930\n",
      "Epoch: 3589 | training loss: 0.002281976864\n",
      "Epoch: 3590 | training loss: 0.002278361470\n",
      "Epoch: 3591 | training loss: 0.002274742350\n",
      "Epoch: 3592 | training loss: 0.002271120204\n",
      "Epoch: 3593 | training loss: 0.002267494565\n",
      "Epoch: 3594 | training loss: 0.002263865899\n",
      "Epoch: 3595 | training loss: 0.002260233974\n",
      "Epoch: 3596 | training loss: 0.002256599022\n",
      "Epoch: 3597 | training loss: 0.002252960810\n",
      "Epoch: 3598 | training loss: 0.002249319339\n",
      "Epoch: 3599 | training loss: 0.002245675307\n",
      "Epoch: 3600 | training loss: 0.002242028713\n",
      "Epoch: 3601 | training loss: 0.002238378627\n",
      "Epoch: 3602 | training loss: 0.002234725747\n",
      "Epoch: 3603 | training loss: 0.002231069840\n",
      "Epoch: 3604 | training loss: 0.002227411373\n",
      "Epoch: 3605 | training loss: 0.002223749179\n",
      "Epoch: 3606 | training loss: 0.002220085589\n",
      "Epoch: 3607 | training loss: 0.002216418507\n",
      "Epoch: 3608 | training loss: 0.002212747931\n",
      "Epoch: 3609 | training loss: 0.002209075028\n",
      "Epoch: 3610 | training loss: 0.002205398167\n",
      "Epoch: 3611 | training loss: 0.002201719675\n",
      "Epoch: 3612 | training loss: 0.002198037691\n",
      "Epoch: 3613 | training loss: 0.002194352448\n",
      "Epoch: 3614 | training loss: 0.002190664876\n",
      "Epoch: 3615 | training loss: 0.002186975209\n",
      "Epoch: 3616 | training loss: 0.002183282981\n",
      "Epoch: 3617 | training loss: 0.002179588424\n",
      "Epoch: 3618 | training loss: 0.002175890841\n",
      "Epoch: 3619 | training loss: 0.002172190230\n",
      "Epoch: 3620 | training loss: 0.002168488223\n",
      "Epoch: 3621 | training loss: 0.002164783888\n",
      "Epoch: 3622 | training loss: 0.002161077224\n",
      "Epoch: 3623 | training loss: 0.002157367999\n",
      "Epoch: 3624 | training loss: 0.002153655281\n",
      "Epoch: 3625 | training loss: 0.002149940468\n",
      "Epoch: 3626 | training loss: 0.002146222629\n",
      "Epoch: 3627 | training loss: 0.002142503159\n",
      "Epoch: 3628 | training loss: 0.002138781361\n",
      "Epoch: 3629 | training loss: 0.002135057468\n",
      "Epoch: 3630 | training loss: 0.002131331712\n",
      "Epoch: 3631 | training loss: 0.002127604093\n",
      "Epoch: 3632 | training loss: 0.002123874612\n",
      "Epoch: 3633 | training loss: 0.002120142337\n",
      "Epoch: 3634 | training loss: 0.002116408898\n",
      "Epoch: 3635 | training loss: 0.002112673596\n",
      "Epoch: 3636 | training loss: 0.002108935267\n",
      "Epoch: 3637 | training loss: 0.002105195075\n",
      "Epoch: 3638 | training loss: 0.002101452788\n",
      "Epoch: 3639 | training loss: 0.002097708639\n",
      "Epoch: 3640 | training loss: 0.002093964024\n",
      "Epoch: 3641 | training loss: 0.002090216614\n",
      "Epoch: 3642 | training loss: 0.002086468041\n",
      "Epoch: 3643 | training loss: 0.002082718071\n",
      "Epoch: 3644 | training loss: 0.002078967169\n",
      "Epoch: 3645 | training loss: 0.002075213473\n",
      "Epoch: 3646 | training loss: 0.002071459079\n",
      "Epoch: 3647 | training loss: 0.002067702822\n",
      "Epoch: 3648 | training loss: 0.002063944470\n",
      "Epoch: 3649 | training loss: 0.002060185652\n",
      "Epoch: 3650 | training loss: 0.002056424972\n",
      "Epoch: 3651 | training loss: 0.002052663127\n",
      "Epoch: 3652 | training loss: 0.002048900351\n",
      "Epoch: 3653 | training loss: 0.002045136876\n",
      "Epoch: 3654 | training loss: 0.002041372238\n",
      "Epoch: 3655 | training loss: 0.002037605969\n",
      "Epoch: 3656 | training loss: 0.002033839002\n",
      "Epoch: 3657 | training loss: 0.002030070638\n",
      "Epoch: 3658 | training loss: 0.002026301809\n",
      "Epoch: 3659 | training loss: 0.002022531582\n",
      "Epoch: 3660 | training loss: 0.002018761588\n",
      "Epoch: 3661 | training loss: 0.002014990430\n",
      "Epoch: 3662 | training loss: 0.002011217875\n",
      "Epoch: 3663 | training loss: 0.002007445553\n",
      "Epoch: 3664 | training loss: 0.002003672067\n",
      "Epoch: 3665 | training loss: 0.001999898115\n",
      "Epoch: 3666 | training loss: 0.001996123930\n",
      "Epoch: 3667 | training loss: 0.001992349280\n",
      "Epoch: 3668 | training loss: 0.001988575095\n",
      "Epoch: 3669 | training loss: 0.001984799281\n",
      "Epoch: 3670 | training loss: 0.001981024165\n",
      "Epoch: 3671 | training loss: 0.001977247884\n",
      "Epoch: 3672 | training loss: 0.001973472303\n",
      "Epoch: 3673 | training loss: 0.001969695790\n",
      "Epoch: 3674 | training loss: 0.001965920441\n",
      "Epoch: 3675 | training loss: 0.001962144859\n",
      "Epoch: 3676 | training loss: 0.001958369510\n",
      "Epoch: 3677 | training loss: 0.001954593696\n",
      "Epoch: 3678 | training loss: 0.001950818230\n",
      "Epoch: 3679 | training loss: 0.001947043813\n",
      "Epoch: 3680 | training loss: 0.001943268464\n",
      "Epoch: 3681 | training loss: 0.001939493348\n",
      "Epoch: 3682 | training loss: 0.001935718930\n",
      "Epoch: 3683 | training loss: 0.001931946608\n",
      "Epoch: 3684 | training loss: 0.001928173937\n",
      "Epoch: 3685 | training loss: 0.001924402430\n",
      "Epoch: 3686 | training loss: 0.001920630224\n",
      "Epoch: 3687 | training loss: 0.001916859183\n",
      "Epoch: 3688 | training loss: 0.001913088607\n",
      "Epoch: 3689 | training loss: 0.001909319777\n",
      "Epoch: 3690 | training loss: 0.001905551646\n",
      "Epoch: 3691 | training loss: 0.001901785610\n",
      "Epoch: 3692 | training loss: 0.001898020157\n",
      "Epoch: 3693 | training loss: 0.001894255518\n",
      "Epoch: 3694 | training loss: 0.001890492043\n",
      "Epoch: 3695 | training loss: 0.001886729384\n",
      "Epoch: 3696 | training loss: 0.001882968587\n",
      "Epoch: 3697 | training loss: 0.001879208372\n",
      "Epoch: 3698 | training loss: 0.001875450602\n",
      "Epoch: 3699 | training loss: 0.001871694927\n",
      "Epoch: 3700 | training loss: 0.001867939951\n",
      "Epoch: 3701 | training loss: 0.001864186139\n",
      "Epoch: 3702 | training loss: 0.001860434422\n",
      "Epoch: 3703 | training loss: 0.001856684685\n",
      "Epoch: 3704 | training loss: 0.001852936577\n",
      "Epoch: 3705 | training loss: 0.001849189983\n",
      "Epoch: 3706 | training loss: 0.001845446066\n",
      "Epoch: 3707 | training loss: 0.001841704710\n",
      "Epoch: 3708 | training loss: 0.001837963704\n",
      "Epoch: 3709 | training loss: 0.001834225608\n",
      "Epoch: 3710 | training loss: 0.001830490539\n",
      "Epoch: 3711 | training loss: 0.001826757099\n",
      "Epoch: 3712 | training loss: 0.001823025988\n",
      "Epoch: 3713 | training loss: 0.001819297438\n",
      "Epoch: 3714 | training loss: 0.001815571217\n",
      "Epoch: 3715 | training loss: 0.001811847556\n",
      "Epoch: 3716 | training loss: 0.001808126224\n",
      "Epoch: 3717 | training loss: 0.001804407570\n",
      "Epoch: 3718 | training loss: 0.001800692640\n",
      "Epoch: 3719 | training loss: 0.001796979574\n",
      "Epoch: 3720 | training loss: 0.001793269534\n",
      "Epoch: 3721 | training loss: 0.001789562055\n",
      "Epoch: 3722 | training loss: 0.001785857952\n",
      "Epoch: 3723 | training loss: 0.001782156993\n",
      "Epoch: 3724 | training loss: 0.001778458129\n",
      "Epoch: 3725 | training loss: 0.001774763106\n",
      "Epoch: 3726 | training loss: 0.001771070994\n",
      "Epoch: 3727 | training loss: 0.001767383190\n",
      "Epoch: 3728 | training loss: 0.001763698296\n",
      "Epoch: 3729 | training loss: 0.001760017243\n",
      "Epoch: 3730 | training loss: 0.001756339334\n",
      "Epoch: 3731 | training loss: 0.001752663869\n",
      "Epoch: 3732 | training loss: 0.001748992712\n",
      "Epoch: 3733 | training loss: 0.001745324931\n",
      "Epoch: 3734 | training loss: 0.001741662156\n",
      "Epoch: 3735 | training loss: 0.001738001825\n",
      "Epoch: 3736 | training loss: 0.001734345802\n",
      "Epoch: 3737 | training loss: 0.001730693737\n",
      "Epoch: 3738 | training loss: 0.001727045979\n",
      "Epoch: 3739 | training loss: 0.001723401714\n",
      "Epoch: 3740 | training loss: 0.001719761407\n",
      "Epoch: 3741 | training loss: 0.001716124942\n",
      "Epoch: 3742 | training loss: 0.001712492434\n",
      "Epoch: 3743 | training loss: 0.001708864933\n",
      "Epoch: 3744 | training loss: 0.001705242321\n",
      "Epoch: 3745 | training loss: 0.001701623434\n",
      "Epoch: 3746 | training loss: 0.001698009321\n",
      "Epoch: 3747 | training loss: 0.001694399049\n",
      "Epoch: 3748 | training loss: 0.001690793200\n",
      "Epoch: 3749 | training loss: 0.001687192591\n",
      "Epoch: 3750 | training loss: 0.001683596405\n",
      "Epoch: 3751 | training loss: 0.001680004410\n",
      "Epoch: 3752 | training loss: 0.001676417771\n",
      "Epoch: 3753 | training loss: 0.001672835439\n",
      "Epoch: 3754 | training loss: 0.001669258345\n",
      "Epoch: 3755 | training loss: 0.001665686141\n",
      "Epoch: 3756 | training loss: 0.001662119059\n",
      "Epoch: 3757 | training loss: 0.001658556634\n",
      "Epoch: 3758 | training loss: 0.001654999447\n",
      "Epoch: 3759 | training loss: 0.001651447499\n",
      "Epoch: 3760 | training loss: 0.001647900557\n",
      "Epoch: 3761 | training loss: 0.001644359087\n",
      "Epoch: 3762 | training loss: 0.001640822273\n",
      "Epoch: 3763 | training loss: 0.001637291745\n",
      "Epoch: 3764 | training loss: 0.001633765758\n",
      "Epoch: 3765 | training loss: 0.001630245708\n",
      "Epoch: 3766 | training loss: 0.001626730198\n",
      "Epoch: 3767 | training loss: 0.001623221207\n",
      "Epoch: 3768 | training loss: 0.001619717106\n",
      "Epoch: 3769 | training loss: 0.001616219059\n",
      "Epoch: 3770 | training loss: 0.001612726599\n",
      "Epoch: 3771 | training loss: 0.001609240426\n",
      "Epoch: 3772 | training loss: 0.001605759375\n",
      "Epoch: 3773 | training loss: 0.001602285076\n",
      "Epoch: 3774 | training loss: 0.001598815084\n",
      "Epoch: 3775 | training loss: 0.001595351496\n",
      "Epoch: 3776 | training loss: 0.001591894194\n",
      "Epoch: 3777 | training loss: 0.001588441548\n",
      "Epoch: 3778 | training loss: 0.001584996004\n",
      "Epoch: 3779 | training loss: 0.001581557211\n",
      "Epoch: 3780 | training loss: 0.001578123309\n",
      "Epoch: 3781 | training loss: 0.001574695227\n",
      "Epoch: 3782 | training loss: 0.001571274712\n",
      "Epoch: 3783 | training loss: 0.001567860018\n",
      "Epoch: 3784 | training loss: 0.001564451493\n",
      "Epoch: 3785 | training loss: 0.001561049139\n",
      "Epoch: 3786 | training loss: 0.001557653886\n",
      "Epoch: 3787 | training loss: 0.001554264803\n",
      "Epoch: 3788 | training loss: 0.001550881658\n",
      "Epoch: 3789 | training loss: 0.001547504216\n",
      "Epoch: 3790 | training loss: 0.001544134459\n",
      "Epoch: 3791 | training loss: 0.001540769706\n",
      "Epoch: 3792 | training loss: 0.001537412521\n",
      "Epoch: 3793 | training loss: 0.001534061157\n",
      "Epoch: 3794 | training loss: 0.001530717127\n",
      "Epoch: 3795 | training loss: 0.001527379965\n",
      "Epoch: 3796 | training loss: 0.001524049323\n",
      "Epoch: 3797 | training loss: 0.001520726015\n",
      "Epoch: 3798 | training loss: 0.001517408062\n",
      "Epoch: 3799 | training loss: 0.001514098374\n",
      "Epoch: 3800 | training loss: 0.001510794507\n",
      "Epoch: 3801 | training loss: 0.001507498091\n",
      "Epoch: 3802 | training loss: 0.001504208776\n",
      "Epoch: 3803 | training loss: 0.001500926446\n",
      "Epoch: 3804 | training loss: 0.001497650985\n",
      "Epoch: 3805 | training loss: 0.001494382392\n",
      "Epoch: 3806 | training loss: 0.001491120551\n",
      "Epoch: 3807 | training loss: 0.001487865811\n",
      "Epoch: 3808 | training loss: 0.001484617242\n",
      "Epoch: 3809 | training loss: 0.001481376821\n",
      "Epoch: 3810 | training loss: 0.001478143269\n",
      "Epoch: 3811 | training loss: 0.001474916469\n",
      "Epoch: 3812 | training loss: 0.001471697236\n",
      "Epoch: 3813 | training loss: 0.001468485454\n",
      "Epoch: 3814 | training loss: 0.001465280890\n",
      "Epoch: 3815 | training loss: 0.001462082029\n",
      "Epoch: 3816 | training loss: 0.001458891202\n",
      "Epoch: 3817 | training loss: 0.001455706893\n",
      "Epoch: 3818 | training loss: 0.001452531549\n",
      "Epoch: 3819 | training loss: 0.001449362608\n",
      "Epoch: 3820 | training loss: 0.001446201117\n",
      "Epoch: 3821 | training loss: 0.001443047076\n",
      "Epoch: 3822 | training loss: 0.001439900487\n",
      "Epoch: 3823 | training loss: 0.001436761115\n",
      "Epoch: 3824 | training loss: 0.001433628378\n",
      "Epoch: 3825 | training loss: 0.001430504373\n",
      "Epoch: 3826 | training loss: 0.001427387353\n",
      "Epoch: 3827 | training loss: 0.001424277667\n",
      "Epoch: 3828 | training loss: 0.001421174966\n",
      "Epoch: 3829 | training loss: 0.001418079832\n",
      "Epoch: 3830 | training loss: 0.001414993312\n",
      "Epoch: 3831 | training loss: 0.001411913196\n",
      "Epoch: 3832 | training loss: 0.001408841694\n",
      "Epoch: 3833 | training loss: 0.001405776828\n",
      "Epoch: 3834 | training loss: 0.001402718946\n",
      "Epoch: 3835 | training loss: 0.001399669913\n",
      "Epoch: 3836 | training loss: 0.001396628213\n",
      "Epoch: 3837 | training loss: 0.001393593848\n",
      "Epoch: 3838 | training loss: 0.001390567166\n",
      "Epoch: 3839 | training loss: 0.001387548167\n",
      "Epoch: 3840 | training loss: 0.001384536736\n",
      "Epoch: 3841 | training loss: 0.001381533337\n",
      "Epoch: 3842 | training loss: 0.001378537621\n",
      "Epoch: 3843 | training loss: 0.001375549356\n",
      "Epoch: 3844 | training loss: 0.001372569008\n",
      "Epoch: 3845 | training loss: 0.001369596226\n",
      "Epoch: 3846 | training loss: 0.001366631477\n",
      "Epoch: 3847 | training loss: 0.001363673946\n",
      "Epoch: 3848 | training loss: 0.001360724564\n",
      "Epoch: 3849 | training loss: 0.001357782399\n",
      "Epoch: 3850 | training loss: 0.001354847569\n",
      "Epoch: 3851 | training loss: 0.001351921586\n",
      "Epoch: 3852 | training loss: 0.001349002589\n",
      "Epoch: 3853 | training loss: 0.001346091391\n",
      "Epoch: 3854 | training loss: 0.001343187410\n",
      "Epoch: 3855 | training loss: 0.001340291579\n",
      "Epoch: 3856 | training loss: 0.001337403781\n",
      "Epoch: 3857 | training loss: 0.001334522734\n",
      "Epoch: 3858 | training loss: 0.001331649837\n",
      "Epoch: 3859 | training loss: 0.001328784856\n",
      "Epoch: 3860 | training loss: 0.001325928606\n",
      "Epoch: 3861 | training loss: 0.001323078526\n",
      "Epoch: 3862 | training loss: 0.001320237061\n",
      "Epoch: 3863 | training loss: 0.001317402464\n",
      "Epoch: 3864 | training loss: 0.001314576715\n",
      "Epoch: 3865 | training loss: 0.001311758766\n",
      "Epoch: 3866 | training loss: 0.001308949315\n",
      "Epoch: 3867 | training loss: 0.001306146616\n",
      "Epoch: 3868 | training loss: 0.001303351950\n",
      "Epoch: 3869 | training loss: 0.001300565433\n",
      "Epoch: 3870 | training loss: 0.001297785668\n",
      "Epoch: 3871 | training loss: 0.001295014052\n",
      "Epoch: 3872 | training loss: 0.001292250818\n",
      "Epoch: 3873 | training loss: 0.001289494219\n",
      "Epoch: 3874 | training loss: 0.001286745770\n",
      "Epoch: 3875 | training loss: 0.001284005004\n",
      "Epoch: 3876 | training loss: 0.001281271689\n",
      "Epoch: 3877 | training loss: 0.001278546290\n",
      "Epoch: 3878 | training loss: 0.001275829971\n",
      "Epoch: 3879 | training loss: 0.001273120288\n",
      "Epoch: 3880 | training loss: 0.001270418405\n",
      "Epoch: 3881 | training loss: 0.001267723972\n",
      "Epoch: 3882 | training loss: 0.001265037106\n",
      "Epoch: 3883 | training loss: 0.001262357924\n",
      "Epoch: 3884 | training loss: 0.001259687240\n",
      "Epoch: 3885 | training loss: 0.001257023774\n",
      "Epoch: 3886 | training loss: 0.001254367991\n",
      "Epoch: 3887 | training loss: 0.001251720125\n",
      "Epoch: 3888 | training loss: 0.001249080524\n",
      "Epoch: 3889 | training loss: 0.001246447675\n",
      "Epoch: 3890 | training loss: 0.001243823674\n",
      "Epoch: 3891 | training loss: 0.001241206191\n",
      "Epoch: 3892 | training loss: 0.001238596393\n",
      "Epoch: 3893 | training loss: 0.001235994510\n",
      "Epoch: 3894 | training loss: 0.001233399147\n",
      "Epoch: 3895 | training loss: 0.001230812049\n",
      "Epoch: 3896 | training loss: 0.001228232286\n",
      "Epoch: 3897 | training loss: 0.001225661719\n",
      "Epoch: 3898 | training loss: 0.001223098370\n",
      "Epoch: 3899 | training loss: 0.001220542006\n",
      "Epoch: 3900 | training loss: 0.001217993675\n",
      "Epoch: 3901 | training loss: 0.001215452328\n",
      "Epoch: 3902 | training loss: 0.001212918898\n",
      "Epoch: 3903 | training loss: 0.001210392336\n",
      "Epoch: 3904 | training loss: 0.001207873691\n",
      "Epoch: 3905 | training loss: 0.001205362845\n",
      "Epoch: 3906 | training loss: 0.001202858984\n",
      "Epoch: 3907 | training loss: 0.001200362458\n",
      "Epoch: 3908 | training loss: 0.001197874197\n",
      "Epoch: 3909 | training loss: 0.001195393386\n",
      "Epoch: 3910 | training loss: 0.001192919794\n",
      "Epoch: 3911 | training loss: 0.001190453768\n",
      "Epoch: 3912 | training loss: 0.001187994611\n",
      "Epoch: 3913 | training loss: 0.001185543835\n",
      "Epoch: 3914 | training loss: 0.001183099695\n",
      "Epoch: 3915 | training loss: 0.001180663938\n",
      "Epoch: 3916 | training loss: 0.001178234466\n",
      "Epoch: 3917 | training loss: 0.001175812562\n",
      "Epoch: 3918 | training loss: 0.001173397992\n",
      "Epoch: 3919 | training loss: 0.001170991454\n",
      "Epoch: 3920 | training loss: 0.001168592251\n",
      "Epoch: 3921 | training loss: 0.001166200498\n",
      "Epoch: 3922 | training loss: 0.001163815730\n",
      "Epoch: 3923 | training loss: 0.001161437831\n",
      "Epoch: 3924 | training loss: 0.001159066916\n",
      "Epoch: 3925 | training loss: 0.001156703103\n",
      "Epoch: 3926 | training loss: 0.001154347090\n",
      "Epoch: 3927 | training loss: 0.001151998993\n",
      "Epoch: 3928 | training loss: 0.001149658230\n",
      "Epoch: 3929 | training loss: 0.001147324219\n",
      "Epoch: 3930 | training loss: 0.001144996844\n",
      "Epoch: 3931 | training loss: 0.001142677269\n",
      "Epoch: 3932 | training loss: 0.001140365028\n",
      "Epoch: 3933 | training loss: 0.001138059306\n",
      "Epoch: 3934 | training loss: 0.001135760802\n",
      "Epoch: 3935 | training loss: 0.001133468584\n",
      "Epoch: 3936 | training loss: 0.001131184748\n",
      "Epoch: 3937 | training loss: 0.001128907781\n",
      "Epoch: 3938 | training loss: 0.001126637799\n",
      "Epoch: 3939 | training loss: 0.001124375034\n",
      "Epoch: 3940 | training loss: 0.001122119138\n",
      "Epoch: 3941 | training loss: 0.001119870110\n",
      "Epoch: 3942 | training loss: 0.001117627486\n",
      "Epoch: 3943 | training loss: 0.001115393708\n",
      "Epoch: 3944 | training loss: 0.001113165985\n",
      "Epoch: 3945 | training loss: 0.001110945130\n",
      "Epoch: 3946 | training loss: 0.001108730910\n",
      "Epoch: 3947 | training loss: 0.001106524142\n",
      "Epoch: 3948 | training loss: 0.001104323426\n",
      "Epoch: 3949 | training loss: 0.001102129929\n",
      "Epoch: 3950 | training loss: 0.001099943416\n",
      "Epoch: 3951 | training loss: 0.001097763772\n",
      "Epoch: 3952 | training loss: 0.001095591113\n",
      "Epoch: 3953 | training loss: 0.001093424624\n",
      "Epoch: 3954 | training loss: 0.001091266051\n",
      "Epoch: 3955 | training loss: 0.001089113415\n",
      "Epoch: 3956 | training loss: 0.001086967881\n",
      "Epoch: 3957 | training loss: 0.001084828633\n",
      "Epoch: 3958 | training loss: 0.001082696486\n",
      "Epoch: 3959 | training loss: 0.001080571557\n",
      "Epoch: 3960 | training loss: 0.001078453148\n",
      "Epoch: 3961 | training loss: 0.001076341025\n",
      "Epoch: 3962 | training loss: 0.001074235304\n",
      "Epoch: 3963 | training loss: 0.001072136103\n",
      "Epoch: 3964 | training loss: 0.001070044236\n",
      "Epoch: 3965 | training loss: 0.001067959471\n",
      "Epoch: 3966 | training loss: 0.001065880526\n",
      "Epoch: 3967 | training loss: 0.001063808333\n",
      "Epoch: 3968 | training loss: 0.001061742660\n",
      "Epoch: 3969 | training loss: 0.001059683738\n",
      "Epoch: 3970 | training loss: 0.001057630638\n",
      "Epoch: 3971 | training loss: 0.001055584755\n",
      "Epoch: 3972 | training loss: 0.001053545042\n",
      "Epoch: 3973 | training loss: 0.001051511965\n",
      "Epoch: 3974 | training loss: 0.001049485058\n",
      "Epoch: 3975 | training loss: 0.001047464437\n",
      "Epoch: 3976 | training loss: 0.001045450103\n",
      "Epoch: 3977 | training loss: 0.001043442753\n",
      "Epoch: 3978 | training loss: 0.001041442621\n",
      "Epoch: 3979 | training loss: 0.001039447729\n",
      "Epoch: 3980 | training loss: 0.001037459238\n",
      "Epoch: 3981 | training loss: 0.001035477151\n",
      "Epoch: 3982 | training loss: 0.001033501350\n",
      "Epoch: 3983 | training loss: 0.001031531254\n",
      "Epoch: 3984 | training loss: 0.001029568841\n",
      "Epoch: 3985 | training loss: 0.001027611666\n",
      "Epoch: 3986 | training loss: 0.001025660429\n",
      "Epoch: 3987 | training loss: 0.001023715711\n",
      "Epoch: 3988 | training loss: 0.001021777280\n",
      "Epoch: 3989 | training loss: 0.001019844785\n",
      "Epoch: 3990 | training loss: 0.001017918577\n",
      "Epoch: 3991 | training loss: 0.001015998656\n",
      "Epoch: 3992 | training loss: 0.001014085836\n",
      "Epoch: 3993 | training loss: 0.001012177556\n",
      "Epoch: 3994 | training loss: 0.001010275795\n",
      "Epoch: 3995 | training loss: 0.001008380321\n",
      "Epoch: 3996 | training loss: 0.001006490551\n",
      "Epoch: 3997 | training loss: 0.001004607300\n",
      "Epoch: 3998 | training loss: 0.001002729405\n",
      "Epoch: 3999 | training loss: 0.001000857446\n",
      "Epoch: 4000 | training loss: 0.000998992473\n",
      "Epoch: 4001 | training loss: 0.000997132738\n",
      "Epoch: 4002 | training loss: 0.000995278591\n",
      "Epoch: 4003 | training loss: 0.000993430847\n",
      "Epoch: 4004 | training loss: 0.000991589040\n",
      "Epoch: 4005 | training loss: 0.000989752705\n",
      "Epoch: 4006 | training loss: 0.000987922656\n",
      "Epoch: 4007 | training loss: 0.000986098661\n",
      "Epoch: 4008 | training loss: 0.000984279788\n",
      "Epoch: 4009 | training loss: 0.000982467202\n",
      "Epoch: 4010 | training loss: 0.000980659970\n",
      "Epoch: 4011 | training loss: 0.000978859374\n",
      "Epoch: 4012 | training loss: 0.000977063901\n",
      "Epoch: 4013 | training loss: 0.000975273666\n",
      "Epoch: 4014 | training loss: 0.000973489427\n",
      "Epoch: 4015 | training loss: 0.000971711997\n",
      "Epoch: 4016 | training loss: 0.000969939807\n",
      "Epoch: 4017 | training loss: 0.000968172331\n",
      "Epoch: 4018 | training loss: 0.000966410968\n",
      "Epoch: 4019 | training loss: 0.000964655599\n",
      "Epoch: 4020 | training loss: 0.000962904538\n",
      "Epoch: 4021 | training loss: 0.000961160229\n",
      "Epoch: 4022 | training loss: 0.000959421275\n",
      "Epoch: 4023 | training loss: 0.000957688317\n",
      "Epoch: 4024 | training loss: 0.000955960015\n",
      "Epoch: 4025 | training loss: 0.000954237708\n",
      "Epoch: 4026 | training loss: 0.000952520350\n",
      "Epoch: 4027 | training loss: 0.000950809568\n",
      "Epoch: 4028 | training loss: 0.000949103967\n",
      "Epoch: 4029 | training loss: 0.000947402907\n",
      "Epoch: 4030 | training loss: 0.000945707201\n",
      "Epoch: 4031 | training loss: 0.000944017956\n",
      "Epoch: 4032 | training loss: 0.000942333310\n",
      "Epoch: 4033 | training loss: 0.000940654078\n",
      "Epoch: 4034 | training loss: 0.000938981073\n",
      "Epoch: 4035 | training loss: 0.000937312783\n",
      "Epoch: 4036 | training loss: 0.000935649965\n",
      "Epoch: 4037 | training loss: 0.000933991862\n",
      "Epoch: 4038 | training loss: 0.000932339812\n",
      "Epoch: 4039 | training loss: 0.000930692826\n",
      "Epoch: 4040 | training loss: 0.000929050089\n",
      "Epoch: 4041 | training loss: 0.000927413406\n",
      "Epoch: 4042 | training loss: 0.000925782078\n",
      "Epoch: 4043 | training loss: 0.000924155640\n",
      "Epoch: 4044 | training loss: 0.000922534848\n",
      "Epoch: 4045 | training loss: 0.000920917839\n",
      "Epoch: 4046 | training loss: 0.000919307407\n",
      "Epoch: 4047 | training loss: 0.000917701866\n",
      "Epoch: 4048 | training loss: 0.000916101038\n",
      "Epoch: 4049 | training loss: 0.000914505101\n",
      "Epoch: 4050 | training loss: 0.000912914693\n",
      "Epoch: 4051 | training loss: 0.000911328942\n",
      "Epoch: 4052 | training loss: 0.000909748487\n",
      "Epoch: 4053 | training loss: 0.000908173039\n",
      "Epoch: 4054 | training loss: 0.000906602363\n",
      "Epoch: 4055 | training loss: 0.000905036519\n",
      "Epoch: 4056 | training loss: 0.000903476495\n",
      "Epoch: 4057 | training loss: 0.000901921070\n",
      "Epoch: 4058 | training loss: 0.000900370185\n",
      "Epoch: 4059 | training loss: 0.000898824597\n",
      "Epoch: 4060 | training loss: 0.000897283666\n",
      "Epoch: 4061 | training loss: 0.000895747740\n",
      "Epoch: 4062 | training loss: 0.000894216122\n",
      "Epoch: 4063 | training loss: 0.000892689568\n",
      "Epoch: 4064 | training loss: 0.000891168194\n",
      "Epoch: 4065 | training loss: 0.000889651943\n",
      "Epoch: 4066 | training loss: 0.000888139708\n",
      "Epoch: 4067 | training loss: 0.000886632828\n",
      "Epoch: 4068 | training loss: 0.000885130721\n",
      "Epoch: 4069 | training loss: 0.000883632514\n",
      "Epoch: 4070 | training loss: 0.000882139080\n",
      "Epoch: 4071 | training loss: 0.000880651700\n",
      "Epoch: 4072 | training loss: 0.000879168161\n",
      "Epoch: 4073 | training loss: 0.000877688988\n",
      "Epoch: 4074 | training loss: 0.000876215636\n",
      "Epoch: 4075 | training loss: 0.000874745776\n",
      "Epoch: 4076 | training loss: 0.000873280806\n",
      "Epoch: 4077 | training loss: 0.000871820608\n",
      "Epoch: 4078 | training loss: 0.000870364660\n",
      "Epoch: 4079 | training loss: 0.000868913368\n",
      "Epoch: 4080 | training loss: 0.000867466850\n",
      "Epoch: 4081 | training loss: 0.000866024755\n",
      "Epoch: 4082 | training loss: 0.000864587491\n",
      "Epoch: 4083 | training loss: 0.000863153604\n",
      "Epoch: 4084 | training loss: 0.000861725479\n",
      "Epoch: 4085 | training loss: 0.000860301661\n",
      "Epoch: 4086 | training loss: 0.000858881336\n",
      "Epoch: 4087 | training loss: 0.000857466192\n",
      "Epoch: 4088 | training loss: 0.000856055645\n",
      "Epoch: 4089 | training loss: 0.000854649523\n",
      "Epoch: 4090 | training loss: 0.000853247358\n",
      "Epoch: 4091 | training loss: 0.000851849967\n",
      "Epoch: 4092 | training loss: 0.000850456534\n",
      "Epoch: 4093 | training loss: 0.000849067583\n",
      "Epoch: 4094 | training loss: 0.000847683288\n",
      "Epoch: 4095 | training loss: 0.000846303068\n",
      "Epoch: 4096 | training loss: 0.000844926690\n",
      "Epoch: 4097 | training loss: 0.000843555317\n",
      "Epoch: 4098 | training loss: 0.000842188194\n",
      "Epoch: 4099 | training loss: 0.000840824679\n",
      "Epoch: 4100 | training loss: 0.000839465763\n",
      "Epoch: 4101 | training loss: 0.000838111271\n",
      "Epoch: 4102 | training loss: 0.000836761494\n",
      "Epoch: 4103 | training loss: 0.000835415034\n",
      "Epoch: 4104 | training loss: 0.000834072474\n",
      "Epoch: 4105 | training loss: 0.000832734804\n",
      "Epoch: 4106 | training loss: 0.000831401325\n",
      "Epoch: 4107 | training loss: 0.000830071629\n",
      "Epoch: 4108 | training loss: 0.000828746182\n",
      "Epoch: 4109 | training loss: 0.000827424577\n",
      "Epoch: 4110 | training loss: 0.000826107513\n",
      "Epoch: 4111 | training loss: 0.000824794290\n",
      "Epoch: 4112 | training loss: 0.000823485258\n",
      "Epoch: 4113 | training loss: 0.000822180184\n",
      "Epoch: 4114 | training loss: 0.000820878660\n",
      "Epoch: 4115 | training loss: 0.000819581968\n",
      "Epoch: 4116 | training loss: 0.000818288652\n",
      "Epoch: 4117 | training loss: 0.000816999935\n",
      "Epoch: 4118 | training loss: 0.000815715175\n",
      "Epoch: 4119 | training loss: 0.000814433966\n",
      "Epoch: 4120 | training loss: 0.000813156366\n",
      "Epoch: 4121 | training loss: 0.000811883539\n",
      "Epoch: 4122 | training loss: 0.000810614089\n",
      "Epoch: 4123 | training loss: 0.000809348887\n",
      "Epoch: 4124 | training loss: 0.000808087469\n",
      "Epoch: 4125 | training loss: 0.000806830241\n",
      "Epoch: 4126 | training loss: 0.000805575983\n",
      "Epoch: 4127 | training loss: 0.000804325449\n",
      "Epoch: 4128 | training loss: 0.000803079223\n",
      "Epoch: 4129 | training loss: 0.000801837537\n",
      "Epoch: 4130 | training loss: 0.000800598878\n",
      "Epoch: 4131 | training loss: 0.000799363945\n",
      "Epoch: 4132 | training loss: 0.000798133260\n",
      "Epoch: 4133 | training loss: 0.000796906184\n",
      "Epoch: 4134 | training loss: 0.000795682427\n",
      "Epoch: 4135 | training loss: 0.000794462976\n",
      "Epoch: 4136 | training loss: 0.000793246203\n",
      "Epoch: 4137 | training loss: 0.000792034029\n",
      "Epoch: 4138 | training loss: 0.000790825696\n",
      "Epoch: 4139 | training loss: 0.000789620331\n",
      "Epoch: 4140 | training loss: 0.000788419100\n",
      "Epoch: 4141 | training loss: 0.000787221943\n",
      "Epoch: 4142 | training loss: 0.000786027638\n",
      "Epoch: 4143 | training loss: 0.000784837524\n",
      "Epoch: 4144 | training loss: 0.000783650437\n",
      "Epoch: 4145 | training loss: 0.000782467425\n",
      "Epoch: 4146 | training loss: 0.000781288021\n",
      "Epoch: 4147 | training loss: 0.000780111761\n",
      "Epoch: 4148 | training loss: 0.000778939109\n",
      "Epoch: 4149 | training loss: 0.000777770591\n",
      "Epoch: 4150 | training loss: 0.000776605273\n",
      "Epoch: 4151 | training loss: 0.000775443739\n",
      "Epoch: 4152 | training loss: 0.000774285349\n",
      "Epoch: 4153 | training loss: 0.000773130567\n",
      "Epoch: 4154 | training loss: 0.000771979161\n",
      "Epoch: 4155 | training loss: 0.000770831539\n",
      "Epoch: 4156 | training loss: 0.000769686652\n",
      "Epoch: 4157 | training loss: 0.000768545957\n",
      "Epoch: 4158 | training loss: 0.000767407939\n",
      "Epoch: 4159 | training loss: 0.000766274170\n",
      "Epoch: 4160 | training loss: 0.000765143370\n",
      "Epoch: 4161 | training loss: 0.000764016819\n",
      "Epoch: 4162 | training loss: 0.000762892654\n",
      "Epoch: 4163 | training loss: 0.000761771691\n",
      "Epoch: 4164 | training loss: 0.000760654861\n",
      "Epoch: 4165 | training loss: 0.000759540941\n",
      "Epoch: 4166 | training loss: 0.000758430455\n",
      "Epoch: 4167 | training loss: 0.000757322880\n",
      "Epoch: 4168 | training loss: 0.000756219495\n",
      "Epoch: 4169 | training loss: 0.000755118439\n",
      "Epoch: 4170 | training loss: 0.000754021050\n",
      "Epoch: 4171 | training loss: 0.000752927386\n",
      "Epoch: 4172 | training loss: 0.000751836807\n",
      "Epoch: 4173 | training loss: 0.000750748499\n",
      "Epoch: 4174 | training loss: 0.000749664847\n",
      "Epoch: 4175 | training loss: 0.000748583640\n",
      "Epoch: 4176 | training loss: 0.000747505866\n",
      "Epoch: 4177 | training loss: 0.000746431353\n",
      "Epoch: 4178 | training loss: 0.000745359925\n",
      "Epoch: 4179 | training loss: 0.000744291348\n",
      "Epoch: 4180 | training loss: 0.000743226381\n",
      "Epoch: 4181 | training loss: 0.000742164091\n",
      "Epoch: 4182 | training loss: 0.000741105177\n",
      "Epoch: 4183 | training loss: 0.000740048592\n",
      "Epoch: 4184 | training loss: 0.000738996489\n",
      "Epoch: 4185 | training loss: 0.000737946655\n",
      "Epoch: 4186 | training loss: 0.000736900256\n",
      "Epoch: 4187 | training loss: 0.000735856476\n",
      "Epoch: 4188 | training loss: 0.000734816480\n",
      "Epoch: 4189 | training loss: 0.000733779627\n",
      "Epoch: 4190 | training loss: 0.000732744695\n",
      "Epoch: 4191 | training loss: 0.000731713546\n",
      "Epoch: 4192 | training loss: 0.000730685191\n",
      "Epoch: 4193 | training loss: 0.000729660445\n",
      "Epoch: 4194 | training loss: 0.000728638319\n",
      "Epoch: 4195 | training loss: 0.000727619627\n",
      "Epoch: 4196 | training loss: 0.000726602622\n",
      "Epoch: 4197 | training loss: 0.000725589925\n",
      "Epoch: 4198 | training loss: 0.000724579790\n",
      "Epoch: 4199 | training loss: 0.000723571982\n",
      "Epoch: 4200 | training loss: 0.000722567434\n",
      "Epoch: 4201 | training loss: 0.000721566030\n",
      "Epoch: 4202 | training loss: 0.000720567710\n",
      "Epoch: 4203 | training loss: 0.000719571894\n",
      "Epoch: 4204 | training loss: 0.000718579511\n",
      "Epoch: 4205 | training loss: 0.000717588991\n",
      "Epoch: 4206 | training loss: 0.000716602022\n",
      "Epoch: 4207 | training loss: 0.000715618313\n",
      "Epoch: 4208 | training loss: 0.000714636291\n",
      "Epoch: 4209 | training loss: 0.000713658053\n",
      "Epoch: 4210 | training loss: 0.000712682551\n",
      "Epoch: 4211 | training loss: 0.000711710018\n",
      "Epoch: 4212 | training loss: 0.000710740162\n",
      "Epoch: 4213 | training loss: 0.000709772459\n",
      "Epoch: 4214 | training loss: 0.000708808308\n",
      "Epoch: 4215 | training loss: 0.000707846950\n",
      "Epoch: 4216 | training loss: 0.000706887804\n",
      "Epoch: 4217 | training loss: 0.000705931452\n",
      "Epoch: 4218 | training loss: 0.000704978418\n",
      "Epoch: 4219 | training loss: 0.000704027247\n",
      "Epoch: 4220 | training loss: 0.000703079393\n",
      "Epoch: 4221 | training loss: 0.000702134101\n",
      "Epoch: 4222 | training loss: 0.000701192010\n",
      "Epoch: 4223 | training loss: 0.000700252072\n",
      "Epoch: 4224 | training loss: 0.000699315045\n",
      "Epoch: 4225 | training loss: 0.000698380580\n",
      "Epoch: 4226 | training loss: 0.000697448093\n",
      "Epoch: 4227 | training loss: 0.000696519564\n",
      "Epoch: 4228 | training loss: 0.000695592898\n",
      "Epoch: 4229 | training loss: 0.000694669201\n",
      "Epoch: 4230 | training loss: 0.000693748589\n",
      "Epoch: 4231 | training loss: 0.000692829606\n",
      "Epoch: 4232 | training loss: 0.000691913359\n",
      "Epoch: 4233 | training loss: 0.000691000081\n",
      "Epoch: 4234 | training loss: 0.000690089189\n",
      "Epoch: 4235 | training loss: 0.000689181092\n",
      "Epoch: 4236 | training loss: 0.000688275206\n",
      "Epoch: 4237 | training loss: 0.000687372114\n",
      "Epoch: 4238 | training loss: 0.000686471583\n",
      "Epoch: 4239 | training loss: 0.000685573323\n",
      "Epoch: 4240 | training loss: 0.000684678147\n",
      "Epoch: 4241 | training loss: 0.000683785474\n",
      "Epoch: 4242 | training loss: 0.000682894723\n",
      "Epoch: 4243 | training loss: 0.000682006823\n",
      "Epoch: 4244 | training loss: 0.000681121252\n",
      "Epoch: 4245 | training loss: 0.000680238300\n",
      "Epoch: 4246 | training loss: 0.000679357443\n",
      "Epoch: 4247 | training loss: 0.000678479671\n",
      "Epoch: 4248 | training loss: 0.000677604519\n",
      "Epoch: 4249 | training loss: 0.000676731346\n",
      "Epoch: 4250 | training loss: 0.000675860734\n",
      "Epoch: 4251 | training loss: 0.000674992334\n",
      "Epoch: 4252 | training loss: 0.000674126204\n",
      "Epoch: 4253 | training loss: 0.000673262519\n",
      "Epoch: 4254 | training loss: 0.000672401860\n",
      "Epoch: 4255 | training loss: 0.000671543181\n",
      "Epoch: 4256 | training loss: 0.000670686481\n",
      "Epoch: 4257 | training loss: 0.000669832458\n",
      "Epoch: 4258 | training loss: 0.000668981287\n",
      "Epoch: 4259 | training loss: 0.000668131979\n",
      "Epoch: 4260 | training loss: 0.000667284592\n",
      "Epoch: 4261 | training loss: 0.000666440697\n",
      "Epoch: 4262 | training loss: 0.000665598200\n",
      "Epoch: 4263 | training loss: 0.000664758205\n",
      "Epoch: 4264 | training loss: 0.000663920830\n",
      "Epoch: 4265 | training loss: 0.000663085666\n",
      "Epoch: 4266 | training loss: 0.000662252656\n",
      "Epoch: 4267 | training loss: 0.000661422324\n",
      "Epoch: 4268 | training loss: 0.000660593738\n",
      "Epoch: 4269 | training loss: 0.000659767888\n",
      "Epoch: 4270 | training loss: 0.000658943958\n",
      "Epoch: 4271 | training loss: 0.000658122415\n",
      "Epoch: 4272 | training loss: 0.000657302735\n",
      "Epoch: 4273 | training loss: 0.000656485558\n",
      "Epoch: 4274 | training loss: 0.000655670825\n",
      "Epoch: 4275 | training loss: 0.000654858188\n",
      "Epoch: 4276 | training loss: 0.000654047937\n",
      "Epoch: 4277 | training loss: 0.000653239200\n",
      "Epoch: 4278 | training loss: 0.000652433140\n",
      "Epoch: 4279 | training loss: 0.000651629525\n",
      "Epoch: 4280 | training loss: 0.000650827657\n",
      "Epoch: 4281 | training loss: 0.000650028582\n",
      "Epoch: 4282 | training loss: 0.000649231370\n",
      "Epoch: 4283 | training loss: 0.000648435322\n",
      "Epoch: 4284 | training loss: 0.000647642533\n",
      "Epoch: 4285 | training loss: 0.000646851666\n",
      "Epoch: 4286 | training loss: 0.000646062603\n",
      "Epoch: 4287 | training loss: 0.000645275984\n",
      "Epoch: 4288 | training loss: 0.000644491462\n",
      "Epoch: 4289 | training loss: 0.000643709151\n",
      "Epoch: 4290 | training loss: 0.000642928877\n",
      "Epoch: 4291 | training loss: 0.000642150524\n",
      "Epoch: 4292 | training loss: 0.000641373976\n",
      "Epoch: 4293 | training loss: 0.000640599988\n",
      "Epoch: 4294 | training loss: 0.000639827631\n",
      "Epoch: 4295 | training loss: 0.000639057427\n",
      "Epoch: 4296 | training loss: 0.000638289261\n",
      "Epoch: 4297 | training loss: 0.000637523597\n",
      "Epoch: 4298 | training loss: 0.000636759854\n",
      "Epoch: 4299 | training loss: 0.000635998207\n",
      "Epoch: 4300 | training loss: 0.000635238481\n",
      "Epoch: 4301 | training loss: 0.000634480326\n",
      "Epoch: 4302 | training loss: 0.000633725082\n",
      "Epoch: 4303 | training loss: 0.000632971001\n",
      "Epoch: 4304 | training loss: 0.000632219191\n",
      "Epoch: 4305 | training loss: 0.000631469418\n",
      "Epoch: 4306 | training loss: 0.000630721916\n",
      "Epoch: 4307 | training loss: 0.000629975984\n",
      "Epoch: 4308 | training loss: 0.000629231741\n",
      "Epoch: 4309 | training loss: 0.000628490234\n",
      "Epoch: 4310 | training loss: 0.000627750007\n",
      "Epoch: 4311 | training loss: 0.000627011992\n",
      "Epoch: 4312 | training loss: 0.000626275549\n",
      "Epoch: 4313 | training loss: 0.000625542016\n",
      "Epoch: 4314 | training loss: 0.000624809472\n",
      "Epoch: 4315 | training loss: 0.000624078792\n",
      "Epoch: 4316 | training loss: 0.000623350730\n",
      "Epoch: 4317 | training loss: 0.000622624357\n",
      "Epoch: 4318 | training loss: 0.000621899730\n",
      "Epoch: 4319 | training loss: 0.000621176499\n",
      "Epoch: 4320 | training loss: 0.000620456005\n",
      "Epoch: 4321 | training loss: 0.000619736791\n",
      "Epoch: 4322 | training loss: 0.000619019789\n",
      "Epoch: 4323 | training loss: 0.000618303893\n",
      "Epoch: 4324 | training loss: 0.000617590558\n",
      "Epoch: 4325 | training loss: 0.000616879552\n",
      "Epoch: 4326 | training loss: 0.000616169476\n",
      "Epoch: 4327 | training loss: 0.000615461613\n",
      "Epoch: 4328 | training loss: 0.000614755205\n",
      "Epoch: 4329 | training loss: 0.000614051241\n",
      "Epoch: 4330 | training loss: 0.000613348617\n",
      "Epoch: 4331 | training loss: 0.000612648146\n",
      "Epoch: 4332 | training loss: 0.000611949188\n",
      "Epoch: 4333 | training loss: 0.000611252093\n",
      "Epoch: 4334 | training loss: 0.000610556977\n",
      "Epoch: 4335 | training loss: 0.000609863549\n",
      "Epoch: 4336 | training loss: 0.000609171751\n",
      "Epoch: 4337 | training loss: 0.000608481758\n",
      "Epoch: 4338 | training loss: 0.000607793452\n",
      "Epoch: 4339 | training loss: 0.000607107242\n",
      "Epoch: 4340 | training loss: 0.000606422429\n",
      "Epoch: 4341 | training loss: 0.000605739187\n",
      "Epoch: 4342 | training loss: 0.000605057634\n",
      "Epoch: 4343 | training loss: 0.000604378292\n",
      "Epoch: 4344 | training loss: 0.000603700522\n",
      "Epoch: 4345 | training loss: 0.000603024382\n",
      "Epoch: 4346 | training loss: 0.000602349930\n",
      "Epoch: 4347 | training loss: 0.000601677340\n",
      "Epoch: 4348 | training loss: 0.000601006497\n",
      "Epoch: 4349 | training loss: 0.000600336876\n",
      "Epoch: 4350 | training loss: 0.000599669700\n",
      "Epoch: 4351 | training loss: 0.000599003513\n",
      "Epoch: 4352 | training loss: 0.000598339073\n",
      "Epoch: 4353 | training loss: 0.000597676728\n",
      "Epoch: 4354 | training loss: 0.000597015431\n",
      "Epoch: 4355 | training loss: 0.000596356229\n",
      "Epoch: 4356 | training loss: 0.000595698599\n",
      "Epoch: 4357 | training loss: 0.000595042540\n",
      "Epoch: 4358 | training loss: 0.000594388286\n",
      "Epoch: 4359 | training loss: 0.000593735429\n",
      "Epoch: 4360 | training loss: 0.000593084784\n",
      "Epoch: 4361 | training loss: 0.000592434837\n",
      "Epoch: 4362 | training loss: 0.000591786811\n",
      "Epoch: 4363 | training loss: 0.000591140531\n",
      "Epoch: 4364 | training loss: 0.000590495707\n",
      "Epoch: 4365 | training loss: 0.000589852803\n",
      "Epoch: 4366 | training loss: 0.000589211297\n",
      "Epoch: 4367 | training loss: 0.000588571420\n",
      "Epoch: 4368 | training loss: 0.000587933522\n",
      "Epoch: 4369 | training loss: 0.000587296614\n",
      "Epoch: 4370 | training loss: 0.000586661685\n",
      "Epoch: 4371 | training loss: 0.000586027745\n",
      "Epoch: 4372 | training loss: 0.000585395726\n",
      "Epoch: 4373 | training loss: 0.000584765628\n",
      "Epoch: 4374 | training loss: 0.000584136695\n",
      "Epoch: 4375 | training loss: 0.000583508809\n",
      "Epoch: 4376 | training loss: 0.000582882785\n",
      "Epoch: 4377 | training loss: 0.000582258741\n",
      "Epoch: 4378 | training loss: 0.000581635977\n",
      "Epoch: 4379 | training loss: 0.000581014669\n",
      "Epoch: 4380 | training loss: 0.000580394699\n",
      "Epoch: 4381 | training loss: 0.000579776475\n",
      "Epoch: 4382 | training loss: 0.000579160056\n",
      "Epoch: 4383 | training loss: 0.000578545034\n",
      "Epoch: 4384 | training loss: 0.000577931176\n",
      "Epoch: 4385 | training loss: 0.000577318307\n",
      "Epoch: 4386 | training loss: 0.000576707767\n",
      "Epoch: 4387 | training loss: 0.000576098391\n",
      "Epoch: 4388 | training loss: 0.000575490645\n",
      "Epoch: 4389 | training loss: 0.000574884238\n",
      "Epoch: 4390 | training loss: 0.000574278936\n",
      "Epoch: 4391 | training loss: 0.000573674799\n",
      "Epoch: 4392 | training loss: 0.000573072990\n",
      "Epoch: 4393 | training loss: 0.000572472229\n",
      "Epoch: 4394 | training loss: 0.000571872981\n",
      "Epoch: 4395 | training loss: 0.000571275013\n",
      "Epoch: 4396 | training loss: 0.000570678734\n",
      "Epoch: 4397 | training loss: 0.000570083444\n",
      "Epoch: 4398 | training loss: 0.000569489668\n",
      "Epoch: 4399 | training loss: 0.000568897696\n",
      "Epoch: 4400 | training loss: 0.000568306947\n",
      "Epoch: 4401 | training loss: 0.000567717711\n",
      "Epoch: 4402 | training loss: 0.000567129522\n",
      "Epoch: 4403 | training loss: 0.000566542614\n",
      "Epoch: 4404 | training loss: 0.000565957511\n",
      "Epoch: 4405 | training loss: 0.000565373339\n",
      "Epoch: 4406 | training loss: 0.000564790447\n",
      "Epoch: 4407 | training loss: 0.000564209477\n",
      "Epoch: 4408 | training loss: 0.000563629786\n",
      "Epoch: 4409 | training loss: 0.000563051028\n",
      "Epoch: 4410 | training loss: 0.000562474423\n",
      "Epoch: 4411 | training loss: 0.000561898749\n",
      "Epoch: 4412 | training loss: 0.000561323890\n",
      "Epoch: 4413 | training loss: 0.000560751127\n",
      "Epoch: 4414 | training loss: 0.000560179469\n",
      "Epoch: 4415 | training loss: 0.000559609034\n",
      "Epoch: 4416 | training loss: 0.000559039880\n",
      "Epoch: 4417 | training loss: 0.000558472122\n",
      "Epoch: 4418 | training loss: 0.000557905703\n",
      "Epoch: 4419 | training loss: 0.000557340507\n",
      "Epoch: 4420 | training loss: 0.000556776475\n",
      "Epoch: 4421 | training loss: 0.000556214014\n",
      "Epoch: 4422 | training loss: 0.000555652950\n",
      "Epoch: 4423 | training loss: 0.000555092876\n",
      "Epoch: 4424 | training loss: 0.000554534781\n",
      "Epoch: 4425 | training loss: 0.000553977326\n",
      "Epoch: 4426 | training loss: 0.000553421327\n",
      "Epoch: 4427 | training loss: 0.000552866491\n",
      "Epoch: 4428 | training loss: 0.000552312704\n",
      "Epoch: 4429 | training loss: 0.000551760662\n",
      "Epoch: 4430 | training loss: 0.000551209436\n",
      "Epoch: 4431 | training loss: 0.000550660247\n",
      "Epoch: 4432 | training loss: 0.000550111465\n",
      "Epoch: 4433 | training loss: 0.000549564254\n",
      "Epoch: 4434 | training loss: 0.000549018441\n",
      "Epoch: 4435 | training loss: 0.000548473210\n",
      "Epoch: 4436 | training loss: 0.000547930016\n",
      "Epoch: 4437 | training loss: 0.000547387695\n",
      "Epoch: 4438 | training loss: 0.000546846422\n",
      "Epoch: 4439 | training loss: 0.000546306721\n",
      "Epoch: 4440 | training loss: 0.000545768125\n",
      "Epoch: 4441 | training loss: 0.000545230578\n",
      "Epoch: 4442 | training loss: 0.000544694427\n",
      "Epoch: 4443 | training loss: 0.000544159091\n",
      "Epoch: 4444 | training loss: 0.000543625269\n",
      "Epoch: 4445 | training loss: 0.000543092494\n",
      "Epoch: 4446 | training loss: 0.000542561640\n",
      "Epoch: 4447 | training loss: 0.000542031019\n",
      "Epoch: 4448 | training loss: 0.000541502028\n",
      "Epoch: 4449 | training loss: 0.000540973968\n",
      "Epoch: 4450 | training loss: 0.000540447538\n",
      "Epoch: 4451 | training loss: 0.000539921632\n",
      "Epoch: 4452 | training loss: 0.000539397064\n",
      "Epoch: 4453 | training loss: 0.000538874301\n",
      "Epoch: 4454 | training loss: 0.000538352062\n",
      "Epoch: 4455 | training loss: 0.000537830812\n",
      "Epoch: 4456 | training loss: 0.000537311076\n",
      "Epoch: 4457 | training loss: 0.000536792562\n",
      "Epoch: 4458 | training loss: 0.000536275213\n",
      "Epoch: 4459 | training loss: 0.000535758270\n",
      "Epoch: 4460 | training loss: 0.000535243074\n",
      "Epoch: 4461 | training loss: 0.000534729217\n",
      "Epoch: 4462 | training loss: 0.000534216117\n",
      "Epoch: 4463 | training loss: 0.000533704471\n",
      "Epoch: 4464 | training loss: 0.000533193583\n",
      "Epoch: 4465 | training loss: 0.000532683975\n",
      "Epoch: 4466 | training loss: 0.000532175589\n",
      "Epoch: 4467 | training loss: 0.000531668018\n",
      "Epoch: 4468 | training loss: 0.000531162019\n",
      "Epoch: 4469 | training loss: 0.000530656194\n",
      "Epoch: 4470 | training loss: 0.000530152582\n",
      "Epoch: 4471 | training loss: 0.000529649784\n",
      "Epoch: 4472 | training loss: 0.000529147394\n",
      "Epoch: 4473 | training loss: 0.000528646866\n",
      "Epoch: 4474 | training loss: 0.000528146920\n",
      "Epoch: 4475 | training loss: 0.000527648372\n",
      "Epoch: 4476 | training loss: 0.000527150696\n",
      "Epoch: 4477 | training loss: 0.000526654010\n",
      "Epoch: 4478 | training loss: 0.000526158779\n",
      "Epoch: 4479 | training loss: 0.000525664131\n",
      "Epoch: 4480 | training loss: 0.000525170763\n",
      "Epoch: 4481 | training loss: 0.000524678326\n",
      "Epoch: 4482 | training loss: 0.000524187286\n",
      "Epoch: 4483 | training loss: 0.000523696945\n",
      "Epoch: 4484 | training loss: 0.000523207651\n",
      "Epoch: 4485 | training loss: 0.000522719405\n",
      "Epoch: 4486 | training loss: 0.000522232265\n",
      "Epoch: 4487 | training loss: 0.000521745882\n",
      "Epoch: 4488 | training loss: 0.000521261187\n",
      "Epoch: 4489 | training loss: 0.000520776841\n",
      "Epoch: 4490 | training loss: 0.000520294125\n",
      "Epoch: 4491 | training loss: 0.000519811816\n",
      "Epoch: 4492 | training loss: 0.000519330497\n",
      "Epoch: 4493 | training loss: 0.000518851040\n",
      "Epoch: 4494 | training loss: 0.000518371875\n",
      "Epoch: 4495 | training loss: 0.000517893641\n",
      "Epoch: 4496 | training loss: 0.000517416396\n",
      "Epoch: 4497 | training loss: 0.000516940490\n",
      "Epoch: 4498 | training loss: 0.000516465574\n",
      "Epoch: 4499 | training loss: 0.000515991240\n",
      "Epoch: 4500 | training loss: 0.000515518070\n",
      "Epoch: 4501 | training loss: 0.000515045482\n",
      "Epoch: 4502 | training loss: 0.000514574873\n",
      "Epoch: 4503 | training loss: 0.000514104613\n",
      "Epoch: 4504 | training loss: 0.000513635576\n",
      "Epoch: 4505 | training loss: 0.000513167062\n",
      "Epoch: 4506 | training loss: 0.000512700004\n",
      "Epoch: 4507 | training loss: 0.000512233702\n",
      "Epoch: 4508 | training loss: 0.000511768158\n",
      "Epoch: 4509 | training loss: 0.000511304010\n",
      "Epoch: 4510 | training loss: 0.000510840211\n",
      "Epoch: 4511 | training loss: 0.000510378100\n",
      "Epoch: 4512 | training loss: 0.000509916688\n",
      "Epoch: 4513 | training loss: 0.000509455916\n",
      "Epoch: 4514 | training loss: 0.000508996018\n",
      "Epoch: 4515 | training loss: 0.000508537516\n",
      "Epoch: 4516 | training loss: 0.000508079713\n",
      "Epoch: 4517 | training loss: 0.000507622724\n",
      "Epoch: 4518 | training loss: 0.000507166667\n",
      "Epoch: 4519 | training loss: 0.000506711600\n",
      "Epoch: 4520 | training loss: 0.000506257231\n",
      "Epoch: 4521 | training loss: 0.000505804259\n",
      "Epoch: 4522 | training loss: 0.000505352276\n",
      "Epoch: 4523 | training loss: 0.000504900236\n",
      "Epoch: 4524 | training loss: 0.000504449650\n",
      "Epoch: 4525 | training loss: 0.000504000054\n",
      "Epoch: 4526 | training loss: 0.000503551972\n",
      "Epoch: 4527 | training loss: 0.000503104180\n",
      "Epoch: 4528 | training loss: 0.000502656971\n",
      "Epoch: 4529 | training loss: 0.000502210925\n",
      "Epoch: 4530 | training loss: 0.000501765579\n",
      "Epoch: 4531 | training loss: 0.000501321687\n",
      "Epoch: 4532 | training loss: 0.000500877970\n",
      "Epoch: 4533 | training loss: 0.000500435184\n",
      "Epoch: 4534 | training loss: 0.000499994028\n",
      "Epoch: 4535 | training loss: 0.000499552698\n",
      "Epoch: 4536 | training loss: 0.000499112823\n",
      "Epoch: 4537 | training loss: 0.000498674344\n",
      "Epoch: 4538 | training loss: 0.000498235808\n",
      "Epoch: 4539 | training loss: 0.000497798144\n",
      "Epoch: 4540 | training loss: 0.000497362111\n",
      "Epoch: 4541 | training loss: 0.000496926834\n",
      "Epoch: 4542 | training loss: 0.000496491673\n",
      "Epoch: 4543 | training loss: 0.000496057793\n",
      "Epoch: 4544 | training loss: 0.000495624554\n",
      "Epoch: 4545 | training loss: 0.000495192537\n",
      "Epoch: 4546 | training loss: 0.000494760985\n",
      "Epoch: 4547 | training loss: 0.000494330423\n",
      "Epoch: 4548 | training loss: 0.000493900909\n",
      "Epoch: 4549 | training loss: 0.000493472093\n",
      "Epoch: 4550 | training loss: 0.000493043801\n",
      "Epoch: 4551 | training loss: 0.000492616789\n",
      "Epoch: 4552 | training loss: 0.000492190418\n",
      "Epoch: 4553 | training loss: 0.000491764746\n",
      "Epoch: 4554 | training loss: 0.000491340004\n",
      "Epoch: 4555 | training loss: 0.000490916020\n",
      "Epoch: 4556 | training loss: 0.000490492792\n",
      "Epoch: 4557 | training loss: 0.000490070204\n",
      "Epoch: 4558 | training loss: 0.000489648548\n",
      "Epoch: 4559 | training loss: 0.000489227998\n",
      "Epoch: 4560 | training loss: 0.000488807913\n",
      "Epoch: 4561 | training loss: 0.000488388760\n",
      "Epoch: 4562 | training loss: 0.000487970072\n",
      "Epoch: 4563 | training loss: 0.000487552374\n",
      "Epoch: 4564 | training loss: 0.000487135723\n",
      "Epoch: 4565 | training loss: 0.000486719975\n",
      "Epoch: 4566 | training loss: 0.000486304663\n",
      "Epoch: 4567 | training loss: 0.000485889963\n",
      "Epoch: 4568 | training loss: 0.000485476630\n",
      "Epoch: 4569 | training loss: 0.000485063531\n",
      "Epoch: 4570 | training loss: 0.000484651100\n",
      "Epoch: 4571 | training loss: 0.000484239834\n",
      "Epoch: 4572 | training loss: 0.000483829295\n",
      "Epoch: 4573 | training loss: 0.000483419572\n",
      "Epoch: 4574 | training loss: 0.000483010605\n",
      "Epoch: 4575 | training loss: 0.000482602190\n",
      "Epoch: 4576 | training loss: 0.000482194388\n",
      "Epoch: 4577 | training loss: 0.000481788011\n",
      "Epoch: 4578 | training loss: 0.000481381692\n",
      "Epoch: 4579 | training loss: 0.000480976392\n",
      "Epoch: 4580 | training loss: 0.000480571907\n",
      "Epoch: 4581 | training loss: 0.000480167946\n",
      "Epoch: 4582 | training loss: 0.000479764858\n",
      "Epoch: 4583 | training loss: 0.000479362527\n",
      "Epoch: 4584 | training loss: 0.000478961098\n",
      "Epoch: 4585 | training loss: 0.000478560134\n",
      "Epoch: 4586 | training loss: 0.000478159578\n",
      "Epoch: 4587 | training loss: 0.000477760099\n",
      "Epoch: 4588 | training loss: 0.000477361958\n",
      "Epoch: 4589 | training loss: 0.000476963673\n",
      "Epoch: 4590 | training loss: 0.000476566260\n",
      "Epoch: 4591 | training loss: 0.000476169982\n",
      "Epoch: 4592 | training loss: 0.000475774345\n",
      "Epoch: 4593 | training loss: 0.000475379580\n",
      "Epoch: 4594 | training loss: 0.000474985049\n",
      "Epoch: 4595 | training loss: 0.000474591536\n",
      "Epoch: 4596 | training loss: 0.000474198634\n",
      "Epoch: 4597 | training loss: 0.000473806111\n",
      "Epoch: 4598 | training loss: 0.000473414664\n",
      "Epoch: 4599 | training loss: 0.000473024003\n",
      "Epoch: 4600 | training loss: 0.000472634158\n",
      "Epoch: 4601 | training loss: 0.000472244225\n",
      "Epoch: 4602 | training loss: 0.000471855397\n",
      "Epoch: 4603 | training loss: 0.000471467472\n",
      "Epoch: 4604 | training loss: 0.000471080246\n",
      "Epoch: 4605 | training loss: 0.000470693398\n",
      "Epoch: 4606 | training loss: 0.000470307423\n",
      "Epoch: 4607 | training loss: 0.000469922059\n",
      "Epoch: 4608 | training loss: 0.000469537772\n",
      "Epoch: 4609 | training loss: 0.000469153543\n",
      "Epoch: 4610 | training loss: 0.000468770071\n",
      "Epoch: 4611 | training loss: 0.000468387239\n",
      "Epoch: 4612 | training loss: 0.000468005426\n",
      "Epoch: 4613 | training loss: 0.000467624050\n",
      "Epoch: 4614 | training loss: 0.000467243488\n",
      "Epoch: 4615 | training loss: 0.000466863770\n",
      "Epoch: 4616 | training loss: 0.000466484315\n",
      "Epoch: 4617 | training loss: 0.000466105470\n",
      "Epoch: 4618 | training loss: 0.000465727819\n",
      "Epoch: 4619 | training loss: 0.000465349993\n",
      "Epoch: 4620 | training loss: 0.000464973215\n",
      "Epoch: 4621 | training loss: 0.000464597397\n",
      "Epoch: 4622 | training loss: 0.000464221783\n",
      "Epoch: 4623 | training loss: 0.000463847042\n",
      "Epoch: 4624 | training loss: 0.000463473349\n",
      "Epoch: 4625 | training loss: 0.000463099394\n",
      "Epoch: 4626 | training loss: 0.000462727039\n",
      "Epoch: 4627 | training loss: 0.000462354452\n",
      "Epoch: 4628 | training loss: 0.000461983203\n",
      "Epoch: 4629 | training loss: 0.000461612246\n",
      "Epoch: 4630 | training loss: 0.000461242627\n",
      "Epoch: 4631 | training loss: 0.000460872485\n",
      "Epoch: 4632 | training loss: 0.000460503536\n",
      "Epoch: 4633 | training loss: 0.000460135518\n",
      "Epoch: 4634 | training loss: 0.000459767762\n",
      "Epoch: 4635 | training loss: 0.000459400588\n",
      "Epoch: 4636 | training loss: 0.000459033763\n",
      "Epoch: 4637 | training loss: 0.000458667870\n",
      "Epoch: 4638 | training loss: 0.000458302791\n",
      "Epoch: 4639 | training loss: 0.000457938091\n",
      "Epoch: 4640 | training loss: 0.000457574381\n",
      "Epoch: 4641 | training loss: 0.000457210786\n",
      "Epoch: 4642 | training loss: 0.000456847774\n",
      "Epoch: 4643 | training loss: 0.000456485461\n",
      "Epoch: 4644 | training loss: 0.000456123671\n",
      "Epoch: 4645 | training loss: 0.000455763133\n",
      "Epoch: 4646 | training loss: 0.000455402711\n",
      "Epoch: 4647 | training loss: 0.000455042755\n",
      "Epoch: 4648 | training loss: 0.000454683555\n",
      "Epoch: 4649 | training loss: 0.000454324618\n",
      "Epoch: 4650 | training loss: 0.000453966815\n",
      "Epoch: 4651 | training loss: 0.000453609624\n",
      "Epoch: 4652 | training loss: 0.000453252782\n",
      "Epoch: 4653 | training loss: 0.000452896202\n",
      "Epoch: 4654 | training loss: 0.000452540931\n",
      "Epoch: 4655 | training loss: 0.000452185719\n",
      "Epoch: 4656 | training loss: 0.000451831031\n",
      "Epoch: 4657 | training loss: 0.000451476895\n",
      "Epoch: 4658 | training loss: 0.000451124273\n",
      "Epoch: 4659 | training loss: 0.000450771040\n",
      "Epoch: 4660 | training loss: 0.000450419000\n",
      "Epoch: 4661 | training loss: 0.000450067135\n",
      "Epoch: 4662 | training loss: 0.000449716114\n",
      "Epoch: 4663 | training loss: 0.000449365936\n",
      "Epoch: 4664 | training loss: 0.000449015846\n",
      "Epoch: 4665 | training loss: 0.000448666455\n",
      "Epoch: 4666 | training loss: 0.000448317325\n",
      "Epoch: 4667 | training loss: 0.000447969272\n",
      "Epoch: 4668 | training loss: 0.000447621627\n",
      "Epoch: 4669 | training loss: 0.000447274360\n",
      "Epoch: 4670 | training loss: 0.000446927763\n",
      "Epoch: 4671 | training loss: 0.000446581602\n",
      "Epoch: 4672 | training loss: 0.000446236343\n",
      "Epoch: 4673 | training loss: 0.000445891201\n",
      "Epoch: 4674 | training loss: 0.000445546728\n",
      "Epoch: 4675 | training loss: 0.000445202983\n",
      "Epoch: 4676 | training loss: 0.000444859674\n",
      "Epoch: 4677 | training loss: 0.000444517122\n",
      "Epoch: 4678 | training loss: 0.000444174599\n",
      "Epoch: 4679 | training loss: 0.000443833065\n",
      "Epoch: 4680 | training loss: 0.000443491765\n",
      "Epoch: 4681 | training loss: 0.000443151512\n",
      "Epoch: 4682 | training loss: 0.000442811113\n",
      "Epoch: 4683 | training loss: 0.000442471704\n",
      "Epoch: 4684 | training loss: 0.000442132703\n",
      "Epoch: 4685 | training loss: 0.000441794225\n",
      "Epoch: 4686 | training loss: 0.000441456039\n",
      "Epoch: 4687 | training loss: 0.000441118493\n",
      "Epoch: 4688 | training loss: 0.000440781936\n",
      "Epoch: 4689 | training loss: 0.000440445496\n",
      "Epoch: 4690 | training loss: 0.000440109812\n",
      "Epoch: 4691 | training loss: 0.000439774420\n",
      "Epoch: 4692 | training loss: 0.000439439289\n",
      "Epoch: 4693 | training loss: 0.000439105643\n",
      "Epoch: 4694 | training loss: 0.000438771414\n",
      "Epoch: 4695 | training loss: 0.000438438263\n",
      "Epoch: 4696 | training loss: 0.000438105373\n",
      "Epoch: 4697 | training loss: 0.000437773211\n",
      "Epoch: 4698 | training loss: 0.000437441107\n",
      "Epoch: 4699 | training loss: 0.000437109848\n",
      "Epoch: 4700 | training loss: 0.000436779141\n",
      "Epoch: 4701 | training loss: 0.000436448783\n",
      "Epoch: 4702 | training loss: 0.000436119095\n",
      "Epoch: 4703 | training loss: 0.000435789989\n",
      "Epoch: 4704 | training loss: 0.000435461261\n",
      "Epoch: 4705 | training loss: 0.000435132912\n",
      "Epoch: 4706 | training loss: 0.000434804853\n",
      "Epoch: 4707 | training loss: 0.000434477755\n",
      "Epoch: 4708 | training loss: 0.000434150745\n",
      "Epoch: 4709 | training loss: 0.000433824811\n",
      "Epoch: 4710 | training loss: 0.000433498994\n",
      "Epoch: 4711 | training loss: 0.000433173613\n",
      "Epoch: 4712 | training loss: 0.000432848843\n",
      "Epoch: 4713 | training loss: 0.000432524539\n",
      "Epoch: 4714 | training loss: 0.000432200410\n",
      "Epoch: 4715 | training loss: 0.000431877066\n",
      "Epoch: 4716 | training loss: 0.000431554159\n",
      "Epoch: 4717 | training loss: 0.000431232038\n",
      "Epoch: 4718 | training loss: 0.000430910150\n",
      "Epoch: 4719 | training loss: 0.000430588785\n",
      "Epoch: 4720 | training loss: 0.000430267741\n",
      "Epoch: 4721 | training loss: 0.000429947075\n",
      "Epoch: 4722 | training loss: 0.000429626787\n",
      "Epoch: 4723 | training loss: 0.000429307343\n",
      "Epoch: 4724 | training loss: 0.000428988365\n",
      "Epoch: 4725 | training loss: 0.000428669649\n",
      "Epoch: 4726 | training loss: 0.000428351545\n",
      "Epoch: 4727 | training loss: 0.000428033818\n",
      "Epoch: 4728 | training loss: 0.000427716732\n",
      "Epoch: 4729 | training loss: 0.000427400053\n",
      "Epoch: 4730 | training loss: 0.000427083898\n",
      "Epoch: 4731 | training loss: 0.000426767976\n",
      "Epoch: 4732 | training loss: 0.000426452752\n",
      "Epoch: 4733 | training loss: 0.000426137674\n",
      "Epoch: 4734 | training loss: 0.000425822975\n",
      "Epoch: 4735 | training loss: 0.000425508828\n",
      "Epoch: 4736 | training loss: 0.000425195583\n",
      "Epoch: 4737 | training loss: 0.000424882106\n",
      "Epoch: 4738 | training loss: 0.000424569298\n",
      "Epoch: 4739 | training loss: 0.000424257480\n",
      "Epoch: 4740 | training loss: 0.000423945428\n",
      "Epoch: 4741 | training loss: 0.000423634192\n",
      "Epoch: 4742 | training loss: 0.000423323188\n",
      "Epoch: 4743 | training loss: 0.000423012651\n",
      "Epoch: 4744 | training loss: 0.000422702578\n",
      "Epoch: 4745 | training loss: 0.000422392943\n",
      "Epoch: 4746 | training loss: 0.000422084355\n",
      "Epoch: 4747 | training loss: 0.000421775418\n",
      "Epoch: 4748 | training loss: 0.000421467004\n",
      "Epoch: 4749 | training loss: 0.000421159697\n",
      "Epoch: 4750 | training loss: 0.000420852040\n",
      "Epoch: 4751 | training loss: 0.000420544908\n",
      "Epoch: 4752 | training loss: 0.000420238415\n",
      "Epoch: 4753 | training loss: 0.000419932243\n",
      "Epoch: 4754 | training loss: 0.000419627351\n",
      "Epoch: 4755 | training loss: 0.000419321732\n",
      "Epoch: 4756 | training loss: 0.000419016811\n",
      "Epoch: 4757 | training loss: 0.000418712269\n",
      "Epoch: 4758 | training loss: 0.000418408308\n",
      "Epoch: 4759 | training loss: 0.000418104755\n",
      "Epoch: 4760 | training loss: 0.000417801697\n",
      "Epoch: 4761 | training loss: 0.000417498872\n",
      "Epoch: 4762 | training loss: 0.000417196425\n",
      "Epoch: 4763 | training loss: 0.000416894298\n",
      "Epoch: 4764 | training loss: 0.000416593248\n",
      "Epoch: 4765 | training loss: 0.000416292110\n",
      "Epoch: 4766 | training loss: 0.000415991177\n",
      "Epoch: 4767 | training loss: 0.000415690884\n",
      "Epoch: 4768 | training loss: 0.000415391172\n",
      "Epoch: 4769 | training loss: 0.000415091461\n",
      "Epoch: 4770 | training loss: 0.000414792972\n",
      "Epoch: 4771 | training loss: 0.000414494338\n",
      "Epoch: 4772 | training loss: 0.000414195994\n",
      "Epoch: 4773 | training loss: 0.000413897680\n",
      "Epoch: 4774 | training loss: 0.000413600297\n",
      "Epoch: 4775 | training loss: 0.000413303380\n",
      "Epoch: 4776 | training loss: 0.000413007161\n",
      "Epoch: 4777 | training loss: 0.000412710913\n",
      "Epoch: 4778 | training loss: 0.000412414869\n",
      "Epoch: 4779 | training loss: 0.000412119494\n",
      "Epoch: 4780 | training loss: 0.000411824411\n",
      "Epoch: 4781 | training loss: 0.000411529792\n",
      "Epoch: 4782 | training loss: 0.000411235233\n",
      "Epoch: 4783 | training loss: 0.000410941633\n",
      "Epoch: 4784 | training loss: 0.000410648441\n",
      "Epoch: 4785 | training loss: 0.000410355104\n",
      "Epoch: 4786 | training loss: 0.000410062348\n",
      "Epoch: 4787 | training loss: 0.000409770350\n",
      "Epoch: 4788 | training loss: 0.000409477798\n",
      "Epoch: 4789 | training loss: 0.000409186323\n",
      "Epoch: 4790 | training loss: 0.000408894994\n",
      "Epoch: 4791 | training loss: 0.000408604217\n",
      "Epoch: 4792 | training loss: 0.000408313965\n",
      "Epoch: 4793 | training loss: 0.000408024032\n",
      "Epoch: 4794 | training loss: 0.000407734391\n",
      "Epoch: 4795 | training loss: 0.000407445244\n",
      "Epoch: 4796 | training loss: 0.000407156098\n",
      "Epoch: 4797 | training loss: 0.000406868174\n",
      "Epoch: 4798 | training loss: 0.000406579551\n",
      "Epoch: 4799 | training loss: 0.000406291685\n",
      "Epoch: 4800 | training loss: 0.000406004139\n",
      "Epoch: 4801 | training loss: 0.000405717350\n",
      "Epoch: 4802 | training loss: 0.000405430677\n",
      "Epoch: 4803 | training loss: 0.000405144470\n",
      "Epoch: 4804 | training loss: 0.000404858496\n",
      "Epoch: 4805 | training loss: 0.000404572638\n",
      "Epoch: 4806 | training loss: 0.000404287654\n",
      "Epoch: 4807 | training loss: 0.000404002843\n",
      "Epoch: 4808 | training loss: 0.000403718150\n",
      "Epoch: 4809 | training loss: 0.000403434358\n",
      "Epoch: 4810 | training loss: 0.000403150771\n",
      "Epoch: 4811 | training loss: 0.000402867357\n",
      "Epoch: 4812 | training loss: 0.000402584177\n",
      "Epoch: 4813 | training loss: 0.000402301783\n",
      "Epoch: 4814 | training loss: 0.000402019068\n",
      "Epoch: 4815 | training loss: 0.000401737285\n",
      "Epoch: 4816 | training loss: 0.000401455472\n",
      "Epoch: 4817 | training loss: 0.000401174359\n",
      "Epoch: 4818 | training loss: 0.000400893565\n",
      "Epoch: 4819 | training loss: 0.000400612829\n",
      "Epoch: 4820 | training loss: 0.000400332618\n",
      "Epoch: 4821 | training loss: 0.000400052726\n",
      "Epoch: 4822 | training loss: 0.000399773300\n",
      "Epoch: 4823 | training loss: 0.000399494078\n",
      "Epoch: 4824 | training loss: 0.000399215351\n",
      "Epoch: 4825 | training loss: 0.000398936885\n",
      "Epoch: 4826 | training loss: 0.000398659031\n",
      "Epoch: 4827 | training loss: 0.000398381264\n",
      "Epoch: 4828 | training loss: 0.000398103904\n",
      "Epoch: 4829 | training loss: 0.000397826720\n",
      "Epoch: 4830 | training loss: 0.000397550000\n",
      "Epoch: 4831 | training loss: 0.000397273456\n",
      "Epoch: 4832 | training loss: 0.000396997406\n",
      "Epoch: 4833 | training loss: 0.000396721502\n",
      "Epoch: 4834 | training loss: 0.000396446208\n",
      "Epoch: 4835 | training loss: 0.000396171119\n",
      "Epoch: 4836 | training loss: 0.000395896495\n",
      "Epoch: 4837 | training loss: 0.000395622337\n",
      "Epoch: 4838 | training loss: 0.000395348383\n",
      "Epoch: 4839 | training loss: 0.000395074603\n",
      "Epoch: 4840 | training loss: 0.000394801318\n",
      "Epoch: 4841 | training loss: 0.000394527946\n",
      "Epoch: 4842 | training loss: 0.000394255272\n",
      "Epoch: 4843 | training loss: 0.000393983035\n",
      "Epoch: 4844 | training loss: 0.000393710623\n",
      "Epoch: 4845 | training loss: 0.000393438793\n",
      "Epoch: 4846 | training loss: 0.000393167255\n",
      "Epoch: 4847 | training loss: 0.000392896385\n",
      "Epoch: 4848 | training loss: 0.000392625370\n",
      "Epoch: 4849 | training loss: 0.000392354850\n",
      "Epoch: 4850 | training loss: 0.000392084534\n",
      "Epoch: 4851 | training loss: 0.000391814683\n",
      "Epoch: 4852 | training loss: 0.000391545298\n",
      "Epoch: 4853 | training loss: 0.000391276611\n",
      "Epoch: 4854 | training loss: 0.000391007547\n",
      "Epoch: 4855 | training loss: 0.000390739064\n",
      "Epoch: 4856 | training loss: 0.000390470843\n",
      "Epoch: 4857 | training loss: 0.000390203058\n",
      "Epoch: 4858 | training loss: 0.000389935565\n",
      "Epoch: 4859 | training loss: 0.000389667985\n",
      "Epoch: 4860 | training loss: 0.000389400928\n",
      "Epoch: 4861 | training loss: 0.000389134308\n",
      "Epoch: 4862 | training loss: 0.000388868008\n",
      "Epoch: 4863 | training loss: 0.000388601911\n",
      "Epoch: 4864 | training loss: 0.000388336339\n",
      "Epoch: 4865 | training loss: 0.000388070999\n",
      "Epoch: 4866 | training loss: 0.000387805921\n",
      "Epoch: 4867 | training loss: 0.000387541018\n",
      "Epoch: 4868 | training loss: 0.000387276261\n",
      "Epoch: 4869 | training loss: 0.000387011794\n",
      "Epoch: 4870 | training loss: 0.000386747997\n",
      "Epoch: 4871 | training loss: 0.000386484258\n",
      "Epoch: 4872 | training loss: 0.000386220956\n",
      "Epoch: 4873 | training loss: 0.000385958178\n",
      "Epoch: 4874 | training loss: 0.000385695486\n",
      "Epoch: 4875 | training loss: 0.000385432970\n",
      "Epoch: 4876 | training loss: 0.000385171094\n",
      "Epoch: 4877 | training loss: 0.000384909159\n",
      "Epoch: 4878 | training loss: 0.000384647632\n",
      "Epoch: 4879 | training loss: 0.000384386483\n",
      "Epoch: 4880 | training loss: 0.000384125480\n",
      "Epoch: 4881 | training loss: 0.000383864710\n",
      "Epoch: 4882 | training loss: 0.000383604551\n",
      "Epoch: 4883 | training loss: 0.000383344362\n",
      "Epoch: 4884 | training loss: 0.000383084611\n",
      "Epoch: 4885 | training loss: 0.000382825499\n",
      "Epoch: 4886 | training loss: 0.000382566330\n",
      "Epoch: 4887 | training loss: 0.000382307393\n",
      "Epoch: 4888 | training loss: 0.000382048747\n",
      "Epoch: 4889 | training loss: 0.000381790625\n",
      "Epoch: 4890 | training loss: 0.000381532620\n",
      "Epoch: 4891 | training loss: 0.000381274789\n",
      "Epoch: 4892 | training loss: 0.000381017540\n",
      "Epoch: 4893 | training loss: 0.000380760350\n",
      "Epoch: 4894 | training loss: 0.000380503392\n",
      "Epoch: 4895 | training loss: 0.000380247278\n",
      "Epoch: 4896 | training loss: 0.000379990524\n",
      "Epoch: 4897 | training loss: 0.000379734556\n",
      "Epoch: 4898 | training loss: 0.000379479083\n",
      "Epoch: 4899 | training loss: 0.000379223929\n",
      "Epoch: 4900 | training loss: 0.000378968514\n",
      "Epoch: 4901 | training loss: 0.000378713798\n",
      "Epoch: 4902 | training loss: 0.000378459372\n",
      "Epoch: 4903 | training loss: 0.000378205441\n",
      "Epoch: 4904 | training loss: 0.000377951277\n",
      "Epoch: 4905 | training loss: 0.000377697725\n",
      "Epoch: 4906 | training loss: 0.000377444288\n",
      "Epoch: 4907 | training loss: 0.000377190940\n",
      "Epoch: 4908 | training loss: 0.000376938173\n",
      "Epoch: 4909 | training loss: 0.000376685784\n",
      "Epoch: 4910 | training loss: 0.000376432872\n",
      "Epoch: 4911 | training loss: 0.000376180920\n",
      "Epoch: 4912 | training loss: 0.000375929521\n",
      "Epoch: 4913 | training loss: 0.000375677773\n",
      "Epoch: 4914 | training loss: 0.000375426258\n",
      "Epoch: 4915 | training loss: 0.000375175325\n",
      "Epoch: 4916 | training loss: 0.000374924683\n",
      "Epoch: 4917 | training loss: 0.000374674477\n",
      "Epoch: 4918 | training loss: 0.000374424068\n",
      "Epoch: 4919 | training loss: 0.000374173978\n",
      "Epoch: 4920 | training loss: 0.000373924326\n",
      "Epoch: 4921 | training loss: 0.000373675139\n",
      "Epoch: 4922 | training loss: 0.000373425923\n",
      "Epoch: 4923 | training loss: 0.000373177056\n",
      "Epoch: 4924 | training loss: 0.000372928509\n",
      "Epoch: 4925 | training loss: 0.000372680370\n",
      "Epoch: 4926 | training loss: 0.000372432318\n",
      "Epoch: 4927 | training loss: 0.000372184470\n",
      "Epoch: 4928 | training loss: 0.000371937029\n",
      "Epoch: 4929 | training loss: 0.000371689763\n",
      "Epoch: 4930 | training loss: 0.000371442788\n",
      "Epoch: 4931 | training loss: 0.000371195812\n",
      "Epoch: 4932 | training loss: 0.000370949419\n",
      "Epoch: 4933 | training loss: 0.000370703026\n",
      "Epoch: 4934 | training loss: 0.000370456924\n",
      "Epoch: 4935 | training loss: 0.000370211288\n",
      "Epoch: 4936 | training loss: 0.000369965797\n",
      "Epoch: 4937 | training loss: 0.000369720918\n",
      "Epoch: 4938 | training loss: 0.000369475631\n",
      "Epoch: 4939 | training loss: 0.000369231042\n",
      "Epoch: 4940 | training loss: 0.000368986948\n",
      "Epoch: 4941 | training loss: 0.000368742825\n",
      "Epoch: 4942 | training loss: 0.000368498557\n",
      "Epoch: 4943 | training loss: 0.000368254958\n",
      "Epoch: 4944 | training loss: 0.000368011446\n",
      "Epoch: 4945 | training loss: 0.000367768575\n",
      "Epoch: 4946 | training loss: 0.000367525703\n",
      "Epoch: 4947 | training loss: 0.000367282948\n",
      "Epoch: 4948 | training loss: 0.000367040455\n",
      "Epoch: 4949 | training loss: 0.000366798224\n",
      "Epoch: 4950 | training loss: 0.000366556633\n",
      "Epoch: 4951 | training loss: 0.000366314693\n",
      "Epoch: 4952 | training loss: 0.000366073422\n",
      "Epoch: 4953 | training loss: 0.000365832355\n",
      "Epoch: 4954 | training loss: 0.000365591171\n",
      "Epoch: 4955 | training loss: 0.000365350570\n",
      "Epoch: 4956 | training loss: 0.000365109940\n",
      "Epoch: 4957 | training loss: 0.000364870124\n",
      "Epoch: 4958 | training loss: 0.000364630221\n",
      "Epoch: 4959 | training loss: 0.000364390289\n",
      "Epoch: 4960 | training loss: 0.000364150590\n",
      "Epoch: 4961 | training loss: 0.000363911444\n",
      "Epoch: 4962 | training loss: 0.000363672589\n",
      "Epoch: 4963 | training loss: 0.000363433326\n",
      "Epoch: 4964 | training loss: 0.000363194675\n",
      "Epoch: 4965 | training loss: 0.000362956722\n",
      "Epoch: 4966 | training loss: 0.000362718245\n",
      "Epoch: 4967 | training loss: 0.000362480321\n",
      "Epoch: 4968 | training loss: 0.000362243154\n",
      "Epoch: 4969 | training loss: 0.000362005376\n",
      "Epoch: 4970 | training loss: 0.000361768471\n",
      "Epoch: 4971 | training loss: 0.000361531769\n",
      "Epoch: 4972 | training loss: 0.000361295242\n",
      "Epoch: 4973 | training loss: 0.000361058628\n",
      "Epoch: 4974 | training loss: 0.000360822247\n",
      "Epoch: 4975 | training loss: 0.000360586564\n",
      "Epoch: 4976 | training loss: 0.000360350445\n",
      "Epoch: 4977 | training loss: 0.000360115257\n",
      "Epoch: 4978 | training loss: 0.000359879632\n",
      "Epoch: 4979 | training loss: 0.000359644473\n",
      "Epoch: 4980 | training loss: 0.000359409809\n",
      "Epoch: 4981 | training loss: 0.000359175378\n",
      "Epoch: 4982 | training loss: 0.000358940684\n",
      "Epoch: 4983 | training loss: 0.000358707039\n",
      "Epoch: 4984 | training loss: 0.000358473015\n",
      "Epoch: 4985 | training loss: 0.000358239631\n",
      "Epoch: 4986 | training loss: 0.000358006073\n",
      "Epoch: 4987 | training loss: 0.000357772748\n",
      "Epoch: 4988 | training loss: 0.000357539917\n",
      "Epoch: 4989 | training loss: 0.000357307232\n",
      "Epoch: 4990 | training loss: 0.000357074547\n",
      "Epoch: 4991 | training loss: 0.000356841949\n",
      "Epoch: 4992 | training loss: 0.000356610108\n",
      "Epoch: 4993 | training loss: 0.000356378354\n",
      "Epoch: 4994 | training loss: 0.000356146600\n",
      "Epoch: 4995 | training loss: 0.000355914992\n",
      "Epoch: 4996 | training loss: 0.000355683966\n",
      "Epoch: 4997 | training loss: 0.000355453143\n",
      "Epoch: 4998 | training loss: 0.000355222204\n",
      "Epoch: 4999 | training loss: 0.000354991964\n",
      "Epoch: 5000 | training loss: 0.000354761258\n",
      "Epoch: 5001 | training loss: 0.000354531192\n",
      "Epoch: 5002 | training loss: 0.000354301417\n",
      "Epoch: 5003 | training loss: 0.000354071497\n",
      "Epoch: 5004 | training loss: 0.000353842042\n",
      "Epoch: 5005 | training loss: 0.000353613228\n",
      "Epoch: 5006 | training loss: 0.000353384152\n",
      "Epoch: 5007 | training loss: 0.000353155134\n",
      "Epoch: 5008 | training loss: 0.000352926640\n",
      "Epoch: 5009 | training loss: 0.000352698145\n",
      "Epoch: 5010 | training loss: 0.000352470030\n",
      "Epoch: 5011 | training loss: 0.000352242176\n",
      "Epoch: 5012 | training loss: 0.000352014438\n",
      "Epoch: 5013 | training loss: 0.000351786788\n",
      "Epoch: 5014 | training loss: 0.000351559633\n",
      "Epoch: 5015 | training loss: 0.000351332536\n",
      "Epoch: 5016 | training loss: 0.000351105351\n",
      "Epoch: 5017 | training loss: 0.000350879011\n",
      "Epoch: 5018 | training loss: 0.000350652408\n",
      "Epoch: 5019 | training loss: 0.000350426300\n",
      "Epoch: 5020 | training loss: 0.000350200396\n",
      "Epoch: 5021 | training loss: 0.000349974551\n",
      "Epoch: 5022 | training loss: 0.000349749113\n",
      "Epoch: 5023 | training loss: 0.000349523703\n",
      "Epoch: 5024 | training loss: 0.000349298323\n",
      "Epoch: 5025 | training loss: 0.000349073525\n",
      "Epoch: 5026 | training loss: 0.000348848640\n",
      "Epoch: 5027 | training loss: 0.000348623958\n",
      "Epoch: 5028 | training loss: 0.000348399713\n",
      "Epoch: 5029 | training loss: 0.000348175323\n",
      "Epoch: 5030 | training loss: 0.000347951602\n",
      "Epoch: 5031 | training loss: 0.000347727735\n",
      "Epoch: 5032 | training loss: 0.000347504159\n",
      "Epoch: 5033 | training loss: 0.000347280613\n",
      "Epoch: 5034 | training loss: 0.000347057241\n",
      "Epoch: 5035 | training loss: 0.000346834597\n",
      "Epoch: 5036 | training loss: 0.000346611778\n",
      "Epoch: 5037 | training loss: 0.000346389221\n",
      "Epoch: 5038 | training loss: 0.000346166984\n",
      "Epoch: 5039 | training loss: 0.000345944660\n",
      "Epoch: 5040 | training loss: 0.000345722277\n",
      "Epoch: 5041 | training loss: 0.000345500914\n",
      "Epoch: 5042 | training loss: 0.000345279084\n",
      "Epoch: 5043 | training loss: 0.000345057895\n",
      "Epoch: 5044 | training loss: 0.000344836735\n",
      "Epoch: 5045 | training loss: 0.000344615895\n",
      "Epoch: 5046 | training loss: 0.000344394997\n",
      "Epoch: 5047 | training loss: 0.000344174652\n",
      "Epoch: 5048 | training loss: 0.000343954336\n",
      "Epoch: 5049 | training loss: 0.000343734020\n",
      "Epoch: 5050 | training loss: 0.000343514141\n",
      "Epoch: 5051 | training loss: 0.000343294465\n",
      "Epoch: 5052 | training loss: 0.000343074964\n",
      "Epoch: 5053 | training loss: 0.000342855084\n",
      "Epoch: 5054 | training loss: 0.000342636020\n",
      "Epoch: 5055 | training loss: 0.000342416868\n",
      "Epoch: 5056 | training loss: 0.000342198065\n",
      "Epoch: 5057 | training loss: 0.000341979321\n",
      "Epoch: 5058 | training loss: 0.000341761013\n",
      "Epoch: 5059 | training loss: 0.000341542880\n",
      "Epoch: 5060 | training loss: 0.000341324834\n",
      "Epoch: 5061 | training loss: 0.000341107021\n",
      "Epoch: 5062 | training loss: 0.000340889470\n",
      "Epoch: 5063 | training loss: 0.000340671773\n",
      "Epoch: 5064 | training loss: 0.000340454222\n",
      "Epoch: 5065 | training loss: 0.000340237544\n",
      "Epoch: 5066 | training loss: 0.000340020779\n",
      "Epoch: 5067 | training loss: 0.000339804043\n",
      "Epoch: 5068 | training loss: 0.000339587335\n",
      "Epoch: 5069 | training loss: 0.000339370890\n",
      "Epoch: 5070 | training loss: 0.000339154969\n",
      "Epoch: 5071 | training loss: 0.000338938640\n",
      "Epoch: 5072 | training loss: 0.000338722719\n",
      "Epoch: 5073 | training loss: 0.000338507467\n",
      "Epoch: 5074 | training loss: 0.000338292040\n",
      "Epoch: 5075 | training loss: 0.000338077109\n",
      "Epoch: 5076 | training loss: 0.000337862119\n",
      "Epoch: 5077 | training loss: 0.000337647041\n",
      "Epoch: 5078 | training loss: 0.000337432430\n",
      "Epoch: 5079 | training loss: 0.000337217614\n",
      "Epoch: 5080 | training loss: 0.000337003323\n",
      "Epoch: 5081 | training loss: 0.000336788915\n",
      "Epoch: 5082 | training loss: 0.000336575235\n",
      "Epoch: 5083 | training loss: 0.000336361583\n",
      "Epoch: 5084 | training loss: 0.000336148165\n",
      "Epoch: 5085 | training loss: 0.000335934688\n",
      "Epoch: 5086 | training loss: 0.000335721590\n",
      "Epoch: 5087 | training loss: 0.000335508405\n",
      "Epoch: 5088 | training loss: 0.000335295685\n",
      "Epoch: 5089 | training loss: 0.000335082936\n",
      "Epoch: 5090 | training loss: 0.000334870507\n",
      "Epoch: 5091 | training loss: 0.000334658369\n",
      "Epoch: 5092 | training loss: 0.000334446173\n",
      "Epoch: 5093 | training loss: 0.000334234355\n",
      "Epoch: 5094 | training loss: 0.000334022567\n",
      "Epoch: 5095 | training loss: 0.000333810749\n",
      "Epoch: 5096 | training loss: 0.000333599659\n",
      "Epoch: 5097 | training loss: 0.000333388307\n",
      "Epoch: 5098 | training loss: 0.000333177013\n",
      "Epoch: 5099 | training loss: 0.000332966272\n",
      "Epoch: 5100 | training loss: 0.000332755648\n",
      "Epoch: 5101 | training loss: 0.000332545111\n",
      "Epoch: 5102 | training loss: 0.000332334661\n",
      "Epoch: 5103 | training loss: 0.000332124415\n",
      "Epoch: 5104 | training loss: 0.000331914547\n",
      "Epoch: 5105 | training loss: 0.000331704738\n",
      "Epoch: 5106 | training loss: 0.000331495627\n",
      "Epoch: 5107 | training loss: 0.000331285817\n",
      "Epoch: 5108 | training loss: 0.000331076531\n",
      "Epoch: 5109 | training loss: 0.000330867799\n",
      "Epoch: 5110 | training loss: 0.000330658426\n",
      "Epoch: 5111 | training loss: 0.000330450042\n",
      "Epoch: 5112 | training loss: 0.000330241513\n",
      "Epoch: 5113 | training loss: 0.000330032955\n",
      "Epoch: 5114 | training loss: 0.000329824805\n",
      "Epoch: 5115 | training loss: 0.000329616596\n",
      "Epoch: 5116 | training loss: 0.000329408707\n",
      "Epoch: 5117 | training loss: 0.000329200964\n",
      "Epoch: 5118 | training loss: 0.000328993367\n",
      "Epoch: 5119 | training loss: 0.000328785914\n",
      "Epoch: 5120 | training loss: 0.000328578899\n",
      "Epoch: 5121 | training loss: 0.000328371592\n",
      "Epoch: 5122 | training loss: 0.000328165072\n",
      "Epoch: 5123 | training loss: 0.000327958027\n",
      "Epoch: 5124 | training loss: 0.000327751593\n",
      "Epoch: 5125 | training loss: 0.000327545335\n",
      "Epoch: 5126 | training loss: 0.000327339250\n",
      "Epoch: 5127 | training loss: 0.000327133195\n",
      "Epoch: 5128 | training loss: 0.000326927402\n",
      "Epoch: 5129 | training loss: 0.000326721551\n",
      "Epoch: 5130 | training loss: 0.000326516252\n",
      "Epoch: 5131 | training loss: 0.000326310692\n",
      "Epoch: 5132 | training loss: 0.000326105219\n",
      "Epoch: 5133 | training loss: 0.000325900619\n",
      "Epoch: 5134 | training loss: 0.000325695582\n",
      "Epoch: 5135 | training loss: 0.000325491128\n",
      "Epoch: 5136 | training loss: 0.000325286557\n",
      "Epoch: 5137 | training loss: 0.000325082161\n",
      "Epoch: 5138 | training loss: 0.000324877998\n",
      "Epoch: 5139 | training loss: 0.000324674009\n",
      "Epoch: 5140 | training loss: 0.000324470049\n",
      "Epoch: 5141 | training loss: 0.000324266468\n",
      "Epoch: 5142 | training loss: 0.000324062974\n",
      "Epoch: 5143 | training loss: 0.000323859509\n",
      "Epoch: 5144 | training loss: 0.000323656830\n",
      "Epoch: 5145 | training loss: 0.000323453511\n",
      "Epoch: 5146 | training loss: 0.000323250890\n",
      "Epoch: 5147 | training loss: 0.000323048152\n",
      "Epoch: 5148 | training loss: 0.000322845532\n",
      "Epoch: 5149 | training loss: 0.000322643231\n",
      "Epoch: 5150 | training loss: 0.000322441338\n",
      "Epoch: 5151 | training loss: 0.000322239532\n",
      "Epoch: 5152 | training loss: 0.000322037726\n",
      "Epoch: 5153 | training loss: 0.000321835454\n",
      "Epoch: 5154 | training loss: 0.000321634521\n",
      "Epoch: 5155 | training loss: 0.000321432919\n",
      "Epoch: 5156 | training loss: 0.000321231608\n",
      "Epoch: 5157 | training loss: 0.000321030268\n",
      "Epoch: 5158 | training loss: 0.000320829422\n",
      "Epoch: 5159 | training loss: 0.000320628693\n",
      "Epoch: 5160 | training loss: 0.000320428022\n",
      "Epoch: 5161 | training loss: 0.000320227584\n",
      "Epoch: 5162 | training loss: 0.000320027117\n",
      "Epoch: 5163 | training loss: 0.000319827435\n",
      "Epoch: 5164 | training loss: 0.000319627114\n",
      "Epoch: 5165 | training loss: 0.000319427199\n",
      "Epoch: 5166 | training loss: 0.000319227518\n",
      "Epoch: 5167 | training loss: 0.000319028186\n",
      "Epoch: 5168 | training loss: 0.000318828825\n",
      "Epoch: 5169 | training loss: 0.000318629696\n",
      "Epoch: 5170 | training loss: 0.000318430626\n",
      "Epoch: 5171 | training loss: 0.000318231818\n",
      "Epoch: 5172 | training loss: 0.000318032806\n",
      "Epoch: 5173 | training loss: 0.000317834609\n",
      "Epoch: 5174 | training loss: 0.000317635771\n",
      "Epoch: 5175 | training loss: 0.000317437924\n",
      "Epoch: 5176 | training loss: 0.000317239785\n",
      "Epoch: 5177 | training loss: 0.000317041646\n",
      "Epoch: 5178 | training loss: 0.000316843623\n",
      "Epoch: 5179 | training loss: 0.000316646183\n",
      "Epoch: 5180 | training loss: 0.000316448393\n",
      "Epoch: 5181 | training loss: 0.000316251302\n",
      "Epoch: 5182 | training loss: 0.000316054065\n",
      "Epoch: 5183 | training loss: 0.000315856858\n",
      "Epoch: 5184 | training loss: 0.000315660058\n",
      "Epoch: 5185 | training loss: 0.000315463461\n",
      "Epoch: 5186 | training loss: 0.000315266778\n",
      "Epoch: 5187 | training loss: 0.000315070909\n",
      "Epoch: 5188 | training loss: 0.000314874284\n",
      "Epoch: 5189 | training loss: 0.000314678531\n",
      "Epoch: 5190 | training loss: 0.000314482430\n",
      "Epoch: 5191 | training loss: 0.000314286503\n",
      "Epoch: 5192 | training loss: 0.000314090954\n",
      "Epoch: 5193 | training loss: 0.000313895434\n",
      "Epoch: 5194 | training loss: 0.000313700060\n",
      "Epoch: 5195 | training loss: 0.000313505006\n",
      "Epoch: 5196 | training loss: 0.000313309836\n",
      "Epoch: 5197 | training loss: 0.000313115132\n",
      "Epoch: 5198 | training loss: 0.000312920369\n",
      "Epoch: 5199 | training loss: 0.000312725431\n",
      "Epoch: 5200 | training loss: 0.000312531309\n",
      "Epoch: 5201 | training loss: 0.000312336953\n",
      "Epoch: 5202 | training loss: 0.000312142656\n",
      "Epoch: 5203 | training loss: 0.000311948796\n",
      "Epoch: 5204 | training loss: 0.000311754731\n",
      "Epoch: 5205 | training loss: 0.000311561074\n",
      "Epoch: 5206 | training loss: 0.000311367679\n",
      "Epoch: 5207 | training loss: 0.000311173731\n",
      "Epoch: 5208 | training loss: 0.000310981064\n",
      "Epoch: 5209 | training loss: 0.000310787902\n",
      "Epoch: 5210 | training loss: 0.000310594856\n",
      "Epoch: 5211 | training loss: 0.000310402131\n",
      "Epoch: 5212 | training loss: 0.000310209580\n",
      "Epoch: 5213 | training loss: 0.000310017087\n",
      "Epoch: 5214 | training loss: 0.000309824653\n",
      "Epoch: 5215 | training loss: 0.000309632218\n",
      "Epoch: 5216 | training loss: 0.000309440395\n",
      "Epoch: 5217 | training loss: 0.000309248688\n",
      "Epoch: 5218 | training loss: 0.000309056573\n",
      "Epoch: 5219 | training loss: 0.000308864866\n",
      "Epoch: 5220 | training loss: 0.000308673509\n",
      "Epoch: 5221 | training loss: 0.000308482180\n",
      "Epoch: 5222 | training loss: 0.000308291084\n",
      "Epoch: 5223 | training loss: 0.000308100018\n",
      "Epoch: 5224 | training loss: 0.000307908980\n",
      "Epoch: 5225 | training loss: 0.000307718525\n",
      "Epoch: 5226 | training loss: 0.000307527691\n",
      "Epoch: 5227 | training loss: 0.000307337061\n",
      "Epoch: 5228 | training loss: 0.000307146925\n",
      "Epoch: 5229 | training loss: 0.000306956616\n",
      "Epoch: 5230 | training loss: 0.000306766422\n",
      "Epoch: 5231 | training loss: 0.000306576374\n",
      "Epoch: 5232 | training loss: 0.000306387024\n",
      "Epoch: 5233 | training loss: 0.000306197355\n",
      "Epoch: 5234 | training loss: 0.000306007656\n",
      "Epoch: 5235 | training loss: 0.000305818307\n",
      "Epoch: 5236 | training loss: 0.000305629103\n",
      "Epoch: 5237 | training loss: 0.000305439782\n",
      "Epoch: 5238 | training loss: 0.000305251044\n",
      "Epoch: 5239 | training loss: 0.000305062131\n",
      "Epoch: 5240 | training loss: 0.000304873567\n",
      "Epoch: 5241 | training loss: 0.000304684945\n",
      "Epoch: 5242 | training loss: 0.000304496643\n",
      "Epoch: 5243 | training loss: 0.000304308545\n",
      "Epoch: 5244 | training loss: 0.000304120476\n",
      "Epoch: 5245 | training loss: 0.000303932466\n",
      "Epoch: 5246 | training loss: 0.000303744921\n",
      "Epoch: 5247 | training loss: 0.000303557143\n",
      "Epoch: 5248 | training loss: 0.000303369772\n",
      "Epoch: 5249 | training loss: 0.000303182373\n",
      "Epoch: 5250 | training loss: 0.000302995002\n",
      "Epoch: 5251 | training loss: 0.000302808010\n",
      "Epoch: 5252 | training loss: 0.000302621134\n",
      "Epoch: 5253 | training loss: 0.000302434433\n",
      "Epoch: 5254 | training loss: 0.000302247819\n",
      "Epoch: 5255 | training loss: 0.000302061177\n",
      "Epoch: 5256 | training loss: 0.000301874592\n",
      "Epoch: 5257 | training loss: 0.000301688560\n",
      "Epoch: 5258 | training loss: 0.000301502529\n",
      "Epoch: 5259 | training loss: 0.000301316293\n",
      "Epoch: 5260 | training loss: 0.000301130232\n",
      "Epoch: 5261 | training loss: 0.000300944608\n",
      "Epoch: 5262 | training loss: 0.000300759420\n",
      "Epoch: 5263 | training loss: 0.000300574000\n",
      "Epoch: 5264 | training loss: 0.000300388841\n",
      "Epoch: 5265 | training loss: 0.000300203683\n",
      "Epoch: 5266 | training loss: 0.000300018815\n",
      "Epoch: 5267 | training loss: 0.000299833715\n",
      "Epoch: 5268 | training loss: 0.000299648673\n",
      "Epoch: 5269 | training loss: 0.000299464096\n",
      "Epoch: 5270 | training loss: 0.000299280102\n",
      "Epoch: 5271 | training loss: 0.000299095642\n",
      "Epoch: 5272 | training loss: 0.000298911153\n",
      "Epoch: 5273 | training loss: 0.000298727216\n",
      "Epoch: 5274 | training loss: 0.000298543280\n",
      "Epoch: 5275 | training loss: 0.000298359257\n",
      "Epoch: 5276 | training loss: 0.000298175524\n",
      "Epoch: 5277 | training loss: 0.000297992287\n",
      "Epoch: 5278 | training loss: 0.000297808758\n",
      "Epoch: 5279 | training loss: 0.000297625200\n",
      "Epoch: 5280 | training loss: 0.000297442049\n",
      "Epoch: 5281 | training loss: 0.000297259045\n",
      "Epoch: 5282 | training loss: 0.000297076331\n",
      "Epoch: 5283 | training loss: 0.000296893530\n",
      "Epoch: 5284 | training loss: 0.000296710874\n",
      "Epoch: 5285 | training loss: 0.000296528538\n",
      "Epoch: 5286 | training loss: 0.000296346261\n",
      "Epoch: 5287 | training loss: 0.000296164362\n",
      "Epoch: 5288 | training loss: 0.000295982027\n",
      "Epoch: 5289 | training loss: 0.000295799924\n",
      "Epoch: 5290 | training loss: 0.000295618491\n",
      "Epoch: 5291 | training loss: 0.000295436679\n",
      "Epoch: 5292 | training loss: 0.000295255450\n",
      "Epoch: 5293 | training loss: 0.000295074133\n",
      "Epoch: 5294 | training loss: 0.000294892758\n",
      "Epoch: 5295 | training loss: 0.000294711557\n",
      "Epoch: 5296 | training loss: 0.000294530735\n",
      "Epoch: 5297 | training loss: 0.000294349855\n",
      "Epoch: 5298 | training loss: 0.000294169149\n",
      "Epoch: 5299 | training loss: 0.000293988647\n",
      "Epoch: 5300 | training loss: 0.000293808349\n",
      "Epoch: 5301 | training loss: 0.000293627789\n",
      "Epoch: 5302 | training loss: 0.000293447607\n",
      "Epoch: 5303 | training loss: 0.000293267512\n",
      "Epoch: 5304 | training loss: 0.000293087563\n",
      "Epoch: 5305 | training loss: 0.000292907818\n",
      "Epoch: 5306 | training loss: 0.000292728335\n",
      "Epoch: 5307 | training loss: 0.000292548735\n",
      "Epoch: 5308 | training loss: 0.000292369223\n",
      "Epoch: 5309 | training loss: 0.000292189943\n",
      "Epoch: 5310 | training loss: 0.000292011129\n",
      "Epoch: 5311 | training loss: 0.000291832228\n",
      "Epoch: 5312 | training loss: 0.000291653152\n",
      "Epoch: 5313 | training loss: 0.000291474222\n",
      "Epoch: 5314 | training loss: 0.000291295757\n",
      "Epoch: 5315 | training loss: 0.000291117321\n",
      "Epoch: 5316 | training loss: 0.000290938682\n",
      "Epoch: 5317 | training loss: 0.000290760712\n",
      "Epoch: 5318 | training loss: 0.000290582597\n",
      "Epoch: 5319 | training loss: 0.000290404307\n",
      "Epoch: 5320 | training loss: 0.000290226482\n",
      "Epoch: 5321 | training loss: 0.000290049036\n",
      "Epoch: 5322 | training loss: 0.000289871561\n",
      "Epoch: 5323 | training loss: 0.000289693882\n",
      "Epoch: 5324 | training loss: 0.000289516465\n",
      "Epoch: 5325 | training loss: 0.000289339048\n",
      "Epoch: 5326 | training loss: 0.000289162155\n",
      "Epoch: 5327 | training loss: 0.000288985204\n",
      "Epoch: 5328 | training loss: 0.000288808602\n",
      "Epoch: 5329 | training loss: 0.000288631913\n",
      "Epoch: 5330 | training loss: 0.000288455514\n",
      "Epoch: 5331 | training loss: 0.000288278970\n",
      "Epoch: 5332 | training loss: 0.000288102514\n",
      "Epoch: 5333 | training loss: 0.000287926232\n",
      "Epoch: 5334 | training loss: 0.000287750619\n",
      "Epoch: 5335 | training loss: 0.000287574512\n",
      "Epoch: 5336 | training loss: 0.000287398580\n",
      "Epoch: 5337 | training loss: 0.000287223142\n",
      "Epoch: 5338 | training loss: 0.000287047558\n",
      "Epoch: 5339 | training loss: 0.000286872440\n",
      "Epoch: 5340 | training loss: 0.000286696712\n",
      "Epoch: 5341 | training loss: 0.000286521943\n",
      "Epoch: 5342 | training loss: 0.000286346767\n",
      "Epoch: 5343 | training loss: 0.000286171969\n",
      "Epoch: 5344 | training loss: 0.000285997259\n",
      "Epoch: 5345 | training loss: 0.000285822724\n",
      "Epoch: 5346 | training loss: 0.000285648130\n",
      "Epoch: 5347 | training loss: 0.000285473594\n",
      "Epoch: 5348 | training loss: 0.000285299553\n",
      "Epoch: 5349 | training loss: 0.000285125396\n",
      "Epoch: 5350 | training loss: 0.000284951442\n",
      "Epoch: 5351 | training loss: 0.000284777372\n",
      "Epoch: 5352 | training loss: 0.000284603942\n",
      "Epoch: 5353 | training loss: 0.000284430338\n",
      "Epoch: 5354 | training loss: 0.000284256588\n",
      "Epoch: 5355 | training loss: 0.000284083420\n",
      "Epoch: 5356 | training loss: 0.000283910253\n",
      "Epoch: 5357 | training loss: 0.000283737056\n",
      "Epoch: 5358 | training loss: 0.000283564150\n",
      "Epoch: 5359 | training loss: 0.000283391244\n",
      "Epoch: 5360 | training loss: 0.000283218717\n",
      "Epoch: 5361 | training loss: 0.000283046073\n",
      "Epoch: 5362 | training loss: 0.000282873545\n",
      "Epoch: 5363 | training loss: 0.000282701163\n",
      "Epoch: 5364 | training loss: 0.000282528519\n",
      "Epoch: 5365 | training loss: 0.000282356748\n",
      "Epoch: 5366 | training loss: 0.000282184861\n",
      "Epoch: 5367 | training loss: 0.000282012799\n",
      "Epoch: 5368 | training loss: 0.000281841087\n",
      "Epoch: 5369 | training loss: 0.000281669287\n",
      "Epoch: 5370 | training loss: 0.000281497894\n",
      "Epoch: 5371 | training loss: 0.000281326415\n",
      "Epoch: 5372 | training loss: 0.000281154847\n",
      "Epoch: 5373 | training loss: 0.000280984124\n",
      "Epoch: 5374 | training loss: 0.000280813023\n",
      "Epoch: 5375 | training loss: 0.000280642271\n",
      "Epoch: 5376 | training loss: 0.000280471373\n",
      "Epoch: 5377 | training loss: 0.000280300883\n",
      "Epoch: 5378 | training loss: 0.000280130422\n",
      "Epoch: 5379 | training loss: 0.000279960106\n",
      "Epoch: 5380 | training loss: 0.000279789820\n",
      "Epoch: 5381 | training loss: 0.000279619358\n",
      "Epoch: 5382 | training loss: 0.000279449509\n",
      "Epoch: 5383 | training loss: 0.000279279629\n",
      "Epoch: 5384 | training loss: 0.000279109809\n",
      "Epoch: 5385 | training loss: 0.000278940250\n",
      "Epoch: 5386 | training loss: 0.000278770487\n",
      "Epoch: 5387 | training loss: 0.000278601510\n",
      "Epoch: 5388 | training loss: 0.000278432271\n",
      "Epoch: 5389 | training loss: 0.000278262916\n",
      "Epoch: 5390 | training loss: 0.000278094027\n",
      "Epoch: 5391 | training loss: 0.000277924788\n",
      "Epoch: 5392 | training loss: 0.000277756160\n",
      "Epoch: 5393 | training loss: 0.000277587300\n",
      "Epoch: 5394 | training loss: 0.000277418585\n",
      "Epoch: 5395 | training loss: 0.000277250016\n",
      "Epoch: 5396 | training loss: 0.000277081970\n",
      "Epoch: 5397 | training loss: 0.000276913779\n",
      "Epoch: 5398 | training loss: 0.000276745675\n",
      "Epoch: 5399 | training loss: 0.000276577426\n",
      "Epoch: 5400 | training loss: 0.000276409497\n",
      "Epoch: 5401 | training loss: 0.000276241917\n",
      "Epoch: 5402 | training loss: 0.000276074366\n",
      "Epoch: 5403 | training loss: 0.000275906757\n",
      "Epoch: 5404 | training loss: 0.000275739556\n",
      "Epoch: 5405 | training loss: 0.000275572296\n",
      "Epoch: 5406 | training loss: 0.000275405240\n",
      "Epoch: 5407 | training loss: 0.000275238184\n",
      "Epoch: 5408 | training loss: 0.000275071536\n",
      "Epoch: 5409 | training loss: 0.000274904742\n",
      "Epoch: 5410 | training loss: 0.000274738239\n",
      "Epoch: 5411 | training loss: 0.000274571707\n",
      "Epoch: 5412 | training loss: 0.000274405291\n",
      "Epoch: 5413 | training loss: 0.000274238933\n",
      "Epoch: 5414 | training loss: 0.000274072838\n",
      "Epoch: 5415 | training loss: 0.000273906742\n",
      "Epoch: 5416 | training loss: 0.000273740879\n",
      "Epoch: 5417 | training loss: 0.000273575279\n",
      "Epoch: 5418 | training loss: 0.000273409532\n",
      "Epoch: 5419 | training loss: 0.000273243815\n",
      "Epoch: 5420 | training loss: 0.000273078447\n",
      "Epoch: 5421 | training loss: 0.000272912963\n",
      "Epoch: 5422 | training loss: 0.000272747915\n",
      "Epoch: 5423 | training loss: 0.000272582809\n",
      "Epoch: 5424 | training loss: 0.000272417878\n",
      "Epoch: 5425 | training loss: 0.000272253063\n",
      "Epoch: 5426 | training loss: 0.000272088044\n",
      "Epoch: 5427 | training loss: 0.000271923636\n",
      "Epoch: 5428 | training loss: 0.000271759345\n",
      "Epoch: 5429 | training loss: 0.000271594850\n",
      "Epoch: 5430 | training loss: 0.000271430588\n",
      "Epoch: 5431 | training loss: 0.000271266734\n",
      "Epoch: 5432 | training loss: 0.000271102675\n",
      "Epoch: 5433 | training loss: 0.000270938763\n",
      "Epoch: 5434 | training loss: 0.000270774879\n",
      "Epoch: 5435 | training loss: 0.000270611024\n",
      "Epoch: 5436 | training loss: 0.000270447548\n",
      "Epoch: 5437 | training loss: 0.000270284072\n",
      "Epoch: 5438 | training loss: 0.000270120625\n",
      "Epoch: 5439 | training loss: 0.000269957760\n",
      "Epoch: 5440 | training loss: 0.000269794604\n",
      "Epoch: 5441 | training loss: 0.000269631651\n",
      "Epoch: 5442 | training loss: 0.000269468932\n",
      "Epoch: 5443 | training loss: 0.000269305863\n",
      "Epoch: 5444 | training loss: 0.000269143813\n",
      "Epoch: 5445 | training loss: 0.000268981035\n",
      "Epoch: 5446 | training loss: 0.000268818578\n",
      "Epoch: 5447 | training loss: 0.000268656644\n",
      "Epoch: 5448 | training loss: 0.000268494099\n",
      "Epoch: 5449 | training loss: 0.000268332253\n",
      "Epoch: 5450 | training loss: 0.000268170377\n",
      "Epoch: 5451 | training loss: 0.000268008735\n",
      "Epoch: 5452 | training loss: 0.000267846597\n",
      "Epoch: 5453 | training loss: 0.000267685391\n",
      "Epoch: 5454 | training loss: 0.000267523865\n",
      "Epoch: 5455 | training loss: 0.000267362717\n",
      "Epoch: 5456 | training loss: 0.000267201249\n",
      "Epoch: 5457 | training loss: 0.000267040596\n",
      "Epoch: 5458 | training loss: 0.000266879448\n",
      "Epoch: 5459 | training loss: 0.000266718562\n",
      "Epoch: 5460 | training loss: 0.000266558054\n",
      "Epoch: 5461 | training loss: 0.000266397285\n",
      "Epoch: 5462 | training loss: 0.000266236952\n",
      "Epoch: 5463 | training loss: 0.000266076648\n",
      "Epoch: 5464 | training loss: 0.000265916227\n",
      "Epoch: 5465 | training loss: 0.000265756389\n",
      "Epoch: 5466 | training loss: 0.000265595852\n",
      "Epoch: 5467 | training loss: 0.000265436247\n",
      "Epoch: 5468 | training loss: 0.000265276612\n",
      "Epoch: 5469 | training loss: 0.000265116774\n",
      "Epoch: 5470 | training loss: 0.000264957169\n",
      "Epoch: 5471 | training loss: 0.000264797709\n",
      "Epoch: 5472 | training loss: 0.000264638220\n",
      "Epoch: 5473 | training loss: 0.000264479168\n",
      "Epoch: 5474 | training loss: 0.000264320057\n",
      "Epoch: 5475 | training loss: 0.000264160917\n",
      "Epoch: 5476 | training loss: 0.000264002243\n",
      "Epoch: 5477 | training loss: 0.000263843394\n",
      "Epoch: 5478 | training loss: 0.000263684866\n",
      "Epoch: 5479 | training loss: 0.000263526221\n",
      "Epoch: 5480 | training loss: 0.000263367954\n",
      "Epoch: 5481 | training loss: 0.000263209571\n",
      "Epoch: 5482 | training loss: 0.000263051246\n",
      "Epoch: 5483 | training loss: 0.000262893445\n",
      "Epoch: 5484 | training loss: 0.000262735441\n",
      "Epoch: 5485 | training loss: 0.000262577698\n",
      "Epoch: 5486 | training loss: 0.000262420072\n",
      "Epoch: 5487 | training loss: 0.000262262416\n",
      "Epoch: 5488 | training loss: 0.000262104906\n",
      "Epoch: 5489 | training loss: 0.000261947513\n",
      "Epoch: 5490 | training loss: 0.000261790323\n",
      "Epoch: 5491 | training loss: 0.000261633220\n",
      "Epoch: 5492 | training loss: 0.000261476001\n",
      "Epoch: 5493 | training loss: 0.000261319074\n",
      "Epoch: 5494 | training loss: 0.000261162641\n",
      "Epoch: 5495 | training loss: 0.000261005596\n",
      "Epoch: 5496 | training loss: 0.000260848843\n",
      "Epoch: 5497 | training loss: 0.000260692672\n",
      "Epoch: 5498 | training loss: 0.000260536443\n",
      "Epoch: 5499 | training loss: 0.000260380388\n",
      "Epoch: 5500 | training loss: 0.000260224071\n",
      "Epoch: 5501 | training loss: 0.000260067813\n",
      "Epoch: 5502 | training loss: 0.000259911962\n",
      "Epoch: 5503 | training loss: 0.000259756285\n",
      "Epoch: 5504 | training loss: 0.000259600609\n",
      "Epoch: 5505 | training loss: 0.000259444874\n",
      "Epoch: 5506 | training loss: 0.000259289576\n",
      "Epoch: 5507 | training loss: 0.000259134220\n",
      "Epoch: 5508 | training loss: 0.000258978806\n",
      "Epoch: 5509 | training loss: 0.000258823682\n",
      "Epoch: 5510 | training loss: 0.000258668908\n",
      "Epoch: 5511 | training loss: 0.000258513959\n",
      "Epoch: 5512 | training loss: 0.000258359243\n",
      "Epoch: 5513 | training loss: 0.000258204760\n",
      "Epoch: 5514 | training loss: 0.000258050073\n",
      "Epoch: 5515 | training loss: 0.000257895590\n",
      "Epoch: 5516 | training loss: 0.000257741543\n",
      "Epoch: 5517 | training loss: 0.000257587206\n",
      "Epoch: 5518 | training loss: 0.000257433567\n",
      "Epoch: 5519 | training loss: 0.000257279462\n",
      "Epoch: 5520 | training loss: 0.000257125444\n",
      "Epoch: 5521 | training loss: 0.000256972038\n",
      "Epoch: 5522 | training loss: 0.000256818254\n",
      "Epoch: 5523 | training loss: 0.000256664789\n",
      "Epoch: 5524 | training loss: 0.000256511325\n",
      "Epoch: 5525 | training loss: 0.000256357976\n",
      "Epoch: 5526 | training loss: 0.000256205152\n",
      "Epoch: 5527 | training loss: 0.000256052241\n",
      "Epoch: 5528 | training loss: 0.000255899446\n",
      "Epoch: 5529 | training loss: 0.000255746301\n",
      "Epoch: 5530 | training loss: 0.000255593768\n",
      "Epoch: 5531 | training loss: 0.000255441264\n",
      "Epoch: 5532 | training loss: 0.000255288556\n",
      "Epoch: 5533 | training loss: 0.000255136430\n",
      "Epoch: 5534 | training loss: 0.000254984072\n",
      "Epoch: 5535 | training loss: 0.000254832237\n",
      "Epoch: 5536 | training loss: 0.000254679966\n",
      "Epoch: 5537 | training loss: 0.000254528190\n",
      "Epoch: 5538 | training loss: 0.000254376268\n",
      "Epoch: 5539 | training loss: 0.000254224811\n",
      "Epoch: 5540 | training loss: 0.000254073006\n",
      "Epoch: 5541 | training loss: 0.000253921637\n",
      "Epoch: 5542 | training loss: 0.000253770326\n",
      "Epoch: 5543 | training loss: 0.000253619102\n",
      "Epoch: 5544 | training loss: 0.000253467879\n",
      "Epoch: 5545 | training loss: 0.000253317063\n",
      "Epoch: 5546 | training loss: 0.000253166101\n",
      "Epoch: 5547 | training loss: 0.000253015285\n",
      "Epoch: 5548 | training loss: 0.000252864615\n",
      "Epoch: 5549 | training loss: 0.000252713915\n",
      "Epoch: 5550 | training loss: 0.000252563390\n",
      "Epoch: 5551 | training loss: 0.000252413214\n",
      "Epoch: 5552 | training loss: 0.000252263097\n",
      "Epoch: 5553 | training loss: 0.000252112863\n",
      "Epoch: 5554 | training loss: 0.000251962745\n",
      "Epoch: 5555 | training loss: 0.000251813122\n",
      "Epoch: 5556 | training loss: 0.000251663208\n",
      "Epoch: 5557 | training loss: 0.000251513819\n",
      "Epoch: 5558 | training loss: 0.000251364312\n",
      "Epoch: 5559 | training loss: 0.000251215068\n",
      "Epoch: 5560 | training loss: 0.000251065649\n",
      "Epoch: 5561 | training loss: 0.000250916491\n",
      "Epoch: 5562 | training loss: 0.000250767538\n",
      "Epoch: 5563 | training loss: 0.000250618410\n",
      "Epoch: 5564 | training loss: 0.000250469835\n",
      "Epoch: 5565 | training loss: 0.000250320707\n",
      "Epoch: 5566 | training loss: 0.000250172103\n",
      "Epoch: 5567 | training loss: 0.000250023702\n",
      "Epoch: 5568 | training loss: 0.000249874924\n",
      "Epoch: 5569 | training loss: 0.000249726756\n",
      "Epoch: 5570 | training loss: 0.000249578676\n",
      "Epoch: 5571 | training loss: 0.000249430654\n",
      "Epoch: 5572 | training loss: 0.000249282399\n",
      "Epoch: 5573 | training loss: 0.000249134580\n",
      "Epoch: 5574 | training loss: 0.000248986762\n",
      "Epoch: 5575 | training loss: 0.000248839206\n",
      "Epoch: 5576 | training loss: 0.000248691649\n",
      "Epoch: 5577 | training loss: 0.000248544151\n",
      "Epoch: 5578 | training loss: 0.000248396886\n",
      "Epoch: 5579 | training loss: 0.000248249446\n",
      "Epoch: 5580 | training loss: 0.000248102471\n",
      "Epoch: 5581 | training loss: 0.000247955555\n",
      "Epoch: 5582 | training loss: 0.000247808726\n",
      "Epoch: 5583 | training loss: 0.000247661636\n",
      "Epoch: 5584 | training loss: 0.000247514865\n",
      "Epoch: 5585 | training loss: 0.000247368414\n",
      "Epoch: 5586 | training loss: 0.000247221789\n",
      "Epoch: 5587 | training loss: 0.000247075659\n",
      "Epoch: 5588 | training loss: 0.000246929441\n",
      "Epoch: 5589 | training loss: 0.000246783369\n",
      "Epoch: 5590 | training loss: 0.000246637384\n",
      "Epoch: 5591 | training loss: 0.000246491312\n",
      "Epoch: 5592 | training loss: 0.000246345327\n",
      "Epoch: 5593 | training loss: 0.000246200070\n",
      "Epoch: 5594 | training loss: 0.000246054493\n",
      "Epoch: 5595 | training loss: 0.000245908916\n",
      "Epoch: 5596 | training loss: 0.000245763687\n",
      "Epoch: 5597 | training loss: 0.000245618314\n",
      "Epoch: 5598 | training loss: 0.000245473348\n",
      "Epoch: 5599 | training loss: 0.000245328469\n",
      "Epoch: 5600 | training loss: 0.000245183328\n",
      "Epoch: 5601 | training loss: 0.000245038449\n",
      "Epoch: 5602 | training loss: 0.000244893890\n",
      "Epoch: 5603 | training loss: 0.000244749594\n",
      "Epoch: 5604 | training loss: 0.000244605006\n",
      "Epoch: 5605 | training loss: 0.000244460796\n",
      "Epoch: 5606 | training loss: 0.000244316296\n",
      "Epoch: 5607 | training loss: 0.000244172290\n",
      "Epoch: 5608 | training loss: 0.000244028211\n",
      "Epoch: 5609 | training loss: 0.000243884337\n",
      "Epoch: 5610 | training loss: 0.000243740273\n",
      "Epoch: 5611 | training loss: 0.000243596674\n",
      "Epoch: 5612 | training loss: 0.000243453105\n",
      "Epoch: 5613 | training loss: 0.000243309623\n",
      "Epoch: 5614 | training loss: 0.000243166156\n",
      "Epoch: 5615 | training loss: 0.000243022791\n",
      "Epoch: 5616 | training loss: 0.000242879527\n",
      "Epoch: 5617 | training loss: 0.000242736656\n",
      "Epoch: 5618 | training loss: 0.000242593436\n",
      "Epoch: 5619 | training loss: 0.000242450682\n",
      "Epoch: 5620 | training loss: 0.000242307986\n",
      "Epoch: 5621 | training loss: 0.000242165319\n",
      "Epoch: 5622 | training loss: 0.000242022623\n",
      "Epoch: 5623 | training loss: 0.000241880160\n",
      "Epoch: 5624 | training loss: 0.000241737842\n",
      "Epoch: 5625 | training loss: 0.000241595902\n",
      "Epoch: 5626 | training loss: 0.000241453818\n",
      "Epoch: 5627 | training loss: 0.000241311587\n",
      "Epoch: 5628 | training loss: 0.000241169735\n",
      "Epoch: 5629 | training loss: 0.000241028232\n",
      "Epoch: 5630 | training loss: 0.000240886351\n",
      "Epoch: 5631 | training loss: 0.000240745023\n",
      "Epoch: 5632 | training loss: 0.000240603229\n",
      "Epoch: 5633 | training loss: 0.000240462046\n",
      "Epoch: 5634 | training loss: 0.000240320893\n",
      "Epoch: 5635 | training loss: 0.000240179652\n",
      "Epoch: 5636 | training loss: 0.000240038586\n",
      "Epoch: 5637 | training loss: 0.000239897723\n",
      "Epoch: 5638 | training loss: 0.000239756715\n",
      "Epoch: 5639 | training loss: 0.000239615983\n",
      "Epoch: 5640 | training loss: 0.000239475456\n",
      "Epoch: 5641 | training loss: 0.000239334971\n",
      "Epoch: 5642 | training loss: 0.000239194531\n",
      "Epoch: 5643 | training loss: 0.000239054149\n",
      "Epoch: 5644 | training loss: 0.000238914101\n",
      "Epoch: 5645 | training loss: 0.000238774141\n",
      "Epoch: 5646 | training loss: 0.000238633715\n",
      "Epoch: 5647 | training loss: 0.000238494104\n",
      "Epoch: 5648 | training loss: 0.000238354405\n",
      "Epoch: 5649 | training loss: 0.000238214969\n",
      "Epoch: 5650 | training loss: 0.000238075372\n",
      "Epoch: 5651 | training loss: 0.000237936067\n",
      "Epoch: 5652 | training loss: 0.000237796718\n",
      "Epoch: 5653 | training loss: 0.000237657514\n",
      "Epoch: 5654 | training loss: 0.000237518630\n",
      "Epoch: 5655 | training loss: 0.000237379427\n",
      "Epoch: 5656 | training loss: 0.000237240543\n",
      "Epoch: 5657 | training loss: 0.000237101835\n",
      "Epoch: 5658 | training loss: 0.000236963169\n",
      "Epoch: 5659 | training loss: 0.000236824839\n",
      "Epoch: 5660 | training loss: 0.000236686406\n",
      "Epoch: 5661 | training loss: 0.000236547930\n",
      "Epoch: 5662 | training loss: 0.000236409978\n",
      "Epoch: 5663 | training loss: 0.000236271793\n",
      "Epoch: 5664 | training loss: 0.000236133579\n",
      "Epoch: 5665 | training loss: 0.000235996020\n",
      "Epoch: 5666 | training loss: 0.000235858344\n",
      "Epoch: 5667 | training loss: 0.000235720596\n",
      "Epoch: 5668 | training loss: 0.000235583022\n",
      "Epoch: 5669 | training loss: 0.000235445608\n",
      "Epoch: 5670 | training loss: 0.000235308369\n",
      "Epoch: 5671 | training loss: 0.000235170970\n",
      "Epoch: 5672 | training loss: 0.000235033905\n",
      "Epoch: 5673 | training loss: 0.000234897074\n",
      "Epoch: 5674 | training loss: 0.000234759966\n",
      "Epoch: 5675 | training loss: 0.000234623105\n",
      "Epoch: 5676 | training loss: 0.000234486768\n",
      "Epoch: 5677 | training loss: 0.000234350067\n",
      "Epoch: 5678 | training loss: 0.000234213352\n",
      "Epoch: 5679 | training loss: 0.000234076957\n",
      "Epoch: 5680 | training loss: 0.000233940722\n",
      "Epoch: 5681 | training loss: 0.000233804414\n",
      "Epoch: 5682 | training loss: 0.000233668674\n",
      "Epoch: 5683 | training loss: 0.000233532352\n",
      "Epoch: 5684 | training loss: 0.000233396626\n",
      "Epoch: 5685 | training loss: 0.000233260871\n",
      "Epoch: 5686 | training loss: 0.000233125378\n",
      "Epoch: 5687 | training loss: 0.000232989696\n",
      "Epoch: 5688 | training loss: 0.000232854305\n",
      "Epoch: 5689 | training loss: 0.000232719045\n",
      "Epoch: 5690 | training loss: 0.000232583843\n",
      "Epoch: 5691 | training loss: 0.000232448700\n",
      "Epoch: 5692 | training loss: 0.000232313469\n",
      "Epoch: 5693 | training loss: 0.000232178601\n",
      "Epoch: 5694 | training loss: 0.000232043618\n",
      "Epoch: 5695 | training loss: 0.000231909289\n",
      "Epoch: 5696 | training loss: 0.000231774349\n",
      "Epoch: 5697 | training loss: 0.000231639802\n",
      "Epoch: 5698 | training loss: 0.000231505343\n",
      "Epoch: 5699 | training loss: 0.000231371174\n",
      "Epoch: 5700 | training loss: 0.000231236918\n",
      "Epoch: 5701 | training loss: 0.000231102866\n",
      "Epoch: 5702 | training loss: 0.000230968872\n",
      "Epoch: 5703 | training loss: 0.000230835140\n",
      "Epoch: 5704 | training loss: 0.000230701349\n",
      "Epoch: 5705 | training loss: 0.000230567704\n",
      "Epoch: 5706 | training loss: 0.000230433972\n",
      "Epoch: 5707 | training loss: 0.000230300589\n",
      "Epoch: 5708 | training loss: 0.000230167090\n",
      "Epoch: 5709 | training loss: 0.000230033911\n",
      "Epoch: 5710 | training loss: 0.000229900848\n",
      "Epoch: 5711 | training loss: 0.000229767757\n",
      "Epoch: 5712 | training loss: 0.000229634927\n",
      "Epoch: 5713 | training loss: 0.000229502097\n",
      "Epoch: 5714 | training loss: 0.000229369412\n",
      "Epoch: 5715 | training loss: 0.000229236990\n",
      "Epoch: 5716 | training loss: 0.000229104189\n",
      "Epoch: 5717 | training loss: 0.000228972087\n",
      "Epoch: 5718 | training loss: 0.000228839766\n",
      "Epoch: 5719 | training loss: 0.000228707533\n",
      "Epoch: 5720 | training loss: 0.000228575678\n",
      "Epoch: 5721 | training loss: 0.000228443605\n",
      "Epoch: 5722 | training loss: 0.000228311634\n",
      "Epoch: 5723 | training loss: 0.000228180113\n",
      "Epoch: 5724 | training loss: 0.000228048273\n",
      "Epoch: 5725 | training loss: 0.000227916957\n",
      "Epoch: 5726 | training loss: 0.000227785553\n",
      "Epoch: 5727 | training loss: 0.000227654149\n",
      "Epoch: 5728 | training loss: 0.000227522964\n",
      "Epoch: 5729 | training loss: 0.000227391749\n",
      "Epoch: 5730 | training loss: 0.000227260840\n",
      "Epoch: 5731 | training loss: 0.000227129931\n",
      "Epoch: 5732 | training loss: 0.000226999080\n",
      "Epoch: 5733 | training loss: 0.000226868331\n",
      "Epoch: 5734 | training loss: 0.000226737786\n",
      "Epoch: 5735 | training loss: 0.000226607255\n",
      "Epoch: 5736 | training loss: 0.000226476710\n",
      "Epoch: 5737 | training loss: 0.000226346543\n",
      "Epoch: 5738 | training loss: 0.000226216362\n",
      "Epoch: 5739 | training loss: 0.000226086442\n",
      "Epoch: 5740 | training loss: 0.000225956290\n",
      "Epoch: 5741 | training loss: 0.000225826283\n",
      "Epoch: 5742 | training loss: 0.000225696567\n",
      "Epoch: 5743 | training loss: 0.000225566764\n",
      "Epoch: 5744 | training loss: 0.000225437223\n",
      "Epoch: 5745 | training loss: 0.000225307696\n",
      "Epoch: 5746 | training loss: 0.000225178592\n",
      "Epoch: 5747 | training loss: 0.000225049080\n",
      "Epoch: 5748 | training loss: 0.000224919990\n",
      "Epoch: 5749 | training loss: 0.000224790943\n",
      "Epoch: 5750 | training loss: 0.000224661751\n",
      "Epoch: 5751 | training loss: 0.000224532821\n",
      "Epoch: 5752 | training loss: 0.000224404037\n",
      "Epoch: 5753 | training loss: 0.000224275413\n",
      "Epoch: 5754 | training loss: 0.000224146643\n",
      "Epoch: 5755 | training loss: 0.000224018222\n",
      "Epoch: 5756 | training loss: 0.000223889496\n",
      "Epoch: 5757 | training loss: 0.000223761497\n",
      "Epoch: 5758 | training loss: 0.000223633280\n",
      "Epoch: 5759 | training loss: 0.000223505165\n",
      "Epoch: 5760 | training loss: 0.000223377225\n",
      "Epoch: 5761 | training loss: 0.000223249473\n",
      "Epoch: 5762 | training loss: 0.000223121606\n",
      "Epoch: 5763 | training loss: 0.000222993986\n",
      "Epoch: 5764 | training loss: 0.000222866511\n",
      "Epoch: 5765 | training loss: 0.000222739036\n",
      "Epoch: 5766 | training loss: 0.000222611765\n",
      "Epoch: 5767 | training loss: 0.000222484523\n",
      "Epoch: 5768 | training loss: 0.000222357310\n",
      "Epoch: 5769 | training loss: 0.000222230476\n",
      "Epoch: 5770 | training loss: 0.000222103408\n",
      "Epoch: 5771 | training loss: 0.000221976865\n",
      "Epoch: 5772 | training loss: 0.000221849870\n",
      "Epoch: 5773 | training loss: 0.000221723341\n",
      "Epoch: 5774 | training loss: 0.000221597002\n",
      "Epoch: 5775 | training loss: 0.000221470487\n",
      "Epoch: 5776 | training loss: 0.000221344060\n",
      "Epoch: 5777 | training loss: 0.000221217866\n",
      "Epoch: 5778 | training loss: 0.000221091730\n",
      "Epoch: 5779 | training loss: 0.000220965827\n",
      "Epoch: 5780 | training loss: 0.000220839982\n",
      "Epoch: 5781 | training loss: 0.000220714137\n",
      "Epoch: 5782 | training loss: 0.000220588394\n",
      "Epoch: 5783 | training loss: 0.000220462898\n",
      "Epoch: 5784 | training loss: 0.000220337213\n",
      "Epoch: 5785 | training loss: 0.000220211849\n",
      "Epoch: 5786 | training loss: 0.000220086484\n",
      "Epoch: 5787 | training loss: 0.000219961279\n",
      "Epoch: 5788 | training loss: 0.000219836162\n",
      "Epoch: 5789 | training loss: 0.000219710928\n",
      "Epoch: 5790 | training loss: 0.000219586102\n",
      "Epoch: 5791 | training loss: 0.000219461392\n",
      "Epoch: 5792 | training loss: 0.000219336827\n",
      "Epoch: 5793 | training loss: 0.000219212205\n",
      "Epoch: 5794 | training loss: 0.000219087786\n",
      "Epoch: 5795 | training loss: 0.000218963280\n",
      "Epoch: 5796 | training loss: 0.000218838788\n",
      "Epoch: 5797 | training loss: 0.000218714616\n",
      "Epoch: 5798 | training loss: 0.000218590896\n",
      "Epoch: 5799 | training loss: 0.000218466594\n",
      "Epoch: 5800 | training loss: 0.000218342902\n",
      "Epoch: 5801 | training loss: 0.000218218964\n",
      "Epoch: 5802 | training loss: 0.000218095025\n",
      "Epoch: 5803 | training loss: 0.000217971305\n",
      "Epoch: 5804 | training loss: 0.000217848152\n",
      "Epoch: 5805 | training loss: 0.000217724446\n",
      "Epoch: 5806 | training loss: 0.000217601308\n",
      "Epoch: 5807 | training loss: 0.000217478184\n",
      "Epoch: 5808 | training loss: 0.000217354973\n",
      "Epoch: 5809 | training loss: 0.000217231951\n",
      "Epoch: 5810 | training loss: 0.000217109002\n",
      "Epoch: 5811 | training loss: 0.000216986082\n",
      "Epoch: 5812 | training loss: 0.000216863351\n",
      "Epoch: 5813 | training loss: 0.000216740649\n",
      "Epoch: 5814 | training loss: 0.000216618209\n",
      "Epoch: 5815 | training loss: 0.000216495580\n",
      "Epoch: 5816 | training loss: 0.000216373446\n",
      "Epoch: 5817 | training loss: 0.000216251123\n",
      "Epoch: 5818 | training loss: 0.000216128974\n",
      "Epoch: 5819 | training loss: 0.000216006825\n",
      "Epoch: 5820 | training loss: 0.000215885113\n",
      "Epoch: 5821 | training loss: 0.000215763372\n",
      "Epoch: 5822 | training loss: 0.000215641572\n",
      "Epoch: 5823 | training loss: 0.000215520122\n",
      "Epoch: 5824 | training loss: 0.000215398410\n",
      "Epoch: 5825 | training loss: 0.000215277003\n",
      "Epoch: 5826 | training loss: 0.000215155727\n",
      "Epoch: 5827 | training loss: 0.000215034292\n",
      "Epoch: 5828 | training loss: 0.000214913205\n",
      "Epoch: 5829 | training loss: 0.000214792177\n",
      "Epoch: 5830 | training loss: 0.000214671178\n",
      "Epoch: 5831 | training loss: 0.000214550499\n",
      "Epoch: 5832 | training loss: 0.000214429776\n",
      "Epoch: 5833 | training loss: 0.000214309053\n",
      "Epoch: 5834 | training loss: 0.000214188709\n",
      "Epoch: 5835 | training loss: 0.000214068161\n",
      "Epoch: 5836 | training loss: 0.000213947758\n",
      "Epoch: 5837 | training loss: 0.000213827559\n",
      "Epoch: 5838 | training loss: 0.000213707433\n",
      "Epoch: 5839 | training loss: 0.000213587395\n",
      "Epoch: 5840 | training loss: 0.000213467283\n",
      "Epoch: 5841 | training loss: 0.000213347797\n",
      "Epoch: 5842 | training loss: 0.000213227991\n",
      "Epoch: 5843 | training loss: 0.000213108142\n",
      "Epoch: 5844 | training loss: 0.000212988685\n",
      "Epoch: 5845 | training loss: 0.000212869258\n",
      "Epoch: 5846 | training loss: 0.000212750048\n",
      "Epoch: 5847 | training loss: 0.000212630752\n",
      "Epoch: 5848 | training loss: 0.000212511528\n",
      "Epoch: 5849 | training loss: 0.000212392333\n",
      "Epoch: 5850 | training loss: 0.000212273211\n",
      "Epoch: 5851 | training loss: 0.000212154599\n",
      "Epoch: 5852 | training loss: 0.000212035753\n",
      "Epoch: 5853 | training loss: 0.000211917039\n",
      "Epoch: 5854 | training loss: 0.000211798571\n",
      "Epoch: 5855 | training loss: 0.000211679915\n",
      "Epoch: 5856 | training loss: 0.000211561593\n",
      "Epoch: 5857 | training loss: 0.000211443301\n",
      "Epoch: 5858 | training loss: 0.000211325096\n",
      "Epoch: 5859 | training loss: 0.000211206774\n",
      "Epoch: 5860 | training loss: 0.000211088860\n",
      "Epoch: 5861 | training loss: 0.000210970844\n",
      "Epoch: 5862 | training loss: 0.000210852857\n",
      "Epoch: 5863 | training loss: 0.000210735161\n",
      "Epoch: 5864 | training loss: 0.000210617844\n",
      "Epoch: 5865 | training loss: 0.000210500206\n",
      "Epoch: 5866 | training loss: 0.000210382961\n",
      "Epoch: 5867 | training loss: 0.000210265338\n",
      "Epoch: 5868 | training loss: 0.000210148151\n",
      "Epoch: 5869 | training loss: 0.000210030965\n",
      "Epoch: 5870 | training loss: 0.000209914171\n",
      "Epoch: 5871 | training loss: 0.000209797116\n",
      "Epoch: 5872 | training loss: 0.000209680104\n",
      "Epoch: 5873 | training loss: 0.000209563470\n",
      "Epoch: 5874 | training loss: 0.000209446880\n",
      "Epoch: 5875 | training loss: 0.000209330174\n",
      "Epoch: 5876 | training loss: 0.000209213496\n",
      "Epoch: 5877 | training loss: 0.000209097314\n",
      "Epoch: 5878 | training loss: 0.000208980971\n",
      "Epoch: 5879 | training loss: 0.000208864891\n",
      "Epoch: 5880 | training loss: 0.000208748854\n",
      "Epoch: 5881 | training loss: 0.000208632788\n",
      "Epoch: 5882 | training loss: 0.000208517042\n",
      "Epoch: 5883 | training loss: 0.000208401121\n",
      "Epoch: 5884 | training loss: 0.000208285477\n",
      "Epoch: 5885 | training loss: 0.000208169920\n",
      "Epoch: 5886 | training loss: 0.000208054320\n",
      "Epoch: 5887 | training loss: 0.000207938981\n",
      "Epoch: 5888 | training loss: 0.000207823556\n",
      "Epoch: 5889 | training loss: 0.000207708392\n",
      "Epoch: 5890 | training loss: 0.000207593170\n",
      "Epoch: 5891 | training loss: 0.000207478239\n",
      "Epoch: 5892 | training loss: 0.000207363162\n",
      "Epoch: 5893 | training loss: 0.000207248260\n",
      "Epoch: 5894 | training loss: 0.000207133664\n",
      "Epoch: 5895 | training loss: 0.000207018893\n",
      "Epoch: 5896 | training loss: 0.000206904253\n",
      "Epoch: 5897 | training loss: 0.000206789875\n",
      "Epoch: 5898 | training loss: 0.000206675511\n",
      "Epoch: 5899 | training loss: 0.000206561133\n",
      "Epoch: 5900 | training loss: 0.000206447017\n",
      "Epoch: 5901 | training loss: 0.000206332741\n",
      "Epoch: 5902 | training loss: 0.000206218858\n",
      "Epoch: 5903 | training loss: 0.000206104829\n",
      "Epoch: 5904 | training loss: 0.000205991208\n",
      "Epoch: 5905 | training loss: 0.000205877383\n",
      "Epoch: 5906 | training loss: 0.000205763747\n",
      "Epoch: 5907 | training loss: 0.000205650242\n",
      "Epoch: 5908 | training loss: 0.000205536926\n",
      "Epoch: 5909 | training loss: 0.000205423741\n",
      "Epoch: 5910 | training loss: 0.000205310236\n",
      "Epoch: 5911 | training loss: 0.000205197110\n",
      "Epoch: 5912 | training loss: 0.000205084070\n",
      "Epoch: 5913 | training loss: 0.000204971118\n",
      "Epoch: 5914 | training loss: 0.000204858254\n",
      "Epoch: 5915 | training loss: 0.000204745476\n",
      "Epoch: 5916 | training loss: 0.000204632714\n",
      "Epoch: 5917 | training loss: 0.000204520184\n",
      "Epoch: 5918 | training loss: 0.000204407668\n",
      "Epoch: 5919 | training loss: 0.000204295327\n",
      "Epoch: 5920 | training loss: 0.000204182870\n",
      "Epoch: 5921 | training loss: 0.000204070690\n",
      "Epoch: 5922 | training loss: 0.000203958596\n",
      "Epoch: 5923 | training loss: 0.000203846605\n",
      "Epoch: 5924 | training loss: 0.000203734671\n",
      "Epoch: 5925 | training loss: 0.000203622825\n",
      "Epoch: 5926 | training loss: 0.000203511154\n",
      "Epoch: 5927 | training loss: 0.000203399482\n",
      "Epoch: 5928 | training loss: 0.000203287811\n",
      "Epoch: 5929 | training loss: 0.000203176489\n",
      "Epoch: 5930 | training loss: 0.000203064861\n",
      "Epoch: 5931 | training loss: 0.000202953815\n",
      "Epoch: 5932 | training loss: 0.000202842552\n",
      "Epoch: 5933 | training loss: 0.000202731448\n",
      "Epoch: 5934 | training loss: 0.000202620256\n",
      "Epoch: 5935 | training loss: 0.000202509371\n",
      "Epoch: 5936 | training loss: 0.000202398660\n",
      "Epoch: 5937 | training loss: 0.000202287833\n",
      "Epoch: 5938 | training loss: 0.000202177063\n",
      "Epoch: 5939 | training loss: 0.000202066542\n",
      "Epoch: 5940 | training loss: 0.000201956049\n",
      "Epoch: 5941 | training loss: 0.000201845833\n",
      "Epoch: 5942 | training loss: 0.000201735325\n",
      "Epoch: 5943 | training loss: 0.000201625080\n",
      "Epoch: 5944 | training loss: 0.000201514980\n",
      "Epoch: 5945 | training loss: 0.000201404924\n",
      "Epoch: 5946 | training loss: 0.000201295043\n",
      "Epoch: 5947 | training loss: 0.000201184957\n",
      "Epoch: 5948 | training loss: 0.000201075323\n",
      "Epoch: 5949 | training loss: 0.000200965733\n",
      "Epoch: 5950 | training loss: 0.000200856040\n",
      "Epoch: 5951 | training loss: 0.000200746537\n",
      "Epoch: 5952 | training loss: 0.000200637121\n",
      "Epoch: 5953 | training loss: 0.000200527968\n",
      "Epoch: 5954 | training loss: 0.000200418552\n",
      "Epoch: 5955 | training loss: 0.000200309529\n",
      "Epoch: 5956 | training loss: 0.000200200418\n",
      "Epoch: 5957 | training loss: 0.000200091497\n",
      "Epoch: 5958 | training loss: 0.000199982707\n",
      "Epoch: 5959 | training loss: 0.000199873815\n",
      "Epoch: 5960 | training loss: 0.000199765345\n",
      "Epoch: 5961 | training loss: 0.000199656613\n",
      "Epoch: 5962 | training loss: 0.000199548231\n",
      "Epoch: 5963 | training loss: 0.000199439819\n",
      "Epoch: 5964 | training loss: 0.000199331640\n",
      "Epoch: 5965 | training loss: 0.000199223301\n",
      "Epoch: 5966 | training loss: 0.000199115326\n",
      "Epoch: 5967 | training loss: 0.000199007365\n",
      "Epoch: 5968 | training loss: 0.000198899404\n",
      "Epoch: 5969 | training loss: 0.000198791560\n",
      "Epoch: 5970 | training loss: 0.000198683789\n",
      "Epoch: 5971 | training loss: 0.000198576279\n",
      "Epoch: 5972 | training loss: 0.000198468595\n",
      "Epoch: 5973 | training loss: 0.000198361085\n",
      "Epoch: 5974 | training loss: 0.000198253838\n",
      "Epoch: 5975 | training loss: 0.000198146532\n",
      "Epoch: 5976 | training loss: 0.000198039357\n",
      "Epoch: 5977 | training loss: 0.000197932095\n",
      "Epoch: 5978 | training loss: 0.000197825007\n",
      "Epoch: 5979 | training loss: 0.000197718109\n",
      "Epoch: 5980 | training loss: 0.000197611284\n",
      "Epoch: 5981 | training loss: 0.000197504472\n",
      "Epoch: 5982 | training loss: 0.000197397923\n",
      "Epoch: 5983 | training loss: 0.000197291316\n",
      "Epoch: 5984 | training loss: 0.000197184752\n",
      "Epoch: 5985 | training loss: 0.000197078523\n",
      "Epoch: 5986 | training loss: 0.000196972265\n",
      "Epoch: 5987 | training loss: 0.000196865789\n",
      "Epoch: 5988 | training loss: 0.000196759705\n",
      "Epoch: 5989 | training loss: 0.000196653622\n",
      "Epoch: 5990 | training loss: 0.000196547742\n",
      "Epoch: 5991 | training loss: 0.000196441644\n",
      "Epoch: 5992 | training loss: 0.000196335910\n",
      "Epoch: 5993 | training loss: 0.000196230161\n",
      "Epoch: 5994 | training loss: 0.000196124398\n",
      "Epoch: 5995 | training loss: 0.000196018897\n",
      "Epoch: 5996 | training loss: 0.000195913541\n",
      "Epoch: 5997 | training loss: 0.000195808127\n",
      "Epoch: 5998 | training loss: 0.000195702771\n",
      "Epoch: 5999 | training loss: 0.000195597619\n",
      "Epoch: 6000 | training loss: 0.000195492525\n",
      "Epoch: 6001 | training loss: 0.000195387489\n",
      "Epoch: 6002 | training loss: 0.000195282337\n",
      "Epoch: 6003 | training loss: 0.000195177592\n",
      "Epoch: 6004 | training loss: 0.000195072716\n",
      "Epoch: 6005 | training loss: 0.000194968030\n",
      "Epoch: 6006 | training loss: 0.000194863504\n",
      "Epoch: 6007 | training loss: 0.000194758919\n",
      "Epoch: 6008 | training loss: 0.000194654582\n",
      "Epoch: 6009 | training loss: 0.000194550346\n",
      "Epoch: 6010 | training loss: 0.000194446038\n",
      "Epoch: 6011 | training loss: 0.000194341643\n",
      "Epoch: 6012 | training loss: 0.000194237771\n",
      "Epoch: 6013 | training loss: 0.000194133754\n",
      "Epoch: 6014 | training loss: 0.000194029650\n",
      "Epoch: 6015 | training loss: 0.000193925807\n",
      "Epoch: 6016 | training loss: 0.000193822169\n",
      "Epoch: 6017 | training loss: 0.000193718472\n",
      "Epoch: 6018 | training loss: 0.000193614818\n",
      "Epoch: 6019 | training loss: 0.000193511427\n",
      "Epoch: 6020 | training loss: 0.000193408021\n",
      "Epoch: 6021 | training loss: 0.000193304673\n",
      "Epoch: 6022 | training loss: 0.000193201355\n",
      "Epoch: 6023 | training loss: 0.000193098342\n",
      "Epoch: 6024 | training loss: 0.000192995125\n",
      "Epoch: 6025 | training loss: 0.000192892345\n",
      "Epoch: 6026 | training loss: 0.000192789332\n",
      "Epoch: 6027 | training loss: 0.000192686392\n",
      "Epoch: 6028 | training loss: 0.000192583801\n",
      "Epoch: 6029 | training loss: 0.000192481224\n",
      "Epoch: 6030 | training loss: 0.000192378415\n",
      "Epoch: 6031 | training loss: 0.000192276028\n",
      "Epoch: 6032 | training loss: 0.000192173844\n",
      "Epoch: 6033 | training loss: 0.000192071326\n",
      "Epoch: 6034 | training loss: 0.000191969331\n",
      "Epoch: 6035 | training loss: 0.000191867031\n",
      "Epoch: 6036 | training loss: 0.000191765081\n",
      "Epoch: 6037 | training loss: 0.000191663130\n",
      "Epoch: 6038 | training loss: 0.000191561383\n",
      "Epoch: 6039 | training loss: 0.000191459520\n",
      "Epoch: 6040 | training loss: 0.000191357904\n",
      "Epoch: 6041 | training loss: 0.000191256302\n",
      "Epoch: 6042 | training loss: 0.000191154802\n",
      "Epoch: 6043 | training loss: 0.000191053114\n",
      "Epoch: 6044 | training loss: 0.000190951745\n",
      "Epoch: 6045 | training loss: 0.000190850638\n",
      "Epoch: 6046 | training loss: 0.000190749386\n",
      "Epoch: 6047 | training loss: 0.000190648163\n",
      "Epoch: 6048 | training loss: 0.000190547231\n",
      "Epoch: 6049 | training loss: 0.000190446182\n",
      "Epoch: 6050 | training loss: 0.000190345396\n",
      "Epoch: 6051 | training loss: 0.000190244667\n",
      "Epoch: 6052 | training loss: 0.000190143939\n",
      "Epoch: 6053 | training loss: 0.000190043356\n",
      "Epoch: 6054 | training loss: 0.000189942890\n",
      "Epoch: 6055 | training loss: 0.000189842365\n",
      "Epoch: 6056 | training loss: 0.000189742146\n",
      "Epoch: 6057 | training loss: 0.000189641869\n",
      "Epoch: 6058 | training loss: 0.000189541810\n",
      "Epoch: 6059 | training loss: 0.000189441693\n",
      "Epoch: 6060 | training loss: 0.000189341619\n",
      "Epoch: 6061 | training loss: 0.000189241691\n",
      "Epoch: 6062 | training loss: 0.000189141982\n",
      "Epoch: 6063 | training loss: 0.000189042315\n",
      "Epoch: 6064 | training loss: 0.000188942198\n",
      "Epoch: 6065 | training loss: 0.000188842969\n",
      "Epoch: 6066 | training loss: 0.000188743259\n",
      "Epoch: 6067 | training loss: 0.000188643986\n",
      "Epoch: 6068 | training loss: 0.000188544567\n",
      "Epoch: 6069 | training loss: 0.000188445483\n",
      "Epoch: 6070 | training loss: 0.000188346254\n",
      "Epoch: 6071 | training loss: 0.000188247155\n",
      "Epoch: 6072 | training loss: 0.000188148057\n",
      "Epoch: 6073 | training loss: 0.000188049336\n",
      "Epoch: 6074 | training loss: 0.000187950558\n",
      "Epoch: 6075 | training loss: 0.000187851605\n",
      "Epoch: 6076 | training loss: 0.000187752885\n",
      "Epoch: 6077 | training loss: 0.000187654310\n",
      "Epoch: 6078 | training loss: 0.000187555794\n",
      "Epoch: 6079 | training loss: 0.000187457248\n",
      "Epoch: 6080 | training loss: 0.000187358935\n",
      "Epoch: 6081 | training loss: 0.000187260564\n",
      "Epoch: 6082 | training loss: 0.000187162339\n",
      "Epoch: 6083 | training loss: 0.000187064259\n",
      "Epoch: 6084 | training loss: 0.000186966354\n",
      "Epoch: 6085 | training loss: 0.000186868361\n",
      "Epoch: 6086 | training loss: 0.000186770514\n",
      "Epoch: 6087 | training loss: 0.000186672638\n",
      "Epoch: 6088 | training loss: 0.000186574791\n",
      "Epoch: 6089 | training loss: 0.000186477177\n",
      "Epoch: 6090 | training loss: 0.000186379359\n",
      "Epoch: 6091 | training loss: 0.000186282094\n",
      "Epoch: 6092 | training loss: 0.000186184770\n",
      "Epoch: 6093 | training loss: 0.000186087113\n",
      "Epoch: 6094 | training loss: 0.000185990037\n",
      "Epoch: 6095 | training loss: 0.000185893005\n",
      "Epoch: 6096 | training loss: 0.000185795725\n",
      "Epoch: 6097 | training loss: 0.000185698591\n",
      "Epoch: 6098 | training loss: 0.000185601661\n",
      "Epoch: 6099 | training loss: 0.000185504978\n",
      "Epoch: 6100 | training loss: 0.000185408135\n",
      "Epoch: 6101 | training loss: 0.000185311335\n",
      "Epoch: 6102 | training loss: 0.000185214798\n",
      "Epoch: 6103 | training loss: 0.000185118362\n",
      "Epoch: 6104 | training loss: 0.000185021796\n",
      "Epoch: 6105 | training loss: 0.000184925419\n",
      "Epoch: 6106 | training loss: 0.000184829129\n",
      "Epoch: 6107 | training loss: 0.000184732795\n",
      "Epoch: 6108 | training loss: 0.000184636549\n",
      "Epoch: 6109 | training loss: 0.000184540579\n",
      "Epoch: 6110 | training loss: 0.000184444536\n",
      "Epoch: 6111 | training loss: 0.000184348755\n",
      "Epoch: 6112 | training loss: 0.000184252771\n",
      "Epoch: 6113 | training loss: 0.000184156874\n",
      "Epoch: 6114 | training loss: 0.000184061500\n",
      "Epoch: 6115 | training loss: 0.000183965603\n",
      "Epoch: 6116 | training loss: 0.000183870157\n",
      "Epoch: 6117 | training loss: 0.000183774784\n",
      "Epoch: 6118 | training loss: 0.000183679615\n",
      "Epoch: 6119 | training loss: 0.000183584169\n",
      "Epoch: 6120 | training loss: 0.000183489072\n",
      "Epoch: 6121 | training loss: 0.000183393830\n",
      "Epoch: 6122 | training loss: 0.000183298631\n",
      "Epoch: 6123 | training loss: 0.000183203694\n",
      "Epoch: 6124 | training loss: 0.000183108903\n",
      "Epoch: 6125 | training loss: 0.000183013995\n",
      "Epoch: 6126 | training loss: 0.000182919030\n",
      "Epoch: 6127 | training loss: 0.000182824442\n",
      "Epoch: 6128 | training loss: 0.000182729913\n",
      "Epoch: 6129 | training loss: 0.000182635093\n",
      "Epoch: 6130 | training loss: 0.000182540869\n",
      "Epoch: 6131 | training loss: 0.000182446398\n",
      "Epoch: 6132 | training loss: 0.000182352203\n",
      "Epoch: 6133 | training loss: 0.000182257863\n",
      "Epoch: 6134 | training loss: 0.000182163756\n",
      "Epoch: 6135 | training loss: 0.000182069693\n",
      "Epoch: 6136 | training loss: 0.000181975833\n",
      "Epoch: 6137 | training loss: 0.000181881740\n",
      "Epoch: 6138 | training loss: 0.000181788113\n",
      "Epoch: 6139 | training loss: 0.000181694399\n",
      "Epoch: 6140 | training loss: 0.000181600699\n",
      "Epoch: 6141 | training loss: 0.000181507057\n",
      "Epoch: 6142 | training loss: 0.000181413532\n",
      "Epoch: 6143 | training loss: 0.000181320007\n",
      "Epoch: 6144 | training loss: 0.000181226613\n",
      "Epoch: 6145 | training loss: 0.000181133189\n",
      "Epoch: 6146 | training loss: 0.000181040130\n",
      "Epoch: 6147 | training loss: 0.000180946809\n",
      "Epoch: 6148 | training loss: 0.000180853851\n",
      "Epoch: 6149 | training loss: 0.000180760777\n",
      "Epoch: 6150 | training loss: 0.000180667834\n",
      "Epoch: 6151 | training loss: 0.000180574905\n",
      "Epoch: 6152 | training loss: 0.000180482268\n",
      "Epoch: 6153 | training loss: 0.000180389456\n",
      "Epoch: 6154 | training loss: 0.000180297022\n",
      "Epoch: 6155 | training loss: 0.000180204021\n",
      "Epoch: 6156 | training loss: 0.000180111849\n",
      "Epoch: 6157 | training loss: 0.000180019488\n",
      "Epoch: 6158 | training loss: 0.000179927069\n",
      "Epoch: 6159 | training loss: 0.000179834751\n",
      "Epoch: 6160 | training loss: 0.000179742492\n",
      "Epoch: 6161 | training loss: 0.000179650393\n",
      "Epoch: 6162 | training loss: 0.000179558265\n",
      "Epoch: 6163 | training loss: 0.000179466413\n",
      "Epoch: 6164 | training loss: 0.000179374518\n",
      "Epoch: 6165 | training loss: 0.000179282652\n",
      "Epoch: 6166 | training loss: 0.000179191076\n",
      "Epoch: 6167 | training loss: 0.000179099472\n",
      "Epoch: 6168 | training loss: 0.000179007620\n",
      "Epoch: 6169 | training loss: 0.000178916292\n",
      "Epoch: 6170 | training loss: 0.000178824848\n",
      "Epoch: 6171 | training loss: 0.000178733419\n",
      "Epoch: 6172 | training loss: 0.000178642251\n",
      "Epoch: 6173 | training loss: 0.000178551025\n",
      "Epoch: 6174 | training loss: 0.000178459683\n",
      "Epoch: 6175 | training loss: 0.000178368675\n",
      "Epoch: 6176 | training loss: 0.000178277842\n",
      "Epoch: 6177 | training loss: 0.000178186689\n",
      "Epoch: 6178 | training loss: 0.000178096117\n",
      "Epoch: 6179 | training loss: 0.000178005343\n",
      "Epoch: 6180 | training loss: 0.000177914742\n",
      "Epoch: 6181 | training loss: 0.000177823968\n",
      "Epoch: 6182 | training loss: 0.000177733426\n",
      "Epoch: 6183 | training loss: 0.000177643175\n",
      "Epoch: 6184 | training loss: 0.000177552778\n",
      "Epoch: 6185 | training loss: 0.000177462382\n",
      "Epoch: 6186 | training loss: 0.000177372087\n",
      "Epoch: 6187 | training loss: 0.000177282025\n",
      "Epoch: 6188 | training loss: 0.000177191861\n",
      "Epoch: 6189 | training loss: 0.000177101843\n",
      "Epoch: 6190 | training loss: 0.000177012029\n",
      "Epoch: 6191 | training loss: 0.000176922142\n",
      "Epoch: 6192 | training loss: 0.000176832458\n",
      "Epoch: 6193 | training loss: 0.000176742687\n",
      "Epoch: 6194 | training loss: 0.000176652975\n",
      "Epoch: 6195 | training loss: 0.000176563204\n",
      "Epoch: 6196 | training loss: 0.000176473652\n",
      "Epoch: 6197 | training loss: 0.000176384376\n",
      "Epoch: 6198 | training loss: 0.000176294852\n",
      "Epoch: 6199 | training loss: 0.000176205533\n",
      "Epoch: 6200 | training loss: 0.000176116300\n",
      "Epoch: 6201 | training loss: 0.000176027301\n",
      "Epoch: 6202 | training loss: 0.000175938083\n",
      "Epoch: 6203 | training loss: 0.000175849040\n",
      "Epoch: 6204 | training loss: 0.000175760157\n",
      "Epoch: 6205 | training loss: 0.000175671274\n",
      "Epoch: 6206 | training loss: 0.000175582303\n",
      "Epoch: 6207 | training loss: 0.000175493653\n",
      "Epoch: 6208 | training loss: 0.000175404988\n",
      "Epoch: 6209 | training loss: 0.000175316265\n",
      "Epoch: 6210 | training loss: 0.000175227819\n",
      "Epoch: 6211 | training loss: 0.000175139314\n",
      "Epoch: 6212 | training loss: 0.000175050925\n",
      "Epoch: 6213 | training loss: 0.000174962566\n",
      "Epoch: 6214 | training loss: 0.000174874411\n",
      "Epoch: 6215 | training loss: 0.000174786168\n",
      "Epoch: 6216 | training loss: 0.000174698085\n",
      "Epoch: 6217 | training loss: 0.000174609944\n",
      "Epoch: 6218 | training loss: 0.000174521803\n",
      "Epoch: 6219 | training loss: 0.000174434026\n",
      "Epoch: 6220 | training loss: 0.000174346118\n",
      "Epoch: 6221 | training loss: 0.000174258428\n",
      "Epoch: 6222 | training loss: 0.000174170738\n",
      "Epoch: 6223 | training loss: 0.000174083063\n",
      "Epoch: 6224 | training loss: 0.000173995475\n",
      "Epoch: 6225 | training loss: 0.000173908120\n",
      "Epoch: 6226 | training loss: 0.000173820503\n",
      "Epoch: 6227 | training loss: 0.000173733279\n",
      "Epoch: 6228 | training loss: 0.000173646040\n",
      "Epoch: 6229 | training loss: 0.000173558801\n",
      "Epoch: 6230 | training loss: 0.000173471577\n",
      "Epoch: 6231 | training loss: 0.000173384469\n",
      "Epoch: 6232 | training loss: 0.000173297609\n",
      "Epoch: 6233 | training loss: 0.000173210778\n",
      "Epoch: 6234 | training loss: 0.000173123743\n",
      "Epoch: 6235 | training loss: 0.000173036984\n",
      "Epoch: 6236 | training loss: 0.000172950269\n",
      "Epoch: 6237 | training loss: 0.000172863613\n",
      "Epoch: 6238 | training loss: 0.000172777072\n",
      "Epoch: 6239 | training loss: 0.000172690576\n",
      "Epoch: 6240 | training loss: 0.000172604166\n",
      "Epoch: 6241 | training loss: 0.000172517772\n",
      "Epoch: 6242 | training loss: 0.000172431493\n",
      "Epoch: 6243 | training loss: 0.000172345055\n",
      "Epoch: 6244 | training loss: 0.000172258937\n",
      "Epoch: 6245 | training loss: 0.000172173095\n",
      "Epoch: 6246 | training loss: 0.000172086817\n",
      "Epoch: 6247 | training loss: 0.000172000844\n",
      "Epoch: 6248 | training loss: 0.000171914944\n",
      "Epoch: 6249 | training loss: 0.000171829102\n",
      "Epoch: 6250 | training loss: 0.000171743362\n",
      "Epoch: 6251 | training loss: 0.000171657608\n",
      "Epoch: 6252 | training loss: 0.000171571941\n",
      "Epoch: 6253 | training loss: 0.000171486434\n",
      "Epoch: 6254 | training loss: 0.000171401116\n",
      "Epoch: 6255 | training loss: 0.000171315623\n",
      "Epoch: 6256 | training loss: 0.000171230131\n",
      "Epoch: 6257 | training loss: 0.000171144900\n",
      "Epoch: 6258 | training loss: 0.000171059568\n",
      "Epoch: 6259 | training loss: 0.000170974439\n",
      "Epoch: 6260 | training loss: 0.000170889281\n",
      "Epoch: 6261 | training loss: 0.000170804356\n",
      "Epoch: 6262 | training loss: 0.000170719475\n",
      "Epoch: 6263 | training loss: 0.000170634346\n",
      "Epoch: 6264 | training loss: 0.000170549785\n",
      "Epoch: 6265 | training loss: 0.000170464773\n",
      "Epoch: 6266 | training loss: 0.000170380197\n",
      "Epoch: 6267 | training loss: 0.000170295505\n",
      "Epoch: 6268 | training loss: 0.000170210755\n",
      "Epoch: 6269 | training loss: 0.000170126383\n",
      "Epoch: 6270 | training loss: 0.000170041923\n",
      "Epoch: 6271 | training loss: 0.000169957522\n",
      "Epoch: 6272 | training loss: 0.000169873281\n",
      "Epoch: 6273 | training loss: 0.000169789157\n",
      "Epoch: 6274 | training loss: 0.000169704872\n",
      "Epoch: 6275 | training loss: 0.000169620762\n",
      "Epoch: 6276 | training loss: 0.000169536637\n",
      "Epoch: 6277 | training loss: 0.000169452716\n",
      "Epoch: 6278 | training loss: 0.000169368781\n",
      "Epoch: 6279 | training loss: 0.000169284933\n",
      "Epoch: 6280 | training loss: 0.000169201085\n",
      "Epoch: 6281 | training loss: 0.000169117528\n",
      "Epoch: 6282 | training loss: 0.000169033883\n",
      "Epoch: 6283 | training loss: 0.000168950151\n",
      "Epoch: 6284 | training loss: 0.000168866711\n",
      "Epoch: 6285 | training loss: 0.000168783270\n",
      "Epoch: 6286 | training loss: 0.000168699873\n",
      "Epoch: 6287 | training loss: 0.000168616418\n",
      "Epoch: 6288 | training loss: 0.000168533123\n",
      "Epoch: 6289 | training loss: 0.000168449973\n",
      "Epoch: 6290 | training loss: 0.000168366649\n",
      "Epoch: 6291 | training loss: 0.000168283615\n",
      "Epoch: 6292 | training loss: 0.000168200466\n",
      "Epoch: 6293 | training loss: 0.000168117578\n",
      "Epoch: 6294 | training loss: 0.000168034647\n",
      "Epoch: 6295 | training loss: 0.000167951919\n",
      "Epoch: 6296 | training loss: 0.000167868973\n",
      "Epoch: 6297 | training loss: 0.000167786333\n",
      "Epoch: 6298 | training loss: 0.000167703576\n",
      "Epoch: 6299 | training loss: 0.000167621067\n",
      "Epoch: 6300 | training loss: 0.000167538383\n",
      "Epoch: 6301 | training loss: 0.000167456048\n",
      "Epoch: 6302 | training loss: 0.000167373553\n",
      "Epoch: 6303 | training loss: 0.000167291204\n",
      "Epoch: 6304 | training loss: 0.000167208971\n",
      "Epoch: 6305 | training loss: 0.000167126840\n",
      "Epoch: 6306 | training loss: 0.000167044520\n",
      "Epoch: 6307 | training loss: 0.000166962389\n",
      "Epoch: 6308 | training loss: 0.000166880462\n",
      "Epoch: 6309 | training loss: 0.000166798563\n",
      "Epoch: 6310 | training loss: 0.000166716753\n",
      "Epoch: 6311 | training loss: 0.000166634680\n",
      "Epoch: 6312 | training loss: 0.000166552985\n",
      "Epoch: 6313 | training loss: 0.000166471276\n",
      "Epoch: 6314 | training loss: 0.000166389567\n",
      "Epoch: 6315 | training loss: 0.000166307931\n",
      "Epoch: 6316 | training loss: 0.000166226440\n",
      "Epoch: 6317 | training loss: 0.000166144891\n",
      "Epoch: 6318 | training loss: 0.000166063459\n",
      "Epoch: 6319 | training loss: 0.000165982201\n",
      "Epoch: 6320 | training loss: 0.000165900681\n",
      "Epoch: 6321 | training loss: 0.000165819656\n",
      "Epoch: 6322 | training loss: 0.000165738515\n",
      "Epoch: 6323 | training loss: 0.000165657300\n",
      "Epoch: 6324 | training loss: 0.000165576392\n",
      "Epoch: 6325 | training loss: 0.000165495294\n",
      "Epoch: 6326 | training loss: 0.000165414327\n",
      "Epoch: 6327 | training loss: 0.000165333506\n",
      "Epoch: 6328 | training loss: 0.000165252713\n",
      "Epoch: 6329 | training loss: 0.000165171921\n",
      "Epoch: 6330 | training loss: 0.000165091304\n",
      "Epoch: 6331 | training loss: 0.000165010599\n",
      "Epoch: 6332 | training loss: 0.000164930054\n",
      "Epoch: 6333 | training loss: 0.000164849625\n",
      "Epoch: 6334 | training loss: 0.000164769212\n",
      "Epoch: 6335 | training loss: 0.000164688972\n",
      "Epoch: 6336 | training loss: 0.000164608369\n",
      "Epoch: 6337 | training loss: 0.000164528174\n",
      "Epoch: 6338 | training loss: 0.000164448036\n",
      "Epoch: 6339 | training loss: 0.000164367870\n",
      "Epoch: 6340 | training loss: 0.000164287863\n",
      "Epoch: 6341 | training loss: 0.000164207799\n",
      "Epoch: 6342 | training loss: 0.000164127981\n",
      "Epoch: 6343 | training loss: 0.000164047990\n",
      "Epoch: 6344 | training loss: 0.000163968303\n",
      "Epoch: 6345 | training loss: 0.000163888530\n",
      "Epoch: 6346 | training loss: 0.000163808989\n",
      "Epoch: 6347 | training loss: 0.000163729157\n",
      "Epoch: 6348 | training loss: 0.000163649791\n",
      "Epoch: 6349 | training loss: 0.000163570163\n",
      "Epoch: 6350 | training loss: 0.000163490680\n",
      "Epoch: 6351 | training loss: 0.000163411358\n",
      "Epoch: 6352 | training loss: 0.000163332064\n",
      "Epoch: 6353 | training loss: 0.000163252873\n",
      "Epoch: 6354 | training loss: 0.000163173710\n",
      "Epoch: 6355 | training loss: 0.000163094403\n",
      "Epoch: 6356 | training loss: 0.000163015357\n",
      "Epoch: 6357 | training loss: 0.000162936281\n",
      "Epoch: 6358 | training loss: 0.000162857541\n",
      "Epoch: 6359 | training loss: 0.000162778641\n",
      "Epoch: 6360 | training loss: 0.000162699696\n",
      "Epoch: 6361 | training loss: 0.000162620883\n",
      "Epoch: 6362 | training loss: 0.000162542245\n",
      "Epoch: 6363 | training loss: 0.000162463577\n",
      "Epoch: 6364 | training loss: 0.000162384866\n",
      "Epoch: 6365 | training loss: 0.000162306344\n",
      "Epoch: 6366 | training loss: 0.000162227923\n",
      "Epoch: 6367 | training loss: 0.000162149270\n",
      "Epoch: 6368 | training loss: 0.000162070908\n",
      "Epoch: 6369 | training loss: 0.000161992779\n",
      "Epoch: 6370 | training loss: 0.000161914446\n",
      "Epoch: 6371 | training loss: 0.000161836084\n",
      "Epoch: 6372 | training loss: 0.000161758042\n",
      "Epoch: 6373 | training loss: 0.000161679942\n",
      "Epoch: 6374 | training loss: 0.000161601987\n",
      "Epoch: 6375 | training loss: 0.000161523989\n",
      "Epoch: 6376 | training loss: 0.000161445962\n",
      "Epoch: 6377 | training loss: 0.000161368051\n",
      "Epoch: 6378 | training loss: 0.000161290372\n",
      "Epoch: 6379 | training loss: 0.000161212418\n",
      "Epoch: 6380 | training loss: 0.000161134929\n",
      "Epoch: 6381 | training loss: 0.000161057222\n",
      "Epoch: 6382 | training loss: 0.000160979689\n",
      "Epoch: 6383 | training loss: 0.000160902011\n",
      "Epoch: 6384 | training loss: 0.000160824740\n",
      "Epoch: 6385 | training loss: 0.000160747353\n",
      "Epoch: 6386 | training loss: 0.000160669908\n",
      "Epoch: 6387 | training loss: 0.000160592637\n",
      "Epoch: 6388 | training loss: 0.000160515468\n",
      "Epoch: 6389 | training loss: 0.000160438329\n",
      "Epoch: 6390 | training loss: 0.000160361160\n",
      "Epoch: 6391 | training loss: 0.000160284035\n",
      "Epoch: 6392 | training loss: 0.000160207012\n",
      "Epoch: 6393 | training loss: 0.000160130148\n",
      "Epoch: 6394 | training loss: 0.000160053285\n",
      "Epoch: 6395 | training loss: 0.000159976407\n",
      "Epoch: 6396 | training loss: 0.000159899748\n",
      "Epoch: 6397 | training loss: 0.000159822826\n",
      "Epoch: 6398 | training loss: 0.000159746283\n",
      "Epoch: 6399 | training loss: 0.000159669638\n",
      "Epoch: 6400 | training loss: 0.000159593183\n",
      "Epoch: 6401 | training loss: 0.000159516625\n",
      "Epoch: 6402 | training loss: 0.000159440198\n",
      "Epoch: 6403 | training loss: 0.000159363801\n",
      "Epoch: 6404 | training loss: 0.000159287520\n",
      "Epoch: 6405 | training loss: 0.000159211180\n",
      "Epoch: 6406 | training loss: 0.000159135088\n",
      "Epoch: 6407 | training loss: 0.000159058865\n",
      "Epoch: 6408 | training loss: 0.000158982861\n",
      "Epoch: 6409 | training loss: 0.000158906740\n",
      "Epoch: 6410 | training loss: 0.000158830837\n",
      "Epoch: 6411 | training loss: 0.000158754818\n",
      "Epoch: 6412 | training loss: 0.000158678944\n",
      "Epoch: 6413 | training loss: 0.000158603099\n",
      "Epoch: 6414 | training loss: 0.000158527284\n",
      "Epoch: 6415 | training loss: 0.000158451439\n",
      "Epoch: 6416 | training loss: 0.000158375944\n",
      "Epoch: 6417 | training loss: 0.000158300289\n",
      "Epoch: 6418 | training loss: 0.000158224575\n",
      "Epoch: 6419 | training loss: 0.000158149152\n",
      "Epoch: 6420 | training loss: 0.000158073788\n",
      "Epoch: 6421 | training loss: 0.000157998409\n",
      "Epoch: 6422 | training loss: 0.000157923030\n",
      "Epoch: 6423 | training loss: 0.000157847739\n",
      "Epoch: 6424 | training loss: 0.000157772432\n",
      "Epoch: 6425 | training loss: 0.000157697243\n",
      "Epoch: 6426 | training loss: 0.000157622067\n",
      "Epoch: 6427 | training loss: 0.000157546951\n",
      "Epoch: 6428 | training loss: 0.000157471863\n",
      "Epoch: 6429 | training loss: 0.000157396993\n",
      "Epoch: 6430 | training loss: 0.000157322065\n",
      "Epoch: 6431 | training loss: 0.000157247065\n",
      "Epoch: 6432 | training loss: 0.000157172341\n",
      "Epoch: 6433 | training loss: 0.000157097500\n",
      "Epoch: 6434 | training loss: 0.000157022703\n",
      "Epoch: 6435 | training loss: 0.000156948081\n",
      "Epoch: 6436 | training loss: 0.000156873517\n",
      "Epoch: 6437 | training loss: 0.000156798895\n",
      "Epoch: 6438 | training loss: 0.000156724462\n",
      "Epoch: 6439 | training loss: 0.000156650101\n",
      "Epoch: 6440 | training loss: 0.000156575843\n",
      "Epoch: 6441 | training loss: 0.000156501366\n",
      "Epoch: 6442 | training loss: 0.000156427064\n",
      "Epoch: 6443 | training loss: 0.000156352864\n",
      "Epoch: 6444 | training loss: 0.000156278693\n",
      "Epoch: 6445 | training loss: 0.000156204595\n",
      "Epoch: 6446 | training loss: 0.000156130525\n",
      "Epoch: 6447 | training loss: 0.000156056412\n",
      "Epoch: 6448 | training loss: 0.000155982561\n",
      "Epoch: 6449 | training loss: 0.000155908390\n",
      "Epoch: 6450 | training loss: 0.000155834510\n",
      "Epoch: 6451 | training loss: 0.000155760776\n",
      "Epoch: 6452 | training loss: 0.000155687085\n",
      "Epoch: 6453 | training loss: 0.000155613467\n",
      "Epoch: 6454 | training loss: 0.000155539688\n",
      "Epoch: 6455 | training loss: 0.000155466085\n",
      "Epoch: 6456 | training loss: 0.000155392481\n",
      "Epoch: 6457 | training loss: 0.000155318921\n",
      "Epoch: 6458 | training loss: 0.000155245376\n",
      "Epoch: 6459 | training loss: 0.000155172107\n",
      "Epoch: 6460 | training loss: 0.000155098693\n",
      "Epoch: 6461 | training loss: 0.000155025438\n",
      "Epoch: 6462 | training loss: 0.000154951995\n",
      "Epoch: 6463 | training loss: 0.000154878959\n",
      "Epoch: 6464 | training loss: 0.000154805879\n",
      "Epoch: 6465 | training loss: 0.000154732727\n",
      "Epoch: 6466 | training loss: 0.000154659792\n",
      "Epoch: 6467 | training loss: 0.000154586683\n",
      "Epoch: 6468 | training loss: 0.000154513822\n",
      "Epoch: 6469 | training loss: 0.000154440742\n",
      "Epoch: 6470 | training loss: 0.000154367895\n",
      "Epoch: 6471 | training loss: 0.000154295092\n",
      "Epoch: 6472 | training loss: 0.000154222289\n",
      "Epoch: 6473 | training loss: 0.000154149573\n",
      "Epoch: 6474 | training loss: 0.000154076843\n",
      "Epoch: 6475 | training loss: 0.000154004345\n",
      "Epoch: 6476 | training loss: 0.000153931833\n",
      "Epoch: 6477 | training loss: 0.000153859204\n",
      "Epoch: 6478 | training loss: 0.000153786736\n",
      "Epoch: 6479 | training loss: 0.000153714442\n",
      "Epoch: 6480 | training loss: 0.000153642046\n",
      "Epoch: 6481 | training loss: 0.000153569737\n",
      "Epoch: 6482 | training loss: 0.000153497414\n",
      "Epoch: 6483 | training loss: 0.000153425164\n",
      "Epoch: 6484 | training loss: 0.000153353118\n",
      "Epoch: 6485 | training loss: 0.000153281013\n",
      "Epoch: 6486 | training loss: 0.000153208908\n",
      "Epoch: 6487 | training loss: 0.000153136862\n",
      "Epoch: 6488 | training loss: 0.000153064844\n",
      "Epoch: 6489 | training loss: 0.000152992841\n",
      "Epoch: 6490 | training loss: 0.000152920955\n",
      "Epoch: 6491 | training loss: 0.000152849301\n",
      "Epoch: 6492 | training loss: 0.000152777473\n",
      "Epoch: 6493 | training loss: 0.000152705878\n",
      "Epoch: 6494 | training loss: 0.000152634035\n",
      "Epoch: 6495 | training loss: 0.000152562599\n",
      "Epoch: 6496 | training loss: 0.000152490931\n",
      "Epoch: 6497 | training loss: 0.000152419350\n",
      "Epoch: 6498 | training loss: 0.000152347988\n",
      "Epoch: 6499 | training loss: 0.000152276552\n",
      "Epoch: 6500 | training loss: 0.000152205175\n",
      "Epoch: 6501 | training loss: 0.000152133885\n",
      "Epoch: 6502 | training loss: 0.000152062479\n",
      "Epoch: 6503 | training loss: 0.000151991277\n",
      "Epoch: 6504 | training loss: 0.000151920132\n",
      "Epoch: 6505 | training loss: 0.000151848915\n",
      "Epoch: 6506 | training loss: 0.000151777960\n",
      "Epoch: 6507 | training loss: 0.000151706932\n",
      "Epoch: 6508 | training loss: 0.000151635933\n",
      "Epoch: 6509 | training loss: 0.000151564978\n",
      "Epoch: 6510 | training loss: 0.000151494140\n",
      "Epoch: 6511 | training loss: 0.000151423359\n",
      "Epoch: 6512 | training loss: 0.000151352535\n",
      "Epoch: 6513 | training loss: 0.000151281813\n",
      "Epoch: 6514 | training loss: 0.000151211207\n",
      "Epoch: 6515 | training loss: 0.000151140528\n",
      "Epoch: 6516 | training loss: 0.000151069893\n",
      "Epoch: 6517 | training loss: 0.000150999404\n",
      "Epoch: 6518 | training loss: 0.000150928812\n",
      "Epoch: 6519 | training loss: 0.000150858366\n",
      "Epoch: 6520 | training loss: 0.000150788124\n",
      "Epoch: 6521 | training loss: 0.000150717591\n",
      "Epoch: 6522 | training loss: 0.000150647465\n",
      "Epoch: 6523 | training loss: 0.000150577223\n",
      "Epoch: 6524 | training loss: 0.000150506792\n",
      "Epoch: 6525 | training loss: 0.000150436797\n",
      "Epoch: 6526 | training loss: 0.000150366614\n",
      "Epoch: 6527 | training loss: 0.000150296575\n",
      "Epoch: 6528 | training loss: 0.000150226406\n",
      "Epoch: 6529 | training loss: 0.000150156469\n",
      "Epoch: 6530 | training loss: 0.000150086620\n",
      "Epoch: 6531 | training loss: 0.000150016771\n",
      "Epoch: 6532 | training loss: 0.000149946849\n",
      "Epoch: 6533 | training loss: 0.000149876956\n",
      "Epoch: 6534 | training loss: 0.000149807252\n",
      "Epoch: 6535 | training loss: 0.000149737636\n",
      "Epoch: 6536 | training loss: 0.000149668034\n",
      "Epoch: 6537 | training loss: 0.000149598258\n",
      "Epoch: 6538 | training loss: 0.000149528947\n",
      "Epoch: 6539 | training loss: 0.000149459389\n",
      "Epoch: 6540 | training loss: 0.000149389860\n",
      "Epoch: 6541 | training loss: 0.000149320462\n",
      "Epoch: 6542 | training loss: 0.000149250976\n",
      "Epoch: 6543 | training loss: 0.000149181782\n",
      "Epoch: 6544 | training loss: 0.000149112457\n",
      "Epoch: 6545 | training loss: 0.000149043350\n",
      "Epoch: 6546 | training loss: 0.000148974010\n",
      "Epoch: 6547 | training loss: 0.000148904903\n",
      "Epoch: 6548 | training loss: 0.000148835898\n",
      "Epoch: 6549 | training loss: 0.000148766921\n",
      "Epoch: 6550 | training loss: 0.000148698062\n",
      "Epoch: 6551 | training loss: 0.000148628838\n",
      "Epoch: 6552 | training loss: 0.000148559993\n",
      "Epoch: 6553 | training loss: 0.000148491119\n",
      "Epoch: 6554 | training loss: 0.000148422492\n",
      "Epoch: 6555 | training loss: 0.000148353691\n",
      "Epoch: 6556 | training loss: 0.000148284962\n",
      "Epoch: 6557 | training loss: 0.000148216306\n",
      "Epoch: 6558 | training loss: 0.000148147505\n",
      "Epoch: 6559 | training loss: 0.000148079052\n",
      "Epoch: 6560 | training loss: 0.000148010542\n",
      "Epoch: 6561 | training loss: 0.000147942032\n",
      "Epoch: 6562 | training loss: 0.000147873696\n",
      "Epoch: 6563 | training loss: 0.000147805229\n",
      "Epoch: 6564 | training loss: 0.000147737010\n",
      "Epoch: 6565 | training loss: 0.000147668674\n",
      "Epoch: 6566 | training loss: 0.000147600324\n",
      "Epoch: 6567 | training loss: 0.000147532293\n",
      "Epoch: 6568 | training loss: 0.000147464103\n",
      "Epoch: 6569 | training loss: 0.000147395913\n",
      "Epoch: 6570 | training loss: 0.000147327810\n",
      "Epoch: 6571 | training loss: 0.000147259911\n",
      "Epoch: 6572 | training loss: 0.000147191895\n",
      "Epoch: 6573 | training loss: 0.000147123879\n",
      "Epoch: 6574 | training loss: 0.000147055995\n",
      "Epoch: 6575 | training loss: 0.000146988139\n",
      "Epoch: 6576 | training loss: 0.000146920327\n",
      "Epoch: 6577 | training loss: 0.000146852602\n",
      "Epoch: 6578 | training loss: 0.000146784849\n",
      "Epoch: 6579 | training loss: 0.000146717255\n",
      "Epoch: 6580 | training loss: 0.000146649472\n",
      "Epoch: 6581 | training loss: 0.000146582097\n",
      "Epoch: 6582 | training loss: 0.000146514591\n",
      "Epoch: 6583 | training loss: 0.000146446982\n",
      "Epoch: 6584 | training loss: 0.000146379389\n",
      "Epoch: 6585 | training loss: 0.000146312086\n",
      "Epoch: 6586 | training loss: 0.000146244769\n",
      "Epoch: 6587 | training loss: 0.000146177423\n",
      "Epoch: 6588 | training loss: 0.000146110251\n",
      "Epoch: 6589 | training loss: 0.000146042978\n",
      "Epoch: 6590 | training loss: 0.000145975777\n",
      "Epoch: 6591 | training loss: 0.000145908693\n",
      "Epoch: 6592 | training loss: 0.000145841521\n",
      "Epoch: 6593 | training loss: 0.000145774524\n",
      "Epoch: 6594 | training loss: 0.000145707600\n",
      "Epoch: 6595 | training loss: 0.000145640501\n",
      "Epoch: 6596 | training loss: 0.000145573693\n",
      "Epoch: 6597 | training loss: 0.000145506783\n",
      "Epoch: 6598 | training loss: 0.000145440034\n",
      "Epoch: 6599 | training loss: 0.000145373167\n",
      "Epoch: 6600 | training loss: 0.000145306403\n",
      "Epoch: 6601 | training loss: 0.000145239610\n",
      "Epoch: 6602 | training loss: 0.000145172962\n",
      "Epoch: 6603 | training loss: 0.000145106242\n",
      "Epoch: 6604 | training loss: 0.000145039783\n",
      "Epoch: 6605 | training loss: 0.000144973179\n",
      "Epoch: 6606 | training loss: 0.000144906662\n",
      "Epoch: 6607 | training loss: 0.000144840247\n",
      "Epoch: 6608 | training loss: 0.000144773920\n",
      "Epoch: 6609 | training loss: 0.000144707490\n",
      "Epoch: 6610 | training loss: 0.000144641032\n",
      "Epoch: 6611 | training loss: 0.000144574762\n",
      "Epoch: 6612 | training loss: 0.000144508434\n",
      "Epoch: 6613 | training loss: 0.000144442194\n",
      "Epoch: 6614 | training loss: 0.000144376172\n",
      "Epoch: 6615 | training loss: 0.000144309946\n",
      "Epoch: 6616 | training loss: 0.000144243822\n",
      "Epoch: 6617 | training loss: 0.000144177728\n",
      "Epoch: 6618 | training loss: 0.000144111749\n",
      "Epoch: 6619 | training loss: 0.000144045873\n",
      "Epoch: 6620 | training loss: 0.000143979982\n",
      "Epoch: 6621 | training loss: 0.000143914134\n",
      "Epoch: 6622 | training loss: 0.000143848214\n",
      "Epoch: 6623 | training loss: 0.000143782207\n",
      "Epoch: 6624 | training loss: 0.000143716490\n",
      "Epoch: 6625 | training loss: 0.000143650919\n",
      "Epoch: 6626 | training loss: 0.000143585115\n",
      "Epoch: 6627 | training loss: 0.000143519515\n",
      "Epoch: 6628 | training loss: 0.000143454032\n",
      "Epoch: 6629 | training loss: 0.000143388286\n",
      "Epoch: 6630 | training loss: 0.000143322817\n",
      "Epoch: 6631 | training loss: 0.000143257363\n",
      "Epoch: 6632 | training loss: 0.000143191952\n",
      "Epoch: 6633 | training loss: 0.000143126483\n",
      "Epoch: 6634 | training loss: 0.000143061203\n",
      "Epoch: 6635 | training loss: 0.000142995879\n",
      "Epoch: 6636 | training loss: 0.000142930774\n",
      "Epoch: 6637 | training loss: 0.000142865407\n",
      "Epoch: 6638 | training loss: 0.000142800054\n",
      "Epoch: 6639 | training loss: 0.000142734934\n",
      "Epoch: 6640 | training loss: 0.000142669902\n",
      "Epoch: 6641 | training loss: 0.000142604855\n",
      "Epoch: 6642 | training loss: 0.000142539895\n",
      "Epoch: 6643 | training loss: 0.000142474892\n",
      "Epoch: 6644 | training loss: 0.000142409874\n",
      "Epoch: 6645 | training loss: 0.000142345089\n",
      "Epoch: 6646 | training loss: 0.000142280085\n",
      "Epoch: 6647 | training loss: 0.000142215489\n",
      "Epoch: 6648 | training loss: 0.000142150631\n",
      "Epoch: 6649 | training loss: 0.000142085744\n",
      "Epoch: 6650 | training loss: 0.000142021076\n",
      "Epoch: 6651 | training loss: 0.000141956436\n",
      "Epoch: 6652 | training loss: 0.000141891796\n",
      "Epoch: 6653 | training loss: 0.000141827302\n",
      "Epoch: 6654 | training loss: 0.000141762808\n",
      "Epoch: 6655 | training loss: 0.000141698256\n",
      "Epoch: 6656 | training loss: 0.000141633849\n",
      "Epoch: 6657 | training loss: 0.000141569268\n",
      "Epoch: 6658 | training loss: 0.000141505123\n",
      "Epoch: 6659 | training loss: 0.000141440585\n",
      "Epoch: 6660 | training loss: 0.000141376251\n",
      "Epoch: 6661 | training loss: 0.000141311975\n",
      "Epoch: 6662 | training loss: 0.000141247685\n",
      "Epoch: 6663 | training loss: 0.000141183380\n",
      "Epoch: 6664 | training loss: 0.000141119323\n",
      "Epoch: 6665 | training loss: 0.000141055090\n",
      "Epoch: 6666 | training loss: 0.000140991164\n",
      "Epoch: 6667 | training loss: 0.000140927208\n",
      "Epoch: 6668 | training loss: 0.000140862903\n",
      "Epoch: 6669 | training loss: 0.000140799166\n",
      "Epoch: 6670 | training loss: 0.000140735196\n",
      "Epoch: 6671 | training loss: 0.000140671342\n",
      "Epoch: 6672 | training loss: 0.000140607372\n",
      "Epoch: 6673 | training loss: 0.000140543489\n",
      "Epoch: 6674 | training loss: 0.000140479882\n",
      "Epoch: 6675 | training loss: 0.000140416028\n",
      "Epoch: 6676 | training loss: 0.000140352335\n",
      "Epoch: 6677 | training loss: 0.000140288728\n",
      "Epoch: 6678 | training loss: 0.000140224962\n",
      "Epoch: 6679 | training loss: 0.000140161574\n",
      "Epoch: 6680 | training loss: 0.000140097880\n",
      "Epoch: 6681 | training loss: 0.000140034317\n",
      "Epoch: 6682 | training loss: 0.000139970987\n",
      "Epoch: 6683 | training loss: 0.000139907454\n",
      "Epoch: 6684 | training loss: 0.000139844182\n",
      "Epoch: 6685 | training loss: 0.000139780823\n",
      "Epoch: 6686 | training loss: 0.000139717449\n",
      "Epoch: 6687 | training loss: 0.000139654177\n",
      "Epoch: 6688 | training loss: 0.000139590877\n",
      "Epoch: 6689 | training loss: 0.000139527736\n",
      "Epoch: 6690 | training loss: 0.000139464595\n",
      "Epoch: 6691 | training loss: 0.000139401658\n",
      "Epoch: 6692 | training loss: 0.000139338386\n",
      "Epoch: 6693 | training loss: 0.000139275173\n",
      "Epoch: 6694 | training loss: 0.000139212265\n",
      "Epoch: 6695 | training loss: 0.000139149241\n",
      "Epoch: 6696 | training loss: 0.000139086260\n",
      "Epoch: 6697 | training loss: 0.000139023468\n",
      "Epoch: 6698 | training loss: 0.000138960517\n",
      "Epoch: 6699 | training loss: 0.000138897783\n",
      "Epoch: 6700 | training loss: 0.000138834876\n",
      "Epoch: 6701 | training loss: 0.000138772361\n",
      "Epoch: 6702 | training loss: 0.000138709467\n",
      "Epoch: 6703 | training loss: 0.000138646792\n",
      "Epoch: 6704 | training loss: 0.000138584190\n",
      "Epoch: 6705 | training loss: 0.000138521631\n",
      "Epoch: 6706 | training loss: 0.000138459000\n",
      "Epoch: 6707 | training loss: 0.000138396470\n",
      "Epoch: 6708 | training loss: 0.000138334013\n",
      "Epoch: 6709 | training loss: 0.000138271484\n",
      "Epoch: 6710 | training loss: 0.000138209129\n",
      "Epoch: 6711 | training loss: 0.000138146876\n",
      "Epoch: 6712 | training loss: 0.000138084390\n",
      "Epoch: 6713 | training loss: 0.000138022166\n",
      "Epoch: 6714 | training loss: 0.000137960029\n",
      "Epoch: 6715 | training loss: 0.000137897790\n",
      "Epoch: 6716 | training loss: 0.000137835537\n",
      "Epoch: 6717 | training loss: 0.000137773430\n",
      "Epoch: 6718 | training loss: 0.000137711366\n",
      "Epoch: 6719 | training loss: 0.000137649244\n",
      "Epoch: 6720 | training loss: 0.000137587165\n",
      "Epoch: 6721 | training loss: 0.000137525203\n",
      "Epoch: 6722 | training loss: 0.000137463154\n",
      "Epoch: 6723 | training loss: 0.000137401308\n",
      "Epoch: 6724 | training loss: 0.000137339390\n",
      "Epoch: 6725 | training loss: 0.000137277661\n",
      "Epoch: 6726 | training loss: 0.000137215786\n",
      "Epoch: 6727 | training loss: 0.000137153940\n",
      "Epoch: 6728 | training loss: 0.000137092284\n",
      "Epoch: 6729 | training loss: 0.000137030598\n",
      "Epoch: 6730 | training loss: 0.000136968738\n",
      "Epoch: 6731 | training loss: 0.000136907169\n",
      "Epoch: 6732 | training loss: 0.000136845687\n",
      "Epoch: 6733 | training loss: 0.000136784074\n",
      "Epoch: 6734 | training loss: 0.000136722665\n",
      "Epoch: 6735 | training loss: 0.000136661081\n",
      "Epoch: 6736 | training loss: 0.000136599585\n",
      "Epoch: 6737 | training loss: 0.000136538220\n",
      "Epoch: 6738 | training loss: 0.000136476810\n",
      "Epoch: 6739 | training loss: 0.000136415430\n",
      "Epoch: 6740 | training loss: 0.000136354225\n",
      "Epoch: 6741 | training loss: 0.000136292991\n",
      "Epoch: 6742 | training loss: 0.000136231683\n",
      "Epoch: 6743 | training loss: 0.000136170507\n",
      "Epoch: 6744 | training loss: 0.000136109273\n",
      "Epoch: 6745 | training loss: 0.000136048227\n",
      "Epoch: 6746 | training loss: 0.000135987095\n",
      "Epoch: 6747 | training loss: 0.000135926050\n",
      "Epoch: 6748 | training loss: 0.000135864975\n",
      "Epoch: 6749 | training loss: 0.000135804090\n",
      "Epoch: 6750 | training loss: 0.000135743045\n",
      "Epoch: 6751 | training loss: 0.000135682058\n",
      "Epoch: 6752 | training loss: 0.000135621260\n",
      "Epoch: 6753 | training loss: 0.000135560273\n",
      "Epoch: 6754 | training loss: 0.000135499751\n",
      "Epoch: 6755 | training loss: 0.000135438750\n",
      "Epoch: 6756 | training loss: 0.000135378039\n",
      "Epoch: 6757 | training loss: 0.000135317285\n",
      "Epoch: 6758 | training loss: 0.000135256589\n",
      "Epoch: 6759 | training loss: 0.000135195995\n",
      "Epoch: 6760 | training loss: 0.000135135226\n",
      "Epoch: 6761 | training loss: 0.000135074806\n",
      "Epoch: 6762 | training loss: 0.000135014358\n",
      "Epoch: 6763 | training loss: 0.000134953822\n",
      "Epoch: 6764 | training loss: 0.000134893417\n",
      "Epoch: 6765 | training loss: 0.000134832866\n",
      "Epoch: 6766 | training loss: 0.000134772461\n",
      "Epoch: 6767 | training loss: 0.000134712114\n",
      "Epoch: 6768 | training loss: 0.000134651724\n",
      "Epoch: 6769 | training loss: 0.000134591552\n",
      "Epoch: 6770 | training loss: 0.000134531190\n",
      "Epoch: 6771 | training loss: 0.000134470916\n",
      "Epoch: 6772 | training loss: 0.000134410686\n",
      "Epoch: 6773 | training loss: 0.000134350514\n",
      "Epoch: 6774 | training loss: 0.000134290385\n",
      "Epoch: 6775 | training loss: 0.000134230417\n",
      "Epoch: 6776 | training loss: 0.000134170230\n",
      "Epoch: 6777 | training loss: 0.000134110203\n",
      "Epoch: 6778 | training loss: 0.000134050148\n",
      "Epoch: 6779 | training loss: 0.000133990281\n",
      "Epoch: 6780 | training loss: 0.000133930240\n",
      "Epoch: 6781 | training loss: 0.000133870315\n",
      "Epoch: 6782 | training loss: 0.000133810347\n",
      "Epoch: 6783 | training loss: 0.000133750538\n",
      "Epoch: 6784 | training loss: 0.000133690861\n",
      "Epoch: 6785 | training loss: 0.000133631067\n",
      "Epoch: 6786 | training loss: 0.000133571215\n",
      "Epoch: 6787 | training loss: 0.000133511523\n",
      "Epoch: 6788 | training loss: 0.000133451860\n",
      "Epoch: 6789 | training loss: 0.000133392183\n",
      "Epoch: 6790 | training loss: 0.000133332593\n",
      "Epoch: 6791 | training loss: 0.000133272901\n",
      "Epoch: 6792 | training loss: 0.000133213471\n",
      "Epoch: 6793 | training loss: 0.000133153924\n",
      "Epoch: 6794 | training loss: 0.000133094392\n",
      "Epoch: 6795 | training loss: 0.000133034904\n",
      "Epoch: 6796 | training loss: 0.000132975634\n",
      "Epoch: 6797 | training loss: 0.000132916190\n",
      "Epoch: 6798 | training loss: 0.000132856949\n",
      "Epoch: 6799 | training loss: 0.000132797548\n",
      "Epoch: 6800 | training loss: 0.000132738322\n",
      "Epoch: 6801 | training loss: 0.000132678892\n",
      "Epoch: 6802 | training loss: 0.000132619636\n",
      "Epoch: 6803 | training loss: 0.000132560483\n",
      "Epoch: 6804 | training loss: 0.000132501329\n",
      "Epoch: 6805 | training loss: 0.000132442277\n",
      "Epoch: 6806 | training loss: 0.000132383138\n",
      "Epoch: 6807 | training loss: 0.000132324014\n",
      "Epoch: 6808 | training loss: 0.000132265035\n",
      "Epoch: 6809 | training loss: 0.000132205882\n",
      "Epoch: 6810 | training loss: 0.000132147048\n",
      "Epoch: 6811 | training loss: 0.000132087895\n",
      "Epoch: 6812 | training loss: 0.000132029265\n",
      "Epoch: 6813 | training loss: 0.000131970417\n",
      "Epoch: 6814 | training loss: 0.000131911627\n",
      "Epoch: 6815 | training loss: 0.000131852663\n",
      "Epoch: 6816 | training loss: 0.000131793931\n",
      "Epoch: 6817 | training loss: 0.000131735302\n",
      "Epoch: 6818 | training loss: 0.000131676381\n",
      "Epoch: 6819 | training loss: 0.000131617795\n",
      "Epoch: 6820 | training loss: 0.000131559093\n",
      "Epoch: 6821 | training loss: 0.000131500565\n",
      "Epoch: 6822 | training loss: 0.000131441921\n",
      "Epoch: 6823 | training loss: 0.000131383335\n",
      "Epoch: 6824 | training loss: 0.000131324719\n",
      "Epoch: 6825 | training loss: 0.000131266337\n",
      "Epoch: 6826 | training loss: 0.000131207897\n",
      "Epoch: 6827 | training loss: 0.000131149471\n",
      "Epoch: 6828 | training loss: 0.000131091074\n",
      "Epoch: 6829 | training loss: 0.000131032721\n",
      "Epoch: 6830 | training loss: 0.000130974397\n",
      "Epoch: 6831 | training loss: 0.000130916014\n",
      "Epoch: 6832 | training loss: 0.000130857719\n",
      "Epoch: 6833 | training loss: 0.000130799512\n",
      "Epoch: 6834 | training loss: 0.000130741188\n",
      "Epoch: 6835 | training loss: 0.000130683155\n",
      "Epoch: 6836 | training loss: 0.000130624991\n",
      "Epoch: 6837 | training loss: 0.000130566841\n",
      "Epoch: 6838 | training loss: 0.000130508648\n",
      "Epoch: 6839 | training loss: 0.000130450673\n",
      "Epoch: 6840 | training loss: 0.000130392567\n",
      "Epoch: 6841 | training loss: 0.000130334534\n",
      "Epoch: 6842 | training loss: 0.000130276632\n",
      "Epoch: 6843 | training loss: 0.000130218628\n",
      "Epoch: 6844 | training loss: 0.000130160770\n",
      "Epoch: 6845 | training loss: 0.000130102810\n",
      "Epoch: 6846 | training loss: 0.000130044995\n",
      "Epoch: 6847 | training loss: 0.000129987122\n",
      "Epoch: 6848 | training loss: 0.000129929249\n",
      "Epoch: 6849 | training loss: 0.000129871507\n",
      "Epoch: 6850 | training loss: 0.000129813925\n",
      "Epoch: 6851 | training loss: 0.000129756227\n",
      "Epoch: 6852 | training loss: 0.000129698456\n",
      "Epoch: 6853 | training loss: 0.000129640976\n",
      "Epoch: 6854 | training loss: 0.000129583263\n",
      "Epoch: 6855 | training loss: 0.000129525666\n",
      "Epoch: 6856 | training loss: 0.000129467939\n",
      "Epoch: 6857 | training loss: 0.000129410590\n",
      "Epoch: 6858 | training loss: 0.000129353037\n",
      "Epoch: 6859 | training loss: 0.000129295571\n",
      "Epoch: 6860 | training loss: 0.000129238237\n",
      "Epoch: 6861 | training loss: 0.000129180946\n",
      "Epoch: 6862 | training loss: 0.000129123568\n",
      "Epoch: 6863 | training loss: 0.000129066204\n",
      "Epoch: 6864 | training loss: 0.000129008928\n",
      "Epoch: 6865 | training loss: 0.000128951680\n",
      "Epoch: 6866 | training loss: 0.000128894491\n",
      "Epoch: 6867 | training loss: 0.000128837186\n",
      "Epoch: 6868 | training loss: 0.000128780157\n",
      "Epoch: 6869 | training loss: 0.000128722866\n",
      "Epoch: 6870 | training loss: 0.000128665823\n",
      "Epoch: 6871 | training loss: 0.000128608735\n",
      "Epoch: 6872 | training loss: 0.000128551619\n",
      "Epoch: 6873 | training loss: 0.000128494721\n",
      "Epoch: 6874 | training loss: 0.000128437692\n",
      "Epoch: 6875 | training loss: 0.000128380751\n",
      "Epoch: 6876 | training loss: 0.000128323853\n",
      "Epoch: 6877 | training loss: 0.000128267013\n",
      "Epoch: 6878 | training loss: 0.000128210188\n",
      "Epoch: 6879 | training loss: 0.000128153333\n",
      "Epoch: 6880 | training loss: 0.000128096435\n",
      "Epoch: 6881 | training loss: 0.000128039566\n",
      "Epoch: 6882 | training loss: 0.000127982799\n",
      "Epoch: 6883 | training loss: 0.000127926192\n",
      "Epoch: 6884 | training loss: 0.000127869629\n",
      "Epoch: 6885 | training loss: 0.000127812935\n",
      "Epoch: 6886 | training loss: 0.000127756328\n",
      "Epoch: 6887 | training loss: 0.000127699765\n",
      "Epoch: 6888 | training loss: 0.000127643070\n",
      "Epoch: 6889 | training loss: 0.000127586565\n",
      "Epoch: 6890 | training loss: 0.000127529958\n",
      "Epoch: 6891 | training loss: 0.000127473424\n",
      "Epoch: 6892 | training loss: 0.000127417035\n",
      "Epoch: 6893 | training loss: 0.000127360545\n",
      "Epoch: 6894 | training loss: 0.000127304171\n",
      "Epoch: 6895 | training loss: 0.000127247709\n",
      "Epoch: 6896 | training loss: 0.000127191495\n",
      "Epoch: 6897 | training loss: 0.000127135048\n",
      "Epoch: 6898 | training loss: 0.000127078820\n",
      "Epoch: 6899 | training loss: 0.000127022562\n",
      "Epoch: 6900 | training loss: 0.000126966261\n",
      "Epoch: 6901 | training loss: 0.000126910047\n",
      "Epoch: 6902 | training loss: 0.000126853905\n",
      "Epoch: 6903 | training loss: 0.000126797735\n",
      "Epoch: 6904 | training loss: 0.000126741434\n",
      "Epoch: 6905 | training loss: 0.000126685482\n",
      "Epoch: 6906 | training loss: 0.000126629398\n",
      "Epoch: 6907 | training loss: 0.000126573257\n",
      "Epoch: 6908 | training loss: 0.000126517320\n",
      "Epoch: 6909 | training loss: 0.000126461280\n",
      "Epoch: 6910 | training loss: 0.000126405401\n",
      "Epoch: 6911 | training loss: 0.000126349551\n",
      "Epoch: 6912 | training loss: 0.000126293511\n",
      "Epoch: 6913 | training loss: 0.000126237748\n",
      "Epoch: 6914 | training loss: 0.000126181840\n",
      "Epoch: 6915 | training loss: 0.000126126106\n",
      "Epoch: 6916 | training loss: 0.000126070430\n",
      "Epoch: 6917 | training loss: 0.000126014726\n",
      "Epoch: 6918 | training loss: 0.000125959021\n",
      "Epoch: 6919 | training loss: 0.000125903272\n",
      "Epoch: 6920 | training loss: 0.000125847509\n",
      "Epoch: 6921 | training loss: 0.000125791936\n",
      "Epoch: 6922 | training loss: 0.000125736275\n",
      "Epoch: 6923 | training loss: 0.000125680803\n",
      "Epoch: 6924 | training loss: 0.000125625171\n",
      "Epoch: 6925 | training loss: 0.000125569626\n",
      "Epoch: 6926 | training loss: 0.000125514256\n",
      "Epoch: 6927 | training loss: 0.000125458697\n",
      "Epoch: 6928 | training loss: 0.000125403254\n",
      "Epoch: 6929 | training loss: 0.000125347811\n",
      "Epoch: 6930 | training loss: 0.000125292368\n",
      "Epoch: 6931 | training loss: 0.000125237042\n",
      "Epoch: 6932 | training loss: 0.000125181788\n",
      "Epoch: 6933 | training loss: 0.000125126506\n",
      "Epoch: 6934 | training loss: 0.000125071136\n",
      "Epoch: 6935 | training loss: 0.000125015853\n",
      "Epoch: 6936 | training loss: 0.000124960832\n",
      "Epoch: 6937 | training loss: 0.000124905418\n",
      "Epoch: 6938 | training loss: 0.000124850238\n",
      "Epoch: 6939 | training loss: 0.000124795202\n",
      "Epoch: 6940 | training loss: 0.000124740094\n",
      "Epoch: 6941 | training loss: 0.000124684972\n",
      "Epoch: 6942 | training loss: 0.000124629863\n",
      "Epoch: 6943 | training loss: 0.000124574814\n",
      "Epoch: 6944 | training loss: 0.000124519836\n",
      "Epoch: 6945 | training loss: 0.000124464888\n",
      "Epoch: 6946 | training loss: 0.000124409926\n",
      "Epoch: 6947 | training loss: 0.000124354934\n",
      "Epoch: 6948 | training loss: 0.000124300088\n",
      "Epoch: 6949 | training loss: 0.000124245184\n",
      "Epoch: 6950 | training loss: 0.000124190294\n",
      "Epoch: 6951 | training loss: 0.000124135375\n",
      "Epoch: 6952 | training loss: 0.000124080601\n",
      "Epoch: 6953 | training loss: 0.000124025770\n",
      "Epoch: 6954 | training loss: 0.000123971084\n",
      "Epoch: 6955 | training loss: 0.000123916412\n",
      "Epoch: 6956 | training loss: 0.000123861624\n",
      "Epoch: 6957 | training loss: 0.000123806996\n",
      "Epoch: 6958 | training loss: 0.000123752339\n",
      "Epoch: 6959 | training loss: 0.000123697697\n",
      "Epoch: 6960 | training loss: 0.000123643214\n",
      "Epoch: 6961 | training loss: 0.000123588572\n",
      "Epoch: 6962 | training loss: 0.000123534061\n",
      "Epoch: 6963 | training loss: 0.000123479505\n",
      "Epoch: 6964 | training loss: 0.000123425125\n",
      "Epoch: 6965 | training loss: 0.000123370628\n",
      "Epoch: 6966 | training loss: 0.000123316277\n",
      "Epoch: 6967 | training loss: 0.000123261940\n",
      "Epoch: 6968 | training loss: 0.000123207530\n",
      "Epoch: 6969 | training loss: 0.000123153237\n",
      "Epoch: 6970 | training loss: 0.000123098769\n",
      "Epoch: 6971 | training loss: 0.000123044563\n",
      "Epoch: 6972 | training loss: 0.000122990110\n",
      "Epoch: 6973 | training loss: 0.000122935977\n",
      "Epoch: 6974 | training loss: 0.000122881873\n",
      "Epoch: 6975 | training loss: 0.000122827681\n",
      "Epoch: 6976 | training loss: 0.000122773345\n",
      "Epoch: 6977 | training loss: 0.000122719386\n",
      "Epoch: 6978 | training loss: 0.000122665107\n",
      "Epoch: 6979 | training loss: 0.000122611018\n",
      "Epoch: 6980 | training loss: 0.000122557060\n",
      "Epoch: 6981 | training loss: 0.000122503086\n",
      "Epoch: 6982 | training loss: 0.000122448924\n",
      "Epoch: 6983 | training loss: 0.000122395082\n",
      "Epoch: 6984 | training loss: 0.000122341080\n",
      "Epoch: 6985 | training loss: 0.000122287107\n",
      "Epoch: 6986 | training loss: 0.000122233178\n",
      "Epoch: 6987 | training loss: 0.000122179277\n",
      "Epoch: 6988 | training loss: 0.000122125508\n",
      "Epoch: 6989 | training loss: 0.000122071709\n",
      "Epoch: 6990 | training loss: 0.000122017918\n",
      "Epoch: 6991 | training loss: 0.000121964054\n",
      "Epoch: 6992 | training loss: 0.000121910402\n",
      "Epoch: 6993 | training loss: 0.000121856647\n",
      "Epoch: 6994 | training loss: 0.000121802928\n",
      "Epoch: 6995 | training loss: 0.000121749123\n",
      "Epoch: 6996 | training loss: 0.000121695703\n",
      "Epoch: 6997 | training loss: 0.000121641977\n",
      "Epoch: 6998 | training loss: 0.000121588222\n",
      "Epoch: 6999 | training loss: 0.000121534773\n",
      "Epoch: 7000 | training loss: 0.000121481222\n",
      "Epoch: 7001 | training loss: 0.000121427714\n",
      "Epoch: 7002 | training loss: 0.000121374280\n",
      "Epoch: 7003 | training loss: 0.000121320663\n",
      "Epoch: 7004 | training loss: 0.000121267381\n",
      "Epoch: 7005 | training loss: 0.000121213830\n",
      "Epoch: 7006 | training loss: 0.000121160381\n",
      "Epoch: 7007 | training loss: 0.000121106947\n",
      "Epoch: 7008 | training loss: 0.000121053679\n",
      "Epoch: 7009 | training loss: 0.000121000354\n",
      "Epoch: 7010 | training loss: 0.000120947152\n",
      "Epoch: 7011 | training loss: 0.000120893717\n",
      "Epoch: 7012 | training loss: 0.000120840574\n",
      "Epoch: 7013 | training loss: 0.000120787445\n",
      "Epoch: 7014 | training loss: 0.000120734185\n",
      "Epoch: 7015 | training loss: 0.000120681027\n",
      "Epoch: 7016 | training loss: 0.000120627912\n",
      "Epoch: 7017 | training loss: 0.000120574798\n",
      "Epoch: 7018 | training loss: 0.000120521676\n",
      "Epoch: 7019 | training loss: 0.000120468692\n",
      "Epoch: 7020 | training loss: 0.000120415571\n",
      "Epoch: 7021 | training loss: 0.000120362514\n",
      "Epoch: 7022 | training loss: 0.000120309604\n",
      "Epoch: 7023 | training loss: 0.000120256664\n",
      "Epoch: 7024 | training loss: 0.000120203607\n",
      "Epoch: 7025 | training loss: 0.000120150726\n",
      "Epoch: 7026 | training loss: 0.000120097939\n",
      "Epoch: 7027 | training loss: 0.000120045064\n",
      "Epoch: 7028 | training loss: 0.000119992110\n",
      "Epoch: 7029 | training loss: 0.000119939330\n",
      "Epoch: 7030 | training loss: 0.000119886579\n",
      "Epoch: 7031 | training loss: 0.000119833814\n",
      "Epoch: 7032 | training loss: 0.000119781194\n",
      "Epoch: 7033 | training loss: 0.000119728473\n",
      "Epoch: 7034 | training loss: 0.000119675569\n",
      "Epoch: 7035 | training loss: 0.000119622913\n",
      "Epoch: 7036 | training loss: 0.000119570483\n",
      "Epoch: 7037 | training loss: 0.000119517696\n",
      "Epoch: 7038 | training loss: 0.000119465163\n",
      "Epoch: 7039 | training loss: 0.000119412609\n",
      "Epoch: 7040 | training loss: 0.000119359960\n",
      "Epoch: 7041 | training loss: 0.000119307420\n",
      "Epoch: 7042 | training loss: 0.000119255019\n",
      "Epoch: 7043 | training loss: 0.000119202465\n",
      "Epoch: 7044 | training loss: 0.000119149983\n",
      "Epoch: 7045 | training loss: 0.000119097655\n",
      "Epoch: 7046 | training loss: 0.000119045202\n",
      "Epoch: 7047 | training loss: 0.000118992859\n",
      "Epoch: 7048 | training loss: 0.000118940457\n",
      "Epoch: 7049 | training loss: 0.000118888114\n",
      "Epoch: 7050 | training loss: 0.000118835742\n",
      "Epoch: 7051 | training loss: 0.000118783551\n",
      "Epoch: 7052 | training loss: 0.000118731361\n",
      "Epoch: 7053 | training loss: 0.000118679040\n",
      "Epoch: 7054 | training loss: 0.000118626762\n",
      "Epoch: 7055 | training loss: 0.000118574615\n",
      "Epoch: 7056 | training loss: 0.000118522410\n",
      "Epoch: 7057 | training loss: 0.000118470321\n",
      "Epoch: 7058 | training loss: 0.000118418116\n",
      "Epoch: 7059 | training loss: 0.000118366166\n",
      "Epoch: 7060 | training loss: 0.000118314114\n",
      "Epoch: 7061 | training loss: 0.000118261989\n",
      "Epoch: 7062 | training loss: 0.000118209791\n",
      "Epoch: 7063 | training loss: 0.000118157797\n",
      "Epoch: 7064 | training loss: 0.000118105949\n",
      "Epoch: 7065 | training loss: 0.000118053838\n",
      "Epoch: 7066 | training loss: 0.000118001903\n",
      "Epoch: 7067 | training loss: 0.000117949930\n",
      "Epoch: 7068 | training loss: 0.000117898133\n",
      "Epoch: 7069 | training loss: 0.000117846423\n",
      "Epoch: 7070 | training loss: 0.000117794421\n",
      "Epoch: 7071 | training loss: 0.000117742631\n",
      "Epoch: 7072 | training loss: 0.000117690826\n",
      "Epoch: 7073 | training loss: 0.000117639080\n",
      "Epoch: 7074 | training loss: 0.000117587202\n",
      "Epoch: 7075 | training loss: 0.000117535492\n",
      "Epoch: 7076 | training loss: 0.000117483854\n",
      "Epoch: 7077 | training loss: 0.000117432166\n",
      "Epoch: 7078 | training loss: 0.000117380390\n",
      "Epoch: 7079 | training loss: 0.000117328847\n",
      "Epoch: 7080 | training loss: 0.000117277246\n",
      "Epoch: 7081 | training loss: 0.000117225543\n",
      "Epoch: 7082 | training loss: 0.000117174044\n",
      "Epoch: 7083 | training loss: 0.000117122472\n",
      "Epoch: 7084 | training loss: 0.000117070944\n",
      "Epoch: 7085 | training loss: 0.000117019445\n",
      "Epoch: 7086 | training loss: 0.000116967858\n",
      "Epoch: 7087 | training loss: 0.000116916461\n",
      "Epoch: 7088 | training loss: 0.000116865034\n",
      "Epoch: 7089 | training loss: 0.000116813695\n",
      "Epoch: 7090 | training loss: 0.000116762181\n",
      "Epoch: 7091 | training loss: 0.000116710871\n",
      "Epoch: 7092 | training loss: 0.000116659518\n",
      "Epoch: 7093 | training loss: 0.000116608229\n",
      "Epoch: 7094 | training loss: 0.000116556919\n",
      "Epoch: 7095 | training loss: 0.000116505653\n",
      "Epoch: 7096 | training loss: 0.000116454292\n",
      "Epoch: 7097 | training loss: 0.000116402996\n",
      "Epoch: 7098 | training loss: 0.000116351817\n",
      "Epoch: 7099 | training loss: 0.000116300638\n",
      "Epoch: 7100 | training loss: 0.000116249576\n",
      "Epoch: 7101 | training loss: 0.000116198418\n",
      "Epoch: 7102 | training loss: 0.000116147232\n",
      "Epoch: 7103 | training loss: 0.000116096235\n",
      "Epoch: 7104 | training loss: 0.000116045092\n",
      "Epoch: 7105 | training loss: 0.000115993971\n",
      "Epoch: 7106 | training loss: 0.000115943003\n",
      "Epoch: 7107 | training loss: 0.000115892006\n",
      "Epoch: 7108 | training loss: 0.000115840950\n",
      "Epoch: 7109 | training loss: 0.000115790070\n",
      "Epoch: 7110 | training loss: 0.000115739007\n",
      "Epoch: 7111 | training loss: 0.000115688032\n",
      "Epoch: 7112 | training loss: 0.000115637347\n",
      "Epoch: 7113 | training loss: 0.000115586423\n",
      "Epoch: 7114 | training loss: 0.000115535513\n",
      "Epoch: 7115 | training loss: 0.000115484698\n",
      "Epoch: 7116 | training loss: 0.000115433933\n",
      "Epoch: 7117 | training loss: 0.000115383169\n",
      "Epoch: 7118 | training loss: 0.000115332354\n",
      "Epoch: 7119 | training loss: 0.000115281597\n",
      "Epoch: 7120 | training loss: 0.000115230810\n",
      "Epoch: 7121 | training loss: 0.000115180141\n",
      "Epoch: 7122 | training loss: 0.000115129500\n",
      "Epoch: 7123 | training loss: 0.000115078699\n",
      "Epoch: 7124 | training loss: 0.000115028117\n",
      "Epoch: 7125 | training loss: 0.000114977462\n",
      "Epoch: 7126 | training loss: 0.000114927054\n",
      "Epoch: 7127 | training loss: 0.000114876435\n",
      "Epoch: 7128 | training loss: 0.000114825787\n",
      "Epoch: 7129 | training loss: 0.000114775066\n",
      "Epoch: 7130 | training loss: 0.000114724746\n",
      "Epoch: 7131 | training loss: 0.000114674229\n",
      "Epoch: 7132 | training loss: 0.000114623792\n",
      "Epoch: 7133 | training loss: 0.000114573333\n",
      "Epoch: 7134 | training loss: 0.000114522838\n",
      "Epoch: 7135 | training loss: 0.000114472408\n",
      "Epoch: 7136 | training loss: 0.000114422030\n",
      "Epoch: 7137 | training loss: 0.000114371724\n",
      "Epoch: 7138 | training loss: 0.000114321389\n",
      "Epoch: 7139 | training loss: 0.000114270973\n",
      "Epoch: 7140 | training loss: 0.000114220784\n",
      "Epoch: 7141 | training loss: 0.000114170449\n",
      "Epoch: 7142 | training loss: 0.000114120150\n",
      "Epoch: 7143 | training loss: 0.000114069917\n",
      "Epoch: 7144 | training loss: 0.000114019655\n",
      "Epoch: 7145 | training loss: 0.000113969494\n",
      "Epoch: 7146 | training loss: 0.000113919334\n",
      "Epoch: 7147 | training loss: 0.000113869144\n",
      "Epoch: 7148 | training loss: 0.000113819107\n",
      "Epoch: 7149 | training loss: 0.000113768925\n",
      "Epoch: 7150 | training loss: 0.000113718910\n",
      "Epoch: 7151 | training loss: 0.000113668750\n",
      "Epoch: 7152 | training loss: 0.000113618749\n",
      "Epoch: 7153 | training loss: 0.000113568763\n",
      "Epoch: 7154 | training loss: 0.000113518610\n",
      "Epoch: 7155 | training loss: 0.000113468748\n",
      "Epoch: 7156 | training loss: 0.000113418791\n",
      "Epoch: 7157 | training loss: 0.000113368900\n",
      "Epoch: 7158 | training loss: 0.000113318893\n",
      "Epoch: 7159 | training loss: 0.000113268965\n",
      "Epoch: 7160 | training loss: 0.000113219183\n",
      "Epoch: 7161 | training loss: 0.000113169168\n",
      "Epoch: 7162 | training loss: 0.000113119386\n",
      "Epoch: 7163 | training loss: 0.000113069618\n",
      "Epoch: 7164 | training loss: 0.000113019792\n",
      "Epoch: 7165 | training loss: 0.000112970156\n",
      "Epoch: 7166 | training loss: 0.000112920403\n",
      "Epoch: 7167 | training loss: 0.000112870592\n",
      "Epoch: 7168 | training loss: 0.000112820919\n",
      "Epoch: 7169 | training loss: 0.000112771348\n",
      "Epoch: 7170 | training loss: 0.000112721595\n",
      "Epoch: 7171 | training loss: 0.000112671943\n",
      "Epoch: 7172 | training loss: 0.000112622416\n",
      "Epoch: 7173 | training loss: 0.000112572932\n",
      "Epoch: 7174 | training loss: 0.000112523223\n",
      "Epoch: 7175 | training loss: 0.000112473739\n",
      "Epoch: 7176 | training loss: 0.000112424212\n",
      "Epoch: 7177 | training loss: 0.000112374611\n",
      "Epoch: 7178 | training loss: 0.000112325215\n",
      "Epoch: 7179 | training loss: 0.000112275731\n",
      "Epoch: 7180 | training loss: 0.000112226306\n",
      "Epoch: 7181 | training loss: 0.000112176924\n",
      "Epoch: 7182 | training loss: 0.000112127367\n",
      "Epoch: 7183 | training loss: 0.000112078080\n",
      "Epoch: 7184 | training loss: 0.000112028647\n",
      "Epoch: 7185 | training loss: 0.000111979251\n",
      "Epoch: 7186 | training loss: 0.000111929963\n",
      "Epoch: 7187 | training loss: 0.000111880719\n",
      "Epoch: 7188 | training loss: 0.000111831483\n",
      "Epoch: 7189 | training loss: 0.000111782181\n",
      "Epoch: 7190 | training loss: 0.000111732894\n",
      "Epoch: 7191 | training loss: 0.000111683854\n",
      "Epoch: 7192 | training loss: 0.000111634501\n",
      "Epoch: 7193 | training loss: 0.000111585352\n",
      "Epoch: 7194 | training loss: 0.000111536181\n",
      "Epoch: 7195 | training loss: 0.000111487068\n",
      "Epoch: 7196 | training loss: 0.000111437956\n",
      "Epoch: 7197 | training loss: 0.000111388799\n",
      "Epoch: 7198 | training loss: 0.000111339643\n",
      "Epoch: 7199 | training loss: 0.000111290676\n",
      "Epoch: 7200 | training loss: 0.000111241709\n",
      "Epoch: 7201 | training loss: 0.000111192698\n",
      "Epoch: 7202 | training loss: 0.000111143687\n",
      "Epoch: 7203 | training loss: 0.000111094647\n",
      "Epoch: 7204 | training loss: 0.000111045709\n",
      "Epoch: 7205 | training loss: 0.000110996858\n",
      "Epoch: 7206 | training loss: 0.000110947796\n",
      "Epoch: 7207 | training loss: 0.000110899127\n",
      "Epoch: 7208 | training loss: 0.000110850146\n",
      "Epoch: 7209 | training loss: 0.000110801324\n",
      "Epoch: 7210 | training loss: 0.000110752400\n",
      "Epoch: 7211 | training loss: 0.000110703666\n",
      "Epoch: 7212 | training loss: 0.000110654859\n",
      "Epoch: 7213 | training loss: 0.000110606008\n",
      "Epoch: 7214 | training loss: 0.000110557419\n",
      "Epoch: 7215 | training loss: 0.000110508576\n",
      "Epoch: 7216 | training loss: 0.000110459834\n",
      "Epoch: 7217 | training loss: 0.000110411231\n",
      "Epoch: 7218 | training loss: 0.000110362627\n",
      "Epoch: 7219 | training loss: 0.000110313871\n",
      "Epoch: 7220 | training loss: 0.000110265159\n",
      "Epoch: 7221 | training loss: 0.000110216606\n",
      "Epoch: 7222 | training loss: 0.000110167995\n",
      "Epoch: 7223 | training loss: 0.000110119494\n",
      "Epoch: 7224 | training loss: 0.000110070789\n",
      "Epoch: 7225 | training loss: 0.000110022447\n",
      "Epoch: 7226 | training loss: 0.000109973902\n",
      "Epoch: 7227 | training loss: 0.000109925197\n",
      "Epoch: 7228 | training loss: 0.000109876797\n",
      "Epoch: 7229 | training loss: 0.000109828383\n",
      "Epoch: 7230 | training loss: 0.000109779910\n",
      "Epoch: 7231 | training loss: 0.000109731496\n",
      "Epoch: 7232 | training loss: 0.000109683097\n",
      "Epoch: 7233 | training loss: 0.000109634799\n",
      "Epoch: 7234 | training loss: 0.000109586326\n",
      "Epoch: 7235 | training loss: 0.000109537999\n",
      "Epoch: 7236 | training loss: 0.000109489578\n",
      "Epoch: 7237 | training loss: 0.000109441433\n",
      "Epoch: 7238 | training loss: 0.000109392990\n",
      "Epoch: 7239 | training loss: 0.000109344764\n",
      "Epoch: 7240 | training loss: 0.000109296467\n",
      "Epoch: 7241 | training loss: 0.000109248242\n",
      "Epoch: 7242 | training loss: 0.000109200017\n",
      "Epoch: 7243 | training loss: 0.000109151879\n",
      "Epoch: 7244 | training loss: 0.000109103596\n",
      "Epoch: 7245 | training loss: 0.000109055502\n",
      "Epoch: 7246 | training loss: 0.000109007284\n",
      "Epoch: 7247 | training loss: 0.000108959212\n",
      "Epoch: 7248 | training loss: 0.000108911103\n",
      "Epoch: 7249 | training loss: 0.000108863038\n",
      "Epoch: 7250 | training loss: 0.000108815046\n",
      "Epoch: 7251 | training loss: 0.000108766981\n",
      "Epoch: 7252 | training loss: 0.000108718901\n",
      "Epoch: 7253 | training loss: 0.000108670851\n",
      "Epoch: 7254 | training loss: 0.000108622946\n",
      "Epoch: 7255 | training loss: 0.000108574997\n",
      "Epoch: 7256 | training loss: 0.000108527049\n",
      "Epoch: 7257 | training loss: 0.000108479057\n",
      "Epoch: 7258 | training loss: 0.000108431093\n",
      "Epoch: 7259 | training loss: 0.000108383298\n",
      "Epoch: 7260 | training loss: 0.000108335371\n",
      "Epoch: 7261 | training loss: 0.000108287626\n",
      "Epoch: 7262 | training loss: 0.000108239736\n",
      "Epoch: 7263 | training loss: 0.000108191918\n",
      "Epoch: 7264 | training loss: 0.000108144173\n",
      "Epoch: 7265 | training loss: 0.000108096428\n",
      "Epoch: 7266 | training loss: 0.000108048611\n",
      "Epoch: 7267 | training loss: 0.000108000997\n",
      "Epoch: 7268 | training loss: 0.000107953201\n",
      "Epoch: 7269 | training loss: 0.000107905551\n",
      "Epoch: 7270 | training loss: 0.000107857872\n",
      "Epoch: 7271 | training loss: 0.000107810185\n",
      "Epoch: 7272 | training loss: 0.000107762549\n",
      "Epoch: 7273 | training loss: 0.000107714921\n",
      "Epoch: 7274 | training loss: 0.000107667278\n",
      "Epoch: 7275 | training loss: 0.000107619766\n",
      "Epoch: 7276 | training loss: 0.000107572268\n",
      "Epoch: 7277 | training loss: 0.000107524611\n",
      "Epoch: 7278 | training loss: 0.000107477172\n",
      "Epoch: 7279 | training loss: 0.000107429762\n",
      "Epoch: 7280 | training loss: 0.000107382148\n",
      "Epoch: 7281 | training loss: 0.000107334672\n",
      "Epoch: 7282 | training loss: 0.000107287211\n",
      "Epoch: 7283 | training loss: 0.000107239801\n",
      "Epoch: 7284 | training loss: 0.000107192391\n",
      "Epoch: 7285 | training loss: 0.000107144981\n",
      "Epoch: 7286 | training loss: 0.000107097701\n",
      "Epoch: 7287 | training loss: 0.000107050233\n",
      "Epoch: 7288 | training loss: 0.000107002765\n",
      "Epoch: 7289 | training loss: 0.000106955544\n",
      "Epoch: 7290 | training loss: 0.000106908265\n",
      "Epoch: 7291 | training loss: 0.000106860934\n",
      "Epoch: 7292 | training loss: 0.000106813633\n",
      "Epoch: 7293 | training loss: 0.000106766427\n",
      "Epoch: 7294 | training loss: 0.000106719090\n",
      "Epoch: 7295 | training loss: 0.000106671985\n",
      "Epoch: 7296 | training loss: 0.000106624728\n",
      "Epoch: 7297 | training loss: 0.000106577456\n",
      "Epoch: 7298 | training loss: 0.000106530439\n",
      "Epoch: 7299 | training loss: 0.000106483218\n",
      "Epoch: 7300 | training loss: 0.000106436084\n",
      "Epoch: 7301 | training loss: 0.000106389038\n",
      "Epoch: 7302 | training loss: 0.000106341860\n",
      "Epoch: 7303 | training loss: 0.000106294799\n",
      "Epoch: 7304 | training loss: 0.000106247651\n",
      "Epoch: 7305 | training loss: 0.000106200663\n",
      "Epoch: 7306 | training loss: 0.000106153559\n",
      "Epoch: 7307 | training loss: 0.000106106672\n",
      "Epoch: 7308 | training loss: 0.000106059641\n",
      "Epoch: 7309 | training loss: 0.000106012623\n",
      "Epoch: 7310 | training loss: 0.000105965613\n",
      "Epoch: 7311 | training loss: 0.000105918778\n",
      "Epoch: 7312 | training loss: 0.000105871819\n",
      "Epoch: 7313 | training loss: 0.000105824976\n",
      "Epoch: 7314 | training loss: 0.000105778032\n",
      "Epoch: 7315 | training loss: 0.000105731175\n",
      "Epoch: 7316 | training loss: 0.000105684347\n",
      "Epoch: 7317 | training loss: 0.000105637562\n",
      "Epoch: 7318 | training loss: 0.000105590705\n",
      "Epoch: 7319 | training loss: 0.000105543935\n",
      "Epoch: 7320 | training loss: 0.000105497078\n",
      "Epoch: 7321 | training loss: 0.000105450352\n",
      "Epoch: 7322 | training loss: 0.000105403633\n",
      "Epoch: 7323 | training loss: 0.000105356870\n",
      "Epoch: 7324 | training loss: 0.000105310181\n",
      "Epoch: 7325 | training loss: 0.000105263607\n",
      "Epoch: 7326 | training loss: 0.000105216859\n",
      "Epoch: 7327 | training loss: 0.000105170271\n",
      "Epoch: 7328 | training loss: 0.000105123632\n",
      "Epoch: 7329 | training loss: 0.000105077008\n",
      "Epoch: 7330 | training loss: 0.000105030529\n",
      "Epoch: 7331 | training loss: 0.000104983919\n",
      "Epoch: 7332 | training loss: 0.000104937397\n",
      "Epoch: 7333 | training loss: 0.000104890816\n",
      "Epoch: 7334 | training loss: 0.000104844396\n",
      "Epoch: 7335 | training loss: 0.000104797924\n",
      "Epoch: 7336 | training loss: 0.000104751336\n",
      "Epoch: 7337 | training loss: 0.000104704894\n",
      "Epoch: 7338 | training loss: 0.000104658538\n",
      "Epoch: 7339 | training loss: 0.000104612162\n",
      "Epoch: 7340 | training loss: 0.000104565741\n",
      "Epoch: 7341 | training loss: 0.000104519277\n",
      "Epoch: 7342 | training loss: 0.000104472987\n",
      "Epoch: 7343 | training loss: 0.000104426646\n",
      "Epoch: 7344 | training loss: 0.000104380350\n",
      "Epoch: 7345 | training loss: 0.000104333914\n",
      "Epoch: 7346 | training loss: 0.000104287763\n",
      "Epoch: 7347 | training loss: 0.000104241444\n",
      "Epoch: 7348 | training loss: 0.000104195176\n",
      "Epoch: 7349 | training loss: 0.000104148829\n",
      "Epoch: 7350 | training loss: 0.000104102699\n",
      "Epoch: 7351 | training loss: 0.000104056584\n",
      "Epoch: 7352 | training loss: 0.000104010229\n",
      "Epoch: 7353 | training loss: 0.000103964237\n",
      "Epoch: 7354 | training loss: 0.000103918042\n",
      "Epoch: 7355 | training loss: 0.000103872051\n",
      "Epoch: 7356 | training loss: 0.000103825776\n",
      "Epoch: 7357 | training loss: 0.000103779697\n",
      "Epoch: 7358 | training loss: 0.000103733633\n",
      "Epoch: 7359 | training loss: 0.000103687678\n",
      "Epoch: 7360 | training loss: 0.000103641549\n",
      "Epoch: 7361 | training loss: 0.000103595507\n",
      "Epoch: 7362 | training loss: 0.000103549413\n",
      "Epoch: 7363 | training loss: 0.000103503473\n",
      "Epoch: 7364 | training loss: 0.000103457613\n",
      "Epoch: 7365 | training loss: 0.000103411548\n",
      "Epoch: 7366 | training loss: 0.000103365499\n",
      "Epoch: 7367 | training loss: 0.000103319639\n",
      "Epoch: 7368 | training loss: 0.000103273735\n",
      "Epoch: 7369 | training loss: 0.000103227736\n",
      "Epoch: 7370 | training loss: 0.000103181817\n",
      "Epoch: 7371 | training loss: 0.000103136153\n",
      "Epoch: 7372 | training loss: 0.000103090104\n",
      "Epoch: 7373 | training loss: 0.000103044353\n",
      "Epoch: 7374 | training loss: 0.000102998485\n",
      "Epoch: 7375 | training loss: 0.000102952763\n",
      "Epoch: 7376 | training loss: 0.000102907026\n",
      "Epoch: 7377 | training loss: 0.000102861173\n",
      "Epoch: 7378 | training loss: 0.000102815306\n",
      "Epoch: 7379 | training loss: 0.000102769642\n",
      "Epoch: 7380 | training loss: 0.000102724021\n",
      "Epoch: 7381 | training loss: 0.000102678277\n",
      "Epoch: 7382 | training loss: 0.000102632621\n",
      "Epoch: 7383 | training loss: 0.000102586950\n",
      "Epoch: 7384 | training loss: 0.000102541351\n",
      "Epoch: 7385 | training loss: 0.000102495695\n",
      "Epoch: 7386 | training loss: 0.000102450089\n",
      "Epoch: 7387 | training loss: 0.000102404520\n",
      "Epoch: 7388 | training loss: 0.000102358943\n",
      "Epoch: 7389 | training loss: 0.000102313323\n",
      "Epoch: 7390 | training loss: 0.000102267804\n",
      "Epoch: 7391 | training loss: 0.000102222199\n",
      "Epoch: 7392 | training loss: 0.000102176848\n",
      "Epoch: 7393 | training loss: 0.000102131278\n",
      "Epoch: 7394 | training loss: 0.000102086000\n",
      "Epoch: 7395 | training loss: 0.000102040343\n",
      "Epoch: 7396 | training loss: 0.000101995014\n",
      "Epoch: 7397 | training loss: 0.000101949670\n",
      "Epoch: 7398 | training loss: 0.000101904167\n",
      "Epoch: 7399 | training loss: 0.000101858983\n",
      "Epoch: 7400 | training loss: 0.000101813486\n",
      "Epoch: 7401 | training loss: 0.000101768106\n",
      "Epoch: 7402 | training loss: 0.000101722842\n",
      "Epoch: 7403 | training loss: 0.000101677564\n",
      "Epoch: 7404 | training loss: 0.000101632177\n",
      "Epoch: 7405 | training loss: 0.000101586898\n",
      "Epoch: 7406 | training loss: 0.000101541693\n",
      "Epoch: 7407 | training loss: 0.000101496407\n",
      "Epoch: 7408 | training loss: 0.000101451180\n",
      "Epoch: 7409 | training loss: 0.000101406069\n",
      "Epoch: 7410 | training loss: 0.000101360813\n",
      "Epoch: 7411 | training loss: 0.000101315658\n",
      "Epoch: 7412 | training loss: 0.000101270460\n",
      "Epoch: 7413 | training loss: 0.000101225349\n",
      "Epoch: 7414 | training loss: 0.000101180180\n",
      "Epoch: 7415 | training loss: 0.000101135040\n",
      "Epoch: 7416 | training loss: 0.000101090045\n",
      "Epoch: 7417 | training loss: 0.000101044869\n",
      "Epoch: 7418 | training loss: 0.000100999881\n",
      "Epoch: 7419 | training loss: 0.000100954872\n",
      "Epoch: 7420 | training loss: 0.000100909820\n",
      "Epoch: 7421 | training loss: 0.000100864658\n",
      "Epoch: 7422 | training loss: 0.000100819772\n",
      "Epoch: 7423 | training loss: 0.000100774880\n",
      "Epoch: 7424 | training loss: 0.000100729871\n",
      "Epoch: 7425 | training loss: 0.000100684949\n",
      "Epoch: 7426 | training loss: 0.000100639962\n",
      "Epoch: 7427 | training loss: 0.000100595193\n",
      "Epoch: 7428 | training loss: 0.000100550104\n",
      "Epoch: 7429 | training loss: 0.000100505320\n",
      "Epoch: 7430 | training loss: 0.000100460427\n",
      "Epoch: 7431 | training loss: 0.000100415593\n",
      "Epoch: 7432 | training loss: 0.000100370853\n",
      "Epoch: 7433 | training loss: 0.000100326019\n",
      "Epoch: 7434 | training loss: 0.000100281104\n",
      "Epoch: 7435 | training loss: 0.000100236342\n",
      "Epoch: 7436 | training loss: 0.000100191610\n",
      "Epoch: 7437 | training loss: 0.000100146761\n",
      "Epoch: 7438 | training loss: 0.000100101999\n",
      "Epoch: 7439 | training loss: 0.000100057398\n",
      "Epoch: 7440 | training loss: 0.000100012658\n",
      "Epoch: 7441 | training loss: 0.000099967961\n",
      "Epoch: 7442 | training loss: 0.000099923243\n",
      "Epoch: 7443 | training loss: 0.000099878635\n",
      "Epoch: 7444 | training loss: 0.000099833967\n",
      "Epoch: 7445 | training loss: 0.000099789337\n",
      "Epoch: 7446 | training loss: 0.000099744779\n",
      "Epoch: 7447 | training loss: 0.000099700177\n",
      "Epoch: 7448 | training loss: 0.000099655706\n",
      "Epoch: 7449 | training loss: 0.000099611047\n",
      "Epoch: 7450 | training loss: 0.000099566532\n",
      "Epoch: 7451 | training loss: 0.000099521974\n",
      "Epoch: 7452 | training loss: 0.000099477475\n",
      "Epoch: 7453 | training loss: 0.000099432880\n",
      "Epoch: 7454 | training loss: 0.000099388366\n",
      "Epoch: 7455 | training loss: 0.000099343990\n",
      "Epoch: 7456 | training loss: 0.000099299621\n",
      "Epoch: 7457 | training loss: 0.000099255005\n",
      "Epoch: 7458 | training loss: 0.000099210723\n",
      "Epoch: 7459 | training loss: 0.000099166289\n",
      "Epoch: 7460 | training loss: 0.000099121753\n",
      "Epoch: 7461 | training loss: 0.000099077450\n",
      "Epoch: 7462 | training loss: 0.000099033176\n",
      "Epoch: 7463 | training loss: 0.000098988763\n",
      "Epoch: 7464 | training loss: 0.000098944525\n",
      "Epoch: 7465 | training loss: 0.000098900084\n",
      "Epoch: 7466 | training loss: 0.000098855853\n",
      "Epoch: 7467 | training loss: 0.000098811666\n",
      "Epoch: 7468 | training loss: 0.000098767297\n",
      "Epoch: 7469 | training loss: 0.000098723118\n",
      "Epoch: 7470 | training loss: 0.000098678865\n",
      "Epoch: 7471 | training loss: 0.000098634584\n",
      "Epoch: 7472 | training loss: 0.000098590390\n",
      "Epoch: 7473 | training loss: 0.000098546327\n",
      "Epoch: 7474 | training loss: 0.000098502132\n",
      "Epoch: 7475 | training loss: 0.000098458040\n",
      "Epoch: 7476 | training loss: 0.000098413802\n",
      "Epoch: 7477 | training loss: 0.000098369623\n",
      "Epoch: 7478 | training loss: 0.000098325545\n",
      "Epoch: 7479 | training loss: 0.000098281489\n",
      "Epoch: 7480 | training loss: 0.000098237433\n",
      "Epoch: 7481 | training loss: 0.000098193377\n",
      "Epoch: 7482 | training loss: 0.000098149168\n",
      "Epoch: 7483 | training loss: 0.000098105244\n",
      "Epoch: 7484 | training loss: 0.000098061319\n",
      "Epoch: 7485 | training loss: 0.000098017190\n",
      "Epoch: 7486 | training loss: 0.000097973330\n",
      "Epoch: 7487 | training loss: 0.000097929398\n",
      "Epoch: 7488 | training loss: 0.000097885422\n",
      "Epoch: 7489 | training loss: 0.000097841461\n",
      "Epoch: 7490 | training loss: 0.000097797507\n",
      "Epoch: 7491 | training loss: 0.000097753698\n",
      "Epoch: 7492 | training loss: 0.000097709744\n",
      "Epoch: 7493 | training loss: 0.000097665979\n",
      "Epoch: 7494 | training loss: 0.000097622091\n",
      "Epoch: 7495 | training loss: 0.000097578231\n",
      "Epoch: 7496 | training loss: 0.000097534488\n",
      "Epoch: 7497 | training loss: 0.000097490643\n",
      "Epoch: 7498 | training loss: 0.000097446893\n",
      "Epoch: 7499 | training loss: 0.000097403157\n",
      "Epoch: 7500 | training loss: 0.000097359429\n",
      "Epoch: 7501 | training loss: 0.000097315620\n",
      "Epoch: 7502 | training loss: 0.000097271884\n",
      "Epoch: 7503 | training loss: 0.000097228251\n",
      "Epoch: 7504 | training loss: 0.000097184435\n",
      "Epoch: 7505 | training loss: 0.000097140772\n",
      "Epoch: 7506 | training loss: 0.000097097174\n",
      "Epoch: 7507 | training loss: 0.000097053548\n",
      "Epoch: 7508 | training loss: 0.000097009914\n",
      "Epoch: 7509 | training loss: 0.000096966309\n",
      "Epoch: 7510 | training loss: 0.000096922689\n",
      "Epoch: 7511 | training loss: 0.000096879026\n",
      "Epoch: 7512 | training loss: 0.000096835473\n",
      "Epoch: 7513 | training loss: 0.000096792006\n",
      "Epoch: 7514 | training loss: 0.000096748408\n",
      "Epoch: 7515 | training loss: 0.000096704971\n",
      "Epoch: 7516 | training loss: 0.000096661504\n",
      "Epoch: 7517 | training loss: 0.000096617921\n",
      "Epoch: 7518 | training loss: 0.000096574586\n",
      "Epoch: 7519 | training loss: 0.000096531032\n",
      "Epoch: 7520 | training loss: 0.000096487624\n",
      "Epoch: 7521 | training loss: 0.000096444259\n",
      "Epoch: 7522 | training loss: 0.000096400829\n",
      "Epoch: 7523 | training loss: 0.000096357537\n",
      "Epoch: 7524 | training loss: 0.000096314136\n",
      "Epoch: 7525 | training loss: 0.000096270735\n",
      "Epoch: 7526 | training loss: 0.000096227435\n",
      "Epoch: 7527 | training loss: 0.000096184114\n",
      "Epoch: 7528 | training loss: 0.000096140779\n",
      "Epoch: 7529 | training loss: 0.000096097487\n",
      "Epoch: 7530 | training loss: 0.000096054202\n",
      "Epoch: 7531 | training loss: 0.000096010917\n",
      "Epoch: 7532 | training loss: 0.000095967684\n",
      "Epoch: 7533 | training loss: 0.000095924537\n",
      "Epoch: 7534 | training loss: 0.000095881216\n",
      "Epoch: 7535 | training loss: 0.000095838041\n",
      "Epoch: 7536 | training loss: 0.000095794967\n",
      "Epoch: 7537 | training loss: 0.000095751755\n",
      "Epoch: 7538 | training loss: 0.000095708492\n",
      "Epoch: 7539 | training loss: 0.000095665542\n",
      "Epoch: 7540 | training loss: 0.000095622345\n",
      "Epoch: 7541 | training loss: 0.000095579191\n",
      "Epoch: 7542 | training loss: 0.000095536161\n",
      "Epoch: 7543 | training loss: 0.000095492956\n",
      "Epoch: 7544 | training loss: 0.000095449926\n",
      "Epoch: 7545 | training loss: 0.000095406976\n",
      "Epoch: 7546 | training loss: 0.000095363830\n",
      "Epoch: 7547 | training loss: 0.000095320909\n",
      "Epoch: 7548 | training loss: 0.000095277835\n",
      "Epoch: 7549 | training loss: 0.000095234791\n",
      "Epoch: 7550 | training loss: 0.000095191936\n",
      "Epoch: 7551 | training loss: 0.000095149036\n",
      "Epoch: 7552 | training loss: 0.000095105963\n",
      "Epoch: 7553 | training loss: 0.000095063107\n",
      "Epoch: 7554 | training loss: 0.000095020143\n",
      "Epoch: 7555 | training loss: 0.000094977353\n",
      "Epoch: 7556 | training loss: 0.000094934461\n",
      "Epoch: 7557 | training loss: 0.000094891569\n",
      "Epoch: 7558 | training loss: 0.000094848729\n",
      "Epoch: 7559 | training loss: 0.000094805815\n",
      "Epoch: 7560 | training loss: 0.000094763105\n",
      "Epoch: 7561 | training loss: 0.000094720395\n",
      "Epoch: 7562 | training loss: 0.000094677467\n",
      "Epoch: 7563 | training loss: 0.000094634772\n",
      "Epoch: 7564 | training loss: 0.000094592062\n",
      "Epoch: 7565 | training loss: 0.000094549134\n",
      "Epoch: 7566 | training loss: 0.000094506460\n",
      "Epoch: 7567 | training loss: 0.000094463852\n",
      "Epoch: 7568 | training loss: 0.000094421077\n",
      "Epoch: 7569 | training loss: 0.000094378396\n",
      "Epoch: 7570 | training loss: 0.000094335832\n",
      "Epoch: 7571 | training loss: 0.000094293180\n",
      "Epoch: 7572 | training loss: 0.000094250558\n",
      "Epoch: 7573 | training loss: 0.000094207833\n",
      "Epoch: 7574 | training loss: 0.000094165283\n",
      "Epoch: 7575 | training loss: 0.000094122661\n",
      "Epoch: 7576 | training loss: 0.000094079980\n",
      "Epoch: 7577 | training loss: 0.000094037503\n",
      "Epoch: 7578 | training loss: 0.000093994924\n",
      "Epoch: 7579 | training loss: 0.000093952265\n",
      "Epoch: 7580 | training loss: 0.000093909897\n",
      "Epoch: 7581 | training loss: 0.000093867289\n",
      "Epoch: 7582 | training loss: 0.000093824834\n",
      "Epoch: 7583 | training loss: 0.000093782306\n",
      "Epoch: 7584 | training loss: 0.000093739887\n",
      "Epoch: 7585 | training loss: 0.000093697512\n",
      "Epoch: 7586 | training loss: 0.000093654919\n",
      "Epoch: 7587 | training loss: 0.000093612529\n",
      "Epoch: 7588 | training loss: 0.000093570190\n",
      "Epoch: 7589 | training loss: 0.000093527779\n",
      "Epoch: 7590 | training loss: 0.000093485316\n",
      "Epoch: 7591 | training loss: 0.000093442999\n",
      "Epoch: 7592 | training loss: 0.000093400653\n",
      "Epoch: 7593 | training loss: 0.000093358263\n",
      "Epoch: 7594 | training loss: 0.000093316077\n",
      "Epoch: 7595 | training loss: 0.000093273702\n",
      "Epoch: 7596 | training loss: 0.000093231458\n",
      "Epoch: 7597 | training loss: 0.000093189170\n",
      "Epoch: 7598 | training loss: 0.000093147006\n",
      "Epoch: 7599 | training loss: 0.000093104682\n",
      "Epoch: 7600 | training loss: 0.000093062539\n",
      "Epoch: 7601 | training loss: 0.000093020353\n",
      "Epoch: 7602 | training loss: 0.000092978044\n",
      "Epoch: 7603 | training loss: 0.000092935850\n",
      "Epoch: 7604 | training loss: 0.000092893781\n",
      "Epoch: 7605 | training loss: 0.000092851558\n",
      "Epoch: 7606 | training loss: 0.000092809503\n",
      "Epoch: 7607 | training loss: 0.000092767317\n",
      "Epoch: 7608 | training loss: 0.000092725240\n",
      "Epoch: 7609 | training loss: 0.000092683244\n",
      "Epoch: 7610 | training loss: 0.000092641043\n",
      "Epoch: 7611 | training loss: 0.000092598988\n",
      "Epoch: 7612 | training loss: 0.000092556947\n",
      "Epoch: 7613 | training loss: 0.000092514856\n",
      "Epoch: 7614 | training loss: 0.000092472888\n",
      "Epoch: 7615 | training loss: 0.000092430862\n",
      "Epoch: 7616 | training loss: 0.000092388931\n",
      "Epoch: 7617 | training loss: 0.000092346920\n",
      "Epoch: 7618 | training loss: 0.000092304959\n",
      "Epoch: 7619 | training loss: 0.000092263042\n",
      "Epoch: 7620 | training loss: 0.000092221104\n",
      "Epoch: 7621 | training loss: 0.000092179274\n",
      "Epoch: 7622 | training loss: 0.000092137270\n",
      "Epoch: 7623 | training loss: 0.000092095273\n",
      "Epoch: 7624 | training loss: 0.000092053473\n",
      "Epoch: 7625 | training loss: 0.000092011745\n",
      "Epoch: 7626 | training loss: 0.000091969807\n",
      "Epoch: 7627 | training loss: 0.000091927912\n",
      "Epoch: 7628 | training loss: 0.000091886177\n",
      "Epoch: 7629 | training loss: 0.000091844224\n",
      "Epoch: 7630 | training loss: 0.000091802547\n",
      "Epoch: 7631 | training loss: 0.000091760754\n",
      "Epoch: 7632 | training loss: 0.000091719106\n",
      "Epoch: 7633 | training loss: 0.000091677241\n",
      "Epoch: 7634 | training loss: 0.000091635513\n",
      "Epoch: 7635 | training loss: 0.000091593800\n",
      "Epoch: 7636 | training loss: 0.000091552109\n",
      "Epoch: 7637 | training loss: 0.000091510396\n",
      "Epoch: 7638 | training loss: 0.000091468799\n",
      "Epoch: 7639 | training loss: 0.000091427057\n",
      "Epoch: 7640 | training loss: 0.000091385475\n",
      "Epoch: 7641 | training loss: 0.000091343863\n",
      "Epoch: 7642 | training loss: 0.000091302136\n",
      "Epoch: 7643 | training loss: 0.000091260576\n",
      "Epoch: 7644 | training loss: 0.000091219154\n",
      "Epoch: 7645 | training loss: 0.000091177470\n",
      "Epoch: 7646 | training loss: 0.000091135880\n",
      "Epoch: 7647 | training loss: 0.000091094262\n",
      "Epoch: 7648 | training loss: 0.000091052789\n",
      "Epoch: 7649 | training loss: 0.000091011141\n",
      "Epoch: 7650 | training loss: 0.000090969712\n",
      "Epoch: 7651 | training loss: 0.000090928224\n",
      "Epoch: 7652 | training loss: 0.000090886766\n",
      "Epoch: 7653 | training loss: 0.000090845191\n",
      "Epoch: 7654 | training loss: 0.000090803718\n",
      "Epoch: 7655 | training loss: 0.000090762274\n",
      "Epoch: 7656 | training loss: 0.000090720918\n",
      "Epoch: 7657 | training loss: 0.000090679488\n",
      "Epoch: 7658 | training loss: 0.000090638219\n",
      "Epoch: 7659 | training loss: 0.000090596644\n",
      "Epoch: 7660 | training loss: 0.000090555302\n",
      "Epoch: 7661 | training loss: 0.000090513946\n",
      "Epoch: 7662 | training loss: 0.000090472575\n",
      "Epoch: 7663 | training loss: 0.000090431247\n",
      "Epoch: 7664 | training loss: 0.000090389905\n",
      "Epoch: 7665 | training loss: 0.000090348651\n",
      "Epoch: 7666 | training loss: 0.000090307316\n",
      "Epoch: 7667 | training loss: 0.000090266047\n",
      "Epoch: 7668 | training loss: 0.000090224850\n",
      "Epoch: 7669 | training loss: 0.000090183479\n",
      "Epoch: 7670 | training loss: 0.000090142377\n",
      "Epoch: 7671 | training loss: 0.000090101195\n",
      "Epoch: 7672 | training loss: 0.000090059875\n",
      "Epoch: 7673 | training loss: 0.000090018730\n",
      "Epoch: 7674 | training loss: 0.000089977635\n",
      "Epoch: 7675 | training loss: 0.000089936337\n",
      "Epoch: 7676 | training loss: 0.000089895133\n",
      "Epoch: 7677 | training loss: 0.000089854067\n",
      "Epoch: 7678 | training loss: 0.000089812995\n",
      "Epoch: 7679 | training loss: 0.000089771849\n",
      "Epoch: 7680 | training loss: 0.000089730805\n",
      "Epoch: 7681 | training loss: 0.000089689667\n",
      "Epoch: 7682 | training loss: 0.000089648616\n",
      "Epoch: 7683 | training loss: 0.000089607551\n",
      "Epoch: 7684 | training loss: 0.000089566587\n",
      "Epoch: 7685 | training loss: 0.000089525478\n",
      "Epoch: 7686 | training loss: 0.000089484529\n",
      "Epoch: 7687 | training loss: 0.000089443442\n",
      "Epoch: 7688 | training loss: 0.000089402456\n",
      "Epoch: 7689 | training loss: 0.000089361514\n",
      "Epoch: 7690 | training loss: 0.000089320514\n",
      "Epoch: 7691 | training loss: 0.000089279507\n",
      "Epoch: 7692 | training loss: 0.000089238754\n",
      "Epoch: 7693 | training loss: 0.000089197798\n",
      "Epoch: 7694 | training loss: 0.000089156871\n",
      "Epoch: 7695 | training loss: 0.000089115958\n",
      "Epoch: 7696 | training loss: 0.000089075111\n",
      "Epoch: 7697 | training loss: 0.000089034329\n",
      "Epoch: 7698 | training loss: 0.000088993460\n",
      "Epoch: 7699 | training loss: 0.000088952700\n",
      "Epoch: 7700 | training loss: 0.000088911896\n",
      "Epoch: 7701 | training loss: 0.000088871006\n",
      "Epoch: 7702 | training loss: 0.000088830231\n",
      "Epoch: 7703 | training loss: 0.000088789515\n",
      "Epoch: 7704 | training loss: 0.000088748682\n",
      "Epoch: 7705 | training loss: 0.000088707995\n",
      "Epoch: 7706 | training loss: 0.000088667213\n",
      "Epoch: 7707 | training loss: 0.000088626541\n",
      "Epoch: 7708 | training loss: 0.000088585861\n",
      "Epoch: 7709 | training loss: 0.000088545101\n",
      "Epoch: 7710 | training loss: 0.000088504559\n",
      "Epoch: 7711 | training loss: 0.000088463828\n",
      "Epoch: 7712 | training loss: 0.000088423098\n",
      "Epoch: 7713 | training loss: 0.000088382498\n",
      "Epoch: 7714 | training loss: 0.000088341898\n",
      "Epoch: 7715 | training loss: 0.000088301342\n",
      "Epoch: 7716 | training loss: 0.000088260713\n",
      "Epoch: 7717 | training loss: 0.000088220171\n",
      "Epoch: 7718 | training loss: 0.000088179600\n",
      "Epoch: 7719 | training loss: 0.000088139052\n",
      "Epoch: 7720 | training loss: 0.000088098532\n",
      "Epoch: 7721 | training loss: 0.000088057932\n",
      "Epoch: 7722 | training loss: 0.000088017448\n",
      "Epoch: 7723 | training loss: 0.000087976980\n",
      "Epoch: 7724 | training loss: 0.000087936452\n",
      "Epoch: 7725 | training loss: 0.000087895969\n",
      "Epoch: 7726 | training loss: 0.000087855529\n",
      "Epoch: 7727 | training loss: 0.000087815024\n",
      "Epoch: 7728 | training loss: 0.000087774621\n",
      "Epoch: 7729 | training loss: 0.000087734210\n",
      "Epoch: 7730 | training loss: 0.000087693799\n",
      "Epoch: 7731 | training loss: 0.000087653425\n",
      "Epoch: 7732 | training loss: 0.000087612956\n",
      "Epoch: 7733 | training loss: 0.000087572655\n",
      "Epoch: 7734 | training loss: 0.000087532295\n",
      "Epoch: 7735 | training loss: 0.000087492052\n",
      "Epoch: 7736 | training loss: 0.000087451685\n",
      "Epoch: 7737 | training loss: 0.000087411332\n",
      "Epoch: 7738 | training loss: 0.000087371154\n",
      "Epoch: 7739 | training loss: 0.000087330867\n",
      "Epoch: 7740 | training loss: 0.000087290537\n",
      "Epoch: 7741 | training loss: 0.000087250330\n",
      "Epoch: 7742 | training loss: 0.000087210188\n",
      "Epoch: 7743 | training loss: 0.000087169901\n",
      "Epoch: 7744 | training loss: 0.000087129556\n",
      "Epoch: 7745 | training loss: 0.000087089415\n",
      "Epoch: 7746 | training loss: 0.000087049222\n",
      "Epoch: 7747 | training loss: 0.000087009212\n",
      "Epoch: 7748 | training loss: 0.000086969041\n",
      "Epoch: 7749 | training loss: 0.000086928878\n",
      "Epoch: 7750 | training loss: 0.000086888795\n",
      "Epoch: 7751 | training loss: 0.000086848777\n",
      "Epoch: 7752 | training loss: 0.000086808563\n",
      "Epoch: 7753 | training loss: 0.000086768574\n",
      "Epoch: 7754 | training loss: 0.000086728447\n",
      "Epoch: 7755 | training loss: 0.000086688437\n",
      "Epoch: 7756 | training loss: 0.000086648404\n",
      "Epoch: 7757 | training loss: 0.000086608430\n",
      "Epoch: 7758 | training loss: 0.000086568354\n",
      "Epoch: 7759 | training loss: 0.000086528336\n",
      "Epoch: 7760 | training loss: 0.000086488362\n",
      "Epoch: 7761 | training loss: 0.000086448403\n",
      "Epoch: 7762 | training loss: 0.000086408458\n",
      "Epoch: 7763 | training loss: 0.000086368520\n",
      "Epoch: 7764 | training loss: 0.000086328539\n",
      "Epoch: 7765 | training loss: 0.000086288710\n",
      "Epoch: 7766 | training loss: 0.000086248852\n",
      "Epoch: 7767 | training loss: 0.000086208951\n",
      "Epoch: 7768 | training loss: 0.000086169064\n",
      "Epoch: 7769 | training loss: 0.000086129192\n",
      "Epoch: 7770 | training loss: 0.000086089429\n",
      "Epoch: 7771 | training loss: 0.000086049542\n",
      "Epoch: 7772 | training loss: 0.000086009735\n",
      "Epoch: 7773 | training loss: 0.000085970038\n",
      "Epoch: 7774 | training loss: 0.000085930194\n",
      "Epoch: 7775 | training loss: 0.000085890533\n",
      "Epoch: 7776 | training loss: 0.000085850683\n",
      "Epoch: 7777 | training loss: 0.000085810854\n",
      "Epoch: 7778 | training loss: 0.000085771244\n",
      "Epoch: 7779 | training loss: 0.000085731546\n",
      "Epoch: 7780 | training loss: 0.000085691805\n",
      "Epoch: 7781 | training loss: 0.000085652122\n",
      "Epoch: 7782 | training loss: 0.000085612446\n",
      "Epoch: 7783 | training loss: 0.000085572829\n",
      "Epoch: 7784 | training loss: 0.000085533276\n",
      "Epoch: 7785 | training loss: 0.000085493622\n",
      "Epoch: 7786 | training loss: 0.000085454027\n",
      "Epoch: 7787 | training loss: 0.000085414380\n",
      "Epoch: 7788 | training loss: 0.000085374857\n",
      "Epoch: 7789 | training loss: 0.000085335319\n",
      "Epoch: 7790 | training loss: 0.000085295702\n",
      "Epoch: 7791 | training loss: 0.000085256113\n",
      "Epoch: 7792 | training loss: 0.000085216641\n",
      "Epoch: 7793 | training loss: 0.000085177126\n",
      "Epoch: 7794 | training loss: 0.000085137646\n",
      "Epoch: 7795 | training loss: 0.000085098094\n",
      "Epoch: 7796 | training loss: 0.000085058578\n",
      "Epoch: 7797 | training loss: 0.000085019157\n",
      "Epoch: 7798 | training loss: 0.000084979692\n",
      "Epoch: 7799 | training loss: 0.000084940271\n",
      "Epoch: 7800 | training loss: 0.000084900799\n",
      "Epoch: 7801 | training loss: 0.000084861356\n",
      "Epoch: 7802 | training loss: 0.000084822066\n",
      "Epoch: 7803 | training loss: 0.000084782616\n",
      "Epoch: 7804 | training loss: 0.000084743246\n",
      "Epoch: 7805 | training loss: 0.000084704006\n",
      "Epoch: 7806 | training loss: 0.000084664513\n",
      "Epoch: 7807 | training loss: 0.000084625208\n",
      "Epoch: 7808 | training loss: 0.000084585903\n",
      "Epoch: 7809 | training loss: 0.000084546627\n",
      "Epoch: 7810 | training loss: 0.000084507337\n",
      "Epoch: 7811 | training loss: 0.000084468033\n",
      "Epoch: 7812 | training loss: 0.000084428662\n",
      "Epoch: 7813 | training loss: 0.000084389489\n",
      "Epoch: 7814 | training loss: 0.000084350308\n",
      "Epoch: 7815 | training loss: 0.000084311061\n",
      "Epoch: 7816 | training loss: 0.000084271829\n",
      "Epoch: 7817 | training loss: 0.000084232524\n",
      "Epoch: 7818 | training loss: 0.000084193452\n",
      "Epoch: 7819 | training loss: 0.000084154162\n",
      "Epoch: 7820 | training loss: 0.000084115047\n",
      "Epoch: 7821 | training loss: 0.000084075917\n",
      "Epoch: 7822 | training loss: 0.000084036663\n",
      "Epoch: 7823 | training loss: 0.000083997613\n",
      "Epoch: 7824 | training loss: 0.000083958454\n",
      "Epoch: 7825 | training loss: 0.000083919418\n",
      "Epoch: 7826 | training loss: 0.000083880259\n",
      "Epoch: 7827 | training loss: 0.000083841267\n",
      "Epoch: 7828 | training loss: 0.000083802086\n",
      "Epoch: 7829 | training loss: 0.000083763080\n",
      "Epoch: 7830 | training loss: 0.000083724066\n",
      "Epoch: 7831 | training loss: 0.000083685052\n",
      "Epoch: 7832 | training loss: 0.000083646039\n",
      "Epoch: 7833 | training loss: 0.000083607105\n",
      "Epoch: 7834 | training loss: 0.000083568062\n",
      "Epoch: 7835 | training loss: 0.000083529056\n",
      "Epoch: 7836 | training loss: 0.000083490086\n",
      "Epoch: 7837 | training loss: 0.000083451196\n",
      "Epoch: 7838 | training loss: 0.000083412247\n",
      "Epoch: 7839 | training loss: 0.000083373379\n",
      "Epoch: 7840 | training loss: 0.000083334453\n",
      "Epoch: 7841 | training loss: 0.000083295512\n",
      "Epoch: 7842 | training loss: 0.000083256688\n",
      "Epoch: 7843 | training loss: 0.000083217754\n",
      "Epoch: 7844 | training loss: 0.000083178929\n",
      "Epoch: 7845 | training loss: 0.000083140134\n",
      "Epoch: 7846 | training loss: 0.000083101244\n",
      "Epoch: 7847 | training loss: 0.000083062478\n",
      "Epoch: 7848 | training loss: 0.000083023609\n",
      "Epoch: 7849 | training loss: 0.000082984836\n",
      "Epoch: 7850 | training loss: 0.000082946113\n",
      "Epoch: 7851 | training loss: 0.000082907376\n",
      "Epoch: 7852 | training loss: 0.000082868624\n",
      "Epoch: 7853 | training loss: 0.000082829880\n",
      "Epoch: 7854 | training loss: 0.000082791150\n",
      "Epoch: 7855 | training loss: 0.000082752405\n",
      "Epoch: 7856 | training loss: 0.000082713792\n",
      "Epoch: 7857 | training loss: 0.000082675106\n",
      "Epoch: 7858 | training loss: 0.000082636296\n",
      "Epoch: 7859 | training loss: 0.000082597675\n",
      "Epoch: 7860 | training loss: 0.000082559054\n",
      "Epoch: 7861 | training loss: 0.000082520419\n",
      "Epoch: 7862 | training loss: 0.000082481798\n",
      "Epoch: 7863 | training loss: 0.000082443250\n",
      "Epoch: 7864 | training loss: 0.000082404636\n",
      "Epoch: 7865 | training loss: 0.000082366023\n",
      "Epoch: 7866 | training loss: 0.000082327519\n",
      "Epoch: 7867 | training loss: 0.000082288869\n",
      "Epoch: 7868 | training loss: 0.000082250328\n",
      "Epoch: 7869 | training loss: 0.000082211904\n",
      "Epoch: 7870 | training loss: 0.000082173312\n",
      "Epoch: 7871 | training loss: 0.000082134749\n",
      "Epoch: 7872 | training loss: 0.000082096332\n",
      "Epoch: 7873 | training loss: 0.000082057843\n",
      "Epoch: 7874 | training loss: 0.000082019367\n",
      "Epoch: 7875 | training loss: 0.000081980885\n",
      "Epoch: 7876 | training loss: 0.000081942475\n",
      "Epoch: 7877 | training loss: 0.000081904058\n",
      "Epoch: 7878 | training loss: 0.000081865684\n",
      "Epoch: 7879 | training loss: 0.000081827231\n",
      "Epoch: 7880 | training loss: 0.000081788741\n",
      "Epoch: 7881 | training loss: 0.000081750477\n",
      "Epoch: 7882 | training loss: 0.000081712154\n",
      "Epoch: 7883 | training loss: 0.000081673774\n",
      "Epoch: 7884 | training loss: 0.000081635517\n",
      "Epoch: 7885 | training loss: 0.000081597260\n",
      "Epoch: 7886 | training loss: 0.000081558886\n",
      "Epoch: 7887 | training loss: 0.000081520528\n",
      "Epoch: 7888 | training loss: 0.000081482300\n",
      "Epoch: 7889 | training loss: 0.000081443985\n",
      "Epoch: 7890 | training loss: 0.000081405771\n",
      "Epoch: 7891 | training loss: 0.000081367543\n",
      "Epoch: 7892 | training loss: 0.000081329315\n",
      "Epoch: 7893 | training loss: 0.000081291219\n",
      "Epoch: 7894 | training loss: 0.000081252969\n",
      "Epoch: 7895 | training loss: 0.000081214792\n",
      "Epoch: 7896 | training loss: 0.000081176571\n",
      "Epoch: 7897 | training loss: 0.000081138467\n",
      "Epoch: 7898 | training loss: 0.000081100356\n",
      "Epoch: 7899 | training loss: 0.000081062157\n",
      "Epoch: 7900 | training loss: 0.000081024016\n",
      "Epoch: 7901 | training loss: 0.000080986014\n",
      "Epoch: 7902 | training loss: 0.000080947815\n",
      "Epoch: 7903 | training loss: 0.000080909842\n",
      "Epoch: 7904 | training loss: 0.000080871730\n",
      "Epoch: 7905 | training loss: 0.000080833663\n",
      "Epoch: 7906 | training loss: 0.000080795660\n",
      "Epoch: 7907 | training loss: 0.000080757673\n",
      "Epoch: 7908 | training loss: 0.000080719583\n",
      "Epoch: 7909 | training loss: 0.000080681668\n",
      "Epoch: 7910 | training loss: 0.000080643702\n",
      "Epoch: 7911 | training loss: 0.000080605707\n",
      "Epoch: 7912 | training loss: 0.000080567770\n",
      "Epoch: 7913 | training loss: 0.000080529819\n",
      "Epoch: 7914 | training loss: 0.000080491838\n",
      "Epoch: 7915 | training loss: 0.000080453901\n",
      "Epoch: 7916 | training loss: 0.000080416008\n",
      "Epoch: 7917 | training loss: 0.000080378144\n",
      "Epoch: 7918 | training loss: 0.000080340222\n",
      "Epoch: 7919 | training loss: 0.000080302329\n",
      "Epoch: 7920 | training loss: 0.000080264421\n",
      "Epoch: 7921 | training loss: 0.000080226542\n",
      "Epoch: 7922 | training loss: 0.000080188693\n",
      "Epoch: 7923 | training loss: 0.000080150967\n",
      "Epoch: 7924 | training loss: 0.000080113132\n",
      "Epoch: 7925 | training loss: 0.000080075260\n",
      "Epoch: 7926 | training loss: 0.000080037571\n",
      "Epoch: 7927 | training loss: 0.000079999751\n",
      "Epoch: 7928 | training loss: 0.000079962017\n",
      "Epoch: 7929 | training loss: 0.000079924183\n",
      "Epoch: 7930 | training loss: 0.000079886508\n",
      "Epoch: 7931 | training loss: 0.000079848767\n",
      "Epoch: 7932 | training loss: 0.000079811114\n",
      "Epoch: 7933 | training loss: 0.000079773410\n",
      "Epoch: 7934 | training loss: 0.000079735604\n",
      "Epoch: 7935 | training loss: 0.000079697973\n",
      "Epoch: 7936 | training loss: 0.000079660269\n",
      "Epoch: 7937 | training loss: 0.000079622667\n",
      "Epoch: 7938 | training loss: 0.000079585094\n",
      "Epoch: 7939 | training loss: 0.000079547361\n",
      "Epoch: 7940 | training loss: 0.000079509715\n",
      "Epoch: 7941 | training loss: 0.000079472164\n",
      "Epoch: 7942 | training loss: 0.000079434576\n",
      "Epoch: 7943 | training loss: 0.000079396981\n",
      "Epoch: 7944 | training loss: 0.000079359466\n",
      "Epoch: 7945 | training loss: 0.000079322039\n",
      "Epoch: 7946 | training loss: 0.000079284335\n",
      "Epoch: 7947 | training loss: 0.000079246835\n",
      "Epoch: 7948 | training loss: 0.000079209407\n",
      "Epoch: 7949 | training loss: 0.000079171914\n",
      "Epoch: 7950 | training loss: 0.000079134370\n",
      "Epoch: 7951 | training loss: 0.000079096833\n",
      "Epoch: 7952 | training loss: 0.000079059442\n",
      "Epoch: 7953 | training loss: 0.000079022051\n",
      "Epoch: 7954 | training loss: 0.000078984638\n",
      "Epoch: 7955 | training loss: 0.000078947181\n",
      "Epoch: 7956 | training loss: 0.000078909783\n",
      "Epoch: 7957 | training loss: 0.000078872428\n",
      "Epoch: 7958 | training loss: 0.000078834986\n",
      "Epoch: 7959 | training loss: 0.000078797646\n",
      "Epoch: 7960 | training loss: 0.000078760277\n",
      "Epoch: 7961 | training loss: 0.000078722929\n",
      "Epoch: 7962 | training loss: 0.000078685582\n",
      "Epoch: 7963 | training loss: 0.000078648314\n",
      "Epoch: 7964 | training loss: 0.000078611054\n",
      "Epoch: 7965 | training loss: 0.000078573663\n",
      "Epoch: 7966 | training loss: 0.000078536403\n",
      "Epoch: 7967 | training loss: 0.000078499099\n",
      "Epoch: 7968 | training loss: 0.000078461933\n",
      "Epoch: 7969 | training loss: 0.000078424622\n",
      "Epoch: 7970 | training loss: 0.000078387369\n",
      "Epoch: 7971 | training loss: 0.000078350276\n",
      "Epoch: 7972 | training loss: 0.000078312980\n",
      "Epoch: 7973 | training loss: 0.000078275836\n",
      "Epoch: 7974 | training loss: 0.000078238649\n",
      "Epoch: 7975 | training loss: 0.000078201527\n",
      "Epoch: 7976 | training loss: 0.000078164347\n",
      "Epoch: 7977 | training loss: 0.000078127217\n",
      "Epoch: 7978 | training loss: 0.000078089994\n",
      "Epoch: 7979 | training loss: 0.000078052974\n",
      "Epoch: 7980 | training loss: 0.000078015808\n",
      "Epoch: 7981 | training loss: 0.000077978693\n",
      "Epoch: 7982 | training loss: 0.000077941579\n",
      "Epoch: 7983 | training loss: 0.000077904580\n",
      "Epoch: 7984 | training loss: 0.000077867444\n",
      "Epoch: 7985 | training loss: 0.000077830511\n",
      "Epoch: 7986 | training loss: 0.000077793375\n",
      "Epoch: 7987 | training loss: 0.000077756355\n",
      "Epoch: 7988 | training loss: 0.000077719320\n",
      "Epoch: 7989 | training loss: 0.000077682460\n",
      "Epoch: 7990 | training loss: 0.000077645396\n",
      "Epoch: 7991 | training loss: 0.000077608420\n",
      "Epoch: 7992 | training loss: 0.000077571545\n",
      "Epoch: 7993 | training loss: 0.000077534569\n",
      "Epoch: 7994 | training loss: 0.000077497723\n",
      "Epoch: 7995 | training loss: 0.000077460791\n",
      "Epoch: 7996 | training loss: 0.000077423836\n",
      "Epoch: 7997 | training loss: 0.000077386954\n",
      "Epoch: 7998 | training loss: 0.000077350058\n",
      "Epoch: 7999 | training loss: 0.000077313263\n",
      "Epoch: 8000 | training loss: 0.000077276301\n",
      "Epoch: 8001 | training loss: 0.000077239463\n",
      "Epoch: 8002 | training loss: 0.000077202785\n",
      "Epoch: 8003 | training loss: 0.000077165911\n",
      "Epoch: 8004 | training loss: 0.000077129109\n",
      "Epoch: 8005 | training loss: 0.000077092314\n",
      "Epoch: 8006 | training loss: 0.000077055520\n",
      "Epoch: 8007 | training loss: 0.000077018907\n",
      "Epoch: 8008 | training loss: 0.000076982091\n",
      "Epoch: 8009 | training loss: 0.000076945384\n",
      "Epoch: 8010 | training loss: 0.000076908633\n",
      "Epoch: 8011 | training loss: 0.000076871918\n",
      "Epoch: 8012 | training loss: 0.000076835306\n",
      "Epoch: 8013 | training loss: 0.000076798635\n",
      "Epoch: 8014 | training loss: 0.000076761935\n",
      "Epoch: 8015 | training loss: 0.000076725308\n",
      "Epoch: 8016 | training loss: 0.000076688550\n",
      "Epoch: 8017 | training loss: 0.000076651981\n",
      "Epoch: 8018 | training loss: 0.000076615332\n",
      "Epoch: 8019 | training loss: 0.000076578668\n",
      "Epoch: 8020 | training loss: 0.000076542201\n",
      "Epoch: 8021 | training loss: 0.000076505465\n",
      "Epoch: 8022 | training loss: 0.000076468888\n",
      "Epoch: 8023 | training loss: 0.000076432319\n",
      "Epoch: 8024 | training loss: 0.000076395831\n",
      "Epoch: 8025 | training loss: 0.000076359211\n",
      "Epoch: 8026 | training loss: 0.000076322729\n",
      "Epoch: 8027 | training loss: 0.000076286116\n",
      "Epoch: 8028 | training loss: 0.000076249693\n",
      "Epoch: 8029 | training loss: 0.000076213226\n",
      "Epoch: 8030 | training loss: 0.000076176722\n",
      "Epoch: 8031 | training loss: 0.000076140175\n",
      "Epoch: 8032 | training loss: 0.000076103672\n",
      "Epoch: 8033 | training loss: 0.000076067328\n",
      "Epoch: 8034 | training loss: 0.000076030861\n",
      "Epoch: 8035 | training loss: 0.000075994452\n",
      "Epoch: 8036 | training loss: 0.000075958065\n",
      "Epoch: 8037 | training loss: 0.000075921605\n",
      "Epoch: 8038 | training loss: 0.000075885262\n",
      "Epoch: 8039 | training loss: 0.000075848868\n",
      "Epoch: 8040 | training loss: 0.000075812524\n",
      "Epoch: 8041 | training loss: 0.000075776174\n",
      "Epoch: 8042 | training loss: 0.000075739794\n",
      "Epoch: 8043 | training loss: 0.000075703501\n",
      "Epoch: 8044 | training loss: 0.000075667238\n",
      "Epoch: 8045 | training loss: 0.000075630960\n",
      "Epoch: 8046 | training loss: 0.000075594697\n",
      "Epoch: 8047 | training loss: 0.000075558411\n",
      "Epoch: 8048 | training loss: 0.000075522112\n",
      "Epoch: 8049 | training loss: 0.000075485885\n",
      "Epoch: 8050 | training loss: 0.000075449658\n",
      "Epoch: 8051 | training loss: 0.000075413540\n",
      "Epoch: 8052 | training loss: 0.000075377277\n",
      "Epoch: 8053 | training loss: 0.000075341020\n",
      "Epoch: 8054 | training loss: 0.000075304852\n",
      "Epoch: 8055 | training loss: 0.000075268617\n",
      "Epoch: 8056 | training loss: 0.000075232529\n",
      "Epoch: 8057 | training loss: 0.000075196440\n",
      "Epoch: 8058 | training loss: 0.000075160198\n",
      "Epoch: 8059 | training loss: 0.000075124219\n",
      "Epoch: 8060 | training loss: 0.000075088021\n",
      "Epoch: 8061 | training loss: 0.000075051983\n",
      "Epoch: 8062 | training loss: 0.000075015829\n",
      "Epoch: 8063 | training loss: 0.000074979849\n",
      "Epoch: 8064 | training loss: 0.000074943775\n",
      "Epoch: 8065 | training loss: 0.000074907708\n",
      "Epoch: 8066 | training loss: 0.000074871685\n",
      "Epoch: 8067 | training loss: 0.000074835654\n",
      "Epoch: 8068 | training loss: 0.000074799653\n",
      "Epoch: 8069 | training loss: 0.000074763630\n",
      "Epoch: 8070 | training loss: 0.000074727723\n",
      "Epoch: 8071 | training loss: 0.000074691699\n",
      "Epoch: 8072 | training loss: 0.000074655734\n",
      "Epoch: 8073 | training loss: 0.000074619849\n",
      "Epoch: 8074 | training loss: 0.000074583833\n",
      "Epoch: 8075 | training loss: 0.000074548028\n",
      "Epoch: 8076 | training loss: 0.000074512078\n",
      "Epoch: 8077 | training loss: 0.000074476222\n",
      "Epoch: 8078 | training loss: 0.000074440366\n",
      "Epoch: 8079 | training loss: 0.000074404437\n",
      "Epoch: 8080 | training loss: 0.000074368611\n",
      "Epoch: 8081 | training loss: 0.000074332798\n",
      "Epoch: 8082 | training loss: 0.000074296913\n",
      "Epoch: 8083 | training loss: 0.000074261130\n",
      "Epoch: 8084 | training loss: 0.000074225281\n",
      "Epoch: 8085 | training loss: 0.000074189549\n",
      "Epoch: 8086 | training loss: 0.000074153722\n",
      "Epoch: 8087 | training loss: 0.000074118019\n",
      "Epoch: 8088 | training loss: 0.000074082258\n",
      "Epoch: 8089 | training loss: 0.000074046518\n",
      "Epoch: 8090 | training loss: 0.000074010793\n",
      "Epoch: 8091 | training loss: 0.000073975105\n",
      "Epoch: 8092 | training loss: 0.000073939322\n",
      "Epoch: 8093 | training loss: 0.000073903706\n",
      "Epoch: 8094 | training loss: 0.000073868010\n",
      "Epoch: 8095 | training loss: 0.000073832329\n",
      "Epoch: 8096 | training loss: 0.000073796662\n",
      "Epoch: 8097 | training loss: 0.000073760981\n",
      "Epoch: 8098 | training loss: 0.000073725387\n",
      "Epoch: 8099 | training loss: 0.000073689618\n",
      "Epoch: 8100 | training loss: 0.000073654053\n",
      "Epoch: 8101 | training loss: 0.000073618496\n",
      "Epoch: 8102 | training loss: 0.000073582894\n",
      "Epoch: 8103 | training loss: 0.000073547402\n",
      "Epoch: 8104 | training loss: 0.000073511750\n",
      "Epoch: 8105 | training loss: 0.000073476258\n",
      "Epoch: 8106 | training loss: 0.000073440679\n",
      "Epoch: 8107 | training loss: 0.000073405223\n",
      "Epoch: 8108 | training loss: 0.000073369585\n",
      "Epoch: 8109 | training loss: 0.000073334049\n",
      "Epoch: 8110 | training loss: 0.000073298594\n",
      "Epoch: 8111 | training loss: 0.000073263196\n",
      "Epoch: 8112 | training loss: 0.000073227638\n",
      "Epoch: 8113 | training loss: 0.000073192205\n",
      "Epoch: 8114 | training loss: 0.000073156720\n",
      "Epoch: 8115 | training loss: 0.000073121337\n",
      "Epoch: 8116 | training loss: 0.000073085845\n",
      "Epoch: 8117 | training loss: 0.000073050527\n",
      "Epoch: 8118 | training loss: 0.000073015210\n",
      "Epoch: 8119 | training loss: 0.000072979805\n",
      "Epoch: 8120 | training loss: 0.000072944415\n",
      "Epoch: 8121 | training loss: 0.000072909083\n",
      "Epoch: 8122 | training loss: 0.000072873794\n",
      "Epoch: 8123 | training loss: 0.000072838389\n",
      "Epoch: 8124 | training loss: 0.000072803145\n",
      "Epoch: 8125 | training loss: 0.000072767827\n",
      "Epoch: 8126 | training loss: 0.000072732539\n",
      "Epoch: 8127 | training loss: 0.000072697221\n",
      "Epoch: 8128 | training loss: 0.000072662020\n",
      "Epoch: 8129 | training loss: 0.000072626659\n",
      "Epoch: 8130 | training loss: 0.000072591502\n",
      "Epoch: 8131 | training loss: 0.000072556169\n",
      "Epoch: 8132 | training loss: 0.000072520939\n",
      "Epoch: 8133 | training loss: 0.000072485709\n",
      "Epoch: 8134 | training loss: 0.000072450617\n",
      "Epoch: 8135 | training loss: 0.000072415438\n",
      "Epoch: 8136 | training loss: 0.000072380331\n",
      "Epoch: 8137 | training loss: 0.000072345159\n",
      "Epoch: 8138 | training loss: 0.000072309937\n",
      "Epoch: 8139 | training loss: 0.000072274932\n",
      "Epoch: 8140 | training loss: 0.000072239738\n",
      "Epoch: 8141 | training loss: 0.000072204588\n",
      "Epoch: 8142 | training loss: 0.000072169481\n",
      "Epoch: 8143 | training loss: 0.000072134477\n",
      "Epoch: 8144 | training loss: 0.000072099290\n",
      "Epoch: 8145 | training loss: 0.000072064220\n",
      "Epoch: 8146 | training loss: 0.000072029230\n",
      "Epoch: 8147 | training loss: 0.000071994175\n",
      "Epoch: 8148 | training loss: 0.000071959039\n",
      "Epoch: 8149 | training loss: 0.000071924012\n",
      "Epoch: 8150 | training loss: 0.000071889153\n",
      "Epoch: 8151 | training loss: 0.000071854192\n",
      "Epoch: 8152 | training loss: 0.000071819130\n",
      "Epoch: 8153 | training loss: 0.000071784212\n",
      "Epoch: 8154 | training loss: 0.000071749222\n",
      "Epoch: 8155 | training loss: 0.000071714348\n",
      "Epoch: 8156 | training loss: 0.000071679373\n",
      "Epoch: 8157 | training loss: 0.000071644383\n",
      "Epoch: 8158 | training loss: 0.000071609655\n",
      "Epoch: 8159 | training loss: 0.000071574737\n",
      "Epoch: 8160 | training loss: 0.000071539820\n",
      "Epoch: 8161 | training loss: 0.000071504954\n",
      "Epoch: 8162 | training loss: 0.000071470022\n",
      "Epoch: 8163 | training loss: 0.000071435221\n",
      "Epoch: 8164 | training loss: 0.000071400384\n",
      "Epoch: 8165 | training loss: 0.000071365546\n",
      "Epoch: 8166 | training loss: 0.000071330745\n",
      "Epoch: 8167 | training loss: 0.000071296017\n",
      "Epoch: 8168 | training loss: 0.000071261195\n",
      "Epoch: 8169 | training loss: 0.000071226401\n",
      "Epoch: 8170 | training loss: 0.000071191622\n",
      "Epoch: 8171 | training loss: 0.000071156857\n",
      "Epoch: 8172 | training loss: 0.000071122042\n",
      "Epoch: 8173 | training loss: 0.000071087285\n",
      "Epoch: 8174 | training loss: 0.000071052666\n",
      "Epoch: 8175 | training loss: 0.000071018047\n",
      "Epoch: 8176 | training loss: 0.000070983311\n",
      "Epoch: 8177 | training loss: 0.000070948678\n",
      "Epoch: 8178 | training loss: 0.000070914029\n",
      "Epoch: 8179 | training loss: 0.000070879316\n",
      "Epoch: 8180 | training loss: 0.000070844646\n",
      "Epoch: 8181 | training loss: 0.000070810012\n",
      "Epoch: 8182 | training loss: 0.000070775452\n",
      "Epoch: 8183 | training loss: 0.000070740818\n",
      "Epoch: 8184 | training loss: 0.000070706257\n",
      "Epoch: 8185 | training loss: 0.000070671616\n",
      "Epoch: 8186 | training loss: 0.000070637121\n",
      "Epoch: 8187 | training loss: 0.000070602611\n",
      "Epoch: 8188 | training loss: 0.000070567978\n",
      "Epoch: 8189 | training loss: 0.000070533439\n",
      "Epoch: 8190 | training loss: 0.000070498951\n",
      "Epoch: 8191 | training loss: 0.000070464441\n",
      "Epoch: 8192 | training loss: 0.000070429953\n",
      "Epoch: 8193 | training loss: 0.000070395377\n",
      "Epoch: 8194 | training loss: 0.000070361013\n",
      "Epoch: 8195 | training loss: 0.000070326496\n",
      "Epoch: 8196 | training loss: 0.000070292117\n",
      "Epoch: 8197 | training loss: 0.000070257593\n",
      "Epoch: 8198 | training loss: 0.000070223294\n",
      "Epoch: 8199 | training loss: 0.000070188806\n",
      "Epoch: 8200 | training loss: 0.000070154303\n",
      "Epoch: 8201 | training loss: 0.000070119939\n",
      "Epoch: 8202 | training loss: 0.000070085604\n",
      "Epoch: 8203 | training loss: 0.000070051188\n",
      "Epoch: 8204 | training loss: 0.000070016889\n",
      "Epoch: 8205 | training loss: 0.000069982591\n",
      "Epoch: 8206 | training loss: 0.000069948335\n",
      "Epoch: 8207 | training loss: 0.000069913949\n",
      "Epoch: 8208 | training loss: 0.000069879628\n",
      "Epoch: 8209 | training loss: 0.000069845264\n",
      "Epoch: 8210 | training loss: 0.000069811023\n",
      "Epoch: 8211 | training loss: 0.000069776725\n",
      "Epoch: 8212 | training loss: 0.000069742440\n",
      "Epoch: 8213 | training loss: 0.000069708272\n",
      "Epoch: 8214 | training loss: 0.000069674046\n",
      "Epoch: 8215 | training loss: 0.000069639791\n",
      "Epoch: 8216 | training loss: 0.000069605594\n",
      "Epoch: 8217 | training loss: 0.000069571404\n",
      "Epoch: 8218 | training loss: 0.000069537193\n",
      "Epoch: 8219 | training loss: 0.000069503090\n",
      "Epoch: 8220 | training loss: 0.000069468893\n",
      "Epoch: 8221 | training loss: 0.000069434696\n",
      "Epoch: 8222 | training loss: 0.000069400601\n",
      "Epoch: 8223 | training loss: 0.000069366448\n",
      "Epoch: 8224 | training loss: 0.000069332316\n",
      "Epoch: 8225 | training loss: 0.000069298221\n",
      "Epoch: 8226 | training loss: 0.000069264141\n",
      "Epoch: 8227 | training loss: 0.000069230053\n",
      "Epoch: 8228 | training loss: 0.000069195943\n",
      "Epoch: 8229 | training loss: 0.000069161935\n",
      "Epoch: 8230 | training loss: 0.000069127884\n",
      "Epoch: 8231 | training loss: 0.000069093796\n",
      "Epoch: 8232 | training loss: 0.000069059810\n",
      "Epoch: 8233 | training loss: 0.000069025868\n",
      "Epoch: 8234 | training loss: 0.000068991838\n",
      "Epoch: 8235 | training loss: 0.000068957779\n",
      "Epoch: 8236 | training loss: 0.000068923924\n",
      "Epoch: 8237 | training loss: 0.000068889996\n",
      "Epoch: 8238 | training loss: 0.000068856039\n",
      "Epoch: 8239 | training loss: 0.000068822061\n",
      "Epoch: 8240 | training loss: 0.000068788060\n",
      "Epoch: 8241 | training loss: 0.000068754183\n",
      "Epoch: 8242 | training loss: 0.000068720306\n",
      "Epoch: 8243 | training loss: 0.000068686291\n",
      "Epoch: 8244 | training loss: 0.000068652531\n",
      "Epoch: 8245 | training loss: 0.000068618720\n",
      "Epoch: 8246 | training loss: 0.000068584799\n",
      "Epoch: 8247 | training loss: 0.000068550951\n",
      "Epoch: 8248 | training loss: 0.000068517162\n",
      "Epoch: 8249 | training loss: 0.000068483321\n",
      "Epoch: 8250 | training loss: 0.000068449444\n",
      "Epoch: 8251 | training loss: 0.000068415742\n",
      "Epoch: 8252 | training loss: 0.000068381967\n",
      "Epoch: 8253 | training loss: 0.000068348207\n",
      "Epoch: 8254 | training loss: 0.000068314395\n",
      "Epoch: 8255 | training loss: 0.000068280657\n",
      "Epoch: 8256 | training loss: 0.000068246867\n",
      "Epoch: 8257 | training loss: 0.000068213179\n",
      "Epoch: 8258 | training loss: 0.000068179579\n",
      "Epoch: 8259 | training loss: 0.000068145760\n",
      "Epoch: 8260 | training loss: 0.000068112058\n",
      "Epoch: 8261 | training loss: 0.000068078414\n",
      "Epoch: 8262 | training loss: 0.000068044799\n",
      "Epoch: 8263 | training loss: 0.000068011032\n",
      "Epoch: 8264 | training loss: 0.000067977555\n",
      "Epoch: 8265 | training loss: 0.000067943918\n",
      "Epoch: 8266 | training loss: 0.000067910281\n",
      "Epoch: 8267 | training loss: 0.000067876645\n",
      "Epoch: 8268 | training loss: 0.000067843015\n",
      "Epoch: 8269 | training loss: 0.000067809444\n",
      "Epoch: 8270 | training loss: 0.000067775996\n",
      "Epoch: 8271 | training loss: 0.000067742352\n",
      "Epoch: 8272 | training loss: 0.000067708796\n",
      "Epoch: 8273 | training loss: 0.000067675239\n",
      "Epoch: 8274 | training loss: 0.000067641726\n",
      "Epoch: 8275 | training loss: 0.000067608154\n",
      "Epoch: 8276 | training loss: 0.000067574729\n",
      "Epoch: 8277 | training loss: 0.000067541361\n",
      "Epoch: 8278 | training loss: 0.000067507848\n",
      "Epoch: 8279 | training loss: 0.000067474328\n",
      "Epoch: 8280 | training loss: 0.000067440851\n",
      "Epoch: 8281 | training loss: 0.000067407404\n",
      "Epoch: 8282 | training loss: 0.000067374036\n",
      "Epoch: 8283 | training loss: 0.000067340654\n",
      "Epoch: 8284 | training loss: 0.000067307214\n",
      "Epoch: 8285 | training loss: 0.000067273795\n",
      "Epoch: 8286 | training loss: 0.000067240413\n",
      "Epoch: 8287 | training loss: 0.000067207075\n",
      "Epoch: 8288 | training loss: 0.000067173773\n",
      "Epoch: 8289 | training loss: 0.000067140441\n",
      "Epoch: 8290 | training loss: 0.000067107008\n",
      "Epoch: 8291 | training loss: 0.000067073692\n",
      "Epoch: 8292 | training loss: 0.000067040310\n",
      "Epoch: 8293 | training loss: 0.000067007066\n",
      "Epoch: 8294 | training loss: 0.000066973866\n",
      "Epoch: 8295 | training loss: 0.000066940542\n",
      "Epoch: 8296 | training loss: 0.000066907276\n",
      "Epoch: 8297 | training loss: 0.000066874025\n",
      "Epoch: 8298 | training loss: 0.000066840788\n",
      "Epoch: 8299 | training loss: 0.000066807595\n",
      "Epoch: 8300 | training loss: 0.000066774344\n",
      "Epoch: 8301 | training loss: 0.000066741079\n",
      "Epoch: 8302 | training loss: 0.000066707929\n",
      "Epoch: 8303 | training loss: 0.000066674744\n",
      "Epoch: 8304 | training loss: 0.000066641493\n",
      "Epoch: 8305 | training loss: 0.000066608380\n",
      "Epoch: 8306 | training loss: 0.000066575332\n",
      "Epoch: 8307 | training loss: 0.000066542074\n",
      "Epoch: 8308 | training loss: 0.000066508896\n",
      "Epoch: 8309 | training loss: 0.000066475841\n",
      "Epoch: 8310 | training loss: 0.000066442750\n",
      "Epoch: 8311 | training loss: 0.000066409615\n",
      "Epoch: 8312 | training loss: 0.000066376509\n",
      "Epoch: 8313 | training loss: 0.000066343389\n",
      "Epoch: 8314 | training loss: 0.000066310393\n",
      "Epoch: 8315 | training loss: 0.000066277302\n",
      "Epoch: 8316 | training loss: 0.000066244269\n",
      "Epoch: 8317 | training loss: 0.000066211272\n",
      "Epoch: 8318 | training loss: 0.000066178291\n",
      "Epoch: 8319 | training loss: 0.000066145287\n",
      "Epoch: 8320 | training loss: 0.000066112341\n",
      "Epoch: 8321 | training loss: 0.000066079236\n",
      "Epoch: 8322 | training loss: 0.000066046297\n",
      "Epoch: 8323 | training loss: 0.000066013294\n",
      "Epoch: 8324 | training loss: 0.000065980465\n",
      "Epoch: 8325 | training loss: 0.000065947504\n",
      "Epoch: 8326 | training loss: 0.000065914544\n",
      "Epoch: 8327 | training loss: 0.000065881650\n",
      "Epoch: 8328 | training loss: 0.000065848784\n",
      "Epoch: 8329 | training loss: 0.000065815912\n",
      "Epoch: 8330 | training loss: 0.000065783046\n",
      "Epoch: 8331 | training loss: 0.000065750210\n",
      "Epoch: 8332 | training loss: 0.000065717293\n",
      "Epoch: 8333 | training loss: 0.000065684479\n",
      "Epoch: 8334 | training loss: 0.000065651620\n",
      "Epoch: 8335 | training loss: 0.000065618879\n",
      "Epoch: 8336 | training loss: 0.000065585962\n",
      "Epoch: 8337 | training loss: 0.000065553228\n",
      "Epoch: 8338 | training loss: 0.000065520624\n",
      "Epoch: 8339 | training loss: 0.000065487759\n",
      "Epoch: 8340 | training loss: 0.000065454973\n",
      "Epoch: 8341 | training loss: 0.000065422173\n",
      "Epoch: 8342 | training loss: 0.000065389562\n",
      "Epoch: 8343 | training loss: 0.000065356748\n",
      "Epoch: 8344 | training loss: 0.000065324042\n",
      "Epoch: 8345 | training loss: 0.000065291330\n",
      "Epoch: 8346 | training loss: 0.000065258675\n",
      "Epoch: 8347 | training loss: 0.000065226028\n",
      "Epoch: 8348 | training loss: 0.000065193337\n",
      "Epoch: 8349 | training loss: 0.000065160639\n",
      "Epoch: 8350 | training loss: 0.000065128050\n",
      "Epoch: 8351 | training loss: 0.000065095432\n",
      "Epoch: 8352 | training loss: 0.000065062770\n",
      "Epoch: 8353 | training loss: 0.000065030239\n",
      "Epoch: 8354 | training loss: 0.000064997505\n",
      "Epoch: 8355 | training loss: 0.000064964894\n",
      "Epoch: 8356 | training loss: 0.000064932399\n",
      "Epoch: 8357 | training loss: 0.000064899876\n",
      "Epoch: 8358 | training loss: 0.000064867258\n",
      "Epoch: 8359 | training loss: 0.000064834734\n",
      "Epoch: 8360 | training loss: 0.000064802225\n",
      "Epoch: 8361 | training loss: 0.000064769702\n",
      "Epoch: 8362 | training loss: 0.000064737156\n",
      "Epoch: 8363 | training loss: 0.000064704655\n",
      "Epoch: 8364 | training loss: 0.000064672247\n",
      "Epoch: 8365 | training loss: 0.000064639709\n",
      "Epoch: 8366 | training loss: 0.000064607259\n",
      "Epoch: 8367 | training loss: 0.000064574779\n",
      "Epoch: 8368 | training loss: 0.000064542393\n",
      "Epoch: 8369 | training loss: 0.000064509943\n",
      "Epoch: 8370 | training loss: 0.000064477550\n",
      "Epoch: 8371 | training loss: 0.000064445128\n",
      "Epoch: 8372 | training loss: 0.000064412779\n",
      "Epoch: 8373 | training loss: 0.000064380409\n",
      "Epoch: 8374 | training loss: 0.000064348031\n",
      "Epoch: 8375 | training loss: 0.000064315631\n",
      "Epoch: 8376 | training loss: 0.000064283318\n",
      "Epoch: 8377 | training loss: 0.000064250984\n",
      "Epoch: 8378 | training loss: 0.000064218679\n",
      "Epoch: 8379 | training loss: 0.000064186359\n",
      "Epoch: 8380 | training loss: 0.000064154010\n",
      "Epoch: 8381 | training loss: 0.000064121778\n",
      "Epoch: 8382 | training loss: 0.000064089523\n",
      "Epoch: 8383 | training loss: 0.000064057196\n",
      "Epoch: 8384 | training loss: 0.000064025051\n",
      "Epoch: 8385 | training loss: 0.000063992804\n",
      "Epoch: 8386 | training loss: 0.000063960542\n",
      "Epoch: 8387 | training loss: 0.000063928237\n",
      "Epoch: 8388 | training loss: 0.000063896092\n",
      "Epoch: 8389 | training loss: 0.000063863947\n",
      "Epoch: 8390 | training loss: 0.000063831758\n",
      "Epoch: 8391 | training loss: 0.000063799518\n",
      "Epoch: 8392 | training loss: 0.000063767438\n",
      "Epoch: 8393 | training loss: 0.000063735235\n",
      "Epoch: 8394 | training loss: 0.000063703075\n",
      "Epoch: 8395 | training loss: 0.000063670923\n",
      "Epoch: 8396 | training loss: 0.000063638923\n",
      "Epoch: 8397 | training loss: 0.000063606727\n",
      "Epoch: 8398 | training loss: 0.000063574655\n",
      "Epoch: 8399 | training loss: 0.000063542437\n",
      "Epoch: 8400 | training loss: 0.000063510481\n",
      "Epoch: 8401 | training loss: 0.000063478336\n",
      "Epoch: 8402 | training loss: 0.000063446379\n",
      "Epoch: 8403 | training loss: 0.000063414278\n",
      "Epoch: 8404 | training loss: 0.000063382256\n",
      "Epoch: 8405 | training loss: 0.000063350271\n",
      "Epoch: 8406 | training loss: 0.000063318279\n",
      "Epoch: 8407 | training loss: 0.000063286265\n",
      "Epoch: 8408 | training loss: 0.000063254280\n",
      "Epoch: 8409 | training loss: 0.000063222273\n",
      "Epoch: 8410 | training loss: 0.000063190389\n",
      "Epoch: 8411 | training loss: 0.000063158368\n",
      "Epoch: 8412 | training loss: 0.000063126514\n",
      "Epoch: 8413 | training loss: 0.000063094507\n",
      "Epoch: 8414 | training loss: 0.000063062529\n",
      "Epoch: 8415 | training loss: 0.000063030719\n",
      "Epoch: 8416 | training loss: 0.000062998886\n",
      "Epoch: 8417 | training loss: 0.000062966938\n",
      "Epoch: 8418 | training loss: 0.000062935156\n",
      "Epoch: 8419 | training loss: 0.000062903171\n",
      "Epoch: 8420 | training loss: 0.000062871361\n",
      "Epoch: 8421 | training loss: 0.000062839550\n",
      "Epoch: 8422 | training loss: 0.000062807718\n",
      "Epoch: 8423 | training loss: 0.000062775915\n",
      "Epoch: 8424 | training loss: 0.000062744162\n",
      "Epoch: 8425 | training loss: 0.000062712345\n",
      "Epoch: 8426 | training loss: 0.000062680585\n",
      "Epoch: 8427 | training loss: 0.000062648745\n",
      "Epoch: 8428 | training loss: 0.000062617000\n",
      "Epoch: 8429 | training loss: 0.000062585270\n",
      "Epoch: 8430 | training loss: 0.000062553525\n",
      "Epoch: 8431 | training loss: 0.000062521780\n",
      "Epoch: 8432 | training loss: 0.000062490180\n",
      "Epoch: 8433 | training loss: 0.000062458465\n",
      "Epoch: 8434 | training loss: 0.000062426741\n",
      "Epoch: 8435 | training loss: 0.000062395047\n",
      "Epoch: 8436 | training loss: 0.000062363470\n",
      "Epoch: 8437 | training loss: 0.000062331659\n",
      "Epoch: 8438 | training loss: 0.000062300132\n",
      "Epoch: 8439 | training loss: 0.000062268424\n",
      "Epoch: 8440 | training loss: 0.000062236948\n",
      "Epoch: 8441 | training loss: 0.000062205188\n",
      "Epoch: 8442 | training loss: 0.000062173618\n",
      "Epoch: 8443 | training loss: 0.000062142033\n",
      "Epoch: 8444 | training loss: 0.000062110499\n",
      "Epoch: 8445 | training loss: 0.000062078936\n",
      "Epoch: 8446 | training loss: 0.000062047424\n",
      "Epoch: 8447 | training loss: 0.000062015883\n",
      "Epoch: 8448 | training loss: 0.000061984276\n",
      "Epoch: 8449 | training loss: 0.000061952800\n",
      "Epoch: 8450 | training loss: 0.000061921295\n",
      "Epoch: 8451 | training loss: 0.000061889848\n",
      "Epoch: 8452 | training loss: 0.000061858314\n",
      "Epoch: 8453 | training loss: 0.000061826875\n",
      "Epoch: 8454 | training loss: 0.000061795450\n",
      "Epoch: 8455 | training loss: 0.000061763916\n",
      "Epoch: 8456 | training loss: 0.000061732608\n",
      "Epoch: 8457 | training loss: 0.000061701161\n",
      "Epoch: 8458 | training loss: 0.000061669780\n",
      "Epoch: 8459 | training loss: 0.000061638297\n",
      "Epoch: 8460 | training loss: 0.000061607017\n",
      "Epoch: 8461 | training loss: 0.000061575534\n",
      "Epoch: 8462 | training loss: 0.000061544219\n",
      "Epoch: 8463 | training loss: 0.000061512867\n",
      "Epoch: 8464 | training loss: 0.000061481544\n",
      "Epoch: 8465 | training loss: 0.000061450104\n",
      "Epoch: 8466 | training loss: 0.000061418854\n",
      "Epoch: 8467 | training loss: 0.000061387487\n",
      "Epoch: 8468 | training loss: 0.000061356281\n",
      "Epoch: 8469 | training loss: 0.000061324972\n",
      "Epoch: 8470 | training loss: 0.000061293686\n",
      "Epoch: 8471 | training loss: 0.000061262414\n",
      "Epoch: 8472 | training loss: 0.000061231229\n",
      "Epoch: 8473 | training loss: 0.000061199884\n",
      "Epoch: 8474 | training loss: 0.000061168619\n",
      "Epoch: 8475 | training loss: 0.000061137376\n",
      "Epoch: 8476 | training loss: 0.000061106199\n",
      "Epoch: 8477 | training loss: 0.000061075014\n",
      "Epoch: 8478 | training loss: 0.000061043829\n",
      "Epoch: 8479 | training loss: 0.000061012659\n",
      "Epoch: 8480 | training loss: 0.000060981423\n",
      "Epoch: 8481 | training loss: 0.000060950348\n",
      "Epoch: 8482 | training loss: 0.000060919156\n",
      "Epoch: 8483 | training loss: 0.000060888022\n",
      "Epoch: 8484 | training loss: 0.000060856928\n",
      "Epoch: 8485 | training loss: 0.000060825721\n",
      "Epoch: 8486 | training loss: 0.000060794737\n",
      "Epoch: 8487 | training loss: 0.000060763552\n",
      "Epoch: 8488 | training loss: 0.000060732498\n",
      "Epoch: 8489 | training loss: 0.000060701423\n",
      "Epoch: 8490 | training loss: 0.000060670343\n",
      "Epoch: 8491 | training loss: 0.000060639217\n",
      "Epoch: 8492 | training loss: 0.000060608283\n",
      "Epoch: 8493 | training loss: 0.000060577266\n",
      "Epoch: 8494 | training loss: 0.000060546154\n",
      "Epoch: 8495 | training loss: 0.000060515173\n",
      "Epoch: 8496 | training loss: 0.000060484203\n",
      "Epoch: 8497 | training loss: 0.000060453174\n",
      "Epoch: 8498 | training loss: 0.000060422095\n",
      "Epoch: 8499 | training loss: 0.000060391212\n",
      "Epoch: 8500 | training loss: 0.000060360268\n",
      "Epoch: 8501 | training loss: 0.000060329261\n",
      "Epoch: 8502 | training loss: 0.000060298364\n",
      "Epoch: 8503 | training loss: 0.000060267317\n",
      "Epoch: 8504 | training loss: 0.000060236489\n",
      "Epoch: 8505 | training loss: 0.000060205639\n",
      "Epoch: 8506 | training loss: 0.000060174745\n",
      "Epoch: 8507 | training loss: 0.000060143866\n",
      "Epoch: 8508 | training loss: 0.000060112987\n",
      "Epoch: 8509 | training loss: 0.000060082140\n",
      "Epoch: 8510 | training loss: 0.000060051258\n",
      "Epoch: 8511 | training loss: 0.000060020429\n",
      "Epoch: 8512 | training loss: 0.000059989532\n",
      "Epoch: 8513 | training loss: 0.000059958817\n",
      "Epoch: 8514 | training loss: 0.000059928010\n",
      "Epoch: 8515 | training loss: 0.000059897146\n",
      "Epoch: 8516 | training loss: 0.000059866408\n",
      "Epoch: 8517 | training loss: 0.000059835609\n",
      "Epoch: 8518 | training loss: 0.000059804905\n",
      "Epoch: 8519 | training loss: 0.000059774182\n",
      "Epoch: 8520 | training loss: 0.000059743325\n",
      "Epoch: 8521 | training loss: 0.000059712620\n",
      "Epoch: 8522 | training loss: 0.000059681955\n",
      "Epoch: 8523 | training loss: 0.000059651240\n",
      "Epoch: 8524 | training loss: 0.000059620594\n",
      "Epoch: 8525 | training loss: 0.000059589875\n",
      "Epoch: 8526 | training loss: 0.000059559155\n",
      "Epoch: 8527 | training loss: 0.000059528593\n",
      "Epoch: 8528 | training loss: 0.000059497903\n",
      "Epoch: 8529 | training loss: 0.000059467260\n",
      "Epoch: 8530 | training loss: 0.000059436650\n",
      "Epoch: 8531 | training loss: 0.000059406048\n",
      "Epoch: 8532 | training loss: 0.000059375452\n",
      "Epoch: 8533 | training loss: 0.000059344828\n",
      "Epoch: 8534 | training loss: 0.000059314261\n",
      "Epoch: 8535 | training loss: 0.000059283688\n",
      "Epoch: 8536 | training loss: 0.000059253165\n",
      "Epoch: 8537 | training loss: 0.000059222497\n",
      "Epoch: 8538 | training loss: 0.000059192018\n",
      "Epoch: 8539 | training loss: 0.000059161357\n",
      "Epoch: 8540 | training loss: 0.000059130922\n",
      "Epoch: 8541 | training loss: 0.000059100486\n",
      "Epoch: 8542 | training loss: 0.000059069927\n",
      "Epoch: 8543 | training loss: 0.000059039405\n",
      "Epoch: 8544 | training loss: 0.000059008860\n",
      "Epoch: 8545 | training loss: 0.000058978527\n",
      "Epoch: 8546 | training loss: 0.000058948081\n",
      "Epoch: 8547 | training loss: 0.000058917583\n",
      "Epoch: 8548 | training loss: 0.000058887155\n",
      "Epoch: 8549 | training loss: 0.000058856687\n",
      "Epoch: 8550 | training loss: 0.000058826379\n",
      "Epoch: 8551 | training loss: 0.000058795973\n",
      "Epoch: 8552 | training loss: 0.000058765538\n",
      "Epoch: 8553 | training loss: 0.000058735146\n",
      "Epoch: 8554 | training loss: 0.000058704827\n",
      "Epoch: 8555 | training loss: 0.000058674457\n",
      "Epoch: 8556 | training loss: 0.000058644153\n",
      "Epoch: 8557 | training loss: 0.000058613754\n",
      "Epoch: 8558 | training loss: 0.000058583471\n",
      "Epoch: 8559 | training loss: 0.000058553138\n",
      "Epoch: 8560 | training loss: 0.000058522823\n",
      "Epoch: 8561 | training loss: 0.000058492464\n",
      "Epoch: 8562 | training loss: 0.000058462334\n",
      "Epoch: 8563 | training loss: 0.000058431979\n",
      "Epoch: 8564 | training loss: 0.000058401678\n",
      "Epoch: 8565 | training loss: 0.000058371454\n",
      "Epoch: 8566 | training loss: 0.000058341269\n",
      "Epoch: 8567 | training loss: 0.000058311030\n",
      "Epoch: 8568 | training loss: 0.000058280813\n",
      "Epoch: 8569 | training loss: 0.000058250567\n",
      "Epoch: 8570 | training loss: 0.000058220419\n",
      "Epoch: 8571 | training loss: 0.000058190264\n",
      "Epoch: 8572 | training loss: 0.000058160105\n",
      "Epoch: 8573 | training loss: 0.000058129859\n",
      "Epoch: 8574 | training loss: 0.000058099824\n",
      "Epoch: 8575 | training loss: 0.000058069680\n",
      "Epoch: 8576 | training loss: 0.000058039550\n",
      "Epoch: 8577 | training loss: 0.000058009420\n",
      "Epoch: 8578 | training loss: 0.000057979283\n",
      "Epoch: 8579 | training loss: 0.000057949295\n",
      "Epoch: 8580 | training loss: 0.000057919118\n",
      "Epoch: 8581 | training loss: 0.000057889003\n",
      "Epoch: 8582 | training loss: 0.000057859077\n",
      "Epoch: 8583 | training loss: 0.000057828911\n",
      "Epoch: 8584 | training loss: 0.000057798854\n",
      "Epoch: 8585 | training loss: 0.000057768895\n",
      "Epoch: 8586 | training loss: 0.000057738882\n",
      "Epoch: 8587 | training loss: 0.000057708952\n",
      "Epoch: 8588 | training loss: 0.000057678837\n",
      "Epoch: 8589 | training loss: 0.000057648900\n",
      "Epoch: 8590 | training loss: 0.000057618934\n",
      "Epoch: 8591 | training loss: 0.000057588957\n",
      "Epoch: 8592 | training loss: 0.000057559046\n",
      "Epoch: 8593 | training loss: 0.000057529072\n",
      "Epoch: 8594 | training loss: 0.000057499194\n",
      "Epoch: 8595 | training loss: 0.000057469268\n",
      "Epoch: 8596 | training loss: 0.000057439363\n",
      "Epoch: 8597 | training loss: 0.000057409474\n",
      "Epoch: 8598 | training loss: 0.000057379570\n",
      "Epoch: 8599 | training loss: 0.000057349753\n",
      "Epoch: 8600 | training loss: 0.000057319885\n",
      "Epoch: 8601 | training loss: 0.000057290039\n",
      "Epoch: 8602 | training loss: 0.000057260178\n",
      "Epoch: 8603 | training loss: 0.000057230387\n",
      "Epoch: 8604 | training loss: 0.000057200603\n",
      "Epoch: 8605 | training loss: 0.000057170735\n",
      "Epoch: 8606 | training loss: 0.000057140991\n",
      "Epoch: 8607 | training loss: 0.000057111094\n",
      "Epoch: 8608 | training loss: 0.000057081437\n",
      "Epoch: 8609 | training loss: 0.000057051642\n",
      "Epoch: 8610 | training loss: 0.000057021964\n",
      "Epoch: 8611 | training loss: 0.000056992234\n",
      "Epoch: 8612 | training loss: 0.000056962421\n",
      "Epoch: 8613 | training loss: 0.000056932742\n",
      "Epoch: 8614 | training loss: 0.000056903038\n",
      "Epoch: 8615 | training loss: 0.000056873279\n",
      "Epoch: 8616 | training loss: 0.000056843666\n",
      "Epoch: 8617 | training loss: 0.000056813959\n",
      "Epoch: 8618 | training loss: 0.000056784374\n",
      "Epoch: 8619 | training loss: 0.000056754703\n",
      "Epoch: 8620 | training loss: 0.000056725068\n",
      "Epoch: 8621 | training loss: 0.000056695422\n",
      "Epoch: 8622 | training loss: 0.000056665696\n",
      "Epoch: 8623 | training loss: 0.000056636160\n",
      "Epoch: 8624 | training loss: 0.000056606616\n",
      "Epoch: 8625 | training loss: 0.000056576973\n",
      "Epoch: 8626 | training loss: 0.000056547484\n",
      "Epoch: 8627 | training loss: 0.000056517827\n",
      "Epoch: 8628 | training loss: 0.000056488243\n",
      "Epoch: 8629 | training loss: 0.000056458695\n",
      "Epoch: 8630 | training loss: 0.000056429151\n",
      "Epoch: 8631 | training loss: 0.000056399564\n",
      "Epoch: 8632 | training loss: 0.000056370118\n",
      "Epoch: 8633 | training loss: 0.000056340636\n",
      "Epoch: 8634 | training loss: 0.000056311157\n",
      "Epoch: 8635 | training loss: 0.000056281642\n",
      "Epoch: 8636 | training loss: 0.000056252204\n",
      "Epoch: 8637 | training loss: 0.000056222747\n",
      "Epoch: 8638 | training loss: 0.000056193301\n",
      "Epoch: 8639 | training loss: 0.000056163852\n",
      "Epoch: 8640 | training loss: 0.000056134333\n",
      "Epoch: 8641 | training loss: 0.000056104982\n",
      "Epoch: 8642 | training loss: 0.000056075580\n",
      "Epoch: 8643 | training loss: 0.000056046210\n",
      "Epoch: 8644 | training loss: 0.000056016797\n",
      "Epoch: 8645 | training loss: 0.000055987486\n",
      "Epoch: 8646 | training loss: 0.000055958124\n",
      "Epoch: 8647 | training loss: 0.000055928711\n",
      "Epoch: 8648 | training loss: 0.000055899436\n",
      "Epoch: 8649 | training loss: 0.000055870049\n",
      "Epoch: 8650 | training loss: 0.000055840770\n",
      "Epoch: 8651 | training loss: 0.000055811452\n",
      "Epoch: 8652 | training loss: 0.000055782235\n",
      "Epoch: 8653 | training loss: 0.000055752851\n",
      "Epoch: 8654 | training loss: 0.000055723587\n",
      "Epoch: 8655 | training loss: 0.000055694331\n",
      "Epoch: 8656 | training loss: 0.000055664947\n",
      "Epoch: 8657 | training loss: 0.000055635857\n",
      "Epoch: 8658 | training loss: 0.000055606579\n",
      "Epoch: 8659 | training loss: 0.000055577322\n",
      "Epoch: 8660 | training loss: 0.000055548109\n",
      "Epoch: 8661 | training loss: 0.000055518954\n",
      "Epoch: 8662 | training loss: 0.000055489807\n",
      "Epoch: 8663 | training loss: 0.000055460601\n",
      "Epoch: 8664 | training loss: 0.000055431407\n",
      "Epoch: 8665 | training loss: 0.000055402368\n",
      "Epoch: 8666 | training loss: 0.000055373101\n",
      "Epoch: 8667 | training loss: 0.000055343935\n",
      "Epoch: 8668 | training loss: 0.000055314813\n",
      "Epoch: 8669 | training loss: 0.000055285687\n",
      "Epoch: 8670 | training loss: 0.000055256583\n",
      "Epoch: 8671 | training loss: 0.000055227447\n",
      "Epoch: 8672 | training loss: 0.000055198368\n",
      "Epoch: 8673 | training loss: 0.000055169268\n",
      "Epoch: 8674 | training loss: 0.000055140234\n",
      "Epoch: 8675 | training loss: 0.000055111144\n",
      "Epoch: 8676 | training loss: 0.000055082157\n",
      "Epoch: 8677 | training loss: 0.000055053126\n",
      "Epoch: 8678 | training loss: 0.000055024091\n",
      "Epoch: 8679 | training loss: 0.000054995126\n",
      "Epoch: 8680 | training loss: 0.000054966084\n",
      "Epoch: 8681 | training loss: 0.000054937049\n",
      "Epoch: 8682 | training loss: 0.000054908065\n",
      "Epoch: 8683 | training loss: 0.000054879078\n",
      "Epoch: 8684 | training loss: 0.000054850141\n",
      "Epoch: 8685 | training loss: 0.000054821190\n",
      "Epoch: 8686 | training loss: 0.000054792203\n",
      "Epoch: 8687 | training loss: 0.000054763423\n",
      "Epoch: 8688 | training loss: 0.000054734395\n",
      "Epoch: 8689 | training loss: 0.000054705546\n",
      "Epoch: 8690 | training loss: 0.000054676639\n",
      "Epoch: 8691 | training loss: 0.000054647742\n",
      "Epoch: 8692 | training loss: 0.000054618926\n",
      "Epoch: 8693 | training loss: 0.000054590011\n",
      "Epoch: 8694 | training loss: 0.000054561118\n",
      "Epoch: 8695 | training loss: 0.000054532335\n",
      "Epoch: 8696 | training loss: 0.000054503435\n",
      "Epoch: 8697 | training loss: 0.000054474600\n",
      "Epoch: 8698 | training loss: 0.000054445802\n",
      "Epoch: 8699 | training loss: 0.000054417065\n",
      "Epoch: 8700 | training loss: 0.000054388271\n",
      "Epoch: 8701 | training loss: 0.000054359472\n",
      "Epoch: 8702 | training loss: 0.000054330674\n",
      "Epoch: 8703 | training loss: 0.000054301971\n",
      "Epoch: 8704 | training loss: 0.000054273198\n",
      "Epoch: 8705 | training loss: 0.000054244505\n",
      "Epoch: 8706 | training loss: 0.000054215736\n",
      "Epoch: 8707 | training loss: 0.000054187032\n",
      "Epoch: 8708 | training loss: 0.000054158329\n",
      "Epoch: 8709 | training loss: 0.000054129636\n",
      "Epoch: 8710 | training loss: 0.000054100976\n",
      "Epoch: 8711 | training loss: 0.000054072265\n",
      "Epoch: 8712 | training loss: 0.000054043521\n",
      "Epoch: 8713 | training loss: 0.000054014967\n",
      "Epoch: 8714 | training loss: 0.000053986201\n",
      "Epoch: 8715 | training loss: 0.000053957778\n",
      "Epoch: 8716 | training loss: 0.000053929103\n",
      "Epoch: 8717 | training loss: 0.000053900403\n",
      "Epoch: 8718 | training loss: 0.000053871823\n",
      "Epoch: 8719 | training loss: 0.000053843294\n",
      "Epoch: 8720 | training loss: 0.000053814685\n",
      "Epoch: 8721 | training loss: 0.000053786156\n",
      "Epoch: 8722 | training loss: 0.000053757583\n",
      "Epoch: 8723 | training loss: 0.000053729069\n",
      "Epoch: 8724 | training loss: 0.000053700423\n",
      "Epoch: 8725 | training loss: 0.000053671953\n",
      "Epoch: 8726 | training loss: 0.000053643416\n",
      "Epoch: 8727 | training loss: 0.000053615047\n",
      "Epoch: 8728 | training loss: 0.000053586424\n",
      "Epoch: 8729 | training loss: 0.000053558004\n",
      "Epoch: 8730 | training loss: 0.000053529526\n",
      "Epoch: 8731 | training loss: 0.000053501011\n",
      "Epoch: 8732 | training loss: 0.000053472548\n",
      "Epoch: 8733 | training loss: 0.000053444088\n",
      "Epoch: 8734 | training loss: 0.000053415777\n",
      "Epoch: 8735 | training loss: 0.000053387295\n",
      "Epoch: 8736 | training loss: 0.000053358897\n",
      "Epoch: 8737 | training loss: 0.000053330561\n",
      "Epoch: 8738 | training loss: 0.000053302123\n",
      "Epoch: 8739 | training loss: 0.000053273743\n",
      "Epoch: 8740 | training loss: 0.000053245385\n",
      "Epoch: 8741 | training loss: 0.000053217038\n",
      "Epoch: 8742 | training loss: 0.000053188654\n",
      "Epoch: 8743 | training loss: 0.000053160387\n",
      "Epoch: 8744 | training loss: 0.000053131927\n",
      "Epoch: 8745 | training loss: 0.000053103697\n",
      "Epoch: 8746 | training loss: 0.000053075342\n",
      "Epoch: 8747 | training loss: 0.000053047093\n",
      "Epoch: 8748 | training loss: 0.000053018768\n",
      "Epoch: 8749 | training loss: 0.000052990392\n",
      "Epoch: 8750 | training loss: 0.000052962227\n",
      "Epoch: 8751 | training loss: 0.000052933996\n",
      "Epoch: 8752 | training loss: 0.000052905754\n",
      "Epoch: 8753 | training loss: 0.000052877498\n",
      "Epoch: 8754 | training loss: 0.000052849260\n",
      "Epoch: 8755 | training loss: 0.000052821120\n",
      "Epoch: 8756 | training loss: 0.000052792857\n",
      "Epoch: 8757 | training loss: 0.000052764663\n",
      "Epoch: 8758 | training loss: 0.000052736417\n",
      "Epoch: 8759 | training loss: 0.000052708368\n",
      "Epoch: 8760 | training loss: 0.000052680123\n",
      "Epoch: 8761 | training loss: 0.000052652009\n",
      "Epoch: 8762 | training loss: 0.000052623851\n",
      "Epoch: 8763 | training loss: 0.000052595758\n",
      "Epoch: 8764 | training loss: 0.000052567630\n",
      "Epoch: 8765 | training loss: 0.000052539519\n",
      "Epoch: 8766 | training loss: 0.000052511445\n",
      "Epoch: 8767 | training loss: 0.000052483359\n",
      "Epoch: 8768 | training loss: 0.000052455325\n",
      "Epoch: 8769 | training loss: 0.000052427229\n",
      "Epoch: 8770 | training loss: 0.000052399206\n",
      "Epoch: 8771 | training loss: 0.000052371106\n",
      "Epoch: 8772 | training loss: 0.000052343115\n",
      "Epoch: 8773 | training loss: 0.000052315103\n",
      "Epoch: 8774 | training loss: 0.000052287065\n",
      "Epoch: 8775 | training loss: 0.000052258983\n",
      "Epoch: 8776 | training loss: 0.000052231069\n",
      "Epoch: 8777 | training loss: 0.000052203082\n",
      "Epoch: 8778 | training loss: 0.000052175124\n",
      "Epoch: 8779 | training loss: 0.000052147181\n",
      "Epoch: 8780 | training loss: 0.000052119183\n",
      "Epoch: 8781 | training loss: 0.000052091291\n",
      "Epoch: 8782 | training loss: 0.000052063264\n",
      "Epoch: 8783 | training loss: 0.000052035437\n",
      "Epoch: 8784 | training loss: 0.000052007534\n",
      "Epoch: 8785 | training loss: 0.000051979649\n",
      "Epoch: 8786 | training loss: 0.000051951698\n",
      "Epoch: 8787 | training loss: 0.000051923846\n",
      "Epoch: 8788 | training loss: 0.000051895935\n",
      "Epoch: 8789 | training loss: 0.000051868104\n",
      "Epoch: 8790 | training loss: 0.000051840194\n",
      "Epoch: 8791 | training loss: 0.000051812378\n",
      "Epoch: 8792 | training loss: 0.000051784598\n",
      "Epoch: 8793 | training loss: 0.000051756761\n",
      "Epoch: 8794 | training loss: 0.000051728926\n",
      "Epoch: 8795 | training loss: 0.000051701172\n",
      "Epoch: 8796 | training loss: 0.000051673378\n",
      "Epoch: 8797 | training loss: 0.000051645606\n",
      "Epoch: 8798 | training loss: 0.000051617812\n",
      "Epoch: 8799 | training loss: 0.000051590003\n",
      "Epoch: 8800 | training loss: 0.000051562361\n",
      "Epoch: 8801 | training loss: 0.000051534622\n",
      "Epoch: 8802 | training loss: 0.000051506897\n",
      "Epoch: 8803 | training loss: 0.000051479175\n",
      "Epoch: 8804 | training loss: 0.000051451498\n",
      "Epoch: 8805 | training loss: 0.000051423831\n",
      "Epoch: 8806 | training loss: 0.000051396128\n",
      "Epoch: 8807 | training loss: 0.000051368457\n",
      "Epoch: 8808 | training loss: 0.000051340816\n",
      "Epoch: 8809 | training loss: 0.000051313167\n",
      "Epoch: 8810 | training loss: 0.000051285526\n",
      "Epoch: 8811 | training loss: 0.000051257804\n",
      "Epoch: 8812 | training loss: 0.000051230236\n",
      "Epoch: 8813 | training loss: 0.000051202671\n",
      "Epoch: 8814 | training loss: 0.000051175055\n",
      "Epoch: 8815 | training loss: 0.000051147355\n",
      "Epoch: 8816 | training loss: 0.000051119961\n",
      "Epoch: 8817 | training loss: 0.000051092269\n",
      "Epoch: 8818 | training loss: 0.000051064810\n",
      "Epoch: 8819 | training loss: 0.000051037197\n",
      "Epoch: 8820 | training loss: 0.000051009702\n",
      "Epoch: 8821 | training loss: 0.000050982224\n",
      "Epoch: 8822 | training loss: 0.000050954615\n",
      "Epoch: 8823 | training loss: 0.000050927134\n",
      "Epoch: 8824 | training loss: 0.000050899594\n",
      "Epoch: 8825 | training loss: 0.000050872208\n",
      "Epoch: 8826 | training loss: 0.000050844668\n",
      "Epoch: 8827 | training loss: 0.000050817252\n",
      "Epoch: 8828 | training loss: 0.000050789822\n",
      "Epoch: 8829 | training loss: 0.000050762344\n",
      "Epoch: 8830 | training loss: 0.000050734932\n",
      "Epoch: 8831 | training loss: 0.000050707451\n",
      "Epoch: 8832 | training loss: 0.000050680101\n",
      "Epoch: 8833 | training loss: 0.000050652670\n",
      "Epoch: 8834 | training loss: 0.000050625345\n",
      "Epoch: 8835 | training loss: 0.000050597988\n",
      "Epoch: 8836 | training loss: 0.000050570558\n",
      "Epoch: 8837 | training loss: 0.000050543284\n",
      "Epoch: 8838 | training loss: 0.000050515828\n",
      "Epoch: 8839 | training loss: 0.000050488496\n",
      "Epoch: 8840 | training loss: 0.000050461153\n",
      "Epoch: 8841 | training loss: 0.000050433933\n",
      "Epoch: 8842 | training loss: 0.000050406637\n",
      "Epoch: 8843 | training loss: 0.000050379233\n",
      "Epoch: 8844 | training loss: 0.000050352075\n",
      "Epoch: 8845 | training loss: 0.000050324699\n",
      "Epoch: 8846 | training loss: 0.000050297451\n",
      "Epoch: 8847 | training loss: 0.000050270210\n",
      "Epoch: 8848 | training loss: 0.000050243038\n",
      "Epoch: 8849 | training loss: 0.000050215807\n",
      "Epoch: 8850 | training loss: 0.000050188501\n",
      "Epoch: 8851 | training loss: 0.000050161285\n",
      "Epoch: 8852 | training loss: 0.000050134040\n",
      "Epoch: 8853 | training loss: 0.000050106861\n",
      "Epoch: 8854 | training loss: 0.000050079747\n",
      "Epoch: 8855 | training loss: 0.000050052491\n",
      "Epoch: 8856 | training loss: 0.000050025297\n",
      "Epoch: 8857 | training loss: 0.000049998191\n",
      "Epoch: 8858 | training loss: 0.000049971110\n",
      "Epoch: 8859 | training loss: 0.000049943974\n",
      "Epoch: 8860 | training loss: 0.000049916787\n",
      "Epoch: 8861 | training loss: 0.000049889743\n",
      "Epoch: 8862 | training loss: 0.000049862603\n",
      "Epoch: 8863 | training loss: 0.000049835515\n",
      "Epoch: 8864 | training loss: 0.000049808383\n",
      "Epoch: 8865 | training loss: 0.000049781360\n",
      "Epoch: 8866 | training loss: 0.000049754337\n",
      "Epoch: 8867 | training loss: 0.000049727227\n",
      "Epoch: 8868 | training loss: 0.000049700226\n",
      "Epoch: 8869 | training loss: 0.000049673221\n",
      "Epoch: 8870 | training loss: 0.000049646133\n",
      "Epoch: 8871 | training loss: 0.000049619153\n",
      "Epoch: 8872 | training loss: 0.000049592109\n",
      "Epoch: 8873 | training loss: 0.000049565126\n",
      "Epoch: 8874 | training loss: 0.000049538190\n",
      "Epoch: 8875 | training loss: 0.000049511160\n",
      "Epoch: 8876 | training loss: 0.000049484242\n",
      "Epoch: 8877 | training loss: 0.000049457318\n",
      "Epoch: 8878 | training loss: 0.000049430309\n",
      "Epoch: 8879 | training loss: 0.000049403388\n",
      "Epoch: 8880 | training loss: 0.000049376453\n",
      "Epoch: 8881 | training loss: 0.000049349575\n",
      "Epoch: 8882 | training loss: 0.000049322669\n",
      "Epoch: 8883 | training loss: 0.000049295748\n",
      "Epoch: 8884 | training loss: 0.000049268852\n",
      "Epoch: 8885 | training loss: 0.000049242022\n",
      "Epoch: 8886 | training loss: 0.000049215101\n",
      "Epoch: 8887 | training loss: 0.000049188209\n",
      "Epoch: 8888 | training loss: 0.000049161434\n",
      "Epoch: 8889 | training loss: 0.000049134611\n",
      "Epoch: 8890 | training loss: 0.000049107781\n",
      "Epoch: 8891 | training loss: 0.000049080918\n",
      "Epoch: 8892 | training loss: 0.000049054128\n",
      "Epoch: 8893 | training loss: 0.000049027429\n",
      "Epoch: 8894 | training loss: 0.000049000599\n",
      "Epoch: 8895 | training loss: 0.000048973823\n",
      "Epoch: 8896 | training loss: 0.000048947113\n",
      "Epoch: 8897 | training loss: 0.000048920345\n",
      "Epoch: 8898 | training loss: 0.000048893573\n",
      "Epoch: 8899 | training loss: 0.000048866896\n",
      "Epoch: 8900 | training loss: 0.000048840135\n",
      "Epoch: 8901 | training loss: 0.000048813527\n",
      "Epoch: 8902 | training loss: 0.000048786849\n",
      "Epoch: 8903 | training loss: 0.000048760026\n",
      "Epoch: 8904 | training loss: 0.000048733404\n",
      "Epoch: 8905 | training loss: 0.000048706745\n",
      "Epoch: 8906 | training loss: 0.000048680016\n",
      "Epoch: 8907 | training loss: 0.000048653361\n",
      "Epoch: 8908 | training loss: 0.000048626702\n",
      "Epoch: 8909 | training loss: 0.000048600130\n",
      "Epoch: 8910 | training loss: 0.000048573431\n",
      "Epoch: 8911 | training loss: 0.000048546914\n",
      "Epoch: 8912 | training loss: 0.000048520276\n",
      "Epoch: 8913 | training loss: 0.000048493661\n",
      "Epoch: 8914 | training loss: 0.000048467104\n",
      "Epoch: 8915 | training loss: 0.000048440503\n",
      "Epoch: 8916 | training loss: 0.000048414015\n",
      "Epoch: 8917 | training loss: 0.000048387446\n",
      "Epoch: 8918 | training loss: 0.000048360904\n",
      "Epoch: 8919 | training loss: 0.000048334383\n",
      "Epoch: 8920 | training loss: 0.000048307855\n",
      "Epoch: 8921 | training loss: 0.000048281436\n",
      "Epoch: 8922 | training loss: 0.000048254871\n",
      "Epoch: 8923 | training loss: 0.000048228387\n",
      "Epoch: 8924 | training loss: 0.000048201946\n",
      "Epoch: 8925 | training loss: 0.000048175425\n",
      "Epoch: 8926 | training loss: 0.000048148937\n",
      "Epoch: 8927 | training loss: 0.000048122525\n",
      "Epoch: 8928 | training loss: 0.000048096081\n",
      "Epoch: 8929 | training loss: 0.000048069647\n",
      "Epoch: 8930 | training loss: 0.000048043228\n",
      "Epoch: 8931 | training loss: 0.000048016780\n",
      "Epoch: 8932 | training loss: 0.000047990427\n",
      "Epoch: 8933 | training loss: 0.000047964124\n",
      "Epoch: 8934 | training loss: 0.000047937639\n",
      "Epoch: 8935 | training loss: 0.000047911257\n",
      "Epoch: 8936 | training loss: 0.000047884983\n",
      "Epoch: 8937 | training loss: 0.000047858564\n",
      "Epoch: 8938 | training loss: 0.000047832225\n",
      "Epoch: 8939 | training loss: 0.000047805959\n",
      "Epoch: 8940 | training loss: 0.000047779620\n",
      "Epoch: 8941 | training loss: 0.000047753347\n",
      "Epoch: 8942 | training loss: 0.000047726953\n",
      "Epoch: 8943 | training loss: 0.000047700712\n",
      "Epoch: 8944 | training loss: 0.000047674424\n",
      "Epoch: 8945 | training loss: 0.000047648085\n",
      "Epoch: 8946 | training loss: 0.000047621885\n",
      "Epoch: 8947 | training loss: 0.000047595633\n",
      "Epoch: 8948 | training loss: 0.000047569411\n",
      "Epoch: 8949 | training loss: 0.000047543122\n",
      "Epoch: 8950 | training loss: 0.000047516973\n",
      "Epoch: 8951 | training loss: 0.000047490619\n",
      "Epoch: 8952 | training loss: 0.000047464539\n",
      "Epoch: 8953 | training loss: 0.000047438334\n",
      "Epoch: 8954 | training loss: 0.000047412126\n",
      "Epoch: 8955 | training loss: 0.000047386013\n",
      "Epoch: 8956 | training loss: 0.000047359790\n",
      "Epoch: 8957 | training loss: 0.000047333684\n",
      "Epoch: 8958 | training loss: 0.000047307516\n",
      "Epoch: 8959 | training loss: 0.000047281374\n",
      "Epoch: 8960 | training loss: 0.000047255184\n",
      "Epoch: 8961 | training loss: 0.000047229136\n",
      "Epoch: 8962 | training loss: 0.000047203015\n",
      "Epoch: 8963 | training loss: 0.000047176971\n",
      "Epoch: 8964 | training loss: 0.000047150817\n",
      "Epoch: 8965 | training loss: 0.000047124759\n",
      "Epoch: 8966 | training loss: 0.000047098671\n",
      "Epoch: 8967 | training loss: 0.000047072655\n",
      "Epoch: 8968 | training loss: 0.000047046509\n",
      "Epoch: 8969 | training loss: 0.000047020563\n",
      "Epoch: 8970 | training loss: 0.000046994472\n",
      "Epoch: 8971 | training loss: 0.000046968467\n",
      "Epoch: 8972 | training loss: 0.000046942521\n",
      "Epoch: 8973 | training loss: 0.000046916452\n",
      "Epoch: 8974 | training loss: 0.000046890465\n",
      "Epoch: 8975 | training loss: 0.000046864534\n",
      "Epoch: 8976 | training loss: 0.000046838544\n",
      "Epoch: 8977 | training loss: 0.000046812518\n",
      "Epoch: 8978 | training loss: 0.000046786652\n",
      "Epoch: 8979 | training loss: 0.000046760695\n",
      "Epoch: 8980 | training loss: 0.000046734851\n",
      "Epoch: 8981 | training loss: 0.000046708898\n",
      "Epoch: 8982 | training loss: 0.000046682930\n",
      "Epoch: 8983 | training loss: 0.000046657020\n",
      "Epoch: 8984 | training loss: 0.000046631179\n",
      "Epoch: 8985 | training loss: 0.000046605281\n",
      "Epoch: 8986 | training loss: 0.000046579364\n",
      "Epoch: 8987 | training loss: 0.000046553498\n",
      "Epoch: 8988 | training loss: 0.000046527664\n",
      "Epoch: 8989 | training loss: 0.000046501897\n",
      "Epoch: 8990 | training loss: 0.000046476085\n",
      "Epoch: 8991 | training loss: 0.000046450179\n",
      "Epoch: 8992 | training loss: 0.000046424349\n",
      "Epoch: 8993 | training loss: 0.000046398640\n",
      "Epoch: 8994 | training loss: 0.000046372788\n",
      "Epoch: 8995 | training loss: 0.000046346981\n",
      "Epoch: 8996 | training loss: 0.000046321329\n",
      "Epoch: 8997 | training loss: 0.000046295441\n",
      "Epoch: 8998 | training loss: 0.000046269735\n",
      "Epoch: 8999 | training loss: 0.000046244000\n",
      "Epoch: 9000 | training loss: 0.000046218214\n",
      "Epoch: 9001 | training loss: 0.000046192581\n",
      "Epoch: 9002 | training loss: 0.000046166882\n",
      "Epoch: 9003 | training loss: 0.000046141104\n",
      "Epoch: 9004 | training loss: 0.000046115481\n",
      "Epoch: 9005 | training loss: 0.000046089757\n",
      "Epoch: 9006 | training loss: 0.000046064084\n",
      "Epoch: 9007 | training loss: 0.000046038444\n",
      "Epoch: 9008 | training loss: 0.000046012756\n",
      "Epoch: 9009 | training loss: 0.000045987093\n",
      "Epoch: 9010 | training loss: 0.000045961533\n",
      "Epoch: 9011 | training loss: 0.000045935871\n",
      "Epoch: 9012 | training loss: 0.000045910296\n",
      "Epoch: 9013 | training loss: 0.000045884670\n",
      "Epoch: 9014 | training loss: 0.000045859015\n",
      "Epoch: 9015 | training loss: 0.000045833476\n",
      "Epoch: 9016 | training loss: 0.000045807908\n",
      "Epoch: 9017 | training loss: 0.000045782381\n",
      "Epoch: 9018 | training loss: 0.000045756780\n",
      "Epoch: 9019 | training loss: 0.000045731256\n",
      "Epoch: 9020 | training loss: 0.000045705721\n",
      "Epoch: 9021 | training loss: 0.000045680154\n",
      "Epoch: 9022 | training loss: 0.000045654673\n",
      "Epoch: 9023 | training loss: 0.000045629182\n",
      "Epoch: 9024 | training loss: 0.000045603658\n",
      "Epoch: 9025 | training loss: 0.000045578199\n",
      "Epoch: 9026 | training loss: 0.000045552828\n",
      "Epoch: 9027 | training loss: 0.000045527249\n",
      "Epoch: 9028 | training loss: 0.000045501725\n",
      "Epoch: 9029 | training loss: 0.000045476318\n",
      "Epoch: 9030 | training loss: 0.000045450928\n",
      "Epoch: 9031 | training loss: 0.000045425404\n",
      "Epoch: 9032 | training loss: 0.000045400000\n",
      "Epoch: 9033 | training loss: 0.000045374654\n",
      "Epoch: 9034 | training loss: 0.000045349261\n",
      "Epoch: 9035 | training loss: 0.000045323846\n",
      "Epoch: 9036 | training loss: 0.000045298413\n",
      "Epoch: 9037 | training loss: 0.000045273093\n",
      "Epoch: 9038 | training loss: 0.000045247685\n",
      "Epoch: 9039 | training loss: 0.000045222347\n",
      "Epoch: 9040 | training loss: 0.000045197019\n",
      "Epoch: 9041 | training loss: 0.000045171633\n",
      "Epoch: 9042 | training loss: 0.000045146320\n",
      "Epoch: 9043 | training loss: 0.000045121091\n",
      "Epoch: 9044 | training loss: 0.000045095770\n",
      "Epoch: 9045 | training loss: 0.000045070417\n",
      "Epoch: 9046 | training loss: 0.000045045072\n",
      "Epoch: 9047 | training loss: 0.000045019831\n",
      "Epoch: 9048 | training loss: 0.000044994595\n",
      "Epoch: 9049 | training loss: 0.000044969391\n",
      "Epoch: 9050 | training loss: 0.000044944081\n",
      "Epoch: 9051 | training loss: 0.000044918779\n",
      "Epoch: 9052 | training loss: 0.000044893626\n",
      "Epoch: 9053 | training loss: 0.000044868459\n",
      "Epoch: 9054 | training loss: 0.000044843153\n",
      "Epoch: 9055 | training loss: 0.000044817985\n",
      "Epoch: 9056 | training loss: 0.000044792818\n",
      "Epoch: 9057 | training loss: 0.000044767592\n",
      "Epoch: 9058 | training loss: 0.000044742395\n",
      "Epoch: 9059 | training loss: 0.000044717221\n",
      "Epoch: 9060 | training loss: 0.000044692169\n",
      "Epoch: 9061 | training loss: 0.000044666966\n",
      "Epoch: 9062 | training loss: 0.000044641842\n",
      "Epoch: 9063 | training loss: 0.000044616787\n",
      "Epoch: 9064 | training loss: 0.000044591601\n",
      "Epoch: 9065 | training loss: 0.000044566470\n",
      "Epoch: 9066 | training loss: 0.000044541397\n",
      "Epoch: 9067 | training loss: 0.000044516295\n",
      "Epoch: 9068 | training loss: 0.000044491295\n",
      "Epoch: 9069 | training loss: 0.000044466251\n",
      "Epoch: 9070 | training loss: 0.000044441149\n",
      "Epoch: 9071 | training loss: 0.000044416094\n",
      "Epoch: 9072 | training loss: 0.000044391105\n",
      "Epoch: 9073 | training loss: 0.000044366076\n",
      "Epoch: 9074 | training loss: 0.000044341024\n",
      "Epoch: 9075 | training loss: 0.000044316021\n",
      "Epoch: 9076 | training loss: 0.000044290973\n",
      "Epoch: 9077 | training loss: 0.000044265980\n",
      "Epoch: 9078 | training loss: 0.000044240929\n",
      "Epoch: 9079 | training loss: 0.000044216031\n",
      "Epoch: 9080 | training loss: 0.000044191100\n",
      "Epoch: 9081 | training loss: 0.000044166100\n",
      "Epoch: 9082 | training loss: 0.000044141241\n",
      "Epoch: 9083 | training loss: 0.000044116343\n",
      "Epoch: 9084 | training loss: 0.000044091430\n",
      "Epoch: 9085 | training loss: 0.000044066481\n",
      "Epoch: 9086 | training loss: 0.000044041597\n",
      "Epoch: 9087 | training loss: 0.000044016655\n",
      "Epoch: 9088 | training loss: 0.000043991797\n",
      "Epoch: 9089 | training loss: 0.000043966887\n",
      "Epoch: 9090 | training loss: 0.000043942076\n",
      "Epoch: 9091 | training loss: 0.000043917193\n",
      "Epoch: 9092 | training loss: 0.000043892356\n",
      "Epoch: 9093 | training loss: 0.000043867578\n",
      "Epoch: 9094 | training loss: 0.000043842807\n",
      "Epoch: 9095 | training loss: 0.000043817912\n",
      "Epoch: 9096 | training loss: 0.000043793127\n",
      "Epoch: 9097 | training loss: 0.000043768370\n",
      "Epoch: 9098 | training loss: 0.000043743610\n",
      "Epoch: 9099 | training loss: 0.000043718770\n",
      "Epoch: 9100 | training loss: 0.000043694010\n",
      "Epoch: 9101 | training loss: 0.000043669246\n",
      "Epoch: 9102 | training loss: 0.000043644417\n",
      "Epoch: 9103 | training loss: 0.000043619675\n",
      "Epoch: 9104 | training loss: 0.000043595057\n",
      "Epoch: 9105 | training loss: 0.000043570362\n",
      "Epoch: 9106 | training loss: 0.000043545588\n",
      "Epoch: 9107 | training loss: 0.000043520922\n",
      "Epoch: 9108 | training loss: 0.000043496264\n",
      "Epoch: 9109 | training loss: 0.000043471591\n",
      "Epoch: 9110 | training loss: 0.000043446944\n",
      "Epoch: 9111 | training loss: 0.000043422282\n",
      "Epoch: 9112 | training loss: 0.000043397660\n",
      "Epoch: 9113 | training loss: 0.000043372995\n",
      "Epoch: 9114 | training loss: 0.000043348347\n",
      "Epoch: 9115 | training loss: 0.000043323762\n",
      "Epoch: 9116 | training loss: 0.000043299187\n",
      "Epoch: 9117 | training loss: 0.000043274486\n",
      "Epoch: 9118 | training loss: 0.000043249893\n",
      "Epoch: 9119 | training loss: 0.000043225380\n",
      "Epoch: 9120 | training loss: 0.000043200718\n",
      "Epoch: 9121 | training loss: 0.000043176151\n",
      "Epoch: 9122 | training loss: 0.000043151616\n",
      "Epoch: 9123 | training loss: 0.000043127067\n",
      "Epoch: 9124 | training loss: 0.000043102555\n",
      "Epoch: 9125 | training loss: 0.000043077947\n",
      "Epoch: 9126 | training loss: 0.000043053420\n",
      "Epoch: 9127 | training loss: 0.000043028958\n",
      "Epoch: 9128 | training loss: 0.000043004431\n",
      "Epoch: 9129 | training loss: 0.000042979918\n",
      "Epoch: 9130 | training loss: 0.000042955489\n",
      "Epoch: 9131 | training loss: 0.000042930998\n",
      "Epoch: 9132 | training loss: 0.000042906555\n",
      "Epoch: 9133 | training loss: 0.000042882079\n",
      "Epoch: 9134 | training loss: 0.000042857631\n",
      "Epoch: 9135 | training loss: 0.000042833180\n",
      "Epoch: 9136 | training loss: 0.000042808773\n",
      "Epoch: 9137 | training loss: 0.000042784326\n",
      "Epoch: 9138 | training loss: 0.000042759952\n",
      "Epoch: 9139 | training loss: 0.000042735432\n",
      "Epoch: 9140 | training loss: 0.000042711072\n",
      "Epoch: 9141 | training loss: 0.000042686719\n",
      "Epoch: 9142 | training loss: 0.000042662370\n",
      "Epoch: 9143 | training loss: 0.000042637996\n",
      "Epoch: 9144 | training loss: 0.000042613603\n",
      "Epoch: 9145 | training loss: 0.000042589221\n",
      "Epoch: 9146 | training loss: 0.000042564992\n",
      "Epoch: 9147 | training loss: 0.000042540643\n",
      "Epoch: 9148 | training loss: 0.000042516367\n",
      "Epoch: 9149 | training loss: 0.000042492007\n",
      "Epoch: 9150 | training loss: 0.000042467669\n",
      "Epoch: 9151 | training loss: 0.000042443426\n",
      "Epoch: 9152 | training loss: 0.000042419178\n",
      "Epoch: 9153 | training loss: 0.000042394840\n",
      "Epoch: 9154 | training loss: 0.000042370608\n",
      "Epoch: 9155 | training loss: 0.000042346343\n",
      "Epoch: 9156 | training loss: 0.000042322092\n",
      "Epoch: 9157 | training loss: 0.000042297834\n",
      "Epoch: 9158 | training loss: 0.000042273678\n",
      "Epoch: 9159 | training loss: 0.000042249485\n",
      "Epoch: 9160 | training loss: 0.000042225256\n",
      "Epoch: 9161 | training loss: 0.000042201031\n",
      "Epoch: 9162 | training loss: 0.000042176918\n",
      "Epoch: 9163 | training loss: 0.000042152715\n",
      "Epoch: 9164 | training loss: 0.000042128537\n",
      "Epoch: 9165 | training loss: 0.000042104366\n",
      "Epoch: 9166 | training loss: 0.000042080235\n",
      "Epoch: 9167 | training loss: 0.000042056046\n",
      "Epoch: 9168 | training loss: 0.000042031978\n",
      "Epoch: 9169 | training loss: 0.000042007832\n",
      "Epoch: 9170 | training loss: 0.000041983760\n",
      "Epoch: 9171 | training loss: 0.000041959604\n",
      "Epoch: 9172 | training loss: 0.000041935560\n",
      "Epoch: 9173 | training loss: 0.000041911451\n",
      "Epoch: 9174 | training loss: 0.000041887375\n",
      "Epoch: 9175 | training loss: 0.000041863292\n",
      "Epoch: 9176 | training loss: 0.000041839252\n",
      "Epoch: 9177 | training loss: 0.000041815198\n",
      "Epoch: 9178 | training loss: 0.000041791231\n",
      "Epoch: 9179 | training loss: 0.000041767191\n",
      "Epoch: 9180 | training loss: 0.000041743180\n",
      "Epoch: 9181 | training loss: 0.000041719177\n",
      "Epoch: 9182 | training loss: 0.000041695144\n",
      "Epoch: 9183 | training loss: 0.000041671199\n",
      "Epoch: 9184 | training loss: 0.000041647167\n",
      "Epoch: 9185 | training loss: 0.000041623200\n",
      "Epoch: 9186 | training loss: 0.000041599335\n",
      "Epoch: 9187 | training loss: 0.000041575277\n",
      "Epoch: 9188 | training loss: 0.000041551415\n",
      "Epoch: 9189 | training loss: 0.000041527433\n",
      "Epoch: 9190 | training loss: 0.000041503583\n",
      "Epoch: 9191 | training loss: 0.000041479601\n",
      "Epoch: 9192 | training loss: 0.000041455769\n",
      "Epoch: 9193 | training loss: 0.000041431857\n",
      "Epoch: 9194 | training loss: 0.000041408039\n",
      "Epoch: 9195 | training loss: 0.000041384126\n",
      "Epoch: 9196 | training loss: 0.000041360221\n",
      "Epoch: 9197 | training loss: 0.000041336407\n",
      "Epoch: 9198 | training loss: 0.000041312502\n",
      "Epoch: 9199 | training loss: 0.000041288724\n",
      "Epoch: 9200 | training loss: 0.000041264801\n",
      "Epoch: 9201 | training loss: 0.000041241048\n",
      "Epoch: 9202 | training loss: 0.000041217267\n",
      "Epoch: 9203 | training loss: 0.000041193409\n",
      "Epoch: 9204 | training loss: 0.000041169609\n",
      "Epoch: 9205 | training loss: 0.000041145853\n",
      "Epoch: 9206 | training loss: 0.000041122083\n",
      "Epoch: 9207 | training loss: 0.000041098396\n",
      "Epoch: 9208 | training loss: 0.000041074549\n",
      "Epoch: 9209 | training loss: 0.000041050829\n",
      "Epoch: 9210 | training loss: 0.000041027139\n",
      "Epoch: 9211 | training loss: 0.000041003444\n",
      "Epoch: 9212 | training loss: 0.000040979718\n",
      "Epoch: 9213 | training loss: 0.000040955976\n",
      "Epoch: 9214 | training loss: 0.000040932253\n",
      "Epoch: 9215 | training loss: 0.000040908591\n",
      "Epoch: 9216 | training loss: 0.000040884941\n",
      "Epoch: 9217 | training loss: 0.000040861210\n",
      "Epoch: 9218 | training loss: 0.000040837578\n",
      "Epoch: 9219 | training loss: 0.000040813906\n",
      "Epoch: 9220 | training loss: 0.000040790299\n",
      "Epoch: 9221 | training loss: 0.000040766598\n",
      "Epoch: 9222 | training loss: 0.000040742965\n",
      "Epoch: 9223 | training loss: 0.000040719424\n",
      "Epoch: 9224 | training loss: 0.000040695777\n",
      "Epoch: 9225 | training loss: 0.000040672247\n",
      "Epoch: 9226 | training loss: 0.000040648618\n",
      "Epoch: 9227 | training loss: 0.000040625062\n",
      "Epoch: 9228 | training loss: 0.000040601535\n",
      "Epoch: 9229 | training loss: 0.000040577892\n",
      "Epoch: 9230 | training loss: 0.000040554267\n",
      "Epoch: 9231 | training loss: 0.000040530766\n",
      "Epoch: 9232 | training loss: 0.000040507308\n",
      "Epoch: 9233 | training loss: 0.000040483847\n",
      "Epoch: 9234 | training loss: 0.000040460312\n",
      "Epoch: 9235 | training loss: 0.000040436709\n",
      "Epoch: 9236 | training loss: 0.000040413266\n",
      "Epoch: 9237 | training loss: 0.000040389845\n",
      "Epoch: 9238 | training loss: 0.000040366300\n",
      "Epoch: 9239 | training loss: 0.000040342806\n",
      "Epoch: 9240 | training loss: 0.000040319392\n",
      "Epoch: 9241 | training loss: 0.000040295949\n",
      "Epoch: 9242 | training loss: 0.000040272498\n",
      "Epoch: 9243 | training loss: 0.000040249084\n",
      "Epoch: 9244 | training loss: 0.000040225685\n",
      "Epoch: 9245 | training loss: 0.000040202311\n",
      "Epoch: 9246 | training loss: 0.000040178937\n",
      "Epoch: 9247 | training loss: 0.000040155501\n",
      "Epoch: 9248 | training loss: 0.000040132076\n",
      "Epoch: 9249 | training loss: 0.000040108680\n",
      "Epoch: 9250 | training loss: 0.000040085393\n",
      "Epoch: 9251 | training loss: 0.000040062019\n",
      "Epoch: 9252 | training loss: 0.000040038642\n",
      "Epoch: 9253 | training loss: 0.000040015340\n",
      "Epoch: 9254 | training loss: 0.000039991945\n",
      "Epoch: 9255 | training loss: 0.000039968625\n",
      "Epoch: 9256 | training loss: 0.000039945335\n",
      "Epoch: 9257 | training loss: 0.000039922004\n",
      "Epoch: 9258 | training loss: 0.000039898732\n",
      "Epoch: 9259 | training loss: 0.000039875456\n",
      "Epoch: 9260 | training loss: 0.000039852177\n",
      "Epoch: 9261 | training loss: 0.000039828934\n",
      "Epoch: 9262 | training loss: 0.000039805644\n",
      "Epoch: 9263 | training loss: 0.000039782433\n",
      "Epoch: 9264 | training loss: 0.000039759216\n",
      "Epoch: 9265 | training loss: 0.000039735951\n",
      "Epoch: 9266 | training loss: 0.000039712759\n",
      "Epoch: 9267 | training loss: 0.000039689516\n",
      "Epoch: 9268 | training loss: 0.000039666353\n",
      "Epoch: 9269 | training loss: 0.000039643124\n",
      "Epoch: 9270 | training loss: 0.000039619947\n",
      "Epoch: 9271 | training loss: 0.000039596824\n",
      "Epoch: 9272 | training loss: 0.000039573599\n",
      "Epoch: 9273 | training loss: 0.000039550523\n",
      "Epoch: 9274 | training loss: 0.000039527342\n",
      "Epoch: 9275 | training loss: 0.000039504146\n",
      "Epoch: 9276 | training loss: 0.000039481030\n",
      "Epoch: 9277 | training loss: 0.000039457860\n",
      "Epoch: 9278 | training loss: 0.000039434854\n",
      "Epoch: 9279 | training loss: 0.000039411680\n",
      "Epoch: 9280 | training loss: 0.000039388666\n",
      "Epoch: 9281 | training loss: 0.000039365528\n",
      "Epoch: 9282 | training loss: 0.000039342402\n",
      "Epoch: 9283 | training loss: 0.000039319442\n",
      "Epoch: 9284 | training loss: 0.000039296348\n",
      "Epoch: 9285 | training loss: 0.000039273345\n",
      "Epoch: 9286 | training loss: 0.000039250292\n",
      "Epoch: 9287 | training loss: 0.000039227274\n",
      "Epoch: 9288 | training loss: 0.000039204202\n",
      "Epoch: 9289 | training loss: 0.000039181265\n",
      "Epoch: 9290 | training loss: 0.000039158229\n",
      "Epoch: 9291 | training loss: 0.000039135215\n",
      "Epoch: 9292 | training loss: 0.000039112198\n",
      "Epoch: 9293 | training loss: 0.000039089224\n",
      "Epoch: 9294 | training loss: 0.000039066235\n",
      "Epoch: 9295 | training loss: 0.000039043291\n",
      "Epoch: 9296 | training loss: 0.000039020379\n",
      "Epoch: 9297 | training loss: 0.000038997347\n",
      "Epoch: 9298 | training loss: 0.000038974526\n",
      "Epoch: 9299 | training loss: 0.000038951570\n",
      "Epoch: 9300 | training loss: 0.000038928669\n",
      "Epoch: 9301 | training loss: 0.000038905724\n",
      "Epoch: 9302 | training loss: 0.000038882834\n",
      "Epoch: 9303 | training loss: 0.000038860002\n",
      "Epoch: 9304 | training loss: 0.000038837192\n",
      "Epoch: 9305 | training loss: 0.000038814273\n",
      "Epoch: 9306 | training loss: 0.000038791397\n",
      "Epoch: 9307 | training loss: 0.000038768580\n",
      "Epoch: 9308 | training loss: 0.000038745740\n",
      "Epoch: 9309 | training loss: 0.000038722945\n",
      "Epoch: 9310 | training loss: 0.000038700160\n",
      "Epoch: 9311 | training loss: 0.000038677230\n",
      "Epoch: 9312 | training loss: 0.000038654442\n",
      "Epoch: 9313 | training loss: 0.000038631682\n",
      "Epoch: 9314 | training loss: 0.000038608952\n",
      "Epoch: 9315 | training loss: 0.000038586142\n",
      "Epoch: 9316 | training loss: 0.000038563405\n",
      "Epoch: 9317 | training loss: 0.000038540657\n",
      "Epoch: 9318 | training loss: 0.000038517857\n",
      "Epoch: 9319 | training loss: 0.000038495127\n",
      "Epoch: 9320 | training loss: 0.000038472434\n",
      "Epoch: 9321 | training loss: 0.000038449729\n",
      "Epoch: 9322 | training loss: 0.000038426966\n",
      "Epoch: 9323 | training loss: 0.000038404280\n",
      "Epoch: 9324 | training loss: 0.000038381611\n",
      "Epoch: 9325 | training loss: 0.000038358885\n",
      "Epoch: 9326 | training loss: 0.000038336242\n",
      "Epoch: 9327 | training loss: 0.000038313585\n",
      "Epoch: 9328 | training loss: 0.000038291000\n",
      "Epoch: 9329 | training loss: 0.000038268292\n",
      "Epoch: 9330 | training loss: 0.000038245704\n",
      "Epoch: 9331 | training loss: 0.000038223086\n",
      "Epoch: 9332 | training loss: 0.000038200415\n",
      "Epoch: 9333 | training loss: 0.000038177845\n",
      "Epoch: 9334 | training loss: 0.000038155227\n",
      "Epoch: 9335 | training loss: 0.000038132610\n",
      "Epoch: 9336 | training loss: 0.000038110025\n",
      "Epoch: 9337 | training loss: 0.000038087477\n",
      "Epoch: 9338 | training loss: 0.000038064907\n",
      "Epoch: 9339 | training loss: 0.000038042363\n",
      "Epoch: 9340 | training loss: 0.000038019829\n",
      "Epoch: 9341 | training loss: 0.000037997252\n",
      "Epoch: 9342 | training loss: 0.000037974663\n",
      "Epoch: 9343 | training loss: 0.000037952195\n",
      "Epoch: 9344 | training loss: 0.000037929705\n",
      "Epoch: 9345 | training loss: 0.000037907204\n",
      "Epoch: 9346 | training loss: 0.000037884736\n",
      "Epoch: 9347 | training loss: 0.000037862221\n",
      "Epoch: 9348 | training loss: 0.000037839767\n",
      "Epoch: 9349 | training loss: 0.000037817194\n",
      "Epoch: 9350 | training loss: 0.000037794794\n",
      "Epoch: 9351 | training loss: 0.000037772388\n",
      "Epoch: 9352 | training loss: 0.000037749935\n",
      "Epoch: 9353 | training loss: 0.000037727496\n",
      "Epoch: 9354 | training loss: 0.000037705031\n",
      "Epoch: 9355 | training loss: 0.000037682741\n",
      "Epoch: 9356 | training loss: 0.000037660291\n",
      "Epoch: 9357 | training loss: 0.000037637914\n",
      "Epoch: 9358 | training loss: 0.000037615482\n",
      "Epoch: 9359 | training loss: 0.000037593134\n",
      "Epoch: 9360 | training loss: 0.000037570731\n",
      "Epoch: 9361 | training loss: 0.000037548423\n",
      "Epoch: 9362 | training loss: 0.000037526042\n",
      "Epoch: 9363 | training loss: 0.000037503702\n",
      "Epoch: 9364 | training loss: 0.000037481310\n",
      "Epoch: 9365 | training loss: 0.000037458994\n",
      "Epoch: 9366 | training loss: 0.000037436726\n",
      "Epoch: 9367 | training loss: 0.000037414458\n",
      "Epoch: 9368 | training loss: 0.000037392168\n",
      "Epoch: 9369 | training loss: 0.000037369857\n",
      "Epoch: 9370 | training loss: 0.000037347552\n",
      "Epoch: 9371 | training loss: 0.000037325339\n",
      "Epoch: 9372 | training loss: 0.000037303020\n",
      "Epoch: 9373 | training loss: 0.000037280777\n",
      "Epoch: 9374 | training loss: 0.000037258571\n",
      "Epoch: 9375 | training loss: 0.000037236336\n",
      "Epoch: 9376 | training loss: 0.000037214035\n",
      "Epoch: 9377 | training loss: 0.000037191854\n",
      "Epoch: 9378 | training loss: 0.000037169644\n",
      "Epoch: 9379 | training loss: 0.000037147442\n",
      "Epoch: 9380 | training loss: 0.000037125174\n",
      "Epoch: 9381 | training loss: 0.000037103022\n",
      "Epoch: 9382 | training loss: 0.000037080914\n",
      "Epoch: 9383 | training loss: 0.000037058722\n",
      "Epoch: 9384 | training loss: 0.000037036589\n",
      "Epoch: 9385 | training loss: 0.000037014521\n",
      "Epoch: 9386 | training loss: 0.000036992369\n",
      "Epoch: 9387 | training loss: 0.000036970196\n",
      "Epoch: 9388 | training loss: 0.000036948080\n",
      "Epoch: 9389 | training loss: 0.000036926020\n",
      "Epoch: 9390 | training loss: 0.000036903941\n",
      "Epoch: 9391 | training loss: 0.000036881749\n",
      "Epoch: 9392 | training loss: 0.000036859743\n",
      "Epoch: 9393 | training loss: 0.000036837646\n",
      "Epoch: 9394 | training loss: 0.000036815578\n",
      "Epoch: 9395 | training loss: 0.000036793499\n",
      "Epoch: 9396 | training loss: 0.000036771453\n",
      "Epoch: 9397 | training loss: 0.000036749389\n",
      "Epoch: 9398 | training loss: 0.000036727339\n",
      "Epoch: 9399 | training loss: 0.000036705354\n",
      "Epoch: 9400 | training loss: 0.000036683366\n",
      "Epoch: 9401 | training loss: 0.000036661400\n",
      "Epoch: 9402 | training loss: 0.000036639372\n",
      "Epoch: 9403 | training loss: 0.000036617403\n",
      "Epoch: 9404 | training loss: 0.000036595404\n",
      "Epoch: 9405 | training loss: 0.000036573438\n",
      "Epoch: 9406 | training loss: 0.000036551501\n",
      "Epoch: 9407 | training loss: 0.000036529542\n",
      "Epoch: 9408 | training loss: 0.000036507659\n",
      "Epoch: 9409 | training loss: 0.000036485682\n",
      "Epoch: 9410 | training loss: 0.000036463643\n",
      "Epoch: 9411 | training loss: 0.000036441837\n",
      "Epoch: 9412 | training loss: 0.000036419922\n",
      "Epoch: 9413 | training loss: 0.000036398003\n",
      "Epoch: 9414 | training loss: 0.000036376154\n",
      "Epoch: 9415 | training loss: 0.000036354155\n",
      "Epoch: 9416 | training loss: 0.000036332349\n",
      "Epoch: 9417 | training loss: 0.000036310477\n",
      "Epoch: 9418 | training loss: 0.000036288668\n",
      "Epoch: 9419 | training loss: 0.000036266836\n",
      "Epoch: 9420 | training loss: 0.000036244965\n",
      "Epoch: 9421 | training loss: 0.000036223115\n",
      "Epoch: 9422 | training loss: 0.000036201323\n",
      "Epoch: 9423 | training loss: 0.000036179517\n",
      "Epoch: 9424 | training loss: 0.000036157740\n",
      "Epoch: 9425 | training loss: 0.000036135978\n",
      "Epoch: 9426 | training loss: 0.000036114227\n",
      "Epoch: 9427 | training loss: 0.000036092424\n",
      "Epoch: 9428 | training loss: 0.000036070665\n",
      "Epoch: 9429 | training loss: 0.000036048856\n",
      "Epoch: 9430 | training loss: 0.000036027108\n",
      "Epoch: 9431 | training loss: 0.000036005400\n",
      "Epoch: 9432 | training loss: 0.000035983670\n",
      "Epoch: 9433 | training loss: 0.000035962003\n",
      "Epoch: 9434 | training loss: 0.000035940298\n",
      "Epoch: 9435 | training loss: 0.000035918507\n",
      "Epoch: 9436 | training loss: 0.000035896857\n",
      "Epoch: 9437 | training loss: 0.000035875120\n",
      "Epoch: 9438 | training loss: 0.000035853489\n",
      "Epoch: 9439 | training loss: 0.000035831748\n",
      "Epoch: 9440 | training loss: 0.000035810124\n",
      "Epoch: 9441 | training loss: 0.000035788449\n",
      "Epoch: 9442 | training loss: 0.000035766850\n",
      "Epoch: 9443 | training loss: 0.000035745292\n",
      "Epoch: 9444 | training loss: 0.000035723617\n",
      "Epoch: 9445 | training loss: 0.000035701989\n",
      "Epoch: 9446 | training loss: 0.000035680350\n",
      "Epoch: 9447 | training loss: 0.000035658744\n",
      "Epoch: 9448 | training loss: 0.000035637204\n",
      "Epoch: 9449 | training loss: 0.000035615631\n",
      "Epoch: 9450 | training loss: 0.000035594079\n",
      "Epoch: 9451 | training loss: 0.000035572433\n",
      "Epoch: 9452 | training loss: 0.000035550816\n",
      "Epoch: 9453 | training loss: 0.000035529330\n",
      "Epoch: 9454 | training loss: 0.000035507786\n",
      "Epoch: 9455 | training loss: 0.000035486293\n",
      "Epoch: 9456 | training loss: 0.000035464764\n",
      "Epoch: 9457 | training loss: 0.000035443263\n",
      "Epoch: 9458 | training loss: 0.000035421785\n",
      "Epoch: 9459 | training loss: 0.000035400328\n",
      "Epoch: 9460 | training loss: 0.000035378842\n",
      "Epoch: 9461 | training loss: 0.000035357341\n",
      "Epoch: 9462 | training loss: 0.000035335906\n",
      "Epoch: 9463 | training loss: 0.000035314384\n",
      "Epoch: 9464 | training loss: 0.000035292946\n",
      "Epoch: 9465 | training loss: 0.000035271525\n",
      "Epoch: 9466 | training loss: 0.000035250123\n",
      "Epoch: 9467 | training loss: 0.000035228666\n",
      "Epoch: 9468 | training loss: 0.000035207202\n",
      "Epoch: 9469 | training loss: 0.000035185825\n",
      "Epoch: 9470 | training loss: 0.000035164390\n",
      "Epoch: 9471 | training loss: 0.000035143050\n",
      "Epoch: 9472 | training loss: 0.000035121688\n",
      "Epoch: 9473 | training loss: 0.000035100369\n",
      "Epoch: 9474 | training loss: 0.000035078978\n",
      "Epoch: 9475 | training loss: 0.000035057561\n",
      "Epoch: 9476 | training loss: 0.000035036268\n",
      "Epoch: 9477 | training loss: 0.000035014924\n",
      "Epoch: 9478 | training loss: 0.000034993620\n",
      "Epoch: 9479 | training loss: 0.000034972283\n",
      "Epoch: 9480 | training loss: 0.000034951012\n",
      "Epoch: 9481 | training loss: 0.000034929690\n",
      "Epoch: 9482 | training loss: 0.000034908408\n",
      "Epoch: 9483 | training loss: 0.000034887125\n",
      "Epoch: 9484 | training loss: 0.000034865821\n",
      "Epoch: 9485 | training loss: 0.000034844583\n",
      "Epoch: 9486 | training loss: 0.000034823308\n",
      "Epoch: 9487 | training loss: 0.000034802077\n",
      "Epoch: 9488 | training loss: 0.000034780809\n",
      "Epoch: 9489 | training loss: 0.000034759585\n",
      "Epoch: 9490 | training loss: 0.000034738361\n",
      "Epoch: 9491 | training loss: 0.000034717221\n",
      "Epoch: 9492 | training loss: 0.000034696044\n",
      "Epoch: 9493 | training loss: 0.000034674762\n",
      "Epoch: 9494 | training loss: 0.000034653567\n",
      "Epoch: 9495 | training loss: 0.000034632387\n",
      "Epoch: 9496 | training loss: 0.000034611247\n",
      "Epoch: 9497 | training loss: 0.000034590023\n",
      "Epoch: 9498 | training loss: 0.000034568940\n",
      "Epoch: 9499 | training loss: 0.000034547778\n",
      "Epoch: 9500 | training loss: 0.000034526660\n",
      "Epoch: 9501 | training loss: 0.000034505458\n",
      "Epoch: 9502 | training loss: 0.000034484408\n",
      "Epoch: 9503 | training loss: 0.000034463214\n",
      "Epoch: 9504 | training loss: 0.000034442146\n",
      "Epoch: 9505 | training loss: 0.000034421086\n",
      "Epoch: 9506 | training loss: 0.000034399953\n",
      "Epoch: 9507 | training loss: 0.000034378878\n",
      "Epoch: 9508 | training loss: 0.000034357836\n",
      "Epoch: 9509 | training loss: 0.000034336765\n",
      "Epoch: 9510 | training loss: 0.000034315759\n",
      "Epoch: 9511 | training loss: 0.000034294699\n",
      "Epoch: 9512 | training loss: 0.000034273715\n",
      "Epoch: 9513 | training loss: 0.000034252629\n",
      "Epoch: 9514 | training loss: 0.000034231638\n",
      "Epoch: 9515 | training loss: 0.000034210669\n",
      "Epoch: 9516 | training loss: 0.000034189663\n",
      "Epoch: 9517 | training loss: 0.000034168697\n",
      "Epoch: 9518 | training loss: 0.000034147692\n",
      "Epoch: 9519 | training loss: 0.000034126733\n",
      "Epoch: 9520 | training loss: 0.000034105789\n",
      "Epoch: 9521 | training loss: 0.000034084827\n",
      "Epoch: 9522 | training loss: 0.000034063836\n",
      "Epoch: 9523 | training loss: 0.000034042936\n",
      "Epoch: 9524 | training loss: 0.000034022069\n",
      "Epoch: 9525 | training loss: 0.000034001132\n",
      "Epoch: 9526 | training loss: 0.000033980126\n",
      "Epoch: 9527 | training loss: 0.000033959288\n",
      "Epoch: 9528 | training loss: 0.000033938439\n",
      "Epoch: 9529 | training loss: 0.000033917495\n",
      "Epoch: 9530 | training loss: 0.000033896686\n",
      "Epoch: 9531 | training loss: 0.000033875800\n",
      "Epoch: 9532 | training loss: 0.000033854914\n",
      "Epoch: 9533 | training loss: 0.000033833981\n",
      "Epoch: 9534 | training loss: 0.000033813227\n",
      "Epoch: 9535 | training loss: 0.000033792378\n",
      "Epoch: 9536 | training loss: 0.000033771532\n",
      "Epoch: 9537 | training loss: 0.000033750715\n",
      "Epoch: 9538 | training loss: 0.000033729913\n",
      "Epoch: 9539 | training loss: 0.000033709155\n",
      "Epoch: 9540 | training loss: 0.000033688368\n",
      "Epoch: 9541 | training loss: 0.000033667537\n",
      "Epoch: 9542 | training loss: 0.000033646775\n",
      "Epoch: 9543 | training loss: 0.000033626049\n",
      "Epoch: 9544 | training loss: 0.000033605276\n",
      "Epoch: 9545 | training loss: 0.000033584511\n",
      "Epoch: 9546 | training loss: 0.000033563774\n",
      "Epoch: 9547 | training loss: 0.000033543030\n",
      "Epoch: 9548 | training loss: 0.000033522367\n",
      "Epoch: 9549 | training loss: 0.000033501601\n",
      "Epoch: 9550 | training loss: 0.000033480959\n",
      "Epoch: 9551 | training loss: 0.000033460212\n",
      "Epoch: 9552 | training loss: 0.000033439544\n",
      "Epoch: 9553 | training loss: 0.000033418844\n",
      "Epoch: 9554 | training loss: 0.000033398159\n",
      "Epoch: 9555 | training loss: 0.000033377502\n",
      "Epoch: 9556 | training loss: 0.000033356853\n",
      "Epoch: 9557 | training loss: 0.000033336226\n",
      "Epoch: 9558 | training loss: 0.000033315573\n",
      "Epoch: 9559 | training loss: 0.000033294884\n",
      "Epoch: 9560 | training loss: 0.000033274304\n",
      "Epoch: 9561 | training loss: 0.000033253724\n",
      "Epoch: 9562 | training loss: 0.000033233078\n",
      "Epoch: 9563 | training loss: 0.000033212484\n",
      "Epoch: 9564 | training loss: 0.000033191980\n",
      "Epoch: 9565 | training loss: 0.000033171335\n",
      "Epoch: 9566 | training loss: 0.000033150754\n",
      "Epoch: 9567 | training loss: 0.000033130218\n",
      "Epoch: 9568 | training loss: 0.000033109653\n",
      "Epoch: 9569 | training loss: 0.000033089083\n",
      "Epoch: 9570 | training loss: 0.000033068565\n",
      "Epoch: 9571 | training loss: 0.000033048113\n",
      "Epoch: 9572 | training loss: 0.000033027540\n",
      "Epoch: 9573 | training loss: 0.000033006989\n",
      "Epoch: 9574 | training loss: 0.000032986503\n",
      "Epoch: 9575 | training loss: 0.000032966036\n",
      "Epoch: 9576 | training loss: 0.000032945543\n",
      "Epoch: 9577 | training loss: 0.000032925069\n",
      "Epoch: 9578 | training loss: 0.000032904620\n",
      "Epoch: 9579 | training loss: 0.000032884171\n",
      "Epoch: 9580 | training loss: 0.000032863696\n",
      "Epoch: 9581 | training loss: 0.000032843236\n",
      "Epoch: 9582 | training loss: 0.000032822798\n",
      "Epoch: 9583 | training loss: 0.000032802407\n",
      "Epoch: 9584 | training loss: 0.000032781998\n",
      "Epoch: 9585 | training loss: 0.000032761614\n",
      "Epoch: 9586 | training loss: 0.000032741205\n",
      "Epoch: 9587 | training loss: 0.000032720800\n",
      "Epoch: 9588 | training loss: 0.000032700409\n",
      "Epoch: 9589 | training loss: 0.000032680073\n",
      "Epoch: 9590 | training loss: 0.000032659656\n",
      "Epoch: 9591 | training loss: 0.000032639306\n",
      "Epoch: 9592 | training loss: 0.000032618977\n",
      "Epoch: 9593 | training loss: 0.000032598608\n",
      "Epoch: 9594 | training loss: 0.000032578297\n",
      "Epoch: 9595 | training loss: 0.000032557989\n",
      "Epoch: 9596 | training loss: 0.000032537671\n",
      "Epoch: 9597 | training loss: 0.000032517419\n",
      "Epoch: 9598 | training loss: 0.000032497119\n",
      "Epoch: 9599 | training loss: 0.000032476833\n",
      "Epoch: 9600 | training loss: 0.000032456523\n",
      "Epoch: 9601 | training loss: 0.000032436226\n",
      "Epoch: 9602 | training loss: 0.000032415966\n",
      "Epoch: 9603 | training loss: 0.000032395750\n",
      "Epoch: 9604 | training loss: 0.000032375527\n",
      "Epoch: 9605 | training loss: 0.000032355234\n",
      "Epoch: 9606 | training loss: 0.000032335043\n",
      "Epoch: 9607 | training loss: 0.000032314805\n",
      "Epoch: 9608 | training loss: 0.000032294603\n",
      "Epoch: 9609 | training loss: 0.000032274373\n",
      "Epoch: 9610 | training loss: 0.000032254236\n",
      "Epoch: 9611 | training loss: 0.000032234035\n",
      "Epoch: 9612 | training loss: 0.000032213888\n",
      "Epoch: 9613 | training loss: 0.000032193722\n",
      "Epoch: 9614 | training loss: 0.000032173510\n",
      "Epoch: 9615 | training loss: 0.000032153377\n",
      "Epoch: 9616 | training loss: 0.000032133248\n",
      "Epoch: 9617 | training loss: 0.000032113057\n",
      "Epoch: 9618 | training loss: 0.000032092928\n",
      "Epoch: 9619 | training loss: 0.000032072836\n",
      "Epoch: 9620 | training loss: 0.000032052722\n",
      "Epoch: 9621 | training loss: 0.000032032589\n",
      "Epoch: 9622 | training loss: 0.000032012540\n",
      "Epoch: 9623 | training loss: 0.000031992458\n",
      "Epoch: 9624 | training loss: 0.000031972409\n",
      "Epoch: 9625 | training loss: 0.000031952295\n",
      "Epoch: 9626 | training loss: 0.000031932286\n",
      "Epoch: 9627 | training loss: 0.000031912205\n",
      "Epoch: 9628 | training loss: 0.000031892170\n",
      "Epoch: 9629 | training loss: 0.000031872165\n",
      "Epoch: 9630 | training loss: 0.000031852083\n",
      "Epoch: 9631 | training loss: 0.000031832089\n",
      "Epoch: 9632 | training loss: 0.000031812066\n",
      "Epoch: 9633 | training loss: 0.000031792079\n",
      "Epoch: 9634 | training loss: 0.000031772135\n",
      "Epoch: 9635 | training loss: 0.000031752064\n",
      "Epoch: 9636 | training loss: 0.000031732139\n",
      "Epoch: 9637 | training loss: 0.000031712196\n",
      "Epoch: 9638 | training loss: 0.000031692172\n",
      "Epoch: 9639 | training loss: 0.000031672211\n",
      "Epoch: 9640 | training loss: 0.000031652256\n",
      "Epoch: 9641 | training loss: 0.000031632368\n",
      "Epoch: 9642 | training loss: 0.000031612472\n",
      "Epoch: 9643 | training loss: 0.000031592517\n",
      "Epoch: 9644 | training loss: 0.000031572639\n",
      "Epoch: 9645 | training loss: 0.000031552783\n",
      "Epoch: 9646 | training loss: 0.000031532818\n",
      "Epoch: 9647 | training loss: 0.000031512933\n",
      "Epoch: 9648 | training loss: 0.000031493084\n",
      "Epoch: 9649 | training loss: 0.000031473170\n",
      "Epoch: 9650 | training loss: 0.000031453361\n",
      "Epoch: 9651 | training loss: 0.000031433512\n",
      "Epoch: 9652 | training loss: 0.000031413685\n",
      "Epoch: 9653 | training loss: 0.000031393822\n",
      "Epoch: 9654 | training loss: 0.000031373984\n",
      "Epoch: 9655 | training loss: 0.000031354142\n",
      "Epoch: 9656 | training loss: 0.000031334323\n",
      "Epoch: 9657 | training loss: 0.000031314601\n",
      "Epoch: 9658 | training loss: 0.000031294796\n",
      "Epoch: 9659 | training loss: 0.000031274987\n",
      "Epoch: 9660 | training loss: 0.000031255240\n",
      "Epoch: 9661 | training loss: 0.000031235420\n",
      "Epoch: 9662 | training loss: 0.000031215699\n",
      "Epoch: 9663 | training loss: 0.000031195948\n",
      "Epoch: 9664 | training loss: 0.000031176227\n",
      "Epoch: 9665 | training loss: 0.000031156476\n",
      "Epoch: 9666 | training loss: 0.000031136740\n",
      "Epoch: 9667 | training loss: 0.000031117037\n",
      "Epoch: 9668 | training loss: 0.000031097279\n",
      "Epoch: 9669 | training loss: 0.000031077630\n",
      "Epoch: 9670 | training loss: 0.000031057927\n",
      "Epoch: 9671 | training loss: 0.000031038231\n",
      "Epoch: 9672 | training loss: 0.000031018586\n",
      "Epoch: 9673 | training loss: 0.000030998861\n",
      "Epoch: 9674 | training loss: 0.000030979209\n",
      "Epoch: 9675 | training loss: 0.000030959454\n",
      "Epoch: 9676 | training loss: 0.000030939907\n",
      "Epoch: 9677 | training loss: 0.000030920230\n",
      "Epoch: 9678 | training loss: 0.000030900614\n",
      "Epoch: 9679 | training loss: 0.000030881012\n",
      "Epoch: 9680 | training loss: 0.000030861396\n",
      "Epoch: 9681 | training loss: 0.000030841780\n",
      "Epoch: 9682 | training loss: 0.000030822252\n",
      "Epoch: 9683 | training loss: 0.000030802596\n",
      "Epoch: 9684 | training loss: 0.000030783045\n",
      "Epoch: 9685 | training loss: 0.000030763425\n",
      "Epoch: 9686 | training loss: 0.000030743933\n",
      "Epoch: 9687 | training loss: 0.000030724368\n",
      "Epoch: 9688 | training loss: 0.000030704748\n",
      "Epoch: 9689 | training loss: 0.000030685223\n",
      "Epoch: 9690 | training loss: 0.000030665688\n",
      "Epoch: 9691 | training loss: 0.000030646195\n",
      "Epoch: 9692 | training loss: 0.000030626736\n",
      "Epoch: 9693 | training loss: 0.000030607196\n",
      "Epoch: 9694 | training loss: 0.000030587638\n",
      "Epoch: 9695 | training loss: 0.000030568182\n",
      "Epoch: 9696 | training loss: 0.000030548683\n",
      "Epoch: 9697 | training loss: 0.000030529238\n",
      "Epoch: 9698 | training loss: 0.000030509764\n",
      "Epoch: 9699 | training loss: 0.000030490330\n",
      "Epoch: 9700 | training loss: 0.000030470936\n",
      "Epoch: 9701 | training loss: 0.000030451451\n",
      "Epoch: 9702 | training loss: 0.000030432038\n",
      "Epoch: 9703 | training loss: 0.000030412619\n",
      "Epoch: 9704 | training loss: 0.000030393236\n",
      "Epoch: 9705 | training loss: 0.000030373831\n",
      "Epoch: 9706 | training loss: 0.000030354377\n",
      "Epoch: 9707 | training loss: 0.000030335035\n",
      "Epoch: 9708 | training loss: 0.000030315692\n",
      "Epoch: 9709 | training loss: 0.000030296334\n",
      "Epoch: 9710 | training loss: 0.000030276948\n",
      "Epoch: 9711 | training loss: 0.000030257555\n",
      "Epoch: 9712 | training loss: 0.000030238283\n",
      "Epoch: 9713 | training loss: 0.000030218927\n",
      "Epoch: 9714 | training loss: 0.000030199575\n",
      "Epoch: 9715 | training loss: 0.000030180228\n",
      "Epoch: 9716 | training loss: 0.000030160958\n",
      "Epoch: 9717 | training loss: 0.000030141597\n",
      "Epoch: 9718 | training loss: 0.000030122341\n",
      "Epoch: 9719 | training loss: 0.000030103041\n",
      "Epoch: 9720 | training loss: 0.000030083798\n",
      "Epoch: 9721 | training loss: 0.000030064526\n",
      "Epoch: 9722 | training loss: 0.000030045230\n",
      "Epoch: 9723 | training loss: 0.000030026007\n",
      "Epoch: 9724 | training loss: 0.000030006802\n",
      "Epoch: 9725 | training loss: 0.000029987541\n",
      "Epoch: 9726 | training loss: 0.000029968307\n",
      "Epoch: 9727 | training loss: 0.000029949066\n",
      "Epoch: 9728 | training loss: 0.000029929814\n",
      "Epoch: 9729 | training loss: 0.000029910640\n",
      "Epoch: 9730 | training loss: 0.000029891435\n",
      "Epoch: 9731 | training loss: 0.000029872230\n",
      "Epoch: 9732 | training loss: 0.000029853001\n",
      "Epoch: 9733 | training loss: 0.000029833871\n",
      "Epoch: 9734 | training loss: 0.000029814677\n",
      "Epoch: 9735 | training loss: 0.000029795541\n",
      "Epoch: 9736 | training loss: 0.000029776380\n",
      "Epoch: 9737 | training loss: 0.000029757248\n",
      "Epoch: 9738 | training loss: 0.000029738136\n",
      "Epoch: 9739 | training loss: 0.000029718984\n",
      "Epoch: 9740 | training loss: 0.000029699830\n",
      "Epoch: 9741 | training loss: 0.000029680756\n",
      "Epoch: 9742 | training loss: 0.000029661720\n",
      "Epoch: 9743 | training loss: 0.000029642557\n",
      "Epoch: 9744 | training loss: 0.000029623534\n",
      "Epoch: 9745 | training loss: 0.000029604438\n",
      "Epoch: 9746 | training loss: 0.000029585379\n",
      "Epoch: 9747 | training loss: 0.000029566325\n",
      "Epoch: 9748 | training loss: 0.000029547249\n",
      "Epoch: 9749 | training loss: 0.000029528201\n",
      "Epoch: 9750 | training loss: 0.000029509203\n",
      "Epoch: 9751 | training loss: 0.000029490118\n",
      "Epoch: 9752 | training loss: 0.000029471163\n",
      "Epoch: 9753 | training loss: 0.000029452134\n",
      "Epoch: 9754 | training loss: 0.000029433100\n",
      "Epoch: 9755 | training loss: 0.000029414128\n",
      "Epoch: 9756 | training loss: 0.000029395142\n",
      "Epoch: 9757 | training loss: 0.000029376186\n",
      "Epoch: 9758 | training loss: 0.000029357176\n",
      "Epoch: 9759 | training loss: 0.000029338253\n",
      "Epoch: 9760 | training loss: 0.000029319261\n",
      "Epoch: 9761 | training loss: 0.000029300343\n",
      "Epoch: 9762 | training loss: 0.000029281400\n",
      "Epoch: 9763 | training loss: 0.000029262439\n",
      "Epoch: 9764 | training loss: 0.000029243587\n",
      "Epoch: 9765 | training loss: 0.000029224624\n",
      "Epoch: 9766 | training loss: 0.000029205683\n",
      "Epoch: 9767 | training loss: 0.000029186829\n",
      "Epoch: 9768 | training loss: 0.000029167966\n",
      "Epoch: 9769 | training loss: 0.000029149072\n",
      "Epoch: 9770 | training loss: 0.000029130204\n",
      "Epoch: 9771 | training loss: 0.000029111321\n",
      "Epoch: 9772 | training loss: 0.000029092440\n",
      "Epoch: 9773 | training loss: 0.000029073652\n",
      "Epoch: 9774 | training loss: 0.000029054763\n",
      "Epoch: 9775 | training loss: 0.000029035964\n",
      "Epoch: 9776 | training loss: 0.000029017148\n",
      "Epoch: 9777 | training loss: 0.000028998340\n",
      "Epoch: 9778 | training loss: 0.000028979506\n",
      "Epoch: 9779 | training loss: 0.000028960692\n",
      "Epoch: 9780 | training loss: 0.000028942028\n",
      "Epoch: 9781 | training loss: 0.000028923225\n",
      "Epoch: 9782 | training loss: 0.000028904371\n",
      "Epoch: 9783 | training loss: 0.000028885604\n",
      "Epoch: 9784 | training loss: 0.000028866831\n",
      "Epoch: 9785 | training loss: 0.000028848128\n",
      "Epoch: 9786 | training loss: 0.000028829339\n",
      "Epoch: 9787 | training loss: 0.000028810622\n",
      "Epoch: 9788 | training loss: 0.000028791903\n",
      "Epoch: 9789 | training loss: 0.000028773251\n",
      "Epoch: 9790 | training loss: 0.000028754428\n",
      "Epoch: 9791 | training loss: 0.000028735776\n",
      "Epoch: 9792 | training loss: 0.000028717030\n",
      "Epoch: 9793 | training loss: 0.000028698432\n",
      "Epoch: 9794 | training loss: 0.000028679726\n",
      "Epoch: 9795 | training loss: 0.000028661052\n",
      "Epoch: 9796 | training loss: 0.000028642346\n",
      "Epoch: 9797 | training loss: 0.000028623723\n",
      "Epoch: 9798 | training loss: 0.000028605038\n",
      "Epoch: 9799 | training loss: 0.000028586463\n",
      "Epoch: 9800 | training loss: 0.000028567825\n",
      "Epoch: 9801 | training loss: 0.000028549188\n",
      "Epoch: 9802 | training loss: 0.000028530509\n",
      "Epoch: 9803 | training loss: 0.000028511970\n",
      "Epoch: 9804 | training loss: 0.000028493396\n",
      "Epoch: 9805 | training loss: 0.000028474798\n",
      "Epoch: 9806 | training loss: 0.000028456212\n",
      "Epoch: 9807 | training loss: 0.000028437624\n",
      "Epoch: 9808 | training loss: 0.000028419096\n",
      "Epoch: 9809 | training loss: 0.000028400475\n",
      "Epoch: 9810 | training loss: 0.000028381895\n",
      "Epoch: 9811 | training loss: 0.000028363404\n",
      "Epoch: 9812 | training loss: 0.000028344892\n",
      "Epoch: 9813 | training loss: 0.000028326278\n",
      "Epoch: 9814 | training loss: 0.000028307779\n",
      "Epoch: 9815 | training loss: 0.000028289272\n",
      "Epoch: 9816 | training loss: 0.000028270795\n",
      "Epoch: 9817 | training loss: 0.000028252367\n",
      "Epoch: 9818 | training loss: 0.000028233804\n",
      "Epoch: 9819 | training loss: 0.000028215334\n",
      "Epoch: 9820 | training loss: 0.000028196908\n",
      "Epoch: 9821 | training loss: 0.000028178383\n",
      "Epoch: 9822 | training loss: 0.000028159953\n",
      "Epoch: 9823 | training loss: 0.000028141507\n",
      "Epoch: 9824 | training loss: 0.000028123133\n",
      "Epoch: 9825 | training loss: 0.000028104696\n",
      "Epoch: 9826 | training loss: 0.000028086204\n",
      "Epoch: 9827 | training loss: 0.000028067785\n",
      "Epoch: 9828 | training loss: 0.000028049466\n",
      "Epoch: 9829 | training loss: 0.000028031041\n",
      "Epoch: 9830 | training loss: 0.000028012579\n",
      "Epoch: 9831 | training loss: 0.000027994221\n",
      "Epoch: 9832 | training loss: 0.000027975835\n",
      "Epoch: 9833 | training loss: 0.000027957516\n",
      "Epoch: 9834 | training loss: 0.000027939104\n",
      "Epoch: 9835 | training loss: 0.000027920783\n",
      "Epoch: 9836 | training loss: 0.000027902464\n",
      "Epoch: 9837 | training loss: 0.000027884083\n",
      "Epoch: 9838 | training loss: 0.000027865812\n",
      "Epoch: 9839 | training loss: 0.000027847447\n",
      "Epoch: 9840 | training loss: 0.000027829210\n",
      "Epoch: 9841 | training loss: 0.000027810867\n",
      "Epoch: 9842 | training loss: 0.000027792554\n",
      "Epoch: 9843 | training loss: 0.000027774255\n",
      "Epoch: 9844 | training loss: 0.000027755965\n",
      "Epoch: 9845 | training loss: 0.000027737675\n",
      "Epoch: 9846 | training loss: 0.000027719492\n",
      "Epoch: 9847 | training loss: 0.000027701224\n",
      "Epoch: 9848 | training loss: 0.000027682987\n",
      "Epoch: 9849 | training loss: 0.000027664712\n",
      "Epoch: 9850 | training loss: 0.000027646516\n",
      "Epoch: 9851 | training loss: 0.000027628306\n",
      "Epoch: 9852 | training loss: 0.000027610093\n",
      "Epoch: 9853 | training loss: 0.000027591908\n",
      "Epoch: 9854 | training loss: 0.000027573631\n",
      "Epoch: 9855 | training loss: 0.000027555452\n",
      "Epoch: 9856 | training loss: 0.000027537259\n",
      "Epoch: 9857 | training loss: 0.000027519112\n",
      "Epoch: 9858 | training loss: 0.000027500968\n",
      "Epoch: 9859 | training loss: 0.000027482820\n",
      "Epoch: 9860 | training loss: 0.000027464659\n",
      "Epoch: 9861 | training loss: 0.000027446487\n",
      "Epoch: 9862 | training loss: 0.000027428317\n",
      "Epoch: 9863 | training loss: 0.000027410289\n",
      "Epoch: 9864 | training loss: 0.000027392158\n",
      "Epoch: 9865 | training loss: 0.000027374092\n",
      "Epoch: 9866 | training loss: 0.000027355964\n",
      "Epoch: 9867 | training loss: 0.000027337912\n",
      "Epoch: 9868 | training loss: 0.000027319820\n",
      "Epoch: 9869 | training loss: 0.000027301718\n",
      "Epoch: 9870 | training loss: 0.000027283639\n",
      "Epoch: 9871 | training loss: 0.000027265614\n",
      "Epoch: 9872 | training loss: 0.000027247497\n",
      "Epoch: 9873 | training loss: 0.000027229506\n",
      "Epoch: 9874 | training loss: 0.000027211408\n",
      "Epoch: 9875 | training loss: 0.000027193426\n",
      "Epoch: 9876 | training loss: 0.000027175434\n",
      "Epoch: 9877 | training loss: 0.000027157399\n",
      "Epoch: 9878 | training loss: 0.000027139335\n",
      "Epoch: 9879 | training loss: 0.000027121385\n",
      "Epoch: 9880 | training loss: 0.000027103448\n",
      "Epoch: 9881 | training loss: 0.000027085420\n",
      "Epoch: 9882 | training loss: 0.000027067445\n",
      "Epoch: 9883 | training loss: 0.000027049511\n",
      "Epoch: 9884 | training loss: 0.000027031558\n",
      "Epoch: 9885 | training loss: 0.000027013602\n",
      "Epoch: 9886 | training loss: 0.000026995694\n",
      "Epoch: 9887 | training loss: 0.000026977745\n",
      "Epoch: 9888 | training loss: 0.000026959773\n",
      "Epoch: 9889 | training loss: 0.000026941896\n",
      "Epoch: 9890 | training loss: 0.000026924012\n",
      "Epoch: 9891 | training loss: 0.000026906073\n",
      "Epoch: 9892 | training loss: 0.000026888178\n",
      "Epoch: 9893 | training loss: 0.000026870282\n",
      "Epoch: 9894 | training loss: 0.000026852427\n",
      "Epoch: 9895 | training loss: 0.000026834594\n",
      "Epoch: 9896 | training loss: 0.000026816728\n",
      "Epoch: 9897 | training loss: 0.000026798898\n",
      "Epoch: 9898 | training loss: 0.000026780963\n",
      "Epoch: 9899 | training loss: 0.000026763220\n",
      "Epoch: 9900 | training loss: 0.000026745405\n",
      "Epoch: 9901 | training loss: 0.000026727586\n",
      "Epoch: 9902 | training loss: 0.000026709688\n",
      "Epoch: 9903 | training loss: 0.000026691914\n",
      "Epoch: 9904 | training loss: 0.000026674155\n",
      "Epoch: 9905 | training loss: 0.000026656344\n",
      "Epoch: 9906 | training loss: 0.000026638561\n",
      "Epoch: 9907 | training loss: 0.000026620781\n",
      "Epoch: 9908 | training loss: 0.000026603015\n",
      "Epoch: 9909 | training loss: 0.000026585265\n",
      "Epoch: 9910 | training loss: 0.000026567464\n",
      "Epoch: 9911 | training loss: 0.000026549755\n",
      "Epoch: 9912 | training loss: 0.000026532096\n",
      "Epoch: 9913 | training loss: 0.000026514317\n",
      "Epoch: 9914 | training loss: 0.000026496584\n",
      "Epoch: 9915 | training loss: 0.000026478847\n",
      "Epoch: 9916 | training loss: 0.000026461186\n",
      "Epoch: 9917 | training loss: 0.000026443489\n",
      "Epoch: 9918 | training loss: 0.000026425749\n",
      "Epoch: 9919 | training loss: 0.000026408099\n",
      "Epoch: 9920 | training loss: 0.000026390389\n",
      "Epoch: 9921 | training loss: 0.000026372787\n",
      "Epoch: 9922 | training loss: 0.000026355121\n",
      "Epoch: 9923 | training loss: 0.000026337428\n",
      "Epoch: 9924 | training loss: 0.000026319885\n",
      "Epoch: 9925 | training loss: 0.000026302161\n",
      "Epoch: 9926 | training loss: 0.000026284491\n",
      "Epoch: 9927 | training loss: 0.000026266938\n",
      "Epoch: 9928 | training loss: 0.000026249341\n",
      "Epoch: 9929 | training loss: 0.000026231730\n",
      "Epoch: 9930 | training loss: 0.000026214126\n",
      "Epoch: 9931 | training loss: 0.000026196594\n",
      "Epoch: 9932 | training loss: 0.000026179012\n",
      "Epoch: 9933 | training loss: 0.000026161419\n",
      "Epoch: 9934 | training loss: 0.000026143840\n",
      "Epoch: 9935 | training loss: 0.000026126307\n",
      "Epoch: 9936 | training loss: 0.000026108763\n",
      "Epoch: 9937 | training loss: 0.000026091224\n",
      "Epoch: 9938 | training loss: 0.000026073707\n",
      "Epoch: 9939 | training loss: 0.000026056165\n",
      "Epoch: 9940 | training loss: 0.000026038644\n",
      "Epoch: 9941 | training loss: 0.000026021149\n",
      "Epoch: 9942 | training loss: 0.000026003665\n",
      "Epoch: 9943 | training loss: 0.000025986163\n",
      "Epoch: 9944 | training loss: 0.000025968757\n",
      "Epoch: 9945 | training loss: 0.000025951253\n",
      "Epoch: 9946 | training loss: 0.000025933809\n",
      "Epoch: 9947 | training loss: 0.000025916353\n",
      "Epoch: 9948 | training loss: 0.000025898828\n",
      "Epoch: 9949 | training loss: 0.000025881400\n",
      "Epoch: 9950 | training loss: 0.000025863985\n",
      "Epoch: 9951 | training loss: 0.000025846522\n",
      "Epoch: 9952 | training loss: 0.000025829162\n",
      "Epoch: 9953 | training loss: 0.000025811692\n",
      "Epoch: 9954 | training loss: 0.000025794303\n",
      "Epoch: 9955 | training loss: 0.000025776932\n",
      "Epoch: 9956 | training loss: 0.000025759535\n",
      "Epoch: 9957 | training loss: 0.000025742207\n",
      "Epoch: 9958 | training loss: 0.000025724843\n",
      "Epoch: 9959 | training loss: 0.000025707475\n",
      "Epoch: 9960 | training loss: 0.000025690195\n",
      "Epoch: 9961 | training loss: 0.000025672794\n",
      "Epoch: 9962 | training loss: 0.000025655443\n",
      "Epoch: 9963 | training loss: 0.000025638110\n",
      "Epoch: 9964 | training loss: 0.000025620819\n",
      "Epoch: 9965 | training loss: 0.000025603504\n",
      "Epoch: 9966 | training loss: 0.000025586189\n",
      "Epoch: 9967 | training loss: 0.000025568937\n",
      "Epoch: 9968 | training loss: 0.000025551635\n",
      "Epoch: 9969 | training loss: 0.000025534315\n",
      "Epoch: 9970 | training loss: 0.000025517031\n",
      "Epoch: 9971 | training loss: 0.000025499743\n",
      "Epoch: 9972 | training loss: 0.000025482619\n",
      "Epoch: 9973 | training loss: 0.000025465324\n",
      "Epoch: 9974 | training loss: 0.000025448038\n",
      "Epoch: 9975 | training loss: 0.000025430836\n",
      "Epoch: 9976 | training loss: 0.000025413647\n",
      "Epoch: 9977 | training loss: 0.000025396406\n",
      "Epoch: 9978 | training loss: 0.000025379150\n",
      "Epoch: 9979 | training loss: 0.000025361989\n",
      "Epoch: 9980 | training loss: 0.000025344834\n",
      "Epoch: 9981 | training loss: 0.000025327594\n",
      "Epoch: 9982 | training loss: 0.000025310444\n",
      "Epoch: 9983 | training loss: 0.000025293240\n",
      "Epoch: 9984 | training loss: 0.000025276062\n",
      "Epoch: 9985 | training loss: 0.000025258942\n",
      "Epoch: 9986 | training loss: 0.000025241818\n",
      "Epoch: 9987 | training loss: 0.000025224646\n",
      "Epoch: 9988 | training loss: 0.000025207515\n",
      "Epoch: 9989 | training loss: 0.000025190378\n",
      "Epoch: 9990 | training loss: 0.000025173313\n",
      "Epoch: 9991 | training loss: 0.000025156194\n",
      "Epoch: 9992 | training loss: 0.000025139083\n",
      "Epoch: 9993 | training loss: 0.000025122026\n",
      "Epoch: 9994 | training loss: 0.000025104913\n",
      "Epoch: 9995 | training loss: 0.000025087909\n",
      "Epoch: 9996 | training loss: 0.000025070764\n",
      "Epoch: 9997 | training loss: 0.000025053694\n",
      "Epoch: 9998 | training loss: 0.000025036639\n",
      "Epoch: 9999 | training loss: 0.000025019659\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    \n",
    "    # Model\n",
    "    input_size        = 2\n",
    "    hidden_size       = 5\n",
    "    output_size       = 1\n",
    "    nof_hidden_layers = 5\n",
    "\n",
    "    pinn = vanilla_PINN(input_size, hidden_size, output_size, nof_hidden_layers)\n",
    "    pinn = pinn.to(device)\n",
    "    \n",
    "    # Problem params (nylon string)\n",
    "    T     = 50   #Newton\n",
    "    mu    = 8e-4 #kg/meter\n",
    "    ro    = 950  #kg/m³ (volumic mass)\n",
    "    d     = 1e-3 #meter (diameter of the string) \n",
    "    L     = 10 #meter (length of the string)\n",
    "    u_max = 1e-3 #meter\n",
    "    t_max = 1000   #seconds\n",
    "    lbda  = 1e-2 #meter (wave length at t=0)\n",
    "\n",
    "    nof_pts = 10000\n",
    "\n",
    "    # Setting first BC (u(x=0, t) = 0 for all t)\n",
    "    x_bc1_pts = torch.zeros((nof_pts, 1))\n",
    "    t_bc1_pts = torch.linspace(0, t_max, nof_pts)\n",
    "    t_bc1_pts = t_bc1_pts.reshape(t_bc1_pts.shape[0], 1)\n",
    "    u_bc1_pts = torch.zeros((nof_pts, 1))\n",
    "\n",
    "    #Setting second BC (u(x=L, t) = 0 for all t)\n",
    "    x_bc2_pts = L + torch.zeros((nof_pts, 1))\n",
    "    t_bc2_pts = torch.linspace(0, t_max, nof_pts)\n",
    "    t_bc2_pts = t_bc2_pts.reshape(t_bc2_pts.shape[0], 1)\n",
    "    u_bc2_pts = torch.zeros((nof_pts, 1))\n",
    "\n",
    "    #Setting third BC (u(x, t=0) = u_max * cos(2 * pi * x / lambda) for all x and where lambda is the wave length)\n",
    "    x_bc3_pts = torch.linspace(0, L, nof_pts)\n",
    "    x_bc3_pts = x_bc3_pts.reshape(x_bc3_pts.shape[0], 1)\n",
    "    t_bc3_pts = torch.zeros((nof_pts, 1))\n",
    "    u_bc3_pts = u_max * torch.cos(2 * pi * x_bc3_pts / lbda)\n",
    "    u_bc3_pts = u_bc3_pts.reshape(u_bc3_pts.shape[0], 1)\n",
    "\n",
    "    # Training\n",
    "    model                = pinn\n",
    "    pb_params_list       = [T, mu]\n",
    "    nof_collocations_pts = 10000\n",
    "    learning_rate        = 0.0001\n",
    "    optimizer            = torch.optim.Adam(pinn.parameters(), lr=learning_rate)\n",
    "    nof_iterations       = 10000\n",
    "\n",
    "    training_loss = train(pinn, nof_collocations_pts,\n",
    "                          x_bc1_pts, t_bc1_pts, u_bc1_pts,\n",
    "                          x_bc2_pts, t_bc2_pts, u_bc2_pts,\n",
    "                          x_bc3_pts, t_bc3_pts, u_bc3_pts,\n",
    "                          pb_params_list,\n",
    "                          optimizer, nof_iterations, \n",
    "                          device)\n",
    "\n",
    "    #Plot training loss\n",
    "    plt.plot(training_loss)\n",
    "  \n",
    "    #Save the model\n",
    "    torch.save(pinn.state_dict(), 'pinn_model_stringvib.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "import numpy as np\n",
    "\n",
    "# Plot inference\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "nof_pts = 100\n",
    "\n",
    "x_axis = np.linspace(0, L, nof_pts)\n",
    "t_axis = np.linspace(0, t_max, nof_pts)\n",
    "ms_t, ms_x = np.meshgrid(t_axis,x_axis, sparse=False)\n",
    "\n",
    "pts_x = ms_x.reshape(-1, 1)\n",
    "pts_t = ms_t.reshape(-1, 1)\n",
    "\n",
    "pts_x = Variable(torch.from_numpy(pts_x).float(), requires_grad=True).to(device)\n",
    "pts_t = Variable(torch.from_numpy(pts_t).float(), requires_grad=True).to(device)\n",
    "\n",
    "pts_u = model(pts_t, pts_x)\n",
    "ms_u = pts_u.to('cpu').detach().numpy()\n",
    "ms_u = ms_u.reshape(ms_x.shape, order='C')\n",
    "\n",
    "surface = ax.plot_surface(ms_t, ms_x, ms_u, \n",
    "                          cmap=cm.coolwarm, linewidth=0)#, antialiased=False)\n",
    "\n",
    "plt.axis('auto')\n",
    "ax.set_xlabel(\"t\")\n",
    "ax.set_ylabel(\"x\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation\n",
    "\n",
    "My neural network makes a lot of mistakes :\n",
    "- where $x$ is equal to $0$ or $L$, $u(x,t)$ must be equal to $0$ but it's not the case here ;\n",
    "- if the curve was right, we shouldn't see for a fixed $t$ that for the all the $x$, $u(x,t)$ is constant.\n",
    "\n",
    "Those errors seem to come from the boundary conditions because we have a superpositions of conditions. It's a contradiction!\n",
    "\n",
    "Ideas to apply after debug:\n",
    "- try to apply weights to loss functions on boundary conditions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c8cc5b7cf7ab45a4eb9c5d10fcde61976ab495d4e3d71a8f87de6440d779c3fd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
